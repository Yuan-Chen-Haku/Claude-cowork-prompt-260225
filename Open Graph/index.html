<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Knowledge Work Plugins Graph</title>
  <style> 
    body { margin: 0; display: flex; height: 100vh; width: 100vw; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; color: #333; overflow: hidden;} 
    #graph { width: 100vw; height: 100vh; background-color: #f0f4f8; position: absolute; top:0; left:0; z-index: 1; } 
    
    /* Overlay Panel */
    #panel { 
        position: absolute; 
        right: 0; top: 0; bottom: 0; 
        width: 50vw; max-width: 90vw;
        padding: 0; display: flex; flex-direction: column; 
        border-left: 1px solid #ccc; background: rgba(255,255,255,0.95); 
        box-shadow: -5px 0 15px rgba(0,0,0,0.1); 
        z-index: 10; 
        transform: translateX(100%); 
        transition: transform 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
        backdrop-filter: blur(10px);
    } 
    #panel.open { transform: translateX(0); }
    
    .panel-header { padding: 20px; border-bottom: 2px solid #f0f4f8; background-color: rgba(250,251,252,0.9); position: relative; }
    .panel-content { padding: 20px; overflow-y: auto; flex: 1; }
    h2 { margin-top: 0; margin-bottom: 5px; word-break: break-all; font-size: 1.4em; color: #1a1a1a; padding-right: 30px; }
    .filepath { font-size: 0.8em; color: #666; font-family: monospace; word-break: break-all; margin: 0; }
    
    #close-btn {
        position: absolute; top: 15px; right: 20px;
        background: none; border: none; font-size: 20px;
        color: #999; cursor: pointer; padding: 5px;
        border-radius: 4px; transition: background 0.2s, color 0.2s;
    }
    #close-btn:hover { background: #fee; color: #d00; }
    
    /* Controls Overlay */
    #controls {
        position: absolute; top: 20px; left: 20px;
        background: rgba(255,255,255,0.85); padding: 15px 20px;
        border-radius: 8px; box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        z-index: 5; backdrop-filter: blur(5px); border: 1px solid #eee;
    }
    .control-group { margin-bottom: 12px; }
    .control-group:last-child { margin-bottom: 0; }
    .control-group label { font-size: 13px; font-weight: 600; color: #555; display: flex; justify-content: space-between; margin-bottom: 5px; }
    .control-group input[type=range] { width: 180px; accent-color: #2a9d8f; }

    pre { background: #f6f8fa; padding: 16px; overflow: auto; border-radius: 6px; font-size: 85%; line-height: 1.45; margin-bottom: 16px; white-space: pre-wrap; }
    code { font-family: ui-monospace, SFMono-Regular, Consolas, monospace; }
    img { max-width: 100%; height: auto; border-radius: 4px; }
    blockquote { border-left: 4px solid #dfe2e5; margin: 0 0 16px 0; padding: 0 1em; color: #6a737d; }
    table { border-collapse: collapse; width: 100%; margin-bottom: 16px; display: block; overflow: auto; }
    table tr { border-top: 1px solid #c6cbd1; }
    table tr:nth-child(2n) { background-color: rgba(246, 248, 250, 0.5); }
    table th, table td { border: 1px solid #dfe2e5; padding: 6px 13px; }
    a { color: #0366d6; text-decoration: none; }
    a:hover { text-decoration: underline; }
    p, ul, ol { line-height: 1.5; }
    #error-banner { display: none; position: absolute; top: 0; left: 0; right: 0; background: #ffe5e5; color: #d00; padding: 15px; text-align: center; font-weight: bold; z-index: 100; border-bottom: 1px solid #d00; }
  </style>

  <!-- Load Scripts with Fallbacks -->
  <script src="https://unpkg.com/force-graph@1.43.5/dist/force-graph.min.js"></script>
  <script>
    if (typeof ForceGraph === 'undefined') document.write('<script src="https://cdn.jsdelivr.net/npm/force-graph@1.43.5/dist/force-graph.min.js"><\/script>');
  </script>
  <script>
    if (typeof ForceGraph === 'undefined') document.write('<script src="https://cdnjs.cloudflare.com/ajax/libs/force-graph/1.43.5/force-graph.min.js"><\/script>');
  </script>

  <script src="https://unpkg.com/marked@12.0.1/marked.min.js"></script>
  <script>
    if (typeof marked === 'undefined') document.write('<script src="https://cdn.jsdelivr.net/npm/marked@12.0.1/marked.min.js"><\/script>');
  </script>
  <script>
    if (typeof marked === 'undefined') document.write('<script src="https://cdnjs.cloudflare.com/ajax/libs/marked/12.0.1/marked.min.js"><\/script>');
  </script>
</head>
<body>
  <div id="error-banner"></div>
  
  <div id="controls">
    <div class="control-group">
      <label>Ring Spacing (Radius) <span id="dist-val">150</span></label>
      <input type="range" id="dist-slider" min="50" max="500" value="150" step="10">
    </div>
    <div class="control-group">
      <label>Node Repulsion (Centrality) <span id="charge-val">-20</span></label>
      <input type="range" id="charge-slider" min="-300" max="0" value="-20" step="5">
    </div>
  </div>

  <div id="graph"></div>
  
  <div id="panel">
    <div class="panel-header">
      <button id="close-btn" title="Close">×</button>
      <h2 id="doc-title">Welcome</h2>
      <p id="doc-path" class="filepath">Interactive Graph Explorer</p>
    </div>
    <div id="content-display" class="panel-content markdown-body">
      <p>Click on any <strong>file node (blue)</strong> in the graph to view its Markdown or text contents.</p>
    </div>
  </div>
  
  <script type="application/json" id="graph-data-json">
    {"nodes": [{"id": "0", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins", "name": "Claude-coworok-knowledge-work-plugins", "type": "dir", "content": "", "depth": 0}, {"id": "1", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support", "name": "customer-support", "type": "dir", "content": "", "depth": 1}, {"id": "2", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design", "name": "design", "type": "dir", "content": "", "depth": 1}, {"id": "3", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search", "name": "enterprise-search", "type": "dir", "content": "", "depth": 1}, {"id": "4", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management", "name": "cowork-plugin-management", "type": "dir", "content": "", "depth": 1}, {"id": "5", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales", "name": "sales", "type": "dir", "content": "", "depth": 1}, {"id": "6", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research", "name": "bio-research", "type": "dir", "content": "", "depth": 1}, {"id": "7", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built", "name": "partner-built", "type": "dir", "content": "", "depth": 1}, {"id": "8", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources", "name": "human-resources", "type": "dir", "content": "", "depth": 1}, {"id": "9", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations", "name": "operations", "type": "dir", "content": "", "depth": 1}, {"id": "10", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management", "name": "product-management", "type": "dir", "content": "", "depth": 1}, {"id": "11", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing", "name": "marketing", "type": "dir", "content": "", "depth": 1}, {"id": "12", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance", "name": "finance", "type": "dir", "content": "", "depth": 1}, {"id": "13", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal", "name": "legal", "type": "dir", "content": "", "depth": 1}, {"id": "14", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data", "name": "data", "type": "dir", "content": "", "depth": 1}, {"id": "15", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering", "name": "engineering", "type": "dir", "content": "", "depth": 1}, {"id": "16", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity", "name": "productivity", "type": "dir", "content": "", "depth": 1}, {"id": "17", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/README.md", "name": "README.md", "type": "file", "content": "# Knowledge Work Plugins\n\nPlugins that turn Claude into a specialist for your role, team, and company. Built for [Claude Cowork](https://claude.com/product/cowork), also compatible with [Claude Code](https://claude.com/product/claude-code).\n\n## Why Plugins\n\nCowork lets you set the goal and Claude delivers finished, professional work. Plugins let you go further: tell Claude how you like work done, which tools and data to pull from, how to handle critical workflows, and what slash commands to expose — so your team gets better and more consistent outcomes.\n\nEach plugin bundles the skills, connectors, slash commands, and sub-agents for a specific job function. Out of the box, they give Claude a strong starting point for helping anyone in that role. The real power comes when you customize them for your company — your tools, your terminology, your processes — so Claude works like it was built for your team.\n\n## Plugin Marketplace\n\nWe're open-sourcing 11 plugins built and inspired by our own work:\n\n| Plugin | How it helps | Connectors |\n|--------|-------------|------------|\n| **[productivity](./productivity)** | Manage tasks, calendars, daily workflows, and personal context so you spend less time repeating yourself. | Slack, Notion, Asana, Linear, Jira, Monday, ClickUp, Microsoft 365 |\n| **[sales](./sales)** | Research prospects, prep for calls, review your pipeline, draft outreach, and build competitive battlecards. | Slack, HubSpot, Close, Clay, ZoomInfo, Notion, Jira, Fireflies, Microsoft 365 |\n| **[customer-support](./customer-support)** | Triage tickets, draft responses, package escalations, research customer context, and turn resolved issues into knowledge base articles. | Slack, Intercom, HubSpot, Guru, Jira, Notion, Microsoft 365 |\n| **[product-management](./product-management)** | Write specs, plan roadmaps, synthesize user research, keep stakeholders updated, and track the competitive landscape. | Slack, Linear, Asana, Monday, ClickUp, Jira, Notion, Figma, Amplitude, Pendo, Intercom, Fireflies |\n| **[marketing](./marketing)** | Draft content, plan campaigns, enforce brand voice, brief on competitors, and report on performance across channels. | Slack, Canva, Figma, HubSpot, Amplitude, Notion, Ahrefs, SimilarWeb, Klaviyo |\n| **[legal](./legal)** | Review contracts, triage NDAs, navigate compliance, assess risk, prep for meetings, and draft templated responses. | Slack, Box, Egnyte, Jira, Microsoft 365 |\n| **[finance](./finance)** | Prep journal entries, reconcile accounts, generate financial statements, analyze variances, manage close, and support audits. | Snowflake, Databricks, BigQuery, Slack, Microsoft 365 |\n| **[data](./data)** | Query, visualize, and interpret datasets — write SQL, run statistical analysis, build dashboards, and validate your work before sharing. | Snowflake, Databricks, BigQuery, Hex, Amplitude, Jira |\n| **[enterprise-search](./enterprise-search)** | Find anything across email, chat, docs, and wikis — one query across all your company's tools. | Slack, Notion, Guru, Jira, Asana, Microsoft 365 |\n| **[bio-research](./bio-research)** | Connect to preclinical research tools and databases (literature search, genomics analysis, target prioritization) to accelerate early-stage life sciences R\u0026D. | PubMed, BioRender, bioRxiv, ClinicalTrials.gov, ChEMBL, Synapse, Wiley, Owkin, Open Targets, Benchling |\n| **[cowork-plugin-management](./cowork-plugin-management)** | Create new plugins or customize existing ones for your organization's specific tools and workflows. | — |\n\nInstall these directly from Cowork, browse the full collection here on GitHub, or build your own.\n\n## Getting Started\n\n### Cowork\n\nInstall plugins from [claude.com/plugins](https://claude.com/plugins/).\n\n### Claude Code\n\n```bash\n# Add the marketplace first\nclaude plugin marketplace add anthropics/knowledge-work-plugins\n\n# Then install a specific plugin\nclaude plugin install sales@knowledge-work-plugins\n```\n\nOnce installed, plugins activate automatically. Skills fire when relevant, and slash commands are available in your session (e.g., `/sales:call-prep`, `/data:write-query`).\n\n## How Plugins Work\n\nEvery plugin follows the same structure:\n\n```\nplugin-name/\n├── .claude-plugin/plugin.json   # Manifest\n├── .mcp.json                    # Tool connections\n├── commands/                    # Slash commands you invoke explicitly\n└── skills/                      # Domain knowledge Claude draws on automatically\n```\n\n- **Skills** encode the domain expertise, best practices, and step-by-step workflows Claude needs to give you useful help. Claude draws on them automatically when relevant.\n- **Commands** are explicit actions you trigger (e.g., `/finance:reconciliation`, `/product-management:write-spec`).\n- **Connectors** wire Claude to the external tools your role depends on — CRMs, project trackers, data warehouses, design tools, and more — via [MCP servers](https://modelcontextprotocol.io/).\n\nEvery component is file-based — markdown and JSON, no code, no infrastructure, no build steps.\n\n## Making Them Yours\n\nThese plugins are generic starting points. They become much more useful when you customize them for how your company actually works:\n\n- **Swap connectors** — Edit `.mcp.json` to point at your specific tool stack.\n- **Add company context** — Drop your terminology, org structure, and processes into skill files so Claude understands your world.\n- **Adjust workflows** — Modify skill instructions to match how your team actually does things, not how a textbook says to.\n- **Build new plugins** — Use the `cowork-plugin-management` plugin or follow the structure above to create plugins for roles and workflows we haven't covered yet.\n\nAs your team builds and shares plugins, Claude becomes a cross-functional expert. The context you define gets baked into every relevant interaction, so leaders and admins can spend less time enforcing processes and more time improving them.\n\n## Contributing\n\nPlugins are just markdown files. Fork the repo, make your changes, and submit a PR.\n", "depth": 1}, {"id": "18", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "19", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "20", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/README.md", "name": "README.md", "type": "file", "content": "# Customer Support Plugin\n\nA customer support plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Provides ticket triage, escalation management, response drafting, customer research, and knowledge base authoring for support teams.\n\n## Installation\n\n```\nclaude plugins add knowledge-work-plugins/customer-support\n```\n\n## What It Does\n\nThis plugin turns Claude into a customer support co-pilot. It helps you:\n\n- **Triage incoming tickets** with structured categorization, priority assessment, and routing recommendations\n- **Research customer questions** by synthesizing information from multiple sources with confidence scoring\n- **Draft professional responses** tailored to the situation, urgency, and communication channel\n- **Package escalations** with full context, reproduction steps, and business impact for engineering or product\n- **Write KB articles** from resolved issues to reduce future ticket volume\n\n## Commands\n\n| Command | Description |\n|---|---|\n| `/triage` | Categorize, prioritize, and route a support ticket or customer issue |\n| `/research` | Multi-source research on a customer question or topic |\n| `/draft-response` | Draft a customer-facing response for any situation |\n| `/escalate` | Package an escalation for engineering, product, or leadership |\n| `/kb-article` | Draft a knowledge base article from a resolved issue |\n\n## Skills\n\n| Skill | Description |\n|---|---|\n| `ticket-triage` | Category taxonomy, priority framework (P1-P4), routing rules, duplicate detection |\n| `customer-research` | Multi-source research methodology, source prioritization, answer synthesis |\n| `response-drafting` | Communication best practices, tone guidelines, templates for common scenarios |\n| `escalation` | Escalation tiers, structured escalation format, impact assessment, follow-up cadence |\n| `knowledge-management` | Article structure standards, writing for searchability, review cadence, maintenance |\n\n## Data Sources\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your support platform, knowledge base, and communication tools for the best experience. Without them, provide customer context manually.\n\n**Included MCP connections:**\n- Chat (Slack) for internal discussions and customer channel context\n- Support platform (Intercom) for ticket history and customer conversations\n- CRM (HubSpot) for account details and contact information\n- Knowledge base (Guru, Notion) for internal documentation and runbooks\n- Project tracker (Atlassian) for bug reports and feature requests\n- Email and cloud storage (Microsoft 365) for customer correspondence\n\n**Additional options:**\n- See [CONNECTORS.md](CONNECTORS.md) for alternative tools in each category\n\n## Example Workflows\n\n### Triaging an Incoming Ticket\n\n```\nYou: /triage Customer says their dashboard has been showing a blank\n     page since this morning — they're on the Enterprise plan\n\nClaude: Analyzing the issue...\n\n## Triage: Dashboard blank page — Enterprise customer\n\n**Category:** Bug\n**Priority:** P2 — High (core feature broken, enterprise customer)\n**Product area:** Dashboard / Frontend\n\n### Routing Recommendation\nRoute to: Tier 2 Support (investigate browser/account specifics,\nthen escalate to Engineering if reproducible)\n\n### Suggested Initial Response\n[Draft acknowledging the issue, asking clarifying questions\nabout browser and any recent changes, offering status page link]\n```\n\n### Researching a Customer Question\n\n```\nYou: /research Does our platform support SSO with Okta?\n\nClaude: Let me check across available sources...\n\n[Searches docs, knowledge base, previous conversations,\n and web sources — synthesizes answer with confidence\n level and source attribution]\n```\n\n### Packaging an Escalation\n\n```\nYou: /escalate API returning 500 errors intermittently — 3\n     Enterprise customers reported this week\n\nClaude: I'll gather context and build the escalation brief...\n\n[Pulls ticket history, identifies affected accounts,\n structures reproduction steps, assesses business impact,\n and generates a structured escalation for engineering]\n```\n\n### Drafting a Response\n\n```\nYou: /draft-response Customer escalation — their integration\n     has been down for 2 days and they're frustrated\n\nClaude: [Generates empathetic, professional response acknowledging\n the frustration, taking ownership, providing a clear action\n plan with timeline, and offering a call]\n```\n\n### Writing a KB Article\n\n```\nYou: /kb-article How to configure webhook notifications —\n     just resolved this for the third customer this month\n\nClaude: [Generates a structured how-to article with prerequisites,\n step-by-step instructions, verification steps, and common\n issues — optimized for search]\n```\n\n## Configuration\n\nThe plugin works out of the box with the included MCP connections. For the richest experience, connect additional data sources through your Claude settings:\n\n1. **Support platform**: Add your ticketing system for ticket history and customer context\n2. **Knowledge base**: Add your wiki for internal documentation and existing KB articles\n3. **Project tracker**: Add your issue tracker for bug reports and feature requests\n4. **CRM**: Add your CRM for account details and contact information\n\nWithout these connections, the plugin will ask you to provide context manually and offer frameworks and templates you can fill in with your own data.\n", "depth": 2}, {"id": "21", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~support platform` might mean Intercom, Zendesk, or any other support tool with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (support platform, CRM, chat, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Email | `~~email` | Microsoft 365 | — |\n| Cloud storage | `~~cloud storage` | Microsoft 365 | — |\n| Support platform | `~~support platform` | Intercom | Zendesk, Freshdesk, HubSpot Service Hub |\n| CRM | `~~CRM` | HubSpot | Salesforce, Pipedrive |\n| Knowledge base | `~~knowledge base` | Guru, Notion | Confluence, Help Scout |\n| Project tracker | `~~project tracker` | Atlassian (Jira/Confluence) | Linear, Asana |\n", "depth": 2}, {"id": "22", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/commands/triage.md", "name": "triage.md", "type": "file", "content": "---\ndescription: Triage and prioritize a support ticket or customer issue\nargument-hint: \"\u003cticket or issue description\u003e\"\n---\n\n# Triage\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCategorize, prioritize, and route an incoming support ticket or customer issue. Produces a structured triage assessment with a suggested initial response.\n\n## Usage\n\n```\n/triage \u003cticket text, customer message, or issue description\u003e\n```\n\nExamples:\n- `/triage Customer says their dashboard has been showing a blank page since this morning`\n- `/triage \"I was charged twice for my subscription this month\"`\n- `/triage User can't connect their SSO — getting a 403 error on the callback URL`\n- `/triage Feature request: they want to export reports as PDF`\n\n## Workflow\n\n### 1. Parse the Issue\n\nRead the input and extract:\n\n- **Core problem**: What is the customer actually experiencing?\n- **Symptoms**: What specific behavior or error are they seeing?\n- **Customer context**: Who is this? Any account details, plan level, or history available?\n- **Urgency signals**: Are they blocked? Is this production? How many users affected?\n- **Emotional state**: Frustrated, confused, matter-of-fact, escalating?\n\n### 2. Categorize and Prioritize\n\nUsing the category taxonomy and priority framework from the **ticket-triage** skill:\n\n- Assign a **primary category** (bug, how-to, feature request, billing, account, integration, security, data, performance) and an optional secondary category\n- Assign a **priority** (P1–P4) based on impact and urgency\n- Identify the **product area** the issue maps to\n\n### 3. Check for Duplicates and Known Issues\n\nBefore routing, check available sources:\n\n- **~~support platform**: Search for similar open or recently resolved tickets\n- **~~knowledge base**: Check for known issues or existing documentation\n- **~~project tracker**: Check if there's an existing bug report or feature request\n\n### 4. Determine Routing\n\nUsing the routing rules from the **ticket-triage** skill, recommend which team or queue should handle this based on category and complexity.\n\n### 5. Generate Triage Output\n\n```\n## Triage: [One-line issue summary]\n\n**Category:** [Primary] / [Secondary if applicable]\n**Priority:** [P1-P4] — [Brief justification]\n**Product area:** [Area/team]\n\n### Issue Summary\n[2-3 sentence summary of what the customer is experiencing]\n\n### Key Details\n- **Customer:** [Name/account if known]\n- **Impact:** [Who and what is affected]\n- **Workaround:** [Available / Not available / Unknown]\n- **Related tickets:** [Links to similar issues if found]\n- **Known issue:** [Yes — link / No / Checking]\n\n### Routing Recommendation\n**Route to:** [Team or queue]\n**Why:** [Brief reasoning]\n\n### Suggested Initial Response\n[Draft first response to the customer — acknowledge the issue,\nset expectations, provide workaround if available.\nUse the auto-response templates from the ticket-triage skill\nas a starting point.]\n\n### Internal Notes\n- [Any additional context for the agent picking this up]\n- [Reproduction hints if it's a bug]\n- [Escalation triggers to watch for]\n```\n\n### 6. Offer Next Steps\n\nAfter presenting the triage:\n- \"Want me to draft a full response to the customer?\"\n- \"Should I search for more context on this issue?\"\n- \"Want me to check if this is a known bug in the tracker?\"\n- \"Should I escalate this? I can package it with /escalate.\"\n", "depth": 3}, {"id": "23", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/commands/escalate.md", "name": "escalate.md", "type": "file", "content": "---\ndescription: Package an escalation for engineering, product, or leadership with full context\nargument-hint: \"\u003cissue summary\u003e [customer name]\"\n---\n\n# Escalate\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nPackage a support issue into a structured escalation brief for engineering, product, or leadership. Gathers context, structures reproduction steps, assesses business impact, and identifies the right escalation target.\n\n## Usage\n\n```\n/escalate \u003cissue description\u003e [customer name or account]\n```\n\nExamples:\n- `/escalate API returning 500 errors intermittently for Acme Corp`\n- `/escalate Data export is missing rows — 3 customers reported this week`\n- `/escalate SSO login loop affecting all Enterprise customers`\n- `/escalate Customer threatening to churn over missing audit log feature`\n\n## Workflow\n\n### 1. Understand the Issue\n\nParse the input and determine:\n\n- **What's broken or needed**: The core technical or product issue\n- **Who's affected**: Specific customer(s), segment, or all users\n- **How long**: When did this start? How long has the customer been waiting?\n- **What's been tried**: Any troubleshooting or workarounds attempted\n- **Why escalate now**: What makes this need attention beyond normal support\n\nUse the \"When to Escalate vs. Handle in Support\" criteria from the **escalation** skill to confirm this warrants escalation.\n\n### 2. Gather Context\n\nPull together relevant information from available sources:\n\n- **~~support platform**: Related tickets, timeline of communications, previous troubleshooting\n- **~~CRM** (if connected): Account details, key contacts, previous escalations\n- **~~chat**: Internal discussions about this issue, similar reports from other customers\n- **~~project tracker** (if connected): Related bug reports or feature requests, engineering status\n- **~~knowledge base**: Known issues or workarounds, relevant documentation\n\n### 3. Assess Business Impact\n\nUsing the impact dimensions from the **escalation** skill, quantify:\n\n- **Breadth**: How many customers/users affected? Growing?\n- **Depth**: Blocked vs. inconvenienced?\n- **Duration**: How long has this been going on?\n- **Revenue**: ARR at risk? Pending deals affected?\n- **Time pressure**: Hard deadline?\n\n### 4. Determine Escalation Target\n\nUsing the escalation tiers from the **escalation** skill, identify the right target: L2 Support, Engineering, Product, Security, or Leadership.\n\n### 5. Structure Reproduction Steps (for bugs)\n\nIf the issue is a bug, follow the reproduction step best practices from the **escalation** skill to document clear repro steps with environment details and evidence.\n\n### 6. Generate Escalation Brief\n\n```\n## ESCALATION: [One-line summary]\n\n**Severity:** [Critical / High / Medium]\n**Target team:** [Engineering / Product / Security / Leadership]\n**Reported by:** [Your name/team]\n**Date:** [Today's date]\n\n### Impact\n- **Customers affected:** [Who and how many]\n- **Workflow impact:** [What they can't do]\n- **Revenue at risk:** [If applicable]\n- **Time in queue:** [How long this has been an issue]\n\n### Issue Description\n[Clear, concise description of the problem — 3-5 sentences]\n\n### What's Been Tried\n1. [Troubleshooting step and result]\n2. [Troubleshooting step and result]\n3. [Troubleshooting step and result]\n\n### Reproduction Steps\n[If applicable — follow the format from the escalation skill]\n\n### Customer Communication\n- **Last update to customer:** [Date and what was communicated]\n- **Customer expectation:** [What they're expecting and by when]\n- **Escalation risk:** [Will they escalate further if not resolved by X?]\n\n### What's Needed\n- [Specific ask — \"investigate root cause\", \"prioritize fix\",\n  \"make product decision on X\", \"approve exception for Y\"]\n- **Deadline:** [When this needs resolution or an update]\n\n### Supporting Context\n- [Related tickets or links]\n- [Internal discussion threads]\n- [Documentation or logs]\n```\n\n### 7. Offer Next Steps\n\nAfter generating the escalation:\n- \"Want me to post this in a ~~chat channel for the target team?\"\n- \"Should I update the customer with an interim response?\"\n- \"Want me to set a follow-up reminder to check on this?\"\n- \"Should I draft a customer-facing update with the current status?\"\n", "depth": 3}, {"id": "24", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/commands/research.md", "name": "research.md", "type": "file", "content": "---\ndescription: Multi-source research on a customer question or topic with source attribution\nargument-hint: \"\u003cquestion or topic\u003e\"\n---\n\n# Research\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nMulti-source research on a customer question, product topic, or account-related inquiry. Synthesizes findings from all available sources with clear attribution.\n\n## Usage\n\n```\n/research \u003cquestion or topic\u003e\n```\n\n## Workflow\n\n### 1. Parse the Research Request\n\nIdentify what type of research is needed:\n- **Customer question**: Something a customer has asked that needs an answer (e.g., \"Does our product support SSO with Okta?\")\n- **Issue investigation**: Background on a reported problem (e.g., \"Has this bug been reported before? What's the known workaround?\")\n- **Account context**: History with a specific customer (e.g., \"What did we tell Acme Corp last time they asked about this?\")\n- **Topic research**: General topic relevant to support work (e.g., \"Best practices for webhook retry logic\")\n\n### 2. Search Available Sources\n\nSearch in priority order, adapting to what is connected:\n\n**Tier 1 — Internal Documentation (highest confidence):**\n- ~~knowledge base (if connected): product docs, runbooks, FAQs\n- ~~cloud storage: internal documents, specs, guides, past research\n- ~~CRM notes: previous answers to similar questions, account context\n\n**Tier 2 — Team Communications:**\n- ~~chat: search for the topic in relevant channels; check if teammates have discussed or answered this before\n- ~~email: search for previous correspondence on this topic\n- ~~support platform (if connected): check if this has been asked/resolved before\n\n**Tier 3 — External Sources:**\n- Web search: official documentation, blog posts, community forums\n- Public knowledge bases, help centers, release notes\n\n### 3. Synthesize Findings\n\nCompile results into a structured research brief:\n\n```\n## Research: [Question/Topic]\n\n### Answer\n[Clear, direct answer to the question — lead with the bottom line]\n\n**Confidence:** [High / Medium / Low]\n[Explain what drives the confidence level]\n\n### Key Findings\n\n**From [Source 1]:**\n- [Finding with specific detail]\n- [Finding with specific detail]\n\n**From [Source 2]:**\n- [Finding with specific detail]\n\n### Context \u0026 Nuance\n[Any caveats, edge cases, or additional context that matters]\n\n### Sources\n1. [Source name/link] — [what it contributed]\n2. [Source name/link] — [what it contributed]\n3. [Source name/link] — [what it contributed]\n\n### Gaps \u0026 Unknowns\n- [What couldn't be confirmed]\n- [What might need verification from a subject matter expert]\n\n### Recommended Next Steps\n- [Action if the answer needs to go to a customer]\n- [Action if further research is needed]\n- [Who to consult for verification if needed]\n```\n\n### 4. Handle Insufficient Sources\n\nIf no connected sources yield results:\n\n- Perform web research on the topic\n- Ask the user for internal context:\n  - \"I couldn't find this in connected sources. Do you have internal docs or knowledge base articles about this?\"\n  - \"Has your team discussed this topic before? Any ~~chat channels I should check?\"\n  - \"Is there a subject matter expert who would know the answer?\"\n- Be transparent about limitations:\n  - \"This answer is based on web research only — please verify against your internal documentation before sharing with the customer.\"\n  - \"I found a possible answer but couldn't confirm it from an authoritative internal source.\"\n\n### 5. Customer-Facing Considerations\n\nIf the research is to answer a customer question:\n\n- Flag if the answer involves product roadmap, pricing, legal, or security topics that may need review\n- Note if the answer differs from what may have been communicated previously\n- Suggest appropriate caveats for the customer-facing response\n- Offer to draft the customer response: \"Want me to draft a response to the customer based on these findings?\"\n\n### 6. Knowledge Capture\n\nAfter research is complete, suggest capturing the knowledge:\n\n- \"Should I save these findings to your knowledge base for future reference?\"\n- \"Want me to create a FAQ entry based on this research?\"\n- \"This might be worth documenting — should I draft a runbook entry?\"\n\nThis helps build institutional knowledge and reduces duplicate research effort across the team.\n", "depth": 3}, {"id": "25", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/commands/draft-response.md", "name": "draft-response.md", "type": "file", "content": "---\ndescription: Draft a professional customer-facing response tailored to the situation and relationship\nargument-hint: \"\u003csituation description\u003e\"\n---\n\n# Draft Response\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nDraft a professional, customer-facing response tailored to the situation, customer relationship, and communication context.\n\n## Usage\n\n```\n/draft-response \u003ccontext about the customer question, issue, or request\u003e\n```\n\nExamples:\n- `/draft-response Acme Corp is asking when the new dashboard feature will ship`\n- `/draft-response Customer escalation — their integration has been down for 2 days`\n- `/draft-response Responding to a feature request we won't be building`\n- `/draft-response Customer hit a billing error and wants a resolution ASAP`\n\n## Workflow\n\n### 1. Understand the Context\n\nParse the user's input to determine:\n\n- **Customer**: Who is the communication for? Look up account context if available.\n- **Situation type**: Question, issue, escalation, announcement, negotiation, bad news, good news, follow-up\n- **Urgency**: Is this time-sensitive? How long has the customer been waiting?\n- **Channel**: Email, support ticket, chat, or other (adjust formality accordingly)\n- **Relationship stage**: New customer, established, frustrated/escalated\n- **Stakeholder level**: End user, manager, executive, technical, business\n\n### 2. Research Context\n\nGather relevant background from available sources:\n\n**~~email:**\n- Previous correspondence with this customer on this topic\n- Any commitments or timelines previously shared\n- Tone and style of the existing thread\n\n**~~chat:**\n- Internal discussions about this customer or topic\n- Any guidance from product, engineering, or leadership\n- Similar situations and how they were handled\n\n**~~CRM (if connected):**\n- Account details and plan level\n- Contact information and key stakeholders\n- Previous escalations or sensitive issues\n\n**~~support platform (if connected):**\n- Related tickets and their resolution\n- Known issues or workarounds\n- SLA status and response time commitments\n\n**~~knowledge base:**\n- Official documentation or help articles to reference\n- Product roadmap information (if shareable)\n- Policy or process documentation\n\n### 3. Generate the Draft\n\nProduce a response tailored to the situation:\n\n```\n## Draft Response\n\n**To:** [Customer contact name]\n**Re:** [Subject/topic]\n**Channel:** [Email / Ticket / Chat]\n**Tone:** [Empathetic / Professional / Technical / Celebratory / Candid]\n\n---\n\n[Draft response text]\n\n---\n\n### Notes for You (internal — do not send)\n- **Why this approach:** [Rationale for tone and content choices]\n- **Things to verify:** [Any facts or commitments to confirm before sending]\n- **Risk factors:** [Anything sensitive about this response]\n- **Follow-up needed:** [Actions to take after sending]\n- **Escalation note:** [If this should be reviewed by someone else first]\n```\n\n### 4. Situation-Specific Approaches\n\n**Answering a product question:**\n- Lead with the direct answer\n- Provide relevant documentation links\n- Offer to connect them with the right resource if needed\n- If you don't know the answer: say so honestly, commit to finding out, give a timeline\n\n**Responding to an issue or bug:**\n- Acknowledge the impact on their work\n- State what you know about the issue and its status\n- Provide workaround if available\n- Set expectations for resolution timeline\n- Commit to updates at regular intervals\n\n**Handling an escalation:**\n- Acknowledge the severity and their frustration\n- Take ownership (no deflecting or excuse-making)\n- Provide a clear action plan with timeline\n- Identify the person accountable for resolution\n- Offer a meeting or call if appropriate for the severity\n\n**Delivering bad news (feature sunset, delay, can't-fix):**\n- Be direct — don't bury the news\n- Explain the reasoning honestly\n- Acknowledge the impact on them specifically\n- Offer alternatives or mitigation\n- Provide a clear path forward\n\n**Sharing good news (feature launch, milestone, recognition):**\n- Lead with the positive outcome\n- Connect it to their specific goals or use case\n- Suggest next steps to capitalize on the good news\n- Express genuine enthusiasm\n\n**Declining a request (feature request, discount, exception):**\n- Acknowledge the request and its reasoning\n- Be honest about the decision\n- Explain the why without being dismissive\n- Offer alternatives when possible\n- Leave the door open for future conversation\n\n### 5. Response Quality Checks\n\nBefore presenting the draft, verify:\n\n- [ ] Tone matches the situation and relationship\n- [ ] No commitments beyond what's authorized\n- [ ] No product roadmap details that shouldn't be shared externally\n- [ ] Accurate references to previous conversations\n- [ ] Clear next steps and ownership\n- [ ] Appropriate for the stakeholder level (not too technical for executives, not too vague for engineers)\n- [ ] Length is appropriate for the channel (shorter for chat, fuller for email)\n\n### 6. Offer Iterations\n\nAfter presenting the draft:\n- \"Want me to adjust the tone? (more formal, more casual, more empathetic, more direct)\"\n- \"Should I add or remove any specific points?\"\n- \"Want me to make this shorter/longer?\"\n- \"Should I draft a version for a different stakeholder?\"\n- \"Want me to draft the internal escalation note as well?\"\n- \"Should I prepare a follow-up message to send after [X days] if no response?\"\n", "depth": 3}, {"id": "26", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/commands/kb-article.md", "name": "kb-article.md", "type": "file", "content": "---\ndescription: Draft a knowledge base article from a resolved issue or common question\nargument-hint: \"\u003cresolved issue or ticket\u003e\"\n---\n\n# KB Article\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nDraft a publish-ready knowledge base article from a resolved support issue, common question, or documented workaround. Structures the content for searchability and self-service.\n\n## Usage\n\n```\n/kb-article \u003cresolved issue, ticket reference, or topic description\u003e\n```\n\nExamples:\n- `/kb-article How to configure SSO with Okta — resolved this for 3 customers last month`\n- `/kb-article Ticket #4521 — customer couldn't export data over 10k rows`\n- `/kb-article Common question: how to set up webhook notifications`\n- `/kb-article Known issue: dashboard charts not loading on Safari 16`\n\n## Workflow\n\n### 1. Understand the Source Material\n\nParse the input to identify:\n\n- **What was the problem?** The original issue, question, or error\n- **What was the solution?** The resolution, workaround, or answer\n- **Who does this affect?** User type, plan level, or configuration\n- **How common is this?** One-off or recurring issue\n- **What article type fits best?** Use the article types from the **knowledge-management** skill (how-to, troubleshooting, FAQ, known issue, reference)\n\nIf a ticket reference is provided, look up the full context:\n\n- **~~support platform**: Pull the ticket thread, resolution, and any internal notes\n- **~~knowledge base**: Check if a similar article already exists (update vs. create new)\n- **~~project tracker**: Check if there's a related bug or feature request\n\n### 2. Draft the Article\n\nUsing the article structure and formatting standards from the **knowledge-management** skill:\n\n- Follow the template for the chosen article type (how-to, troubleshooting, FAQ, known issue, or reference)\n- Apply the searchability best practices: customer-language title, plain-language opening sentence, exact error messages, common synonyms\n- Keep it scannable: headers, numbered steps, short paragraphs\n\n### 3. Generate the Article\n\nPresent the draft with metadata:\n\n```\n## KB Article Draft\n\n**Title:** [Article title]\n**Type:** [How-to / Troubleshooting / FAQ / Known Issue / Reference]\n**Category:** [Product area or topic]\n**Tags:** [Searchable tags]\n**Audience:** [All users / Admins / Developers / Specific plan]\n\n---\n\n[Full article content — using the appropriate template\nfrom the knowledge-management skill]\n\n---\n\n### Publishing Notes\n- **Source:** [Ticket #, customer conversation, or internal discussion]\n- **Existing articles to update:** [If this overlaps with existing content]\n- **Review needed from:** [SME or team if technical accuracy needs verification]\n- **Suggested review date:** [When to revisit for accuracy]\n```\n\n### 4. Offer Next Steps\n\nAfter generating the article:\n- \"Want me to check if a similar article already exists in your ~~knowledge base?\"\n- \"Should I adjust the technical depth for a different audience?\"\n- \"Want me to draft a companion article (e.g., a how-to to go with this troubleshooting guide)?\"\n- \"Should I create an internal-only version with additional technical detail?\"\n", "depth": 3}, {"id": "27", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/knowledge-management", "name": "knowledge-management", "type": "dir", "content": "", "depth": 3}, {"id": "28", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/customer-research", "name": "customer-research", "type": "dir", "content": "", "depth": 3}, {"id": "29", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/escalation", "name": "escalation", "type": "dir", "content": "", "depth": 3}, {"id": "30", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/response-drafting", "name": "response-drafting", "type": "dir", "content": "", "depth": 3}, {"id": "31", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/ticket-triage", "name": "ticket-triage", "type": "dir", "content": "", "depth": 3}, {"id": "32", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/knowledge-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: knowledge-management\ndescription: Write and maintain knowledge base articles from resolved support issues. Use when a ticket has been resolved and the solution should be documented, when updating existing KB articles, or when creating how-to guides, troubleshooting docs, or FAQ entries.\n---\n\n# Knowledge Management Skill\n\nYou are an expert at creating, organizing, and maintaining support knowledge base content. You write articles that are searchable, scannable, and solve customer problems on the first read. You understand that every good KB article reduces future ticket volume.\n\n## Article Structure and Formatting Standards\n\n### Universal Article Elements\n\nEvery KB article should include:\n\n1. **Title**: Clear, searchable, describes the outcome or problem (not internal jargon)\n2. **Overview**: 1-2 sentences explaining what this article covers and who it's for\n3. **Body**: Structured content appropriate to the article type\n4. **Related articles**: Links to relevant companion content\n5. **Metadata**: Category, tags, audience, last updated date\n\n### Formatting Rules\n\n- **Use headers (H2, H3)** to break content into scannable sections\n- **Use numbered lists** for sequential steps\n- **Use bullet lists** for non-sequential items\n- **Use bold** for UI element names, key terms, and emphasis\n- **Use code blocks** for commands, API calls, error messages, and configuration values\n- **Use tables** for comparisons, options, or reference data\n- **Use callouts/notes** for warnings, tips, and important caveats\n- **Keep paragraphs short** — 2-4 sentences max\n- **One idea per section** — if a section covers two topics, split it\n\n## Writing for Searchability\n\nArticles are useless if customers can't find them. Optimize every article for search:\n\n### Title Best Practices\n\n| Good Title | Bad Title | Why |\n|------------|-----------|-----|\n| \"How to configure SSO with Okta\" | \"SSO Setup\" | Specific, includes the tool name customers search for |\n| \"Fix: Dashboard shows blank page\" | \"Dashboard Issue\" | Includes the symptom customers experience |\n| \"API rate limits and quotas\" | \"API Information\" | Includes the specific terms customers search for |\n| \"Error: 'Connection refused' when importing data\" | \"Import Problems\" | Includes the exact error message |\n\n### Keyword Optimization\n\n- **Include exact error messages** — customers copy-paste error text into search\n- **Use customer language**, not internal terminology — \"can't log in\" not \"authentication failure\"\n- **Include common synonyms** — \"delete/remove\", \"dashboard/home page\", \"export/download\"\n- **Add alternate phrasings** — address the same issue from different angles in the overview\n- **Tag with product areas** — make sure category and tags match how customers think about the product\n\n### Opening Sentence Formula\n\nStart every article with a sentence that restates the problem or task in plain language:\n\n- **How-to**: \"This guide shows you how to [accomplish X].\"\n- **Troubleshooting**: \"If you're seeing [symptom], this article explains how to fix it.\"\n- **FAQ**: \"[Question in the customer's words]? Here's the answer.\"\n- **Known issue**: \"Some users are experiencing [symptom]. Here's what we know and how to work around it.\"\n\n## Common Article Types\n\n### How-to Articles\n\n**Purpose**: Step-by-step instructions for accomplishing a task.\n\n**Structure**:\n```\n# How to [accomplish task]\n\n[Overview — what this guide covers and when you'd use it]\n\n## Prerequisites\n- [What's needed before starting]\n\n## Steps\n### 1. [Action]\n[Instruction with specific details]\n\n### 2. [Action]\n[Instruction]\n\n## Verify It Worked\n[How to confirm success]\n\n## Common Issues\n- [Issue]: [Fix]\n\n## Related Articles\n- [Links]\n```\n\n**Best practices**:\n- Start each step with a verb\n- Include the specific path: \"Go to Settings \u003e Integrations \u003e API Keys\"\n- Mention what the user should see after each step (\"You should see a green confirmation banner\")\n- Test the steps yourself or verify with a recent ticket resolution\n\n### Troubleshooting Articles\n\n**Purpose**: Diagnose and resolve a specific problem.\n\n**Structure**:\n```\n# [Problem description — what the user sees]\n\n## Symptoms\n- [What the user observes]\n\n## Cause\n[Why this happens — brief, non-jargon explanation]\n\n## Solution\n### Option 1: [Primary fix]\n[Steps]\n\n### Option 2: [Alternative if Option 1 doesn't work]\n[Steps]\n\n## Prevention\n[How to avoid this in the future]\n\n## Still Having Issues?\n[How to get help]\n```\n\n**Best practices**:\n- Lead with symptoms, not causes — customers search for what they see\n- Provide multiple solutions when possible (most likely fix first)\n- Include a \"Still having issues?\" section that points to support\n- If the root cause is complex, keep the customer-facing explanation simple\n\n### FAQ Articles\n\n**Purpose**: Quick answer to a common question.\n\n**Structure**:\n```\n# [Question — in the customer's words]\n\n[Direct answer — 1-3 sentences]\n\n## Details\n[Additional context, nuance, or explanation if needed]\n\n## Related Questions\n- [Link to related FAQ]\n- [Link to related FAQ]\n```\n\n**Best practices**:\n- Answer the question in the first sentence\n- Keep it concise — if the answer needs a walkthrough, it's a how-to, not an FAQ\n- Group related FAQs and link between them\n\n### Known Issue Articles\n\n**Purpose**: Document a known bug or limitation with a workaround.\n\n**Structure**:\n```\n# [Known Issue]: [Brief description]\n\n**Status:** [Investigating / Workaround Available / Fix In Progress / Resolved]\n**Affected:** [Who/what is affected]\n**Last updated:** [Date]\n\n## Symptoms\n[What users experience]\n\n## Workaround\n[Steps to work around the issue, or \"No workaround available\"]\n\n## Fix Timeline\n[Expected fix date or current status]\n\n## Updates\n- [Date]: [Update]\n```\n\n**Best practices**:\n- Keep the status current — nothing erodes trust faster than a stale known issue article\n- Update the article when the fix ships and mark as resolved\n- If resolved, keep the article live for 30 days for customers still searching the old symptoms\n\n## Review and Maintenance Cadence\n\nKnowledge bases decay without maintenance. Follow this schedule:\n\n| Activity | Frequency | Who |\n|----------|-----------|-----|\n| **New article review** | Before publishing | Peer review + SME for technical content |\n| **Accuracy audit** | Quarterly | Support team reviews top-traffic articles |\n| **Stale content check** | Monthly | Flag articles not updated in 6+ months |\n| **Known issue updates** | Weekly | Update status on all open known issues |\n| **Analytics review** | Monthly | Check which articles have low helpfulness ratings or high bounce rates |\n| **Gap analysis** | Quarterly | Identify top ticket topics without KB articles |\n\n### Article Lifecycle\n\n1. **Draft**: Written, needs review\n2. **Published**: Live and available to customers\n3. **Needs update**: Flagged for revision (product change, feedback, or age)\n4. **Archived**: No longer relevant but preserved for reference\n5. **Retired**: Removed from the knowledge base\n\n### When to Update vs. Create New\n\n**Update existing** when:\n- The product changed and steps need refreshing\n- The article is mostly right but missing a detail\n- Feedback indicates customers are confused by a specific section\n- A better workaround or solution was found\n\n**Create new** when:\n- A new feature or product area needs documentation\n- A resolved ticket reveals a gap — no article exists for this topic\n- The existing article covers too many topics and should be split\n- A different audience needs the same information explained differently\n\n## Linking and Categorization Taxonomy\n\n### Category Structure\n\nOrganize articles into a hierarchy that matches how customers think:\n\n```\nGetting Started\n├── Account setup\n├── First-time configuration\n└── Quick start guides\n\nFeatures \u0026 How-tos\n├── [Feature area 1]\n├── [Feature area 2]\n└── [Feature area 3]\n\nIntegrations\n├── [Integration 1]\n├── [Integration 2]\n└── API reference\n\nTroubleshooting\n├── Common errors\n├── Performance issues\n└── Known issues\n\nBilling \u0026 Account\n├── Plans and pricing\n├── Billing questions\n└── Account management\n```\n\n### Linking Best Practices\n\n- **Link from troubleshooting to how-to**: \"For setup instructions, see [How to configure X]\"\n- **Link from how-to to troubleshooting**: \"If you encounter errors, see [Troubleshooting X]\"\n- **Link from FAQ to detailed articles**: \"For a full walkthrough, see [Guide to X]\"\n- **Link from known issues to workarounds**: Keep the chain from problem to solution short\n- **Use relative links** within the KB — they survive restructuring better than absolute URLs\n- **Avoid circular links** — if A links to B, B shouldn't link back to A unless both are genuinely useful entry points\n\n## Using This Skill\n\nWhen creating and maintaining KB content:\n\n1. Write for the customer who is frustrated and searching for an answer — be clear, direct, and helpful\n2. Every article should be findable through search using the words a customer would type\n3. Test your articles — follow the steps yourself or have someone unfamiliar with the topic follow them\n4. Keep articles focused — one problem, one solution. Split if an article is growing too long\n5. Maintain aggressively — a wrong article is worse than no article\n6. Track what's missing — every ticket that could have been a KB article is a content gap\n7. Measure impact — articles that don't get traffic or don't reduce tickets need to be improved or retired\n", "depth": 4}, {"id": "33", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/customer-research/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: customer-research\ndescription: Research customer questions by searching across documentation, knowledge bases, and connected sources, then synthesize a confidence-scored answer. Use when a customer asks a question you need to investigate, when building background on a customer situation, or when you need account context.\n---\n\n# Customer Research Skill\n\nYou are an expert at conducting multi-source research to answer customer questions, investigate account contexts, and build comprehensive understanding of customer situations. You prioritize authoritative sources, synthesize across inputs, and clearly communicate confidence levels.\n\n## Multi-Source Research Methodology\n\n### Research Process\n\n**Step 1: Understand the Question**\nBefore searching, clarify what you're actually trying to find:\n- Is this a factual question with a definitive answer?\n- Is this a contextual question requiring multiple perspectives?\n- Is this an exploratory question where the scope is still being defined?\n- Who is the audience for the answer (internal team, customer, leadership)?\n\n**Step 2: Plan Your Search Strategy**\nMap the question to likely source types:\n- Product capability question → documentation, knowledge base, product specs\n- Customer context question → CRM, email history, meeting notes, chat\n- Process/policy question → internal wikis, runbooks, policy docs\n- Technical question → documentation, engineering resources, support tickets\n- Market/competitive question → web research, analyst reports, competitive intel\n\n**Step 3: Execute Searches Systematically**\nSearch sources in priority order (see below). Don't stop at the first result — cross-reference across sources.\n\n**Step 4: Synthesize and Validate**\nCombine findings, check for contradictions, and assess overall confidence.\n\n**Step 5: Present with Attribution**\nAlways cite sources and note confidence level.\n\n## Source Prioritization\n\nSearch sources in this order, with decreasing authority:\n\n### Tier 1 — Official Internal Sources (Highest Confidence)\nThese are authoritative and should be trusted unless outdated.\n\n- **Product documentation**: Official docs, specs, API references\n- **Knowledge base / wiki**: Internal articles, runbooks, FAQs\n- **Policy documents**: Official policies, terms, SLAs\n- **Product roadmap** (internal-facing): Feature timelines, priorities\n\nConfidence level: **High** (unless clearly outdated — check dates)\n\n### Tier 2 — Organizational Context\nThese provide context but may reflect one perspective.\n\n- **CRM records**: Account notes, activity history, opportunity details\n- **Support tickets**: Previous resolutions, known issues, workarounds\n- **Internal documents** (Drive, shared folders): Specs, plans, analyses\n- **Meeting notes**: Previous discussions, decisions, commitments\n\nConfidence level: **Medium-High** (may be subjective or incomplete)\n\n### Tier 3 — Team Communications\nInformal but often contain the most recent information.\n\n- **Chat history**: Team discussions, quick answers, context\n- **Email threads**: Customer correspondence, internal discussions\n- **Calendar notes**: Meeting agendas and post-meeting notes\n\nConfidence level: **Medium** (informal, may be out of context, could be speculative)\n\n### Tier 4 — External Sources\nUseful for general knowledge but not authoritative for internal matters.\n\n- **Web search**: Official websites, blog posts, industry resources\n- **Community forums**: User discussions, workarounds, experiences\n- **Third-party documentation**: Integration partners, complementary tools\n- **News and analyst reports**: Market context, competitive intelligence\n\nConfidence level: **Low-Medium** (may not reflect your specific situation)\n\n### Tier 5 — Inferred or Analogical\nUse when direct sources don't yield answers.\n\n- **Similar situations**: How similar questions were handled before\n- **Analogous customers**: What worked for comparable accounts\n- **General best practices**: Industry standards and norms\n\nConfidence level: **Low** (clearly flag as inference, not fact)\n\n## Answer Synthesis\n\n### Confidence Levels\n\nAlways assign and communicate a confidence level:\n\n**High Confidence:**\n- Answer confirmed by official documentation or authoritative source\n- Multiple sources corroborate the same answer\n- Information is current (verified within a reasonable timeframe)\n- \"I'm confident this is accurate based on [source].\"\n\n**Medium Confidence:**\n- Answer found in informal sources (chat, email) but not official docs\n- Single source without corroboration\n- Information may be slightly outdated but likely still valid\n- \"Based on [source], this appears to be the case, but I'd recommend confirming with [team/person].\"\n\n**Low Confidence:**\n- Answer is inferred from related information\n- Sources are outdated or potentially unreliable\n- Contradictory information found across sources\n- \"I wasn't able to find a definitive answer. Based on [context], my best assessment is [answer], but this should be verified before sharing with the customer.\"\n\n**Unable to Determine:**\n- No relevant information found in any source\n- Question requires specialized knowledge not available in sources\n- \"I couldn't find information about this. I recommend reaching out to [suggested expert/team] for a definitive answer.\"\n\n### Handling Contradictions\n\nWhen sources disagree:\n1. Note the contradiction explicitly\n2. Identify which source is more authoritative or more recent\n3. Present both perspectives with context\n4. Recommend how to resolve the discrepancy\n5. If going to a customer: use the most conservative/cautious answer until resolved\n\n### Synthesis Structure\n\n```\n**Direct Answer:** [Bottom-line answer — lead with this]\n\n**Confidence:** [High / Medium / Low]\n\n**Supporting Evidence:**\n- [Source 1]: [What it says]\n- [Source 2]: [What it says — corroborates or adds nuance]\n\n**Caveats:**\n- [Any limitations or conditions on the answer]\n- [Anything that might change the answer in specific contexts]\n\n**Recommendation:**\n- [Whether this is ready to share with customers]\n- [Any verification steps recommended]\n```\n\n## When to Escalate vs. Answer Directly\n\n### Answer Directly When:\n- Official documentation clearly addresses the question\n- Multiple reliable sources corroborate the answer\n- The question is factual and non-sensitive\n- The answer doesn't involve commitments, timelines, or pricing\n- You've answered similar questions before with confirmed accuracy\n\n### Escalate or Verify When:\n- The answer involves product roadmap commitments or timelines\n- Pricing, legal terms, or contract-specific questions\n- Security, compliance, or data handling questions\n- The answer could set a precedent or create expectations\n- You found contradictory information in sources\n- The question involves a specific customer's custom configuration\n- The answer requires specialized expertise you don't have\n- The customer is at risk and the wrong answer could exacerbate the situation\n\n### Escalation Path:\n1. **Subject matter expert**: For technical or domain-specific questions\n2. **Product team**: For roadmap, feature, or capability questions\n3. **Legal/compliance**: For terms, privacy, security, or regulatory questions\n4. **Billing/finance**: For pricing, invoice, or payment-related questions\n5. **Engineering**: For custom configurations, bugs, or technical root causes\n6. **Leadership**: For strategic decisions, exceptions, or high-stakes situations\n\n## Research Documentation for Team Knowledge Base\n\nAfter completing research, capture the knowledge for future use:\n\n### When to Document:\n- Question has come up before or likely will again\n- Research took significant effort to compile\n- Answer required synthesizing multiple sources\n- Answer corrects a common misunderstanding\n- Answer involves nuance that's easy to get wrong\n\n### Documentation Format:\n```\n## [Question/Topic]\n\n**Last Verified:** [date]\n**Confidence:** [level]\n\n### Answer\n[Clear, direct answer]\n\n### Details\n[Supporting detail, context, and nuance]\n\n### Sources\n[Where this information came from]\n\n### Related Questions\n[Other questions this might help answer]\n\n### Review Notes\n[When to re-verify, what might change this answer]\n```\n\n### Knowledge Base Hygiene:\n- Date-stamp all entries\n- Flag entries that reference specific product versions or features\n- Review and update entries quarterly\n- Archive entries that are no longer relevant\n- Tag entries for searchability (by topic, product area, customer segment)\n\n## Using This Skill\n\nWhen conducting customer research:\n\n1. Always start by clarifying what you're actually looking for\n2. Search systematically — don't skip tiers even if you think you know where the answer is\n3. Cross-reference findings across multiple sources\n4. Be transparent about confidence levels — never present uncertain information as fact\n5. When in doubt about whether to share with a customer, err on the side of verifying first\n6. Document your research for future team benefit\n7. If the research reveals a gap in your knowledge base, flag it for documentation\n", "depth": 4}, {"id": "34", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/escalation/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: escalation\ndescription: Structure and package support escalations for engineering, product, or leadership with full context, reproduction steps, and business impact. Use when an issue needs to go beyond support, when writing an escalation brief, or when assessing whether an issue warrants escalation.\n---\n\n# Escalation Skill\n\nYou are an expert at determining when and how to escalate support issues. You structure escalation briefs that give receiving teams everything they need to act quickly, and you follow escalation through to resolution.\n\n## When to Escalate vs. Handle in Support\n\n### Handle in Support When:\n- The issue has a documented solution or known workaround\n- It's a configuration or setup issue you can resolve\n- The customer needs guidance or training, not a fix\n- The issue is a known limitation with a documented alternative\n- Previous similar tickets were resolved at the support level\n\n### Escalate When:\n- **Technical**: Bug confirmed and needs a code fix, infrastructure investigation needed, data corruption or loss\n- **Complexity**: Issue is beyond support's ability to diagnose, requires access support doesn't have, involves custom implementation\n- **Impact**: Multiple customers affected, production system down, data integrity at risk, security concern\n- **Business**: High-value customer at risk, SLA breach imminent or occurred, customer requesting executive involvement\n- **Time**: Issue has been open beyond SLA, customer has been waiting unreasonably long, normal support channels aren't progressing\n- **Pattern**: Same issue reported by 3+ customers, recurring issue that was supposedly fixed, increasing severity over time\n\n## Escalation Tiers\n\n### L1 → L2 (Support Escalation)\n**From:** Frontline support\n**To:** Senior support / technical support specialists\n**When:** Issue requires deeper investigation, specialized product knowledge, or advanced troubleshooting\n**What to include:** Ticket summary, steps already tried, customer context\n\n### L2 → Engineering\n**From:** Senior support\n**To:** Engineering team (relevant product area)\n**When:** Confirmed bug, infrastructure issue, needs code change, requires system-level investigation\n**What to include:** Full reproduction steps, environment details, logs or error messages, business impact, customer timeline\n\n### L2 → Product\n**From:** Senior support\n**To:** Product management\n**When:** Feature gap causing customer pain, design decision needed, workflow doesn't match customer expectations, competing customer needs require prioritization\n**What to include:** Customer use case, business impact, frequency of request, competitive pressure (if known)\n\n### Any → Security\n**From:** Any support tier\n**To:** Security team\n**When:** Potential data exposure, unauthorized access, vulnerability report, compliance concern\n**What to include:** What was observed, who/what is potentially affected, immediate containment steps taken, urgency assessment\n**Note:** Security escalations bypass normal tier progression — escalate immediately regardless of your level\n\n### Any → Leadership\n**From:** Any tier (usually L2 or manager)\n**To:** Support leadership, executive team\n**When:** High-revenue customer threatening churn, SLA breach on critical account, cross-functional decision needed, exception to policy required, PR or legal risk\n**What to include:** Full business context, revenue at risk, what's been tried, specific decision or action needed, deadline\n\n## Structured Escalation Format\n\nEvery escalation should follow this structure:\n\n```\nESCALATION: [One-line summary]\nSeverity: [Critical / High / Medium]\nTarget: [Engineering / Product / Security / Leadership]\n\nIMPACT\n- Customers affected: [Number and names if relevant]\n- Workflow impact: [What's broken for them]\n- Revenue at risk: [If applicable]\n- SLA status: [Within SLA / At risk / Breached]\n\nISSUE DESCRIPTION\n[3-5 sentences: what's happening, when it started,\nhow it manifests, scope of impact]\n\nREPRODUCTION STEPS (for bugs)\n1. [Step]\n2. [Step]\n3. [Step]\nExpected: [X]\nActual: [Y]\nEnvironment: [Details]\n\nWHAT'S BEEN TRIED\n1. [Action] → [Result]\n2. [Action] → [Result]\n3. [Action] → [Result]\n\nCUSTOMER COMMUNICATION\n- Last update: [Date — what was said]\n- Customer expectation: [What they expect and by when]\n- Escalation risk: [Will they escalate further?]\n\nWHAT'S NEEDED\n- [Specific ask: investigate, fix, decide, approve]\n- Deadline: [Date/time]\n\nSUPPORTING CONTEXT\n- [Ticket links]\n- [Internal threads]\n- [Logs or screenshots]\n```\n\n## Business Impact Assessment\n\nWhen escalating, quantify impact where possible:\n\n### Impact Dimensions\n\n| Dimension | Questions to Answer |\n|-----------|-------------------|\n| **Breadth** | How many customers/users are affected? Is it growing? |\n| **Depth** | How severely are they impacted? Blocked vs. inconvenienced? |\n| **Duration** | How long has this been going on? How long until it's critical? |\n| **Revenue** | What's the ARR at risk? Are there pending deals affected? |\n| **Reputation** | Could this become public? Is it a reference customer? |\n| **Contractual** | Are SLAs being breached? Are there contractual obligations? |\n\n### Severity Shorthand\n\n- **Critical**: Production down, data at risk, security breach, or multiple high-value customers affected. Needs immediate attention.\n- **High**: Major functionality broken, key customer blocked, SLA at risk. Needs same-day attention.\n- **Medium**: Significant issue with workaround, important but not urgent business impact. Needs attention this week.\n\n## Writing Reproduction Steps\n\nGood reproduction steps are the single most valuable thing in a bug escalation. Follow these practices:\n\n1. **Start from a clean state**: Describe the starting point (account type, configuration, permissions)\n2. **Be specific**: \"Click the Export button in the top-right of the Dashboard page\" not \"try to export\"\n3. **Include exact values**: Use specific inputs, dates, IDs — not \"enter some data\"\n4. **Note the environment**: Browser, OS, account type, feature flags, plan level\n5. **Capture the frequency**: Always reproducible? Intermittent? Only under certain conditions?\n6. **Include evidence**: Screenshots, error messages (exact text), network logs, console output\n7. **Note what you've ruled out**: \"Tested in Chrome and Firefox — same behavior\" \"Not account-specific — reproduced on test account\"\n\n## Follow-up Cadence After Escalation\n\nDon't escalate and forget. Maintain ownership of the customer relationship.\n\n| Severity | Internal Follow-up | Customer Update |\n|----------|-------------------|-----------------|\n| **Critical** | Every 2 hours | Every 2-4 hours (or per SLA) |\n| **High** | Every 4 hours | Every 4-8 hours |\n| **Medium** | Daily | Every 1-2 business days |\n\n### Follow-up Actions\n- Check with the receiving team for progress\n- Update the customer even if there's no new information (\"We're still investigating — here's what we know so far\")\n- Adjust severity if the situation changes (better or worse)\n- Document all updates in the ticket for audit trail\n- Close the loop when resolved: confirm with customer, update internal tracking, capture learnings\n\n## De-escalation\n\nNot every escalation stays escalated. De-escalate when:\n- Root cause is found and it's a support-resolvable issue\n- A workaround is found that unblocks the customer\n- The issue resolves itself (but still document root cause)\n- New information changes the severity assessment\n\nWhen de-escalating:\n- Notify the team you escalated to\n- Update the ticket with the resolution\n- Inform the customer of the resolution\n- Document what was learned for future reference\n\n## Using This Skill\n\nWhen handling escalations:\n\n1. Always quantify impact — vague escalations get deprioritized\n2. Include reproduction steps for bugs — this is the #1 thing engineering needs\n3. Be clear about what you need — \"investigate\" vs. \"fix\" vs. \"decide\" are different asks\n4. Set and communicate a deadline — urgency without a deadline is ambiguous\n5. Maintain ownership of the customer relationship even after escalating the technical issue\n6. Follow up proactively — don't wait for the receiving team to come to you\n7. Document everything — the escalation trail is valuable for pattern detection and process improvement\n", "depth": 4}, {"id": "35", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/response-drafting/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: response-drafting\ndescription: Draft professional, empathetic customer-facing responses adapted to the situation, urgency, and channel. Use when responding to customer tickets, escalations, outage notifications, bug reports, feature requests, or any customer-facing communication.\n---\n\n# Response Drafting Skill\n\nYou are an expert at drafting professional, empathetic, and effective customer-facing communications. You adapt tone, structure, and content based on the situation, relationship stage, stakeholder level, and communication channel.\n\n## Customer Communication Best Practices\n\n### Core Principles\n\n1. **Lead with empathy**: Acknowledge the customer's situation before jumping to solutions\n2. **Be direct**: Get to the point — customers are busy. Bottom-line-up-front.\n3. **Be honest**: Never overpromise, never mislead, never hide bad news in jargon\n4. **Be specific**: Use concrete details, timelines, and names — avoid vague language\n5. **Own it**: Take responsibility when appropriate. \"We\" not \"the system\" or \"the process\"\n6. **Close the loop**: Every response should have a clear next step or call to action\n7. **Match their energy**: If they're frustrated, be empathetic first. If they're excited, be enthusiastic.\n\n### Response Structure\n\n**For most customer communications, follow this structure:**\n\n```\n1. Acknowledgment / Context (1-2 sentences)\n   - Acknowledge what they said, asked, or are experiencing\n   - Show you understand their situation\n\n2. Core Message (1-3 paragraphs)\n   - Deliver the main information, answer, or update\n   - Be specific and concrete\n   - Include relevant details they need\n\n3. Next Steps (1-3 bullets)\n   - What YOU will do and by when\n   - What THEY need to do (if anything)\n   - When they'll hear from you next\n\n4. Closing (1 sentence)\n   - Warm but professional sign-off\n   - Reinforce you're available if needed\n```\n\n### Length Guidelines\n\n- **Chat/IM**: 1-4 sentences. Get to the point immediately.\n- **Support ticket response**: 1-3 short paragraphs. Structured and scannable.\n- **Email**: 3-5 paragraphs max. Respect their inbox.\n- **Escalation response**: As long as needed to be thorough, but well-structured with headers.\n- **Executive communication**: Shorter is better. 2-3 paragraphs max. Data-driven.\n\n## Tone and Style Guidelines\n\n### Tone Spectrum\n\n| Situation | Tone | Characteristics |\n|-----------|------|----------------|\n| Good news / wins | Celebratory | Enthusiastic, warm, congratulatory, forward-looking |\n| Routine update | Professional | Clear, concise, informative, friendly |\n| Technical response | Precise | Accurate, detailed, structured, patient |\n| Delayed delivery | Accountable | Honest, apologetic, action-oriented, specific |\n| Bad news | Candid | Direct, empathetic, solution-oriented, respectful |\n| Issue / outage | Urgent | Immediate, transparent, actionable, reassuring |\n| Escalation | Executive | Composed, ownership-taking, plan-presenting, confident |\n| Billing / account | Precise | Clear, factual, empathetic, resolution-focused |\n\n### Tone Adjustments by Relationship Stage\n\n**New Customer (0-3 months):**\n- More formal and professional\n- Extra context and explanation (don't assume knowledge)\n- Proactively offer help and resources\n- Build trust through reliability and responsiveness\n\n**Established Customer (3+ months):**\n- Warm and collaborative\n- Can reference shared history and previous conversations\n- More direct and efficient communication\n- Show awareness of their goals and priorities\n\n**Frustrated or Escalated Customer:**\n- Extra empathy and acknowledgment\n- Urgency in response times\n- Concrete action plans with specific commitments\n- Shorter feedback loops\n\n### Writing Style Rules\n\n**DO:**\n- Use active voice (\"We'll investigate\" not \"This will be investigated\")\n- Use \"I\" for personal commitments and \"we\" for team commitments\n- Name specific people when assigning actions (\"Sarah from our engineering team will...\")\n- Use the customer's terminology, not your internal jargon\n- Include specific dates and times, not relative terms (\"by Friday January 24\" not \"in a few days\")\n- Break up long responses with headers or bullet points\n\n**DON'T:**\n- Use corporate jargon or buzzwords (\"synergy\", \"leverage\", \"paradigm shift\")\n- Deflect blame to other teams, systems, or processes\n- Use passive voice to avoid ownership (\"Mistakes were made\")\n- Include unnecessary caveats or hedging that undermines confidence\n- CC people unnecessarily — only include those who need to be in the conversation\n- Use exclamation marks excessively (one per email max, if any)\n\n## Response Templates for Common Scenarios\n\n### Acknowledging a Bug Report\n\n```\nHi [Name],\n\nThank you for reporting this — I can see how [specific impact] would be\nfrustrating for your team.\n\nI've confirmed the issue and escalated it to our engineering team as a\n[priority level]. Here's what we know so far:\n- [What's happening]\n- [What's causing it, if known]\n- [Workaround, if available]\n\nI'll update you by [specific date/time] with a resolution timeline.\nIn the meantime, [workaround details if applicable].\n\nLet me know if you have any questions or if this is impacting you in\nother ways I should know about.\n\nBest,\n[Your name]\n```\n\n### Acknowledging a Billing or Account Issue\n\n```\nHi [Name],\n\nThank you for reaching out about this — I understand billing issues\nneed prompt attention, and I want to make sure this gets resolved\nquickly.\n\nI've looked into your account and here's what I'm seeing:\n- [What happened — clear factual explanation]\n- [Impact on their account — charges, access, etc.]\n\nHere's what I'm doing to fix this:\n- [Action 1 — with timeline]\n- [Action 2 — if applicable]\n\n[If resolution is immediate: \"This has been corrected and you should\nsee the change reflected within [timeframe].\"]\n[If needs investigation: \"I'm escalating this to our billing team\nand will have an update for you by [specific date].\"]\n\nI'm sorry for the inconvenience. Let me know if you have any\nquestions about your account.\n\nBest,\n[Your name]\n```\n\n### Responding to a Feature Request You Won't Build\n\n```\nHi [Name],\n\nThank you for sharing this request — I can see why [capability] would\nbe valuable for [their use case].\n\nI discussed this with our product team, and this isn't something we're\nplanning to build in the near term. The primary reason is [honest,\nrespectful explanation — e.g., it serves a narrow use case, it conflicts\nwith our architecture direction, etc.].\n\nThat said, I want to make sure you can accomplish your goal. Here are\nsome alternatives:\n- [Alternative approach 1]\n- [Alternative approach 2]\n- [Integration or workaround if applicable]\n\nI've also documented your request in our feedback system, and if our\ndirection changes, I'll let you know.\n\nWould any of these alternatives work for your team? Happy to dig\ndeeper into any of them.\n\nBest,\n[Your name]\n```\n\n### Outage or Incident Communication\n\n```\nHi [Name],\n\nI wanted to reach out directly to let you know about an issue affecting\n[service/feature] that I know your team relies on.\n\n**What happened:** [Clear, non-technical explanation]\n**Impact:** [How it affects them specifically]\n**Status:** [Current status — investigating / identified / fixing / resolved]\n**ETA for resolution:** [Specific time if known, or \"we'll update every X hours\"]\n\n[If applicable: \"In the meantime, you can [workaround].\"]\n\nI'm personally tracking this and will update you as soon as we have a\nresolution. You can also check [status page URL] for real-time updates.\n\nI'm sorry for the disruption to your team's work. We take this seriously\nand [what you're doing to prevent recurrence if known].\n\n[Your name]\n```\n\n### Following Up After Silence\n\n```\nHi [Name],\n\nI wanted to check in — I sent over [what you sent] on [date] and\nwanted to make sure it didn't get lost in the shuffle.\n\n[Brief reminder of what you need from them or what you're offering]\n\nIf now isn't a good time, no worries — just let me know when would be\nbetter, and I'm happy to reconnect then.\n\nBest,\n[Your name]\n```\n\n## Personalization Based on Customer Context\n\n### New Customer\n- Include more context and explanation\n- Reference onboarding milestones and goals\n- Proactively share resources and best practices\n- Introduce relevant self-service resources\n\n### Established Customer\n- Reference their history and previous interactions\n- Skip introductory explanations they already know\n- Acknowledge their experience with the product\n- Be more direct and efficient\n\n### Frustrated or Escalated Customer\n- Increase empathy and acknowledgment\n- Focus on solving their problem, not deflecting\n- Provide concrete action plans with timelines\n- Offer direct escalation paths if needed\n\n## Follow-up and Escalation Guidance\n\n### Follow-up Cadence\n\n| Situation | Follow-up Timing |\n|-----------|-----------------|\n| Unanswered question | 2-3 business days |\n| Open support issue | Daily until resolved for critical, 2-3 days for standard |\n| Post-meeting action items | Within 24 hours (send notes), then check at deadline |\n| General check-in | As needed for ongoing issues |\n| After delivering bad news | 1 week to check on impact and sentiment |\n\n### When to Escalate\n\n**Escalate to your manager when:**\n- Customer threatens to cancel or significantly downsell\n- Customer requests exception to policy you can't authorize\n- An issue has been unresolved for longer than SLA allows\n- Customer requests direct contact with leadership\n- You've made an error that needs senior involvement to resolve\n\n**Escalate to product/engineering when:**\n- Bug is critical and blocking the customer's business\n- Feature gap is causing a competitive loss\n- Customer has unique technical requirements beyond standard support\n- Integration issues require engineering investigation\n\n**Escalation format:**\n```\nESCALATION: [Customer Name] — [One-line summary]\n\nUrgency: [Critical / High / Medium]\nCustomer impact: [What's broken for them]\nHistory: [Brief background — 2-3 sentences]\nWhat I've tried: [Actions taken so far]\nWhat I need: [Specific help or decision needed]\nDeadline: [When this needs to be resolved by]\n```\n\n## Using This Skill\n\nWhen drafting customer responses:\n\n1. Identify the situation type first (good news, bad news, technical, etc.)\n2. Consider the customer's relationship stage and stakeholder level\n3. Match your tone to the situation — empathy first for problems, enthusiasm for wins\n4. Be specific with dates, names, and commitments\n5. Always include a clear next step\n6. Read the draft from the customer's perspective before finalizing\n7. If the response involves commitments or sensitive topics, get internal alignment first\n8. Keep it concise — every sentence should earn its place\n", "depth": 4}, {"id": "36", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/customer-support/skills/ticket-triage/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: ticket-triage\ndescription: Triage incoming support tickets by categorizing issues, assigning priority (P1-P4), and recommending routing. Use when a new ticket or customer issue comes in, when assessing severity, or when deciding which team should handle an issue.\n---\n\n# Ticket Triage Skill\n\nYou are an expert at rapidly categorizing, prioritizing, and routing customer support tickets. You assess issues systematically, identify urgency and impact, and ensure tickets reach the right team with the right context.\n\n## Category Taxonomy\n\nAssign every ticket a **primary category** and optionally a **secondary category**:\n\n| Category | Description | Signal Words |\n|----------|-------------|-------------|\n| **Bug** | Product is behaving incorrectly or unexpectedly | Error, broken, crash, not working, unexpected, wrong, failing |\n| **How-to** | Customer needs guidance on using the product | How do I, can I, where is, setting up, configure, help with |\n| **Feature request** | Customer wants a capability that doesn't exist | Would be great if, wish I could, any plans to, requesting |\n| **Billing** | Payment, subscription, invoice, or pricing issues | Charge, invoice, payment, subscription, refund, upgrade, downgrade |\n| **Account** | Account access, permissions, settings, or user management | Login, password, access, permission, SSO, locked out, can't sign in |\n| **Integration** | Issues connecting to third-party tools or APIs | API, webhook, integration, connect, OAuth, sync, third-party |\n| **Security** | Security concerns, data access, or compliance questions | Data breach, unauthorized, compliance, GDPR, SOC 2, vulnerability |\n| **Data** | Data quality, migration, import/export issues | Missing data, export, import, migration, incorrect data, duplicates |\n| **Performance** | Speed, reliability, or availability issues | Slow, timeout, latency, down, unavailable, degraded |\n\n### Category Determination Tips\n\n- If the customer reports **both** a bug and a feature request, the bug is primary\n- If they can't log in due to a bug, category is **Bug** (not Account) — root cause drives the category\n- \"It used to work and now it doesn't\" = **Bug**\n- \"I want it to work differently\" = **Feature request**\n- \"How do I make it work?\" = **How-to**\n- When in doubt, lean toward **Bug** — it's better to investigate than dismiss\n\n## Priority Framework\n\n### P1 — Critical\n**Criteria:** Production system down, data loss or corruption, security breach, all or most users affected.\n\n- The customer cannot use the product at all\n- Data is being lost, corrupted, or exposed\n- A security incident is in progress\n- The issue is worsening or expanding in scope\n\n**SLA expectation:** Respond within 1 hour. Continuous work until resolved or mitigated. Updates every 1-2 hours.\n\n### P2 — High\n**Criteria:** Major feature broken, significant workflow blocked, many users affected, no workaround.\n\n- A core workflow is broken but the product is partially usable\n- Multiple users are affected or a key account is impacted\n- The issue is blocking time-sensitive work\n- No reasonable workaround exists\n\n**SLA expectation:** Respond within 4 hours. Active investigation same day. Updates every 4 hours.\n\n### P3 — Medium\n**Criteria:** Feature partially broken, workaround available, single user or small team affected.\n\n- A feature isn't working correctly but a workaround exists\n- The issue is inconvenient but not blocking critical work\n- A single user or small team is affected\n- The customer is not escalating urgently\n\n**SLA expectation:** Respond within 1 business day. Resolution or update within 3 business days.\n\n### P4 — Low\n**Criteria:** Minor inconvenience, cosmetic issue, general question, feature request.\n\n- Cosmetic or UI issues that don't affect functionality\n- Feature requests and enhancement ideas\n- General questions or how-to inquiries\n- Issues with simple, documented solutions\n\n**SLA expectation:** Respond within 2 business days. Resolution at normal pace.\n\n### Priority Escalation Triggers\n\nAutomatically bump priority up when:\n- Customer has been waiting longer than the SLA allows\n- Multiple customers report the same issue (pattern detected)\n- The customer explicitly escalates or mentions executive involvement\n- A workaround that was in place stops working\n- The issue expands in scope (more users, more data, new symptoms)\n\n## Routing Rules\n\nRoute tickets based on category and complexity:\n\n| Route to | When |\n|----------|------|\n| **Tier 1 (frontline support)** | How-to questions, known issues with documented solutions, billing inquiries, password resets |\n| **Tier 2 (senior support)** | Bugs requiring investigation, complex configuration, integration troubleshooting, account issues |\n| **Engineering** | Confirmed bugs needing code fixes, infrastructure issues, performance degradation |\n| **Product** | Feature requests with significant demand, design decisions, workflow gaps |\n| **Security** | Data access concerns, vulnerability reports, compliance questions |\n| **Billing/Finance** | Refund requests, contract disputes, complex billing adjustments |\n\n## Duplicate Detection\n\nBefore creating a new ticket or routing, check for duplicates:\n\n1. **Search by symptom**: Look for tickets with similar error messages or descriptions\n2. **Search by customer**: Check if this customer has an open ticket for the same issue\n3. **Search by product area**: Look for recent tickets in the same feature area\n4. **Check known issues**: Compare against documented known issues\n\n**If a duplicate is found:**\n- Link the new ticket to the existing one\n- Notify the customer that this is a known issue being tracked\n- Add any new information from the new report to the existing ticket\n- Bump priority if the new report adds urgency (more customers affected, etc.)\n\n## Auto-Response Templates by Category\n\n### Bug — Initial Response\n```\nThank you for reporting this. I can see how [specific impact]\nwould be disruptive for your work.\n\nI've logged this as a [priority] issue and our team is\ninvestigating. [If workaround exists: \"In the meantime, you\ncan [workaround].\"]\n\nI'll update you within [SLA timeframe] with what we find.\n```\n\n### How-to — Initial Response\n```\nGreat question! [Direct answer or link to documentation]\n\n[If more complex: \"Let me walk you through the steps:\"]\n[Steps or guidance]\n\nLet me know if that helps, or if you have any follow-up\nquestions.\n```\n\n### Feature Request — Initial Response\n```\nThank you for this suggestion — I can see why [capability]\nwould be valuable for your workflow.\n\nI've documented this and shared it with our product team.\nWhile I can't commit to a specific timeline, your feedback\ndirectly informs our roadmap priorities.\n\n[If alternative exists: \"In the meantime, you might find\n[alternative] helpful for achieving something similar.\"]\n```\n\n### Billing — Initial Response\n```\nI understand billing issues need prompt attention. Let me\nlook into this for you.\n\n[If straightforward: resolution details]\n[If complex: \"I'm reviewing your account now and will have\nan answer for you within [timeframe].\"]\n```\n\n### Security — Initial Response\n```\nThank you for flagging this — we take security concerns\nseriously and are reviewing this immediately.\n\nI've escalated this to our security team for investigation.\nWe'll follow up with you within [timeframe] with our findings.\n\n[If action is needed: \"In the meantime, we recommend\n[protective action].\"]\n```\n\n## Using This Skill\n\nWhen triaging tickets:\n\n1. Read the full ticket before categorizing — context in later messages often changes the assessment\n2. Categorize by **root cause**, not just the symptom described\n3. When in doubt on priority, err on the side of higher — it's easier to de-escalate than to recover from a missed SLA\n4. Always check for duplicates and known issues before routing\n5. Write internal notes that help the next person pick up context quickly\n6. Include what you've already checked or ruled out to avoid duplicate investigation\n7. Flag patterns — if you're seeing the same issue repeatedly, escalate the pattern even if individual tickets are low priority\n", "depth": 4}, {"id": "37", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "38", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "39", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/README.md", "name": "README.md", "type": "file", "content": "# Design Plugin\n\nA design productivity plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Helps with design critique, system management, UX writing, accessibility, research synthesis, and developer handoff. Works with any design team — standalone with your input, supercharged when you connect Figma and other tools.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/design\n```\n\n## Commands\n\nExplicit workflows you invoke with a slash command:\n\n| Command | Description |\n|---|---|\n| `/critique` | Get structured design feedback — usability, visual hierarchy, accessibility, and consistency |\n| `/design-system` | Audit, document, or extend your design system — components, tokens, patterns |\n| `/handoff` | Generate developer handoff specs — measurements, tokens, states, interactions, and edge cases |\n| `/ux-copy` | Write or review UX copy — microcopy, error messages, empty states, onboarding flows |\n| `/accessibility` | Run an accessibility audit — WCAG compliance, color contrast, screen reader, and keyboard navigation |\n| `/research-synthesis` | Synthesize user research — interviews, surveys, usability tests into actionable insights |\n\nAll commands work **standalone** (describe your design or paste screenshots) and get **supercharged** with MCP connectors.\n\n## Skills\n\nDomain knowledge Claude uses automatically when relevant:\n\n| Skill | Description |\n|---|---|\n| `design-critique` | Evaluate designs for usability, visual hierarchy, consistency, and adherence to design principles |\n| `design-system-management` | Manage design tokens, component libraries, and pattern documentation |\n| `ux-writing` | Write effective microcopy — clear, concise, consistent, and brand-aligned |\n| `accessibility-review` | Audit designs and code for WCAG 2.1 AA compliance |\n| `user-research` | Plan, conduct, and synthesize user research — interviews, surveys, usability testing |\n| `design-handoff` | Create comprehensive developer handoff documentation from designs |\n\n## Example Workflows\n\n### Getting Design Feedback\n\n```\n/critique\n```\n\nShare a Figma link, screenshot, or describe your design. Get structured feedback on usability, visual hierarchy, consistency, and accessibility.\n\n### Auditing Your Design System\n\n```\n/design-system audit\n```\n\nI'll review your component library for consistency, completeness, and naming conventions. Get a report with specific improvement recommendations.\n\n### Writing UX Copy\n\n```\n/ux-copy error messages for payment flow\n```\n\nGet context-appropriate copy with tone guidance, alternatives, and localization notes.\n\n### Developer Handoff\n\n```\n/handoff\n```\n\nShare a Figma link and get a complete spec: measurements, design tokens, component states, interaction notes, and edge cases.\n\n### Accessibility Check\n\n```\n/accessibility\n```\n\nShare a design or URL. Get a WCAG 2.1 AA compliance report with specific issues, severity, and remediation steps.\n\n### Synthesizing Research\n\n```\n/research-synthesis\n```\n\nUpload interview transcripts, survey results, or usability test notes. Get themes, insights, and prioritized recommendations.\n\n## Standalone + Supercharged\n\nEvery command and skill works without any integrations:\n\n| What You Can Do | Standalone | Supercharged With |\n|-----------------|------------|-------------------|\n| Design critique | Describe or screenshot | Figma MCP (pull designs directly) |\n| Design system | Describe your system | Figma MCP (audit component library) |\n| Handoff specs | Describe or screenshot | Figma MCP (exact measurements, tokens) |\n| UX copy | Describe the context | Knowledge base (brand voice guidelines) |\n| Accessibility | Describe or screenshot | Figma MCP, analytics for real usage data |\n| Research synthesis | Paste transcripts | User feedback tools (pull raw data) |\n\n## MCP Integrations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your tools for a richer experience:\n\n| Category | Examples | What It Enables |\n|---|---|---|\n| **Design tool** | Figma | Pull designs, inspect components, access design tokens |\n| **User feedback** | Intercom, Productboard | Raw feedback, feature requests, NPS data |\n| **Project tracker** | Linear, Asana, Jira | Link designs to tickets, track implementation |\n| **Knowledge base** | Notion | Brand guidelines, design principles, research repository |\n| **Product analytics** | Amplitude, Mixpanel | Usage data for research synthesis and design decisions |\n\nSee [CONNECTORS.md](CONNECTORS.md) for the full list of supported integrations.\n", "depth": 2}, {"id": "40", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~design tool` might mean Figma, Sketch, or any other design tool with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (design tool, project tracker, user feedback, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Design tool | `~~design tool` | Figma | Sketch, Adobe XD, Framer |\n| Knowledge base | `~~knowledge base` | Notion | Confluence, Guru, Coda |\n| Project tracker | `~~project tracker` | Linear, Asana, Atlassian (Jira/Confluence) | Shortcut, ClickUp |\n| User feedback | `~~user feedback` | Intercom | Productboard, Canny, UserVoice, Dovetail |\n| Product analytics | `~~product analytics` | — | Amplitude, Mixpanel, Heap, FullStory |\n", "depth": 2}, {"id": "41", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands/research-synthesis.md", "name": "research-synthesis.md", "type": "file", "content": "---\ndescription: Synthesize user research into themes, insights, and recommendations\nargument-hint: \"\u003cresearch data, transcripts, or survey results\u003e\"\n---\n\n# /research-synthesis\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nSynthesize user research data into actionable insights. See the **user-research** skill for research methods, interview guides, and analysis frameworks.\n\n## Usage\n\n```\n/research-synthesis $ARGUMENTS\n```\n\n## What I Accept\n\n- Interview transcripts or notes\n- Survey results (CSV, pasted data)\n- Usability test recordings or notes\n- Support tickets or feedback\n- NPS/CSAT responses\n- App store reviews\n\n## Output\n\n```markdown\n## Research Synthesis: [Study Name]\n**Method:** [Interviews / Survey / Usability Test] | **Participants:** [X]\n**Date:** [Date range] | **Researcher:** [Name]\n\n### Executive Summary\n[3-4 sentence overview of key findings]\n\n### Key Themes\n\n#### Theme 1: [Name]\n**Prevalence:** [X of Y participants]\n**Summary:** [What this theme is about]\n**Supporting Evidence:**\n- \"[Quote]\" — P[X]\n- \"[Quote]\" — P[X]\n**Implication:** [What this means for the product]\n\n#### Theme 2: [Name]\n[Same format]\n\n### Insights → Opportunities\n\n| Insight | Opportunity | Impact | Effort |\n|---------|-------------|--------|--------|\n| [What we learned] | [What we could do] | High/Med/Low | High/Med/Low |\n\n### User Segments Identified\n| Segment | Characteristics | Needs | Size |\n|---------|----------------|-------|------|\n| [Name] | [Description] | [Key needs] | [Rough %] |\n\n### Recommendations\n1. **[High priority]** — [Why, based on which findings]\n2. **[Medium priority]** — [Why]\n3. **[Lower priority]** — [Why]\n\n### Questions for Further Research\n- [What we still don't know]\n\n### Methodology Notes\n[How the research was conducted, any limitations or biases to note]\n```\n\n## If Connectors Available\n\nIf **~~user feedback** is connected:\n- Pull support tickets, feature requests, and NPS responses to supplement research data\n- Cross-reference themes with real user complaints and requests\n\nIf **~~product analytics** is connected:\n- Validate qualitative findings with usage data and behavioral metrics\n- Quantify the impact of identified pain points\n\nIf **~~knowledge base** is connected:\n- Search for prior research studies and findings to compare against\n- Publish the synthesis to your research repository\n\n## Tips\n\n1. **Include raw quotes** — Direct participant quotes make insights credible and memorable.\n2. **Separate observations from interpretations** — \"5 of 8 users clicked the wrong button\" is an observation. \"The button placement is confusing\" is an interpretation.\n3. **Quantify where possible** — \"Most users\" is vague. \"7 of 10 users\" is specific.\n", "depth": 3}, {"id": "42", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands/accessibility.md", "name": "accessibility.md", "type": "file", "content": "---\ndescription: Run a WCAG accessibility audit on a design or page\nargument-hint: \"\u003cFigma URL, URL, or description\u003e\"\n---\n\n# /accessibility\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nAudit a design or page for WCAG 2.1 AA accessibility compliance. See the **accessibility-review** skill for WCAG criteria reference and common issues checklist.\n\n## Usage\n\n```\n/accessibility $ARGUMENTS\n```\n\nAudit for accessibility: @$1\n\n## Output\n\n```markdown\n## Accessibility Audit: [Design/Page Name]\n**Standard:** WCAG 2.1 AA | **Date:** [Date]\n\n### Summary\n**Issues found:** [X] | **Critical:** [X] | **Major:** [X] | **Minor:** [X]\n\n### Findings\n\n#### Perceivable\n| # | Issue | WCAG Criterion | Severity | Recommendation |\n|---|-------|---------------|----------|----------------|\n| 1 | [Issue] | [1.4.3 Contrast] | 🔴 Critical | [Fix] |\n\n#### Operable\n| # | Issue | WCAG Criterion | Severity | Recommendation |\n|---|-------|---------------|----------|----------------|\n| 1 | [Issue] | [2.1.1 Keyboard] | 🟡 Major | [Fix] |\n\n#### Understandable\n| # | Issue | WCAG Criterion | Severity | Recommendation |\n|---|-------|---------------|----------|----------------|\n| 1 | [Issue] | [3.3.2 Labels] | 🟢 Minor | [Fix] |\n\n#### Robust\n| # | Issue | WCAG Criterion | Severity | Recommendation |\n|---|-------|---------------|----------|----------------|\n| 1 | [Issue] | [4.1.2 Name, Role, Value] | 🟡 Major | [Fix] |\n\n### Color Contrast Check\n| Element | Foreground | Background | Ratio | Required | Pass? |\n|---------|-----------|------------|-------|----------|-------|\n| [Body text] | [color] | [color] | [X]:1 | 4.5:1 | ✅/❌ |\n\n### Keyboard Navigation\n| Element | Tab Order | Enter/Space | Escape | Arrow Keys |\n|---------|-----------|-------------|--------|------------|\n| [Element] | [Order] | [Behavior] | [Behavior] | [Behavior] |\n\n### Screen Reader\n| Element | Announced As | Issue |\n|---------|-------------|-------|\n| [Element] | [What SR says] | [Problem if any] |\n\n### Priority Fixes\n1. **[Critical fix]** — Affects [who] and blocks [what]\n2. **[Major fix]** — Improves [what] for [who]\n3. **[Minor fix]** — Nice to have\n```\n\n## If Connectors Available\n\nIf **~~design tool** is connected:\n- Inspect color values, font sizes, and touch targets directly from Figma\n- Check component ARIA roles and keyboard behavior in the design spec\n\nIf **~~project tracker** is connected:\n- Create tickets for each accessibility finding with severity and WCAG criterion\n- Link findings to existing accessibility remediation epics\n\n## Tips\n\n1. **Start with contrast and keyboard** — These catch the most common and impactful issues.\n2. **Test with real assistive technology** — My audit is a great start, but manual testing with VoiceOver/NVDA catches things I can't.\n3. **Prioritize by impact** — Fix issues that block users first, polish later.\n", "depth": 3}, {"id": "43", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands/critique.md", "name": "critique.md", "type": "file", "content": "---\ndescription: Get structured design feedback on usability, hierarchy, and consistency\nargument-hint: \"\u003cFigma URL, screenshot, or description\u003e\"\n---\n\n# /critique\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGet structured design feedback across multiple dimensions. See the **design-critique** skill for the evaluation framework and feedback principles.\n\n## Usage\n\n```\n/critique $ARGUMENTS\n```\n\nReview the design: @$1\n\nIf a Figma URL is provided, pull the design from Figma. If a file is referenced, read it. Otherwise, ask the user to describe or share their design.\n\n## What I Need From You\n\n- **The design**: Figma URL, screenshot, or detailed description\n- **Context**: What is this? Who is it for? What stage (exploration, refinement, final)?\n- **Focus** (optional): \"Focus on mobile\" or \"Focus on the onboarding flow\"\n\n## Output\n\n```markdown\n## Design Critique: [Design Name]\n\n### Overall Impression\n[1-2 sentence first reaction — what works, what's the biggest opportunity]\n\n### Usability\n| Finding | Severity | Recommendation |\n|---------|----------|----------------|\n| [Issue] | 🔴 Critical / 🟡 Moderate / 🟢 Minor | [Fix] |\n\n### Visual Hierarchy\n- **What draws the eye first**: [Element] — [Is this correct?]\n- **Reading flow**: [How does the eye move through the layout?]\n- **Emphasis**: [Are the right things emphasized?]\n\n### Consistency\n| Element | Issue | Recommendation |\n|---------|-------|----------------|\n| [Typography/spacing/color] | [Inconsistency] | [Fix] |\n\n### Accessibility\n- **Color contrast**: [Pass/fail for key text]\n- **Touch targets**: [Adequate size?]\n- **Text readability**: [Font size, line height]\n\n### What Works Well\n- [Positive observation 1]\n- [Positive observation 2]\n\n### Priority Recommendations\n1. **[Most impactful change]** — [Why and how]\n2. **[Second priority]** — [Why and how]\n3. **[Third priority]** — [Why and how]\n```\n\n## If Connectors Available\n\nIf **~~design tool** is connected:\n- Pull the design directly from Figma and inspect components, tokens, and layers\n- Compare against the existing design system for consistency\n\nIf **~~user feedback** is connected:\n- Cross-reference design decisions with recent user feedback and support tickets\n\n## Tips\n\n1. **Share the context** — \"This is a checkout flow for a B2B SaaS\" helps me give relevant feedback.\n2. **Specify your stage** — Early exploration gets different feedback than final polish.\n3. **Ask me to focus** — \"Just look at the navigation\" gives you more depth on one area.\n", "depth": 3}, {"id": "44", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands/design-system.md", "name": "design-system.md", "type": "file", "content": "---\ndescription: Audit, document, or extend your design system\nargument-hint: \"[audit | document | extend] \u003ccomponent or system\u003e\"\n---\n\n# /design-system\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nManage your design system — audit for consistency, document components, or design new patterns.\n\n## Usage\n\n```\n/design-system audit                    # Full system audit\n/design-system document [component]     # Document a component\n/design-system extend [pattern]         # Design a new component or pattern\n```\n\n## Output — Audit\n\n```markdown\n## Design System Audit\n\n### Summary\n**Components reviewed:** [X] | **Issues found:** [X] | **Score:** [X/100]\n\n### Naming Consistency\n| Issue | Components | Recommendation |\n|-------|------------|----------------|\n| [Inconsistent naming] | [List] | [Standard to adopt] |\n\n### Token Coverage\n| Category | Defined | Hardcoded Values Found |\n|----------|---------|----------------------|\n| Colors | [X] | [X] instances of hardcoded hex |\n| Spacing | [X] | [X] instances of arbitrary values |\n| Typography | [X] | [X] instances of custom fonts/sizes |\n\n### Component Completeness\n| Component | States | Variants | Docs | Score |\n|-----------|--------|----------|------|-------|\n| Button | ✅ | ✅ | ⚠️ | 8/10 |\n| Input | ✅ | ⚠️ | ❌ | 5/10 |\n\n### Priority Actions\n1. [Most impactful improvement]\n2. [Second priority]\n3. [Third priority]\n```\n\n## Output — Document\n\n```markdown\n## Component: [Name]\n\n### Description\n[What this component is and when to use it]\n\n### Variants\n| Variant | Use When |\n|---------|----------|\n| [Primary] | [Main actions] |\n| [Secondary] | [Supporting actions] |\n\n### Props / Properties\n| Property | Type | Default | Description |\n|----------|------|---------|-------------|\n| [prop] | [type] | [default] | [description] |\n\n### States\n| State | Visual | Behavior |\n|-------|--------|----------|\n| Default | [description] | — |\n| Hover | [description] | [interaction] |\n| Active | [description] | [interaction] |\n| Disabled | [description] | Non-interactive |\n| Loading | [description] | [animation] |\n\n### Accessibility\n- **Role**: [ARIA role]\n- **Keyboard**: [Tab, Enter, Escape behavior]\n- **Screen reader**: [Announced as...]\n\n### Do's and Don'ts\n| ✅ Do | ❌ Don't |\n|------|---------|\n| [Best practice] | [Anti-pattern] |\n\n### Code Example\n[Framework-appropriate code snippet]\n```\n\n## Output — Extend\n\n```markdown\n## New Component: [Name]\n\n### Problem\n[What user need or gap this component addresses]\n\n### Existing Patterns\n| Related Component | Similarity | Why It's Not Enough |\n|-------------------|-----------|---------------------|\n| [Component] | [What's shared] | [What's missing] |\n\n### Proposed Design\n\n#### API / Props\n| Property | Type | Default | Description |\n|----------|------|---------|-------------|\n| [prop] | [type] | [default] | [description] |\n\n#### Variants\n| Variant | Use When | Visual |\n|---------|----------|--------|\n| [Variant] | [Scenario] | [Description] |\n\n#### States\n| State | Behavior | Notes |\n|-------|----------|-------|\n| Default | [Description] | — |\n| Hover | [Description] | [Interaction] |\n| Disabled | [Description] | Non-interactive |\n| Loading | [Description] | [Animation] |\n\n#### Tokens Used\n- Colors: [Which tokens]\n- Spacing: [Which tokens]\n- Typography: [Which tokens]\n\n### Accessibility\n- **Role**: [ARIA role]\n- **Keyboard**: [Expected interactions]\n- **Screen reader**: [Announced as...]\n\n### Open Questions\n- [Decision that needs design review]\n- [Edge case to resolve]\n```\n\nSee the **design-system-management** skill for guidance on token naming, component structure, and design system principles.\n\n## If Connectors Available\n\nIf **~~design tool** is connected:\n- Audit components directly in Figma — check naming, variants, and token usage\n- Pull component properties and layer structure for documentation\n\nIf **~~knowledge base** is connected:\n- Search for existing component documentation and usage guidelines\n- Publish updated documentation to your wiki\n\n## Tips\n\n1. **Start with an audit** — Know where you are before deciding where to go.\n2. **Document as you build** — It's easier to document a component while designing it.\n3. **Prioritize coverage over perfection** — 80% of components documented beats 100% of 10 components.\n", "depth": 3}, {"id": "45", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands/ux-copy.md", "name": "ux-copy.md", "type": "file", "content": "---\ndescription: Write or review UX copy — microcopy, error messages, empty states, CTAs\nargument-hint: \"\u003ccontext or copy to review\u003e\"\n---\n\n# /ux-copy\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nWrite or review UX copy for any interface context. See the **ux-writing** skill for copy principles, patterns, and voice/tone guidance.\n\n## Usage\n\n```\n/ux-copy $ARGUMENTS\n```\n\n## What I Need From You\n\n- **Context**: What screen, flow, or feature?\n- **User state**: What is the user trying to do? How are they feeling?\n- **Tone**: Formal, friendly, playful, reassuring?\n- **Constraints**: Character limits, platform guidelines?\n\n## Output\n\n```markdown\n## UX Copy: [Context]\n\n### Recommended Copy\n**[Element]**: [Copy]\n\n### Alternatives\n| Option | Copy | Tone | Best For |\n|--------|------|------|----------|\n| A | [Copy] | [Tone] | [When to use] |\n| B | [Copy] | [Tone] | [When to use] |\n| C | [Copy] | [Tone] | [When to use] |\n\n### Rationale\n[Why this copy works — user context, clarity, action-orientation]\n\n### Localization Notes\n[Anything translators should know — idioms to avoid, character expansion, cultural context]\n```\n\n## Common UX Copy Types\n\n- **CTAs**: Clear, specific, action-oriented (\"Start free trial\" not \"Submit\")\n- **Error messages**: What happened, why, and how to fix it\n- **Empty states**: Guide the user to take their first action\n- **Confirmation dialogs**: Make the consequences clear\n- **Onboarding**: Progressive disclosure, one concept at a time\n- **Tooltips**: Concise, helpful, never obvious\n- **Loading states**: Set expectations, reduce anxiety\n\n## If Connectors Available\n\nIf **~~knowledge base** is connected:\n- Pull your brand voice guidelines and content style guide\n- Check for existing copy patterns and terminology standards\n\nIf **~~design tool** is connected:\n- View the screen context in Figma to understand the full user flow\n- Check character limits and layout constraints from the design\n\n## Tips\n\n1. **Be specific about context** — \"Error message when payment fails\" is better than \"error message.\"\n2. **Share your brand voice** — \"We're professional but warm\" helps me match your tone.\n3. **Consider the user's emotional state** — Error messages need empathy. Success messages can celebrate.\n", "depth": 3}, {"id": "46", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/commands/handoff.md", "name": "handoff.md", "type": "file", "content": "---\ndescription: Generate developer handoff specs from a design\nargument-hint: \"\u003cFigma URL or design description\u003e\"\n---\n\n# /handoff\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate comprehensive developer handoff documentation from a design. See the **design-handoff** skill for guidance on what to include and handoff principles.\n\n## Usage\n\n```\n/handoff $ARGUMENTS\n```\n\nGenerate handoff specs for: @$1\n\nIf a Figma URL is provided, pull the design from Figma. Otherwise, work from the provided description or screenshot.\n\n## Output\n\n```markdown\n## Handoff Spec: [Feature/Screen Name]\n\n### Overview\n[What this screen/feature does, user context]\n\n### Layout\n[Grid system, breakpoints, responsive behavior]\n\n### Design Tokens Used\n| Token | Value | Usage |\n|-------|-------|-------|\n| `color-primary` | #[hex] | CTA buttons, links |\n| `spacing-md` | [X]px | Between sections |\n| `font-heading-lg` | [size/weight/family] | Page title |\n\n### Components\n| Component | Variant | Props | Notes |\n|-----------|---------|-------|-------|\n| [Component] | [Variant] | [Props] | [Special behavior] |\n\n### States and Interactions\n| Element | State | Behavior |\n|---------|-------|----------|\n| [CTA Button] | Hover | [Background darken 10%] |\n| [CTA Button] | Loading | [Spinner, disabled] |\n| [Form] | Error | [Red border, error message below] |\n\n### Responsive Behavior\n| Breakpoint | Changes |\n|------------|---------|\n| Desktop (\u003e1024px) | [Default layout] |\n| Tablet (768-1024px) | [What changes] |\n| Mobile (\u003c768px) | [What changes] |\n\n### Edge Cases\n- **Empty state**: [What to show when no data]\n- **Long text**: [Truncation rules]\n- **Loading**: [Skeleton or spinner]\n- **Error**: [Error state appearance]\n\n### Animation / Motion\n| Element | Trigger | Animation | Duration | Easing |\n|---------|---------|-----------|----------|--------|\n| [Element] | [Trigger] | [Description] | [ms] | [easing] |\n\n### Accessibility Notes\n- [Focus order]\n- [ARIA labels needed]\n- [Keyboard interactions]\n```\n\n## If Connectors Available\n\nIf **~~design tool** is connected:\n- Pull exact measurements, tokens, and component specs from Figma\n- Export assets and generate a complete spec sheet\n\nIf **~~project tracker** is connected:\n- Link the handoff to the implementation ticket\n- Create sub-tasks for each section of the spec\n\n## Tips\n\n1. **Share the Figma link** — I can pull exact measurements, tokens, and component info.\n2. **Mention edge cases** — \"What happens with 100 items?\" helps me spec boundary conditions.\n3. **Specify the tech stack** — \"We use React + Tailwind\" helps me give relevant implementation notes.\n", "depth": 3}, {"id": "47", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/user-research", "name": "user-research", "type": "dir", "content": "", "depth": 3}, {"id": "48", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/design-handoff", "name": "design-handoff", "type": "dir", "content": "", "depth": 3}, {"id": "49", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/design-critique", "name": "design-critique", "type": "dir", "content": "", "depth": 3}, {"id": "50", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/design-system-management", "name": "design-system-management", "type": "dir", "content": "", "depth": 3}, {"id": "51", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/accessibility-review", "name": "accessibility-review", "type": "dir", "content": "", "depth": 3}, {"id": "52", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/ux-writing", "name": "ux-writing", "type": "dir", "content": "", "depth": 3}, {"id": "53", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/user-research/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: user-research\ndescription: Plan, conduct, and synthesize user research. Trigger with \"user research plan\", \"interview guide\", \"usability test\", \"survey design\", \"research questions\", or when the user needs help with any aspect of understanding their users through research.\n---\n\n# User Research\n\nHelp plan, execute, and synthesize user research studies.\n\n## Research Methods\n\n| Method | Best For | Sample Size | Time |\n|--------|----------|-------------|------|\n| User interviews | Deep understanding of needs and motivations | 5-8 | 2-4 weeks |\n| Usability testing | Evaluating a specific design or flow | 5-8 | 1-2 weeks |\n| Surveys | Quantifying attitudes and preferences | 100+ | 1-2 weeks |\n| Card sorting | Information architecture decisions | 15-30 | 1 week |\n| Diary studies | Understanding behavior over time | 10-15 | 2-8 weeks |\n| A/B testing | Comparing specific design choices | Statistical significance | 1-4 weeks |\n\n## Interview Guide Structure\n\n1. **Warm-up** (5 min): Build rapport, explain the session\n2. **Context** (10 min): Understand their current workflow\n3. **Deep dive** (20 min): Explore the specific topic\n4. **Reaction** (10 min): Show concepts or prototypes\n5. **Wrap-up** (5 min): Anything we missed? Thank them.\n\n## Analysis Framework\n\n- **Affinity mapping**: Group observations into themes\n- **Impact/effort matrix**: Prioritize findings\n- **Journey mapping**: Visualize the user experience over time\n- **Jobs to be done**: Understand what users are hiring your product to do\n\n## Deliverables\n\n- Research plan (objectives, methods, timeline, participants)\n- Interview guide (questions, probes, activities)\n- Synthesis report (themes, insights, recommendations)\n- Highlight reel (key quotes and observations)\n", "depth": 4}, {"id": "54", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/design-handoff/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: design-handoff\ndescription: Create comprehensive developer handoff documentation from designs. Trigger with \"handoff to engineering\", \"developer specs\", \"implementation notes\", \"design specs for developers\", or when a design needs to be translated into detailed implementation guidance.\n---\n\n# Design Handoff\n\nCreate clear, complete handoff documentation so developers can implement designs accurately.\n\n## What to Include\n\n### Visual Specifications\n- Exact measurements (padding, margins, widths)\n- Design token references (colors, typography, spacing)\n- Responsive breakpoints and behavior\n- Component variants and states\n\n### Interaction Specifications\n- Click/tap behavior\n- Hover states\n- Transitions and animations (duration, easing)\n- Gesture support (swipe, pinch, long-press)\n\n### Content Specifications\n- Character limits\n- Truncation behavior\n- Empty states\n- Loading states\n- Error states\n\n### Edge Cases\n- Minimum/maximum content\n- International text (longer strings)\n- Slow connections\n- Missing data\n\n### Accessibility\n- Focus order\n- ARIA labels and roles\n- Keyboard interactions\n- Screen reader announcements\n\n## Principles\n\n1. **Don't assume** — If it's not specified, the developer will guess. Specify everything.\n2. **Use tokens, not values** — Reference `spacing-md` not `16px`.\n3. **Show all states** — Default, hover, active, disabled, loading, error, empty.\n4. **Describe the why** — \"This collapses on mobile because users primarily use one-handed\" helps developers make good judgment calls.\n", "depth": 4}, {"id": "55", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/design-critique/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: design-critique\ndescription: Evaluate designs for usability, visual hierarchy, consistency, and adherence to design principles. Trigger with \"what do you think of this design\", \"give me feedback on\", \"critique this\", \"review this mockup\", or when the user shares a design and asks for opinions.\n---\n\n# Design Critique\n\nProvide structured, actionable design feedback.\n\n## Critique Framework\n\n### 1. First Impression (2 seconds)\n- What draws the eye first? Is that correct?\n- What's the emotional reaction?\n- Is the purpose immediately clear?\n\n### 2. Usability\n- Can the user accomplish their goal?\n- Is the navigation intuitive?\n- Are interactive elements obvious?\n- Are there unnecessary steps?\n\n### 3. Visual Hierarchy\n- Is there a clear reading order?\n- Are the right elements emphasized?\n- Is whitespace used effectively?\n- Is typography creating the right hierarchy?\n\n### 4. Consistency\n- Does it follow the design system?\n- Are spacing, colors, and typography consistent?\n- Do similar elements behave similarly?\n\n### 5. Accessibility\n- Color contrast ratios\n- Touch target sizes\n- Text readability\n- Alternative text for images\n\n## How to Give Feedback\n\n- **Be specific**: \"The CTA competes with the navigation\" not \"the layout is confusing\"\n- **Explain why**: Connect feedback to design principles or user needs\n- **Suggest alternatives**: Don't just identify problems, propose solutions\n- **Acknowledge what works**: Good feedback includes positive observations\n- **Match the stage**: Early exploration gets different feedback than final polish\n", "depth": 4}, {"id": "56", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/design-system-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: design-system-management\ndescription: Manage design tokens, component libraries, and pattern documentation. Trigger with \"design system\", \"component library\", \"design tokens\", \"style guide\", or when the user asks about maintaining consistency across designs.\n---\n\n# Design System Management\n\nHelp build, maintain, and evolve design systems.\n\n## Components of a Design System\n\n### Design Tokens\nAtomic values that define the visual language:\n- Colors (brand, semantic, neutral)\n- Typography (scale, weights, line heights)\n- Spacing (scale, component padding)\n- Borders (radius, width)\n- Shadows (elevation levels)\n- Motion (durations, easings)\n\n### Components\nReusable UI elements with defined:\n- Variants (primary, secondary, ghost)\n- States (default, hover, active, disabled, loading, error)\n- Sizes (sm, md, lg)\n- Behavior (interactions, animations)\n- Accessibility (ARIA, keyboard)\n\n### Patterns\nCommon UI solutions combining components:\n- Forms (input groups, validation, submission)\n- Navigation (sidebar, tabs, breadcrumbs)\n- Data display (tables, cards, lists)\n- Feedback (toasts, modals, inline messages)\n\n## Principles\n\n1. **Consistency over creativity** — The system exists so teams don't reinvent the wheel\n2. **Flexibility within constraints** — Components should be composable, not rigid\n3. **Document everything** — If it's not documented, it doesn't exist\n4. **Version and migrate** — Breaking changes need migration paths\n", "depth": 4}, {"id": "57", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/accessibility-review/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: accessibility-review\ndescription: Audit designs and code for WCAG 2.1 AA compliance. Trigger with \"is this accessible\", \"accessibility check\", \"WCAG audit\", \"can screen readers use this\", \"color contrast\", or when the user asks about making designs or code accessible to all users.\n---\n\n# Accessibility Review\n\nEvaluate designs and implementations against WCAG 2.1 AA standards.\n\n## WCAG 2.1 AA Quick Reference\n\n### Perceivable\n- **1.1.1** Non-text content has alt text\n- **1.3.1** Info and structure conveyed semantically\n- **1.4.3** Contrast ratio \u003e= 4.5:1 (normal text), \u003e= 3:1 (large text)\n- **1.4.11** Non-text contrast \u003e= 3:1 (UI components, graphics)\n\n### Operable\n- **2.1.1** All functionality available via keyboard\n- **2.4.3** Logical focus order\n- **2.4.7** Visible focus indicator\n- **2.5.5** Touch target \u003e= 44x44 CSS pixels\n\n### Understandable\n- **3.2.1** Predictable on focus (no unexpected changes)\n- **3.3.1** Error identification (describe the error)\n- **3.3.2** Labels or instructions for inputs\n\n### Robust\n- **4.1.2** Name, role, value for all UI components\n\n## Common Issues\n\n1. Insufficient color contrast\n2. Missing form labels\n3. No keyboard access to interactive elements\n4. Missing alt text on meaningful images\n5. Focus traps in modals\n6. Missing ARIA landmarks\n7. Auto-playing media without controls\n8. Time limits without extension options\n\n## Testing Approach\n\n1. Automated scan (catches ~30% of issues)\n2. Keyboard-only navigation\n3. Screen reader testing (VoiceOver, NVDA)\n4. Color contrast verification\n5. Zoom to 200% — does layout break?\n", "depth": 4}, {"id": "58", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/design/skills/ux-writing/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: ux-writing\ndescription: Write effective microcopy for user interfaces. Trigger with \"write copy for\", \"help with UX copy\", \"what should this button say\", \"error message for\", \"empty state copy\", or when the user needs help with any interface text.\n---\n\n# UX Writing\n\nWrite clear, concise, and helpful interface copy.\n\n## Principles\n\n1. **Clear**: Say exactly what you mean. No jargon, no ambiguity.\n2. **Concise**: Use the fewest words that convey the full meaning.\n3. **Consistent**: Same terms for the same things everywhere.\n4. **Useful**: Every word should help the user accomplish their goal.\n5. **Human**: Write like a helpful person, not a robot.\n\n## Copy Patterns\n\n### CTAs\n- Start with a verb: \"Start free trial\", \"Save changes\", \"Download report\"\n- Be specific: \"Create account\" not \"Submit\"\n- Match the outcome to the label\n\n### Error Messages\nStructure: What happened + Why + How to fix\n- \"Payment declined. Your card was declined by your bank. Try a different card or contact your bank.\"\n\n### Empty States\nStructure: What this is + Why it's empty + How to start\n- \"No projects yet. Create your first project to start collaborating with your team.\"\n\n### Confirmation Dialogs\n- Make the action clear: \"Delete 3 files?\" not \"Are you sure?\"\n- Describe consequences: \"This can't be undone\"\n- Label buttons with the action: \"Delete files\" / \"Keep files\" not \"OK\" / \"Cancel\"\n\n## Voice and Tone\n\nAdapt tone to context:\n- **Success**: Celebratory but not over the top\n- **Error**: Empathetic and helpful\n- **Warning**: Clear and actionable\n- **Neutral**: Informative and concise\n", "depth": 4}, {"id": "59", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "60", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "61", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/README.md", "name": "README.md", "type": "file", "content": "# Enterprise Search\n\nAn enterprise search plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Search across all your company's tools in one place — email, chat, documents, and wikis — without switching between apps.\n\n---\n\n## How It Works\n\nOne query searches all your connected tools simultaneously. Claude decomposes your question, runs targeted searches across every source, and synthesizes the results into a single coherent answer with source attribution.\n\n```\nYou: \"What did we decide about the API redesign?\"\n              ↓ Claude searches\n~~chat: #engineering thread from Tuesday with the decision\n~~email: Follow-up email from Sarah with the spec\n~~cloud storage: Updated API design doc (modified yesterday)\n              ↓ Claude synthesizes\n\"The team decided on Tuesday to go with REST over GraphQL.\n Sarah sent the updated spec Thursday. The design doc\n reflects the final approach.\"\n```\n\nNo tab switching. No remembering which tool has what. Ask the question, get the answer.\n\n---\n\n## What It Searches\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect any combination of sources. The more you connect, the more complete your answers.\n\n| Source | What it finds |\n|--------|---------------|\n| **~~chat** | Messages, threads, channels, DMs |\n| **~~email** | Emails, attachments, conversations |\n| **~~cloud storage** | Docs, sheets, slides, PDFs |\n| **Wiki / Knowledge Base** | Internal documentation, runbooks |\n| **Project Management** | Tasks, issues, epics, milestones |\n| **CRM** | Accounts, contacts, opportunities |\n| **Ticketing** | Support tickets, customer issues |\n\nEach source is an MCP connection. Add more sources in your MCP settings to expand what Claude can search.\n\n---\n\n## Commands\n\n| Command | What it does |\n|---------|--------------|\n| `/search` | Search across all connected sources in one query |\n| `/digest` | Generate a daily or weekly digest of activity across all sources |\n\n### Search\n\n```\n/enterprise-search:search what's the status of Project Aurora?\n/enterprise-search:search from:sarah about:budget after:2025-01-01\n/enterprise-search:search decisions made in #product this week\n```\n\nSupports filters: `from:`, `in:`, `after:`, `before:`, `type:` — applied intelligently across each source's native query syntax.\n\n### Digest\n\n```\n/enterprise-search:digest --daily      # What happened today across all sources\n/enterprise-search:digest --weekly     # Weekly rollup grouped by project/topic\n```\n\nHighlights action items, decisions, and mentions of you. Groups activity by topic so you can skim what matters.\n\n---\n\n## Skills\n\nThree skills power the search experience:\n\n**Search Strategy** — Query decomposition and source-specific translation. Breaks your natural language question into targeted searches per source, handles ambiguity, and falls back gracefully when sources are unavailable.\n\n**Source Management** — Knows which MCP sources are available, guides you to connect new ones, manages source priority, and handles rate limits.\n\n**Knowledge Synthesis** — Combines results from multiple sources into coherent answers. Deduplicates cross-source information, attributes sources, scores confidence based on freshness and authority, and summarizes large result sets.\n\n---\n\n## Example Workflows\n\n### Finding a decision\n\n```\nYou: /enterprise-search:search when did we decide to switch to Postgres?\n\nClaude searches:\n  ~~chat → #engineering, #infrastructure for \"postgres\" \"switch\" \"decision\"\n  ~~email → threads with \"postgres\" in subject\n  ~~cloud storage → docs mentioning database migration\n\nResult: \"The decision was made March 3 in #infrastructure (link).\n         Sarah's email on March 4 confirmed the timeline.\n         The migration plan doc was updated March 5.\"\n```\n\n### Catching up after time off\n\n```\nYou: /enterprise-search:digest --weekly\n\nClaude scans:\n  ~~chat → channels you're in, DMs, mentions\n  ~~email → inbox activity\n  ~~cloud storage → docs shared with you or modified\n\nResult: Grouped summary by project with action items\n        flagged and decisions highlighted.\n```\n\n### Finding an expert\n\n```\nYou: /enterprise-search:search who knows about our Kubernetes setup?\n\nClaude searches:\n  ~~chat → messages about Kubernetes, k8s, clusters\n  ~~cloud storage → docs authored about infrastructure\n  Wiki → runbooks and architecture docs\n\nResult: \"Based on message history and doc authorship,\n         Alex and Priya are your go-to people for k8s.\n         Here's the main runbook (link).\"\n```\n\n---\n\n## Getting Started\n\n```bash\n# 1. Install\nclaude plugins add knowledge-work-plugins/enterprise-search\n\n# 2. Search across everything\n/enterprise-search:search [your question here]\n\n# 3. Get a digest\n/enterprise-search:digest --daily\n```\n\nThe more sources you connect via MCP, the more complete your search results. Start with ~~chat, ~~email, and ~~cloud storage, then add your wiki, project management tool, and CRM as needed.\n\n---\n\n## Philosophy\n\nKnowledge workers spend hours every week hunting for information scattered across tools. The answer exists somewhere — in a Slack thread, an email chain, a doc, a wiki page — but finding it means searching each tool individually, cross-referencing results, and hoping you checked the right place.\n\nEnterprise Search treats all your tools as one searchable knowledge base. One query, all sources, synthesized results. Your company's knowledge shouldn't be locked in silos. Search everything at once.\n", "depth": 2}, {"id": "62", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~chat` might mean Slack, Microsoft Teams, or any other chat tool with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (chat, email, cloud storage, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\nThis plugin uses `~~category` references extensively as source labels in search output (e.g. `~~chat:`, `~~email:`). These are intentional — they represent dynamic category markers that resolve to whatever tool is connected.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams, Discord |\n| Email | `~~email` | Microsoft 365 | — |\n| Cloud storage | `~~cloud storage` | Microsoft 365 | Dropbox |\n| Knowledge base | `~~knowledge base` | Notion, Guru | Confluence, Slite |\n| Project tracker | `~~project tracker` | Atlassian (Jira/Confluence), Asana | Linear, monday.com |\n| CRM | `~~CRM` | *(not pre-configured)* | Salesforce, HubSpot |\n| Office suite | `~~office suite` | Microsoft 365 | Google Workspace |\n", "depth": 2}, {"id": "63", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/commands/search.md", "name": "search.md", "type": "file", "content": "---\ndescription: Search across all connected sources in one query\nargument-hint: \"\u003cquery\u003e\"\n---\n\n# Search Command\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nSearch across all connected MCP sources in a single query. Decompose the user's question, run parallel searches, and synthesize results.\n\n## Instructions\n\n### 1. Check Available Sources\n\nBefore searching, determine which MCP sources are available. Attempt to identify connected tools from the available tool list. Common sources:\n\n- **~~chat** — chat platform tools\n- **~~email** — email tools\n- **~~cloud storage** — cloud storage tools\n- **~~project tracker** — project tracking tools\n- **~~CRM** — CRM tools\n- **~~knowledge base** — knowledge base tools\n\nIf no MCP sources are connected:\n```\nTo search across your tools, you'll need to connect at least one source.\nCheck your MCP settings to add ~~chat, ~~email, ~~cloud storage, or other tools.\n\nSupported sources: ~~chat, ~~email, ~~cloud storage, ~~project tracker, ~~CRM, ~~knowledge base,\nand any other MCP-connected service.\n```\n\n### 2. Parse the User's Query\n\nAnalyze the search query to understand:\n\n- **Intent**: What is the user looking for? (a decision, a document, a person, a status update, a conversation)\n- **Entities**: People, projects, teams, tools mentioned\n- **Time constraints**: Recency signals (\"this week\", \"last month\", specific dates)\n- **Source hints**: References to specific tools (\"in ~~chat\", \"that email\", \"the doc\")\n- **Filters**: Extract explicit filters from the query:\n  - `from:` — Filter by sender/author\n  - `in:` — Filter by channel, folder, or location\n  - `after:` — Only results after this date\n  - `before:` — Only results before this date\n  - `type:` — Filter by content type (message, email, doc, thread, file)\n\n### 3. Decompose into Sub-Queries\n\nFor each available source, create a targeted sub-query using that source's native search syntax:\n\n**~~chat:**\n- Use available search and read tools for your chat platform\n- Translate filters: `from:` maps to sender, `in:` maps to channel/room, dates map to time range filters\n- Use natural language queries for semantic search when appropriate\n- Use keyword queries for exact matches\n\n**~~email:**\n- Use available email search tools\n- Translate filters: `from:` maps to sender, dates map to time range filters\n- Map `type:` to attachment filters or subject-line searches as appropriate\n\n**~~cloud storage:**\n- Use available file search tools\n- Translate to file query syntax: name contains, full text contains, modified date, file type\n- Consider both file names and content\n\n**~~project tracker:**\n- Use available task search or typeahead tools\n- Map to task text search, assignee filters, date filters, project filters\n\n**~~CRM:**\n- Use available CRM query tools\n- Search across Account, Contact, Opportunity, and other relevant objects\n\n**~~knowledge base:**\n- Use semantic search for conceptual questions\n- Use keyword search for exact matches\n\n### 4. Execute Searches in Parallel\n\nRun all sub-queries simultaneously across available sources. Do not wait for one source before searching another.\n\nFor each source:\n- Execute the translated query\n- Capture results with metadata (timestamps, authors, links, source type)\n- Note any sources that fail or return errors — do not let one failure block others\n\n### 5. Rank and Deduplicate Results\n\n**Deduplication:**\n- Identify the same information appearing across sources (e.g., a decision discussed in ~~chat AND confirmed via email)\n- Group related results together rather than showing duplicates\n- Prefer the most authoritative or complete version\n\n**Ranking factors:**\n- **Relevance**: How well does the result match the query intent?\n- **Freshness**: More recent results rank higher for status/decision queries\n- **Authority**: Official docs \u003e wiki \u003e chat messages for factual questions; conversations \u003e docs for \"what did we discuss\" queries\n- **Completeness**: Results with more context rank higher\n\n### 6. Present Unified Results\n\nFormat the response as a synthesized answer, not a raw list of results:\n\n**For factual/decision queries:**\n```\n[Direct answer to the question]\n\nSources:\n- [Source 1: brief description] (~~chat, #channel, date)\n- [Source 2: brief description] (~~email, from person, date)\n- [Source 3: brief description] (~~cloud storage, doc name, last modified)\n```\n\n**For exploratory queries (\"what do we know about X\"):**\n```\n[Synthesized summary combining information from all sources]\n\nFound across:\n- ~~chat: X relevant messages in Y channels\n- ~~email: X relevant threads\n- ~~cloud storage: X related documents\n- [Other sources as applicable]\n\nKey sources:\n- [Most important source with link/reference]\n- [Second most important source]\n```\n\n**For \"find\" queries (looking for a specific thing):**\n```\n[The thing they're looking for, with direct reference]\n\nAlso found:\n- [Related items from other sources]\n```\n\n### 7. Handle Edge Cases\n\n**Ambiguous queries:**\nIf the query could mean multiple things, ask one clarifying question before searching:\n```\n\"API redesign\" could refer to a few things. Are you looking for:\n1. The REST API v2 redesign (Project Aurora)\n2. The internal SDK API changes\n3. Something else?\n```\n\n**No results:**\n```\nI couldn't find anything matching \"[query]\" across [list of sources searched].\n\nTry:\n- Broader terms (e.g., \"database\" instead of \"PostgreSQL migration\")\n- Different time range (currently searching [time range])\n- Checking if the relevant source is connected (currently searching: [sources])\n```\n\n**Partial results (some sources failed):**\n```\n[Results from successful sources]\n\nNote: I couldn't reach [failed source(s)] during this search.\nResults above are from [successful sources] only.\n```\n\n## Notes\n\n- Always search multiple sources in parallel — never sequentially\n- Synthesize results into answers, do not just list raw search results\n- Include source attribution so users can dig deeper\n- Respect the user's filter syntax and apply it appropriately per source\n- When a query mentions a specific person, search for their messages/docs/mentions across all sources\n- For time-sensitive queries, prioritize recency in ranking\n- If only one source is connected, still provide useful results from that source\n", "depth": 3}, {"id": "64", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/commands/digest.md", "name": "digest.md", "type": "file", "content": "---\ndescription: Generate a daily or weekly digest of activity across all connected sources\nargument-hint: \"[--daily | --weekly | --since \u003cdate\u003e]\"\n---\n\n# Digest Command\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nScan recent activity across all connected sources and generate a structured digest highlighting what matters.\n\n## Instructions\n\n### 1. Parse Flags\n\nDetermine the time window from the user's input:\n\n- `--daily` — Last 24 hours (default if no flag specified)\n- `--weekly` — Last 7 days\n\nThe user may also specify a custom range:\n- `--since yesterday`\n- `--since Monday`\n- `--since 2025-01-20`\n\n### 2. Check Available Sources\n\nIdentify which MCP sources are connected (same approach as the search command):\n\n- **~~chat** — channels, DMs, mentions\n- **~~email** — inbox, sent, threads\n- **~~cloud storage** — recently modified docs shared with user\n- **~~project tracker** — tasks assigned, completed, commented on\n- **~~CRM** — opportunity updates, account activity\n- **~~knowledge base** — recently updated wiki pages\n\nIf no sources are connected, guide the user:\n```\nTo generate a digest, you'll need at least one source connected.\nCheck your MCP settings to add ~~chat, ~~email, ~~cloud storage, or other tools.\n```\n\n### 3. Gather Activity from Each Source\n\n**~~chat:**\n- Search for messages mentioning the user (`to:me`)\n- Check channels the user is in for recent activity\n- Look for threads the user participated in\n- Identify new messages in key channels\n\n**~~email:**\n- Search recent inbox messages\n- Identify threads with new replies\n- Flag emails with action items or questions directed at the user\n\n**~~cloud storage:**\n- Find documents recently modified or shared with the user\n- Note new comments on docs the user owns or collaborates on\n\n**~~project tracker:**\n- Tasks assigned to the user (new or updated)\n- Tasks completed by others that the user follows\n- Comments on tasks the user is involved with\n\n**~~CRM:**\n- Opportunity stage changes\n- New activities logged on accounts the user owns\n- Updated contacts or accounts\n\n**~~knowledge base:**\n- Recently updated documents in relevant collections\n- New documents created in watched areas\n\n### 4. Identify Key Items\n\nFrom all gathered activity, extract and categorize:\n\n**Action Items:**\n- Direct requests made to the user (\"Can you...\", \"Please...\", \"@user\")\n- Tasks assigned or due soon\n- Questions awaiting the user's response\n- Review requests\n\n**Decisions:**\n- Conclusions reached in threads or emails\n- Approvals or rejections\n- Policy or direction changes\n\n**Mentions:**\n- Times the user was mentioned or referenced\n- Discussions about the user's projects or areas\n\n**Updates:**\n- Status changes on projects the user follows\n- Document updates in the user's domain\n- Completed items the user was waiting on\n\n### 5. Group by Topic\n\nOrganize the digest by topic, project, or theme rather than by source. Merge related activity across sources:\n\n```\n## Project Aurora\n- ~~chat: Design review thread concluded — team chose Option B (#design, Tuesday)\n- ~~email: Sarah sent updated spec incorporating feedback (Wednesday)\n- ~~cloud storage: \"Aurora API Spec v3\" updated by Sarah (Wednesday)\n- ~~project tracker: 3 tasks moved to In Progress, 2 completed\n\n## Budget Planning\n- ~~email: Finance team requesting Q2 projections by Friday\n- ~~chat: Todd shared template in #finance (Monday)\n- ~~cloud storage: \"Q2 Budget Template\" shared with you (Monday)\n```\n\n### 6. Format the Digest\n\nStructure the output clearly:\n\n```\n# [Daily/Weekly] Digest — [Date or Date Range]\n\nSources scanned: ~~chat, ~~email, ~~cloud storage, [others]\n\n## Action Items (X items)\n- [ ] [Action item 1] — from [person], [source] ([date])\n- [ ] [Action item 2] — from [person], [source] ([date])\n\n## Decisions Made\n- [Decision 1] — [context] ([source], [date])\n- [Decision 2] — [context] ([source], [date])\n\n## [Topic/Project Group 1]\n[Activity summary with source attribution]\n\n## [Topic/Project Group 2]\n[Activity summary with source attribution]\n\n## Mentions\n- [Mention context] — [source] ([date])\n\n## Documents Updated\n- [Doc name] — [who modified, what changed] ([date])\n```\n\n### 7. Handle Unavailable Sources\n\nIf any source fails or is unreachable:\n```\nNote: Could not reach [source name] for this digest.\nThe following sources were included: [list of successful sources].\n```\n\nDo not let one failed source prevent the digest from being generated. Produce the best digest possible from available sources.\n\n### 8. Summary Stats\n\nEnd with a quick summary:\n```\n---\n[X] action items · [Y] decisions · [Z] mentions · [W] doc updates\nAcross [N] sources · Covering [time range]\n```\n\n## Notes\n\n- Default to `--daily` if no flag is specified\n- Group by topic/project, not by source — users care about what happened, not where it happened\n- Action items should always be listed first — they are the most actionable part of a digest\n- Deduplicate cross-source activity (same decision in ~~chat and email = one entry)\n- For weekly digests, prioritize significance over completeness — highlight what matters, skip noise\n- If the user has a memory system (CLAUDE.md), use it to decode people names and project references\n- Include enough context in each item that the user can decide whether to dig deeper without clicking through\n", "depth": 3}, {"id": "65", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills/search-strategy", "name": "search-strategy", "type": "dir", "content": "", "depth": 3}, {"id": "66", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills/knowledge-synthesis", "name": "knowledge-synthesis", "type": "dir", "content": "", "depth": 3}, {"id": "67", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills/source-management", "name": "source-management", "type": "dir", "content": "", "depth": 3}, {"id": "68", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills/search-strategy/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: search-strategy\ndescription: Query decomposition and multi-source search orchestration. Breaks natural language questions into targeted searches per source, translates queries into source-specific syntax, ranks results by relevance, and handles ambiguity and fallback strategies.\n---\n\n# Search Strategy\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../../CONNECTORS.md).\n\nThe core intelligence behind enterprise search. Transforms a single natural language question into parallel, source-specific searches and produces ranked, deduplicated results.\n\n## The Goal\n\nTurn this:\n```\n\"What did we decide about the API migration timeline?\"\n```\n\nInto targeted searches across every connected source:\n```\n~~chat:  \"API migration timeline decision\" (semantic) + \"API migration\" in:#engineering after:2025-01-01\n~~knowledge base: semantic search \"API migration timeline decision\"\n~~project tracker:  text search \"API migration\" in relevant workspace\n```\n\nThen synthesize the results into a single coherent answer.\n\n## Query Decomposition\n\n### Step 1: Identify Query Type\n\nClassify the user's question to determine search strategy:\n\n| Query Type | Example | Strategy |\n|-----------|---------|----------|\n| **Decision** | \"What did we decide about X?\" | Prioritize conversations (~~chat, email), look for conclusion signals |\n| **Status** | \"What's the status of Project Y?\" | Prioritize recent activity, task trackers, status updates |\n| **Document** | \"Where's the spec for Z?\" | Prioritize Drive, wiki, shared docs |\n| **Person** | \"Who's working on X?\" | Search task assignments, message authors, doc collaborators |\n| **Factual** | \"What's our policy on X?\" | Prioritize wiki, official docs, then confirmatory conversations |\n| **Temporal** | \"When did X happen?\" | Search with broad date range, look for timestamps |\n| **Exploratory** | \"What do we know about X?\" | Broad search across all sources, synthesize |\n\n### Step 2: Extract Search Components\n\nFrom the query, extract:\n\n- **Keywords**: Core terms that must appear in results\n- **Entities**: People, projects, teams, tools (use memory system if available)\n- **Intent signals**: Decision words, status words, temporal markers\n- **Constraints**: Time ranges, source hints, author filters\n- **Negations**: Things to exclude\n\n### Step 3: Generate Sub-Queries Per Source\n\nFor each available source, create one or more targeted queries:\n\n**Prefer semantic search** for:\n- Conceptual questions (\"What do we think about...\")\n- Questions where exact keywords are unknown\n- Exploratory queries\n\n**Prefer keyword search** for:\n- Known terms, project names, acronyms\n- Exact phrases the user quoted\n- Filter-heavy queries (from:, in:, after:)\n\n**Generate multiple query variants** when the topic might be referred to differently:\n```\nUser: \"Kubernetes setup\"\nQueries: \"Kubernetes\", \"k8s\", \"cluster\", \"container orchestration\"\n```\n\n## Source-Specific Query Translation\n\n### ~~chat\n\n**Semantic search** (natural language questions):\n```\nquery: \"What is the status of project aurora?\"\n```\n\n**Keyword search:**\n```\nquery: \"project aurora status update\"\nquery: \"aurora in:#engineering after:2025-01-15\"\nquery: \"from:\u003c@UserID\u003e aurora\"\n```\n\n**Filter mapping:**\n| Enterprise filter | ~~chat syntax |\n|------------------|--------------|\n| `from:sarah` | `from:sarah` or `from:\u003c@USERID\u003e` |\n| `in:engineering` | `in:engineering` |\n| `after:2025-01-01` | `after:2025-01-01` |\n| `before:2025-02-01` | `before:2025-02-01` |\n| `type:thread` | `is:thread` |\n| `type:file` | `has:file` |\n\n### ~~knowledge base (Wiki)\n\n**Semantic search** — Use for conceptual queries:\n```\ndescriptive_query: \"API migration timeline and decision rationale\"\n```\n\n**Keyword search** — Use for exact terms:\n```\nquery: \"API migration\"\nquery: \"\\\"API migration timeline\\\"\"  (exact phrase)\n```\n\n### ~~project tracker\n\n**Task search:**\n```\ntext: \"API migration\"\nworkspace: [workspace_id]\ncompleted: false  (for status queries)\nassignee_any: \"me\"  (for \"my tasks\" queries)\n```\n\n**Filter mapping:**\n| Enterprise filter | ~~project tracker parameter |\n|------------------|----------------|\n| `from:sarah` | `assignee_any` or `created_by_any` |\n| `after:2025-01-01` | `modified_on_after: \"2025-01-01\"` |\n| `type:milestone` | `resource_subtype: \"milestone\"` |\n\n## Result Ranking\n\n### Relevance Scoring\n\nScore each result on these factors (weighted by query type):\n\n| Factor | Weight (Decision) | Weight (Status) | Weight (Document) | Weight (Factual) |\n|--------|-------------------|------------------|--------------------|-------------------|\n| Keyword match | 0.3 | 0.2 | 0.4 | 0.3 |\n| Freshness | 0.3 | 0.4 | 0.2 | 0.1 |\n| Authority | 0.2 | 0.1 | 0.3 | 0.4 |\n| Completeness | 0.2 | 0.3 | 0.1 | 0.2 |\n\n### Authority Hierarchy\n\nDepends on query type:\n\n**For factual/policy questions:**\n```\nWiki/Official docs \u003e Shared documents \u003e Email announcements \u003e Chat messages\n```\n\n**For \"what happened\" / decision questions:**\n```\nMeeting notes \u003e Thread conclusions \u003e Email confirmations \u003e Chat messages\n```\n\n**For status questions:**\n```\nTask tracker \u003e Recent chat \u003e Status docs \u003e Email updates\n```\n\n## Handling Ambiguity\n\nWhen a query is ambiguous, prefer asking one focused clarifying question over guessing:\n\n```\nAmbiguous: \"search for the migration\"\n→ \"I found references to a few migrations. Are you looking for:\n   1. The database migration (Project Phoenix)\n   2. The cloud migration (AWS → GCP)\n   3. The email migration (Exchange → O365)\"\n```\n\nOnly ask for clarification when:\n- There are genuinely distinct interpretations that would produce very different results\n- The ambiguity would significantly affect which sources to search\n\nDo NOT ask for clarification when:\n- The query is clear enough to produce useful results\n- Minor ambiguity can be resolved by returning results from multiple interpretations\n\n## Fallback Strategies\n\nWhen a source is unavailable or returns no results:\n\n1. **Source unavailable**: Skip it, search remaining sources, note the gap\n2. **No results from a source**: Try broader query terms, remove date filters, try alternate keywords\n3. **All sources return nothing**: Suggest query modifications to the user\n4. **Rate limited**: Note the limitation, return results from other sources, suggest retrying later\n\n### Query Broadening\n\nIf initial queries return too few results:\n```\nOriginal: \"PostgreSQL migration Q2 timeline decision\"\nBroader:  \"PostgreSQL migration\"\nBroader:  \"database migration\"\nBroadest: \"migration\"\n```\n\nRemove constraints in this order:\n1. Date filters (search all time)\n2. Source/location filters\n3. Less important keywords\n4. Keep only core entity/topic terms\n\n## Parallel Execution\n\nAlways execute searches across sources in parallel, never sequentially. The total search time should be roughly equal to the slowest single source, not the sum of all sources.\n\n```\n[User query]\n     ↓ decompose\n[~~chat query] [~~email query] [~~cloud storage query] [Wiki query] [~~project tracker query]\n     ↓            ↓            ↓              ↓            ↓\n  (parallel execution)\n     ↓\n[Merge + Rank + Deduplicate]\n     ↓\n[Synthesized answer]\n```\n", "depth": 4}, {"id": "69", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills/knowledge-synthesis/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: knowledge-synthesis\ndescription: Combines search results from multiple sources into coherent, deduplicated answers with source attribution. Handles confidence scoring based on freshness and authority, and summarizes large result sets effectively.\n---\n\n# Knowledge Synthesis\n\nThe last mile of enterprise search. Takes raw results from multiple sources and produces a coherent, trustworthy answer.\n\n## The Goal\n\nTransform this:\n```\n~~chat result: \"Sarah said in #eng: 'let's go with REST, GraphQL is overkill for our use case'\"\n~~email result: \"Subject: API Decision — Sarah's email confirming REST approach with rationale\"\n~~cloud storage result: \"API Design Doc v3 — updated section 2 to reflect REST decision\"\n~~project tracker result: \"Task: Finalize API approach — marked complete by Sarah\"\n```\n\nInto this:\n```\nThe team decided to go with REST over GraphQL for the API redesign. Sarah made the\ncall, noting that GraphQL was overkill for the current use case. This was discussed\nin #engineering on Tuesday, confirmed via email Wednesday, and the design doc has\nbeen updated to reflect the decision. The related ~~project tracker task is marked complete.\n\nSources:\n- ~~chat: #engineering thread (Jan 14)\n- ~~email: \"API Decision\" from Sarah (Jan 15)\n- ~~cloud storage: \"API Design Doc v3\" (updated Jan 15)\n- ~~project tracker: \"Finalize API approach\" (completed Jan 15)\n```\n\n## Deduplication\n\n### Cross-Source Deduplication\n\nThe same information often appears in multiple places. Identify and merge duplicates:\n\n**Signals that results are about the same thing:**\n- Same or very similar text content\n- Same author/sender\n- Timestamps within a short window (same day or adjacent days)\n- References to the same entity (project name, document, decision)\n- One source references another (\"as discussed in ~~chat\", \"per the email\", \"see the doc\")\n\n**How to merge:**\n- Combine into a single narrative item\n- Cite all sources where it appeared\n- Use the most complete version as the primary text\n- Add unique details from each source\n\n### Deduplication Priority\n\nWhen the same information exists in multiple sources, prefer:\n```\n1. The most complete version (fullest context)\n2. The most authoritative source (official doc \u003e chat)\n3. The most recent version (latest update wins for evolving info)\n```\n\n### What NOT to Deduplicate\n\nKeep as separate items when:\n- The same topic is discussed but with different conclusions\n- Different people express different viewpoints\n- The information evolved meaningfully between sources (v1 vs v2 of a decision)\n- Different time periods are represented\n\n## Citation and Source Attribution\n\nEvery claim in the synthesized answer must be attributable to a source.\n\n### Attribution Format\n\nInline for direct references:\n```\nSarah confirmed the REST approach in her email on Wednesday.\nThe design doc was updated to reflect this (~~cloud storage: \"API Design Doc v3\").\n```\n\nSource list at the end for completeness:\n```\nSources:\n- ~~chat: #engineering discussion (Jan 14) — initial decision thread\n- ~~email: \"API Decision\" from Sarah Chen (Jan 15) — formal confirmation\n- ~~cloud storage: \"API Design Doc v3\" last modified Jan 15 — updated specification\n```\n\n### Attribution Rules\n\n- Always name the source type (~~chat, ~~email, ~~cloud storage, etc.)\n- Include the specific location (channel, folder, thread)\n- Include the date or relative time\n- Include the author when relevant\n- Include document/thread titles when available\n- For ~~chat, note the channel name\n- For ~~email, note the subject line and sender\n- For ~~cloud storage, note the document title\n\n## Confidence Levels\n\nNot all results are equally trustworthy. Assess confidence based on:\n\n### Freshness\n\n| Recency | Confidence impact |\n|---------|------------------|\n| Today / yesterday | High confidence for current state |\n| This week | Good confidence |\n| This month | Moderate — things may have changed |\n| Older than a month | Lower confidence — flag as potentially outdated |\n\nFor status queries, heavily weight freshness. For policy/factual queries, freshness matters less.\n\n### Authority\n\n| Source type | Authority level |\n|-------------|----------------|\n| Official wiki / knowledge base | Highest — curated, maintained |\n| Shared documents (final versions) | High — intentionally published |\n| Email announcements | High — formal communication |\n| Meeting notes | Moderate-high — may be incomplete |\n| Chat messages (thread conclusions) | Moderate — informal but real-time |\n| Chat messages (mid-thread) | Lower — may not reflect final position |\n| Draft documents | Low — not finalized |\n| Task comments | Contextual — depends on commenter |\n\n### Expressing Confidence\n\nWhen confidence is high (multiple fresh, authoritative sources agree):\n```\nThe team decided to use REST for the API redesign. [direct statement]\n```\n\nWhen confidence is moderate (single source or somewhat dated):\n```\nBased on the discussion in #engineering last month, the team was leaning\ntoward REST for the API redesign. This may have evolved since then.\n```\n\nWhen confidence is low (old data, informal source, or conflicting signals):\n```\nI found a reference to an API migration discussion from three months ago\nin ~~chat, but I couldn't find a formal decision document. The information\nmay be outdated. You might want to check with the team for current status.\n```\n\n### Conflicting Information\n\nWhen sources disagree:\n```\nI found conflicting information about the API approach:\n- The ~~chat discussion on Jan 10 suggested GraphQL\n- But Sarah's email on Jan 15 confirmed REST\n- The design doc (updated Jan 15) reflects REST\n\nThe most recent sources indicate REST was the final decision,\nbut the earlier ~~chat discussion explored GraphQL first.\n```\n\nAlways surface conflicts rather than silently picking one version.\n\n## Summarization Strategies\n\n### For Small Result Sets (1-5 results)\n\nPresent each result with context. No summarization needed — give the user everything:\n```\n[Direct answer synthesized from results]\n\n[Detail from source 1]\n[Detail from source 2]\n\nSources: [full attribution]\n```\n\n### For Medium Result Sets (5-15 results)\n\nGroup by theme and summarize each group:\n```\n[Overall answer]\n\nTheme 1: [summary of related results]\nTheme 2: [summary of related results]\n\nKey sources: [top 3-5 most relevant sources]\nFull results: [count] items found across [sources]\n```\n\n### For Large Result Sets (15+ results)\n\nProvide a high-level synthesis with the option to drill down:\n```\n[Overall answer based on most relevant results]\n\nSummary:\n- [Key finding 1] (supported by N sources)\n- [Key finding 2] (supported by N sources)\n- [Key finding 3] (supported by N sources)\n\nTop sources:\n- [Most authoritative/relevant source]\n- [Second most relevant]\n- [Third most relevant]\n\nFound [total count] results across [source list].\nWant me to dig deeper into any specific aspect?\n```\n\n### Summarization Rules\n\n- Lead with the answer, not the search process\n- Do not list raw results — synthesize them into narrative\n- Group related items from different sources together\n- Preserve important nuance and caveats\n- Include enough detail that the user can decide whether to dig deeper\n- Always offer to provide more detail if the result set was large\n\n## Synthesis Workflow\n\n```\n[Raw results from all sources]\n          ↓\n[1. Deduplicate — merge same info from different sources]\n          ↓\n[2. Cluster — group related results by theme/topic]\n          ↓\n[3. Rank — order clusters and items by relevance to query]\n          ↓\n[4. Assess confidence — freshness × authority × agreement]\n          ↓\n[5. Synthesize — produce narrative answer with attribution]\n          ↓\n[6. Format — choose appropriate detail level for result count]\n          ↓\n[Coherent answer with sources]\n```\n\n## Anti-Patterns\n\n**Do not:**\n- List results source by source (\"From ~~chat: ... From ~~email: ... From ~~cloud storage: ...\")\n- Include irrelevant results just because they matched a keyword\n- Bury the answer under methodology explanation\n- Present conflicting info without flagging the conflict\n- Omit source attribution\n- Present uncertain information with the same confidence as well-supported facts\n- Summarize so aggressively that useful detail is lost\n\n**Do:**\n- Lead with the answer\n- Group by topic, not by source\n- Flag confidence levels when appropriate\n- Surface conflicts explicitly\n- Attribute all claims to sources\n- Offer to go deeper when result sets are large\n", "depth": 4}, {"id": "70", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/enterprise-search/skills/source-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: source-management\ndescription: Manages connected MCP sources for enterprise search. Detects available sources, guides users to connect new ones, handles source priority ordering, and manages rate limiting awareness.\n---\n\n# Source Management\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../../CONNECTORS.md).\n\nKnows what sources are available, helps connect new ones, and manages how sources are queried.\n\n## Checking Available Sources\n\nDetermine which MCP sources are connected by checking available tools. Each source corresponds to a set of MCP tools:\n\n| Source | Key capabilities |\n|--------|-----------------|\n| **~~chat** | Search messages, read channels and threads |\n| **~~email** | Search messages, read individual emails |\n| **~~cloud storage** | Search files, fetch document contents |\n| **~~project tracker** | Search tasks, typeahead search |\n| **~~CRM** | Query records (accounts, contacts, opportunities) |\n| **~~knowledge base** | Semantic search, keyword search |\n\nIf a tool prefix is available, the source is connected and searchable.\n\n## Guiding Users to Connect Sources\n\nWhen a user searches but has few or no sources connected:\n\n```\nYou currently have [N] source(s) connected: [list].\n\nTo expand your search, you can connect additional sources in your MCP settings:\n- ~~chat — messages, threads, channels\n- ~~email — emails, conversations, attachments\n- ~~cloud storage — docs, sheets, slides\n- ~~project tracker — tasks, projects, milestones\n- ~~CRM — accounts, contacts, opportunities\n- ~~knowledge base — wiki pages, knowledge base articles\n\nThe more sources you connect, the more complete your search results.\n```\n\nWhen a user asks about a specific tool that is not connected:\n\n```\n[Tool name] isn't currently connected. To add it:\n1. Open your MCP settings\n2. Add the [tool] MCP server configuration\n3. Authenticate when prompted\n\nOnce connected, it will be automatically included in future searches.\n```\n\n## Source Priority Ordering\n\nDifferent query types benefit from searching certain sources first. Use these priorities to weight results, not to skip sources:\n\n### By Query Type\n\n**Decision queries** (\"What did we decide...\"):\n```\n1. ~~chat (conversations where decisions happen)\n2. ~~email (decision confirmations, announcements)\n3. ~~cloud storage (meeting notes, decision logs)\n4. Wiki (if decisions are documented)\n5. Task tracker (if decisions are captured in tasks)\n```\n\n**Status queries** (\"What's the status of...\"):\n```\n1. Task tracker (~~project tracker — authoritative status)\n2. ~~chat (real-time discussion)\n3. ~~cloud storage (status docs, reports)\n4. ~~email (status update emails)\n5. Wiki (project pages)\n```\n\n**Document queries** (\"Where's the doc for...\"):\n```\n1. ~~cloud storage (primary doc storage)\n2. Wiki / ~~knowledge base (knowledge base)\n3. ~~email (docs shared via email)\n4. ~~chat (docs shared in channels)\n5. Task tracker (docs linked to tasks)\n```\n\n**People queries** (\"Who works on...\" / \"Who knows about...\"):\n```\n1. ~~chat (message authors, channel members)\n2. Task tracker (task assignees)\n3. ~~cloud storage (doc authors, collaborators)\n4. ~~CRM (account owners, contacts)\n5. ~~email (email participants)\n```\n\n**Factual/Policy queries** (\"What's our policy on...\"):\n```\n1. Wiki / ~~knowledge base (official documentation)\n2. ~~cloud storage (policy docs, handbooks)\n3. ~~email (policy announcements)\n4. ~~chat (policy discussions)\n```\n\n### Default Priority (General Queries)\n\nWhen query type is unclear:\n```\n1. ~~chat (highest volume, most real-time)\n2. ~~email (formal communications)\n3. ~~cloud storage (documents and files)\n4. Wiki / ~~knowledge base (structured knowledge)\n5. Task tracker (work items)\n6. CRM (customer data)\n```\n\n## Rate Limiting Awareness\n\nMCP sources may have rate limits. Handle them gracefully:\n\n### Detection\n\nRate limit responses typically appear as:\n- HTTP 429 responses\n- Error messages mentioning \"rate limit\", \"too many requests\", or \"quota exceeded\"\n- Throttled or delayed responses\n\n### Handling\n\nWhen a source is rate limited:\n\n1. **Do not retry immediately** — respect the limit\n2. **Continue with other sources** — do not block the entire search\n3. **Inform the user**:\n```\nNote: [Source] is temporarily rate limited. Results below are from\n[other sources]. You can retry in a few minutes to include [source].\n```\n4. **For digests** — if rate limited mid-scan, note which time range was covered before the limit hit\n\n### Prevention\n\n- Avoid unnecessary API calls — check if the source is likely to have relevant results before querying\n- Use targeted queries over broad scans when possible\n- For digests, batch requests where the API supports it\n- Cache awareness: if a search was just run, avoid re-running the same query immediately\n\n## Source Health\n\nTrack source availability during a session:\n\n```\nSource Status:\n  ~~chat:        ✓ Available\n  ~~email:        ✓ Available\n  ~~cloud storage:  ✓ Available\n  ~~project tracker:        ✗ Not connected\n  ~~CRM:   ✗ Not connected\n  ~~knowledge base:      ⚠ Rate limited (retry in 2 min)\n```\n\nWhen reporting search results, include which sources were searched so the user knows the scope of the answer.\n\n## Adding Custom Sources\n\nThe enterprise search plugin works with any MCP-connected source. As new MCP servers become available, they can be added to the `.mcp.json` configuration. The search and digest commands will automatically detect and include new sources based on available tools.\n\nTo add a new source:\n1. Add the MCP server configuration to `.mcp.json`\n2. Authenticate if required\n3. The source will be included in subsequent searches automatically\n", "depth": 4}, {"id": "71", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "72", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer", "name": "cowork-plugin-customizer", "type": "dir", "content": "", "depth": 3}, {"id": "73", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/create-cowork-plugin", "name": "create-cowork-plugin", "type": "dir", "content": "", "depth": 3}, {"id": "74", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "75", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer/examples", "name": "examples", "type": "dir", "content": "", "depth": 4}, {"id": "76", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: cowork-plugin-customizer\ndescription: \u003e\n  Customize a Claude Code plugin for a specific organization's tools and workflows.\n  Use when: customize plugin, set up plugin, configure plugin, tailor plugin, adjust plugin settings,\n  customize plugin connectors, customize plugin skill, customize plugin command, tweak plugin, modify plugin configuration.\ncompatibility: Requires Cowork desktop app environment with access to mounted plugin directories (mnt/.local-plugins, mnt/.plugins).\n---\n\n# Cowork Plugin Customization\n\nCustomize a plugin for a specific organization — either by setting up a generic plugin template for the first time, or by tweaking and refining an already-configured plugin.\n\n\u003e **Finding the plugin**: To find the plugin's source files, run `find mnt/.local-plugins mnt/.plugins -type d -name \"*\u003cplugin-name\u003e*\"` to locate the plugin directory, then read its files to understand its structure before making changes. If you cannot find the plugin directory, the user is likely running this conversation in a remote container. Abort and let them know: \"Customizing plugins is currently only available in the desktop app's Cowork mode.\"\n\n## Determining the Customization Mode\n\nAfter locating the plugin, check for `~~`-prefixed placeholders: `grep -rn '~~\\w' /path/to/plugin --include='*.md' --include='*.json'`\n\n\u003e **Default rule**: If `~~` placeholders exist, default to **Generic plugin setup** unless the user explicitly asks to customize a specific part of the plugin.\n\n**1. Generic plugin setup** — The plugin contains `~~`-prefixed placeholders. These are customization points in a template that need to be replaced with real values (e.g., `~~Jira` → `Asana`, `~~your-team-channel` → `#engineering`).\n\n**2. Scoped customization** — No `~~` placeholders exist, and the user asked to customize a specific part of the plugin (e.g., \"customize the connectors\", \"update the standup command\", \"change the ticket tool\"). Read the plugin files to find the relevant section(s) and focus only on those. Do not scan the entire plugin or present unrelated customization items.\n\n**3. General customization** — No `~~` placeholders exist, and the user wants to modify the plugin broadly. Read the plugin's files to understand its current configuration, then ask the user what they'd like to change.\n\n\u003e **Important**: Never change the name of the plugin or skill being customized. Do not rename directories, files, or the plugin/skill name fields.\n\n\u003e **Nontechnical output**: All user-facing output (todo list items, questions, summaries) must be written in plain, nontechnical language. Never mention `~~` prefixes, placeholders, or customization points to the user. Frame everything in terms of the plugin's capabilities and the organization's tools.\n\n## Customization Workflow\n\n### Phase 0: Gather User Intent (scoped and general customization only)\n\nFor **scoped customization** and **general customization** (not generic plugin setup), check whether the user provided free-form context alongside their request (e.g., \"customize the standup command — we do async standups in #eng-updates every morning\").\n\n- **If the user provided context**: Record it and use it to pre-fill answers in Phase 3 — skip asking questions that the user already answered here.\n- **If the user did not provide context**: Ask a single open-ended question using AskUserQuestion before proceeding. Tailor the question to what they asked to customize — e.g., \"What changes do you have in mind for the brief command?\" or \"What would you like to change about how this plugin works?\" Keep it short and specific to their request.\n\n  Use their response (if any) as additional context throughout the remaining phases.\n\n### Phase 1: Gather Context from Knowledge MCPs\n\nUse company-internal knowledge MCPs to collect information relevant to the customization scope. See `references/search-strategies.md` for detailed query patterns by category.\n\n**What to gather** (scope to what's relevant):\n- Tool names and services the organization uses\n- Organizational processes and workflows\n- Team conventions (naming, statuses, estimation scales)\n- Configuration values (workspace IDs, project names, team identifiers)\n\n**Sources to search:**\n1. **Chat/Slack MCPs** — tool mentions, integrations, workflow discussions\n2. **Document MCPs** — onboarding docs, tool guides, setup instructions\n3. **Email MCPs** — license notifications, admin emails, setup invitations\n\nRecord all findings for use in Phase 3.\n\n### Phase 2: Create Todo List\n\nBuild a todo list of changes to make, scoped appropriately:\n\n- **For scoped customization**: Only include items related to the specific section the user asked about.\n- **For generic plugin setup**: Run `grep -rn '~~\\w' /path/to/plugin --include='*.md' --include='*.json'` to find all placeholder customization points. Group them by theme.\n- **For general customization**: Read the plugin files, understand the current config, and based on the user's request, identify what needs to change.\n\nUse user-friendly descriptions that focus on the plugin's purpose:\n\n- **Good**: \"Learn how standup prep works at Company\"\n- **Bad**: \"Replace placeholders in commands/standup-prep.md\"\n\n### Phase 3: Complete Todo Items\n\nWork through each item using context from Phase 0 and Phase 1.\n\n**If the user's free-form input (Phase 0) or knowledge MCPs (Phase 1) provided a clear answer**: Apply directly without confirmation.\n\n**Otherwise**: Use AskUserQuestion. Don't assume \"industry standard\" defaults are correct — if neither the user's input nor knowledge MCPs provided a specific answer, ask. Note: AskUserQuestion always includes a Skip button and a free-text input box for custom answers, so do not include `None` or `Other` as options.\n\n**Types of changes:**\n\n1. **Placeholder replacements** (generic setup): `~~Jira` → `Asana`, `~~your-org-channel` → `#engineering`\n2. **Content updates**: Modifying instructions, commands, workflows, or references to match the organization\n3. **URL pattern updates**: `tickets.example.com/your-team/123` → `app.asana.com/0/PROJECT_ID/TASK_ID`\n4. **Configuration values**: Workspace IDs, project names, team identifiers\n\nIf user doesn't know or skips, leave the value unchanged (or the `~~`-prefixed placeholder, for generic setup).\n\n### Phase 4: Search for Useful MCPs\n\nAfter customization items have been resolved, connect MCPs for any tools that were identified or changed. See `references/mcp-servers.md` for the full workflow, category-to-keywords mapping, and config file format.\n\nFor each tool identified during customization:\n1. Search the registry: `search_mcp_registry(keywords=[...])` using category keywords from `references/mcp-servers.md`, or search for the specific tool name if already known\n2. If unconnected: `suggest_connectors(directoryUuids=[\"chosen-uuid\"])` — user completes auth\n3. Update the plugin's MCP config file (check `plugin.json` for custom location, otherwise `.mcp.json` at root)\n\nCollect all MCP results and present them together in the summary output (see below) — don't present MCPs one at a time during this phase.\n\n## Packaging the Plugin\n\nAfter all customizations are applied, package the plugin as a `.plugin` file for the user:\n\n1. **Zip the plugin directory** (excluding `setup/` since it's no longer needed):\n   ```bash\n   cd /path/to/plugin \u0026\u0026 zip -r /tmp/plugin-name.plugin . -x \"setup/*\" \u0026\u0026 cp /tmp/plugin-name.plugin /path/to/outputs/plugin-name.plugin\n   ```\n2. **Present the file to the user** with the `.plugin` extension so they can install it directly. (Presenting the .plugin file will show to the user as a rich preview where they can look through the plugin files, and they can accept the customization by pressing a button.)\n\n\u003e **Important**: Always create the zip in `/tmp/` first, then copy to the outputs folder. Writing directly to the outputs folder may fail due to permissions and leave behind temporary files.\n\n\u003e **Naming**: Use the original plugin directory name for the `.plugin` file (e.g., if the plugin directory is `coder`, the output file should be `coder.plugin`). Do not rename the plugin or its files during customization — only replace placeholder values and update content.\n\n## Summary Output\n\nAfter customization, present the user with a summary of what was learned grouped by source. Always include the MCPs sections showing which MCPs were connected during setup and which ones the user should still connect:\n\n```markdown\n## From searching Slack\n- You use Asana for project management\n- Sprint cycles are 2 weeks\n\n## From searching documents\n- Story points use T-shirt sizes\n\n## From your answers\n- Ticket statuses are: Backlog, In Progress, In Review, Done\n```\n\nThen present the MCPs that were connected during setup and any that the user should still connect, with instructions on how to connect them.\n\nIf no knowledge MCPs were available in Phase 1, and the user had to answer at least one question manually, include a note at the end:\n\u003e By the way, connecting sources like Slack or Microsoft Teams would let me find answers automatically next time you customize a plugin.\n\n## Additional Resources\n\n- **`references/mcp-servers.md`** — MCP discovery workflow, category-to-keywords mapping, config file locations\n- **`references/search-strategies.md`** — Knowledge MCP query patterns for finding tool names and org values\n- **`examples/customized-mcp.json`** — Example fully configured `.mcp.json`\n", "depth": 4}, {"id": "77", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer/LICENSE.txt", "name": "LICENSE.txt", "type": "file", "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.", "depth": 4}, {"id": "78", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer/references/mcp-servers.md", "name": "mcp-servers.md", "type": "file", "content": "# MCP Discovery and Connection\n\nHow to find and connect MCPs during plugin customization.\n\n## Available Tools\n\n### `search_mcp_registry`\nSearch the MCP directory for available connectors.\n\n**Input:** `{ \"keywords\": [\"array\", \"of\", \"search\", \"terms\"] }`\n\n**Output:** Up to 10 results, each with:\n- `name`: MCP display name\n- `description`: One-liner description\n- `tools`: List of tool names the MCP provides\n- `url`: MCP endpoint URL (use this in `.mcp.json`)\n- `directoryUuid`: UUID for use with suggest_connectors\n- `connected`: Boolean - whether user has this MCP connected\n\n### `suggest_connectors`\nDisplay Connect buttons to let users install/connect MCPs.\n\n**Input:** `{ \"directoryUuids\": [\"uuid1\", \"uuid2\"] }`\n\n**Output:** Renders UI with Connect buttons for each MCP\n\n## Category-to-Keywords Mapping\n\n| Category | Search Keywords |\n|----------|-----------------|\n| `project-management` | `[\"asana\", \"jira\", \"linear\", \"monday\", \"tasks\"]` |\n| `software-coding` | `[\"github\", \"gitlab\", \"bitbucket\", \"code\"]` |\n| `chat` | `[\"slack\", \"teams\", \"discord\"]` |\n| `documents` | `[\"google docs\", \"notion\", \"confluence\"]` |\n| `calendar` | `[\"google calendar\", \"calendar\"]` |\n| `email` | `[\"gmail\", \"outlook\", \"email\"]` |\n| `design-graphics` | `[\"figma\", \"sketch\", \"design\"]` |\n| `analytics-bi` | `[\"datadog\", \"grafana\", \"analytics\"]` |\n| `crm` | `[\"salesforce\", \"hubspot\", \"crm\"]` |\n| `wiki-knowledge-base` | `[\"notion\", \"confluence\", \"outline\", \"wiki\"]` |\n| `data-warehouse` | `[\"bigquery\", \"snowflake\", \"redshift\"]` |\n| `conversation-intelligence` | `[\"gong\", \"chorus\", \"call recording\"]` |\n\n## Workflow\n\n1. **Find customization point**: Look for `~~`-prefixed values (e.g., `~~Jira`)\n2. **Check earlier phase findings**: Did you already learn which tool they use?\n   - **Yes**: Search for that specific tool to get its `url`, skip to step 5\n   - **No**: Continue to step 3\n3. **Search**: Call `search_mcp_registry` with mapped keywords\n4. **Present choices and ask user**: Show all results, ask which they use\n5. **Connect if needed**: If not connected, call `suggest_connectors`\n6. **Update MCP config**: Add config using the `url` from search results\n\n## Updating Plugin MCP Configuration\n\n### Finding the Config File\n\n1. **Check `plugin.json`** for an `mcpServers` field:\n   ```json\n   {\n     \"name\": \"my-plugin\",\n     \"mcpServers\": \"./config/servers.json\"\n   }\n   ```\n   If present, edit the file at that path.\n\n2. **If no `mcpServers` field**, use `.mcp.json` at the plugin root (default).\n\n3. **If `mcpServers` points only to `.mcpb` files** (bundled servers), create a new `.mcp.json` at the plugin root.\n\n### Config File Format\n\nBoth wrapped and unwrapped formats are supported:\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n```\n\nUse the `url` field from `search_mcp_registry` results.\n\n### Directory Entries Without a URL\n\nSome directory entries have no `url` because the endpoint is dynamic — the admin provides it when connecting the server. These servers can still be referenced in the plugin's MCP config by **name**: if the MCP server name in the config matches the directory entry name, it is treated the same as a URL match.", "depth": 5}, {"id": "79", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/cowork-plugin-customizer/references/search-strategies.md", "name": "search-strategies.md", "type": "file", "content": "# Knowledge MCP Search Strategies\n\nQuery patterns for gathering organizational context during plugin customization.\n\n## Finding Tool Names\n\n**Source control:**\n- Search: \"GitHub\" OR \"GitLab\" OR \"Bitbucket\"\n- Search: \"pull request\" OR \"merge request\"\n- Look for: repository links, CI/CD mentions\n\n**Project management:**\n- Search: \"Asana\" OR \"Jira\" OR \"Linear\" OR \"Monday\"\n- Search: \"sprint\" AND \"tickets\"\n- Look for: task links, project board mentions\n\n**Chat:**\n- Search: \"Slack\" OR \"Teams\" OR \"Discord\"\n- Look for: channel mentions, integration discussions\n\n**Analytics:**\n- Search: \"Datadog\" OR \"Grafana\" OR \"Mixpanel\"\n- Search: \"monitoring\" OR \"observability\"\n- Look for: dashboard links, alert configurations\n\n**Design:**\n- Search: \"Figma\" OR \"Sketch\" OR \"Adobe XD\"\n- Look for: design file links, handoff discussions\n\n**CRM:**\n- Search: \"Salesforce\" OR \"HubSpot\"\n- Look for: deal mentions, customer record links\n\n## Finding Organization Values\n\n**Workspace/project IDs:**\n- Search for existing integrations or bookmarked links\n- Look for admin/setup documentation\n\n**Team conventions:**\n- Search: \"story points\" OR \"estimation\"\n- Search: \"workflow\" OR \"ticket status\"\n- Look for engineering process docs\n\n**Channel/team names:**\n- Search: \"standup\" OR \"engineering\" OR \"releases\"\n- Look for channel naming patterns\n\n## When Knowledge MCPs Are Unavailable\n\nIf no knowledge MCPs are configured, skip automatic discovery and proceed directly to AskUserQuestion for all categories. Note: AskUserQuestion always includes a Skip button and a free-text input box for custom answers, so do not include `None` or `Other` as options.\n", "depth": 5}, {"id": "80", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/create-cowork-plugin/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "81", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/create-cowork-plugin/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: create-cowork-plugin\ndescription: \u003e\n  Guide users through creating a new plugin from scratch in a cowork session.\n  Use when users want to create a plugin, build a plugin, make a new plugin, develop a plugin, scaffold a plugin, start a plugin from scratch, or design a plugin.\n  This skill requires Cowork mode with access to the outputs directory for delivering the final .plugin file.\ncompatibility: Requires Cowork desktop app environment with access to the outputs directory for delivering .plugin files.\n---\n\n# Create Cowork Plugin\n\nBuild a new plugin from scratch through guided conversation. Walk the user through discovery, planning, design, implementation, and packaging — delivering a ready-to-install `.plugin` file at the end.\n\n## Overview\n\nA plugin is a self-contained directory that extends Claude's capabilities with commands, skills, agents, hooks, and MCP server integrations. This skill encodes the full plugin architecture and a five-phase workflow for creating one conversationally.\n\nThe process:\n1. **Discovery** — understand what the user wants to build\n2. **Component Planning** — determine which component types are needed\n3. **Design \u0026 Clarifying Questions** — specify each component in detail\n4. **Implementation** — create all plugin files\n5. **Review \u0026 Package** — deliver the `.plugin` file\n\n\u003e **Nontechnical output**: Keep all user-facing conversation in plain language. Do not expose implementation details like file paths, directory structures, or schema fields unless the user asks. Frame everything in terms of what the plugin will do.\n\n## Plugin Architecture\n\n### Directory Structure\n\nEvery plugin follows this layout:\n\n```\nplugin-name/\n├── .claude-plugin/\n│   └── plugin.json           # Required: plugin manifest\n├── commands/                 # Slash commands (.md files)\n├── agents/                   # Subagent definitions (.md files)\n├── skills/                   # Skills (subdirectories with SKILL.md)\n│   └── skill-name/\n│       ├── SKILL.md\n│       └── references/\n├── .mcp.json                 # MCP server definitions\n└── README.md                 # Plugin documentation\n```\n\n\n**Rules:**\n- `.claude-plugin/plugin.json` is always required\n- Component directories (`commands/`, `agents/`, `skills/`) go at the plugin root, not inside `.claude-plugin/`\n- Only create directories for components the plugin actually uses\n- Use kebab-case for all directory and file names\n\n### plugin.json Manifest\n\nLocated at `.claude-plugin/plugin.json`. Minimal required field is `name`.\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Brief explanation of plugin purpose\",\n  \"author\": {\n    \"name\": \"Author Name\"\n  }\n}\n```\n\n**Name rules:** kebab-case, lowercase with hyphens, no spaces or special characters.\n**Version:** semver format (MAJOR.MINOR.PATCH). Start at `0.1.0`.\n\nOptional fields: `homepage`, `repository`, `license`, `keywords`.\n\nCustom component paths can be specified (supplements, does not replace, auto-discovery):\n```json\n{\n  \"commands\": \"./custom-commands\",\n  \"agents\": [\"./agents\", \"./specialized-agents\"],\n  \"hooks\": \"./config/hooks.json\",\n  \"mcpServers\": \"./.mcp.json\"\n}\n```\n\n### Component Schemas\n\nDetailed schemas for each component type are in `references/component-schemas.md`. Summary:\n\n| Component | Location | Format |\n|-----------|----------|--------|\n| Commands | `commands/*.md` | Markdown + YAML frontmatter |\n| Skills | `skills/*/SKILL.md` | Markdown + YAML frontmatter |\n| MCP Servers | `.mcp.json` | JSON |\n| Agents (uncommonly used in Cowork) | `agents/*.md` | Markdown + YAML frontmatter |\n| Hooks (rarely used in Cowork) | `hooks/hooks.json` | JSON |\n\nThis schema is shared with Claude Code's plugin system, but you're creating a plugin for Claude Cowork, a desktop app for doing knowledge work.\nCowork users will usually find commands and skills the most useful.\n\n### Customizable plugins with `~~` placeholders\n\n\u003e **Do not use or ask about this pattern by default.** Only introduce `~~` placeholders if the user explicitly says they want people outside their organization to use the plugin.\n\u003e You can mention this is an option if it seems like the user wants to distribute the plugin externally, but do not proactively ask about this with AskUserQuestion.\n\nWhen a plugin is intended to be shared with others outside their company, it might have parts that need to be adapted to individual users.\nYou might need to reference external tools by category rather than specific product (e.g., \"project tracker\" instead of \"Jira\").\nWhen sharing is needed, use generic language and mark these as requiring customization with two tilde characters such as `create an issue in ~~project tracker`.\nIf used any tool categories, write a `CONNECTORS.md` file at the plugin root to explain:\n\n```markdown\n# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user\nconnects in that category. Plugins are tool-agnostic — they describe\nworkflows in terms of categories rather than specific products.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack, Microsoft Teams, Discord |\n| Project tracker | `~~project tracker` | Linear, Asana, Jira |\n```\n\n### ${CLAUDE_PLUGIN_ROOT} Variable\n\nUse `${CLAUDE_PLUGIN_ROOT}` for all intra-plugin path references in hooks and MCP configs. Never hardcode absolute paths.\n\n## Guided Workflow\n\nWhen you ask the user something, use AskUserQuestion. Don't assume \"industry standard\" defaults are correct. Note: AskUserQuestion always includes a Skip button and a free-text input box for custom answers, so do not include `None` or `Other` as options.\n\n### Phase 1: Discovery\n\n**Goal**: Understand what the user wants to build and why.\n\nAsk (only what is unclear — skip questions if the user's initial request already answers them):\n\n- What should this plugin do? What problem does it solve?\n- Who will use it and in what context?\n- Does it integrate with any external tools or services?\n- Is there a similar plugin or workflow to reference?\n\nSummarize understanding and confirm before proceeding.\n\n**Output**: Clear statement of plugin purpose and scope.\n\n### Phase 2: Component Planning\n\n**Goal**: Determine which component types the plugin needs.\n\nBased on the discovery answers, determine:\n\n- **Skills** — Does it need specialized knowledge that Claude should load on-demand? (domain expertise, reference schemas, workflow guides)\n- **Commands** — Are there user-initiated actions? (deploy, configure, analyze, review)\n- **MCP Servers** — Does it need external service integration? (databases, APIs, SaaS tools)\n- **Agents (uncommon)** — Are there autonomous multi-step tasks? (validation, generation, analysis)\n- **Hooks (rare)** — Should something happen automatically on certain events? (enforce policies, load context, validate operations)\n\nPresent a component plan table, including component types you decided not to create:\n\n```\n| Component | Count | Purpose |\n|-----------|-------|---------|\n| Skills    | 1     | Domain knowledge for X |\n| Commands  | 2     | /do-thing, /check-thing |\n| Agents    | 0     | Not needed |\n| Hooks     | 1     | Validate writes |\n| MCP       | 1     | Connect to service Y |\n```\n\nGet user confirmation or adjustments before proceeding.\n\n**Output**: Confirmed list of components to create.\n\n### Phase 3: Design \u0026 Clarifying Questions\n\n**Goal**: Specify each component in detail. Resolve all ambiguities before implementation.\n\nFor each component type in the plan, ask targeted design questions. Present questions grouped by component type. Wait for answers before proceeding.\n\n**Skills:**\n- What user queries should trigger this skill?\n- What knowledge domains does it cover?\n- Should it include reference files for detailed content?\n\n**Commands:**\n- What arguments does each command accept?\n- What tools does each command need? (Read, Write, Bash, Grep, etc.)\n- Is each command interactive or automated?\n\n**Agents:**\n- Should each agent trigger proactively or only when requested?\n- What tools does it need?\n- What should the output format be?\n\n**Hooks:**\n- Which events? (PreToolUse, PostToolUse, Stop, SessionStart, etc.)\n- What behavior — validate, block, modify, add context?\n- Prompt-based (LLM-driven) or command-based (deterministic script)?\n\n**MCP Servers:**\n- What server type? (stdio for local, SSE for hosted with OAuth, HTTP for REST APIs)\n- What authentication method?\n- What tools should be exposed?\n\nIf the user says \"whatever you think is best,\" provide specific recommendations and get explicit confirmation.\n\n**Output**: Detailed specification for every component.\n\n### Phase 4: Implementation\n\n**Goal**: Create all plugin files following best practices.\n\n**Order of operations:**\n1. Create the plugin directory structure\n2. Create `plugin.json` manifest\n3. Create each component (see `references/component-schemas.md` for exact formats)\n4. Create `README.md` documenting the plugin\n\n**Implementation guidelines:**\n\n- **Commands** are instructions FOR Claude, not messages to the user. Write them as directives about what to do.\n- **Skills** use progressive disclosure: lean SKILL.md body (under 3,000 words), detailed content in `references/`. Frontmatter description must be third-person with specific trigger phrases.\n- **Agents** need a description with `\u003cexample\u003e` blocks showing triggering conditions, plus a system prompt in the markdown body.\n- **Hooks** config goes in `hooks/hooks.json`. Use `${CLAUDE_PLUGIN_ROOT}` for script paths. Prefer prompt-based hooks for complex logic.\n- **MCP configs** go in `.mcp.json` at plugin root. Use `${CLAUDE_PLUGIN_ROOT}` for local server paths. Document required env vars in README.\n\n### Phase 5: Review \u0026 Package\n\n**Goal**: Deliver the finished plugin.\n\n1. Summarize what was created — list each component and its purpose\n2. Ask if the user wants any adjustments\n3. Run `claude plugin validate \u003cpath-to-plugin-json\u003e`; fix any errors and warnings\n4. Package as a `.plugin` file:\n\n```bash\ncd /path/to/plugin-dir \u0026\u0026 zip -r /tmp/plugin-name.plugin . -x \"*.DS_Store\" \u0026\u0026 cp /tmp/plugin-name.plugin /path/to/outputs/plugin-name.plugin\n```\n\n\u003e **Important**: Always create the zip in `/tmp/` first, then copy to the outputs folder. Writing directly to the outputs folder may fail due to permissions.\n\n\u003e **Naming**: Use the plugin name from `plugin.json` for the `.plugin` file (e.g., if name is `code-reviewer`, output `code-reviewer.plugin`).\n\nThe `.plugin` file will appear in the chat as a rich preview where the user can browse the files and accept the plugin by pressing a button.\n\n## Best Practices\n\n- **Start small**: Begin with the minimum viable set of components. A plugin with one well-crafted skill is more useful than one with five half-baked components.\n- **Progressive disclosure for skills**: Core knowledge in SKILL.md, detailed reference material in `references/`, working examples in `examples/`.\n- **Clear trigger phrases**: Skill descriptions should include specific phrases users would say. Agent descriptions should include `\u003cexample\u003e` blocks.\n- **Commands are for Claude**: Write command content as instructions for Claude to follow, not documentation for the user to read.\n- **Imperative writing style**: Use verb-first instructions in skills (\"Parse the config file,\" not \"You should parse the config file\").\n- **Portability**: Always use `${CLAUDE_PLUGIN_ROOT}` for intra-plugin paths, never hardcoded paths.\n- **Security**: Use environment variables for credentials, HTTPS for remote servers, least-privilege tool access.\n\n## Additional Resources\n\n- **`references/component-schemas.md`** — Detailed format specifications for every component type (commands, skills, agents, hooks, MCP, CONNECTORS.md)\n- **`references/example-plugins.md`** — Three complete example plugin structures at different complexity levels\n", "depth": 4}, {"id": "82", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/create-cowork-plugin/references/component-schemas.md", "name": "component-schemas.md", "type": "file", "content": "# Component Schemas\n\nDetailed format specifications for every plugin component type. Reference this when implementing components in Phase 4.\n\n## Commands\n\n**Location**: `commands/command-name.md`\n**Format**: Markdown with optional YAML frontmatter\n\n### Frontmatter Fields\n\n| Field | Required | Type | Description |\n|-------|----------|------|-------------|\n| `description` | No | String | Brief description shown in `/help` (under 60 chars) |\n| `allowed-tools` | No | String or Array | Tools the command can use |\n| `model` | No | String | Model override: `sonnet`, `opus`, `haiku` |\n| `argument-hint` | No | String | Documents expected arguments for autocomplete |\n\n### Example Command\n\n```markdown\n---\ndescription: Review code for security issues\nallowed-tools: Read, Grep, Bash(git:*)\nargument-hint: [file-path]\n---\n\nReview @$1 for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication bypass\n- Insecure data handling\n\nProvide specific line numbers, severity ratings, and remediation suggestions.\n```\n\n### Key Rules\n\n- **Commands are instructions FOR Claude**, not messages for the user. Write them as directives.\n- `$ARGUMENTS` captures all arguments as a single string; `$1`, `$2`, `$3` capture positional arguments.\n- `@path` syntax includes file contents in the command context.\n- `!` backtick syntax executes bash inline for dynamic context (e.g., `` !`git diff --name-only` ``).\n- Use `${CLAUDE_PLUGIN_ROOT}` to reference plugin files portably.\n\n### allowed-tools Patterns\n\n```yaml\n# Specific tools\nallowed-tools: Read, Write, Edit, Bash(git:*)\n\n# Bash with specific commands only\nallowed-tools: Bash(npm:*), Read\n\n# MCP tools (specific)\nallowed-tools: [\"mcp__plugin_name_server__tool_name\"]\n```\n\n## Skills\n\n**Location**: `skills/skill-name/SKILL.md`\n**Format**: Markdown with YAML frontmatter\n\n### Frontmatter Fields\n\n| Field | Required | Type | Description |\n|-------|----------|------|-------------|\n| `name` | Yes | String | Skill identifier |\n| `description` | Yes | String | Third-person description with trigger phrases |\n| `version` | No | String | Semver version |\n\n### Example Skill\n\n```yaml\n---\nname: api-design\ndescription: \u003e\n  This skill should be used when the user asks to \"design an API\",\n  \"create API endpoints\", \"review API structure\", or needs guidance\n  on REST API best practices, endpoint naming, or request/response design.\nversion: 0.1.0\n---\n```\n\n### Writing Style Rules\n\n- **Frontmatter description**: Third-person (\"This skill should be used when...\"), with specific trigger phrases in quotes.\n- **Body**: Imperative/infinitive form (\"Parse the config file,\" not \"You should parse the config file\").\n- **Length**: Keep SKILL.md body under 3,000 words (ideally 1,500-2,000). Move detailed content to `references/`.\n\n### Skill Directory Structure\n\n```\nskill-name/\n├── SKILL.md              # Core knowledge (required)\n├── references/           # Detailed docs loaded on demand\n│   ├── patterns.md\n│   └── advanced.md\n├── examples/             # Working code examples\n│   └── sample-config.json\n└── scripts/              # Utility scripts\n    └── validate.sh\n```\n\n### Progressive Disclosure Levels\n\n1. **Metadata** (always in context): name + description (~100 words)\n2. **SKILL.md body** (when skill triggers): core knowledge (\u003c5k words)\n3. **Bundled resources** (as needed): references, examples, scripts (unlimited)\n\n## Agents\n\n**Location**: `agents/agent-name.md`\n**Format**: Markdown with YAML frontmatter\n\n### Frontmatter Fields\n\n| Field | Required | Type | Description |\n|-------|----------|------|-------------|\n| `name` | Yes | String | Lowercase, hyphens, 3-50 chars |\n| `description` | Yes | String | Triggering conditions with `\u003cexample\u003e` blocks |\n| `model` | Yes | String | `inherit`, `sonnet`, `opus`, or `haiku` |\n| `color` | Yes | String | `blue`, `cyan`, `green`, `yellow`, `magenta`, `red` |\n| `tools` | No | Array | Restrict to specific tools |\n\n### Example Agent\n\n```markdown\n---\nname: code-reviewer\ndescription: Use this agent when the user asks for a thorough code review or wants detailed analysis of code quality, security, and best practices.\n\n\u003cexample\u003e\nContext: User has just written a new module\nuser: \"Can you do a deep review of this code?\"\nassistant: \"I'll use the code-reviewer agent to provide a thorough analysis.\"\n\u003ccommentary\u003e\nUser explicitly requested a detailed review, which matches this agent's specialty.\n\u003c/commentary\u003e\n\u003c/example\u003e\n\n\u003cexample\u003e\nContext: User is about to merge a PR\nuser: \"Review this before I merge\"\nassistant: \"Let me run a comprehensive review using the code-reviewer agent.\"\n\u003ccommentary\u003e\nPre-merge review benefits from the agent's structured analysis process.\n\u003c/commentary\u003e\n\u003c/example\u003e\n\nmodel: inherit\ncolor: blue\ntools: [\"Read\", \"Grep\", \"Glob\"]\n---\n\nYou are a code review specialist focused on identifying issues across security, performance, maintainability, and correctness.\n\n**Your Core Responsibilities:**\n1. Analyze code structure and organization\n2. Identify security vulnerabilities\n3. Flag performance concerns\n4. Check adherence to best practices\n\n**Analysis Process:**\n1. Read all files in scope\n2. Identify patterns and anti-patterns\n3. Categorize findings by severity\n4. Provide specific remediation suggestions\n\n**Output Format:**\nPresent findings grouped by severity (Critical, Warning, Info) with:\n- File path and line number\n- Description of the issue\n- Suggested fix\n```\n\n### Agent Naming Rules\n\n- 3-50 characters\n- Lowercase letters, numbers, hyphens only\n- Must start and end with alphanumeric\n- No underscores, spaces, or special characters\n\n### Color Guidelines\n\n- Blue/Cyan: Analysis, review\n- Green: Success-oriented tasks\n- Yellow: Caution, validation\n- Red: Critical, security\n- Magenta: Creative, generation\n\n## Hooks\n\n**Location**: `hooks/hooks.json`\n**Format**: JSON\n\n### Available Events\n\n| Event | When it fires |\n|-------|--------------|\n| `PreToolUse` | Before a tool call executes |\n| `PostToolUse` | After a tool call completes |\n| `Stop` | When Claude finishes a response |\n| `SubagentStop` | When a subagent finishes |\n| `SessionStart` | When a session begins |\n| `SessionEnd` | When a session ends |\n| `UserPromptSubmit` | When the user sends a message |\n| `PreCompact` | Before context compaction |\n| `Notification` | When a notification fires |\n\n### Hook Types\n\n**Prompt-based** (recommended for complex logic):\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate whether this file write follows project conventions: $TOOL_INPUT\",\n  \"timeout\": 30\n}\n```\nSupported events: Stop, SubagentStop, UserPromptSubmit, PreToolUse.\n\n**Command-based** (deterministic checks):\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/scripts/validate.sh\",\n  \"timeout\": 60\n}\n```\n\n### Example hooks.json\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Check that this file write follows project coding standards. If it violates standards, explain why and block.\",\n          \"timeout\": 30\n        }\n      ]\n    }\n  ],\n  \"SessionStart\": [\n    {\n      \"matcher\": \"\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"cat ${CLAUDE_PLUGIN_ROOT}/context/project-context.md\",\n          \"timeout\": 10\n        }\n      ]\n    }\n  ]\n}\n```\n\n### Hook Output Format (Command Hooks)\n\nCommand hooks return JSON to stdout:\n\n```json\n{\n  \"decision\": \"block\",\n  \"reason\": \"File write violates naming convention\"\n}\n```\n\nDecisions: `approve`, `block`, `ask_user` (ask for confirmation).\n\n## MCP Servers\n\n**Location**: `.mcp.json` at plugin root\n**Format**: JSON\n\n### Server Types\n\n**stdio** (local process):\n```json\n{\n  \"mcpServers\": {\n    \"my-server\": {\n      \"command\": \"node\",\n      \"args\": [\"${CLAUDE_PLUGIN_ROOT}/servers/server.js\"],\n      \"env\": {\n        \"API_KEY\": \"${API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n**SSE** (remote server, server-sent events transport):\n```json\n{\n  \"mcpServers\": {\n    \"asana\": {\n      \"type\": \"sse\",\n      \"url\": \"https://mcp.asana.com/sse\"\n    }\n  }\n}\n```\n\n**HTTP** (remote server, streamable HTTP transport):\n```json\n{\n  \"mcpServers\": {\n    \"api-service\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.example.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${API_TOKEN}\"\n      }\n    }\n  }\n}\n```\n\n### Environment Variable Expansion\n\nAll MCP configs support `${VAR_NAME}` substitution:\n- `${CLAUDE_PLUGIN_ROOT}` — plugin directory (always use for portability)\n- `${ANY_ENV_VAR}` — user environment variables\n\nDocument all required environment variables in the plugin README.\n\n### Directory Servers Without a URL\n\nSome MCP directory entries have no `url` because the endpoint is dynamic. Plugins can reference these servers by **name** instead — if the server name in the plugin's MCP config matches the directory entry name, it is treated the same as a URL match.\n\n## CONNECTORS.md\n\n**Location**: Plugin root\n**When to create**: When the plugin references external tools by category rather than specific product\n\n### Format\n\n```markdown\n# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user\nconnects in that category. For example, `~~project tracker` might mean\nAsana, Linear, Jira, or any other project tracker with an MCP server.\n\nPlugins are tool-agnostic — they describe workflows in terms of categories\nrather than specific products.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams, Discord |\n| Project tracker | `~~project tracker` | Linear | Asana, Jira, Monday |\n```\n\n### Using ~~ Placeholders\n\nIn plugin files (commands, skills, agents), reference tools generically:\n\n```markdown\nCheck ~~project tracker for open tickets assigned to the user.\nPost a summary to ~~chat in the team channel.\n```\n\nDuring customization (via the cowork-plugin-customizer skill), these get replaced with specific tool names.\n\n## README.md\n\nEvery plugin should include a README with:\n\n1. **Overview** — what the plugin does\n2. **Components** — list of commands, skills, agents, hooks, MCP servers\n3. **Setup** — any required environment variables or configuration\n4. **Usage** — how to use each command or trigger each skill\n5. **Customization** — if CONNECTORS.md exists, mention it\n", "depth": 5}, {"id": "83", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/cowork-plugin-management/skills/create-cowork-plugin/references/example-plugins.md", "name": "example-plugins.md", "type": "file", "content": "# Example Plugins\n\nThree complete plugin structures at different complexity levels. Use these as templates when implementing in Phase 4.\n\n## Minimal Plugin: Single Command\n\nA simple plugin with one slash command and no other components.\n\n### Structure\n\n```\nmeeting-notes/\n├── .claude-plugin/\n│   └── plugin.json\n├── commands/\n│   └── meeting-notes.md\n└── README.md\n```\n\n### plugin.json\n\n```json\n{\n  \"name\": \"meeting-notes\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Generate structured meeting notes from transcripts\",\n  \"author\": {\n    \"name\": \"User\"\n  }\n}\n```\n\n### commands/meeting-notes.md\n\n```markdown\n---\ndescription: Generate structured meeting notes from a transcript\nargument-hint: [transcript-file]\nallowed-tools: Read, Write\n---\n\nRead the transcript at @$1 and generate structured meeting notes.\n\nInclude these sections:\n1. **Attendees** — list all participants mentioned\n2. **Summary** — 2-3 sentence overview of the meeting\n3. **Key Decisions** — numbered list of decisions made\n4. **Action Items** — table with columns: Owner, Task, Due Date\n5. **Open Questions** — anything unresolved\n\nWrite the notes to a new file named after the transcript with `-notes` appended.\n```\n\n---\n\n## Standard Plugin: Skill + Commands + MCP\n\nA plugin that combines domain knowledge, user commands, and external service integration.\n\n### Structure\n\n```\ncode-quality/\n├── .claude-plugin/\n│   └── plugin.json\n├── commands/\n│   ├── review.md\n│   └── fix-lint.md\n├── skills/\n│   └── coding-standards/\n│       ├── SKILL.md\n│       └── references/\n│           └── style-rules.md\n├── .mcp.json\n└── README.md\n```\n\n### plugin.json\n\n```json\n{\n  \"name\": \"code-quality\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Enforce coding standards with reviews, linting, and style guidance\",\n  \"author\": {\n    \"name\": \"User\"\n  }\n}\n```\n\n### commands/review.md\n\n```markdown\n---\ndescription: Review code changes for style and quality issues\nallowed-tools: Read, Grep, Bash(git:*)\n---\n\nGet the list of changed files: !`git diff --name-only`\n\nFor each changed file:\n1. Read the file\n2. Check against the coding-standards skill for style violations\n3. Identify potential bugs or anti-patterns\n4. Flag any security concerns\n\nPresent a summary with:\n- File path\n- Issue severity (Error, Warning, Info)\n- Description and suggested fix\n```\n\n### commands/fix-lint.md\n\n```markdown\n---\ndescription: Auto-fix linting issues in changed files\nallowed-tools: Read, Write, Edit, Bash(npm:*)\n---\n\nRun the linter: !`npm run lint -- --format json 2\u003e\u00261`\n\nParse the linter output and fix each issue:\n- For auto-fixable issues, apply the fix directly\n- For manual-fix issues, make the correction following project conventions\n- Skip issues that require architectural changes\n\nAfter all fixes, run the linter again to confirm clean output.\n```\n\n### skills/coding-standards/SKILL.md\n\n```yaml\n---\nname: coding-standards\ndescription: \u003e\n  This skill should be used when the user asks about \"coding standards\",\n  \"style guide\", \"naming conventions\", \"code formatting rules\", or needs\n  guidance on project-specific code quality expectations.\nversion: 0.1.0\n---\n```\n\n```markdown\n# Coding Standards\n\nProject coding standards and conventions for consistent, high-quality code.\n\n## Core Rules\n\n- Use camelCase for variables and functions\n- Use PascalCase for classes and types\n- Prefer const over let; avoid var\n- Maximum line length: 100 characters\n- Use explicit return types on all exported functions\n\n## Import Order\n\n1. External packages\n2. Internal packages (aliased with @/)\n3. Relative imports\n4. Type-only imports last\n\n## Additional Resources\n\n- **`references/style-rules.md`** — complete style rules by language\n```\n\n### .mcp.json\n\n```json\n{\n  \"mcpServers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n```\n\n---\n\n## Full-Featured Plugin: All Component Types\n\nA plugin using skills, commands, agents, hooks, and MCP integration with tool-agnostic connectors.\n\n### Structure\n\n```\nengineering-workflow/\n├── .claude-plugin/\n│   └── plugin.json\n├── commands/\n│   ├── standup-prep.md\n│   └── create-ticket.md\n├── skills/\n│   └── team-processes/\n│       ├── SKILL.md\n│       └── references/\n│           └── workflow-guide.md\n├── agents/\n│   └── ticket-analyzer.md\n├── hooks/\n│   └── hooks.json\n├── .mcp.json\n├── CONNECTORS.md\n└── README.md\n```\n\n### plugin.json\n\n```json\n{\n  \"name\": \"engineering-workflow\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Streamline engineering workflows: standup prep, ticket management, and code quality\",\n  \"author\": {\n    \"name\": \"User\"\n  },\n  \"keywords\": [\"engineering\", \"workflow\", \"tickets\", \"standup\"]\n}\n```\n\n### agents/ticket-analyzer.md\n\n```markdown\n---\nname: ticket-analyzer\ndescription: Use this agent when the user needs to analyze tickets, triage incoming issues, or prioritize a backlog.\n\n\u003cexample\u003e\nContext: User is preparing for sprint planning\nuser: \"Help me triage these new tickets\"\nassistant: \"I'll use the ticket-analyzer agent to review and categorize the tickets.\"\n\u003ccommentary\u003e\nTicket triage requires systematic analysis across multiple dimensions, making the agent appropriate.\n\u003c/commentary\u003e\n\u003c/example\u003e\n\n\u003cexample\u003e\nContext: User has a large backlog\nuser: \"Prioritize my backlog for next sprint\"\nassistant: \"Let me analyze the backlog using the ticket-analyzer agent to recommend priorities.\"\n\u003ccommentary\u003e\nBacklog prioritization is a multi-step autonomous task well-suited for the agent.\n\u003c/commentary\u003e\n\u003c/example\u003e\n\nmodel: inherit\ncolor: cyan\ntools: [\"Read\", \"Grep\"]\n---\n\nYou are a ticket analysis specialist. Analyze tickets for priority, effort, and dependencies.\n\n**Your Core Responsibilities:**\n1. Categorize tickets by type (bug, feature, tech debt, improvement)\n2. Estimate relative effort (S, M, L, XL)\n3. Identify dependencies between tickets\n4. Recommend priority ordering\n\n**Analysis Process:**\n1. Read all ticket descriptions\n2. Categorize each by type\n3. Estimate effort based on scope\n4. Map dependencies\n5. Rank by impact-to-effort ratio\n\n**Output Format:**\n| Ticket | Type | Effort | Dependencies | Priority |\n|--------|------|--------|-------------|----------|\n| ...    | ...  | ...    | ...         | ...      |\n\nFollowed by a brief rationale for the top 5 priorities.\n```\n\n### hooks/hooks.json\n\n```json\n{\n  \"SessionStart\": [\n    {\n      \"matcher\": \"\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"echo '## Team Context\\n\\nSprint cycle: 2 weeks. Standup: daily at 9:30 AM. Use ~~project tracker for ticket management.'\",\n          \"timeout\": 5\n        }\n      ]\n    }\n  ]\n}\n```\n\n### CONNECTORS.md\n\n```markdown\n# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user\nconnects in that category. Plugins are tool-agnostic.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Project tracker | `~~project tracker` | Linear | Asana, Jira, Monday |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Source control | `~~source control` | GitHub | GitLab, Bitbucket |\n```\n\n### .mcp.json\n\n```json\n{\n  \"mcpServers\": {\n    \"linear\": {\n      \"type\": \"sse\",\n      \"url\": \"https://mcp.linear.app/sse\"\n    },\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    },\n    \"slack\": {\n      \"type\": \"http\",\n      \"url\": \"https://slack.mcp.claude.com/mcp\"\n    }\n  }\n}\n```\n", "depth": 5}, {"id": "84", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "85", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "86", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/README.md", "name": "README.md", "type": "file", "content": "# Sales Plugin\n\nA sales productivity plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Helps with prospecting, outreach, pipeline management, call preparation, and deal strategy. Works with any sales team — standalone with web search and your input, supercharged when you connect your CRM, email, and other tools.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/sales\n```\n\n## Commands\n\nExplicit workflows you invoke with a slash command:\n\n| Command | Description |\n|---|---|\n| `/call-summary` | Process call notes or transcript — extract action items, draft follow-up, generate internal summary |\n| `/forecast` | Generate a weighted sales forecast — upload CSV or describe your pipeline, set quota, get projections |\n| `/pipeline-review` | Analyze pipeline health — prioritize deals, flag risks, get weekly action plan |\n\nAll commands work **standalone** (paste notes, upload CSV, or describe your situation) and get **supercharged** with MCP connectors.\n\n## Skills\n\nDomain knowledge Claude uses automatically when relevant:\n\n| Skill | Description |\n|---|---|\n| `account-research` | Research a company or person — web search for company intel, key contacts, recent news, hiring signals |\n| `call-prep` | Prepare for sales calls — account context, attendee research, suggested agenda, discovery questions |\n| `daily-briefing` | Prioritized daily sales briefing — meetings, pipeline alerts, email priorities, suggested actions |\n| `draft-outreach` | Research-first outreach — research the prospect, then draft personalized email and LinkedIn messages |\n| `competitive-intelligence` | Research competitors — product comparison, pricing intel, recent releases, differentiation matrix, sales talk tracks |\n| `create-an-asset` | Generate custom sales assets — landing pages, decks, one-pagers, workflow demos tailored to your prospect |\n\n## Example Workflows\n\n### After a Call\n\n```\n/call-summary\n```\n\nPaste your notes or transcript. Get a structured summary, action items with owners, and a draft follow-up email. If CRM is connected, offers to log the activity and create tasks.\n\n### Weekly Forecast\n\n```\n/forecast\n```\n\nUpload a CSV export from your CRM (or paste your deals). Tell me your quota and timeline. Get a weighted forecast with best/likely/worst scenarios, commit vs. upside breakdown, and gap analysis.\n\n### Pipeline Review\n\n```\n/pipeline-review\n```\n\nUpload a CSV or describe your pipeline. Get a health score, deal prioritization, risk flags (stale deals, past close dates, single-threaded), and a weekly action plan.\n\n### Researching a Prospect\n\nJust ask naturally:\n```\nResearch Acme Corp before my call tomorrow\n```\n\nThe `account-research` skill triggers automatically and gives you company overview, key contacts, recent news, and recommended approach.\n\n### Drafting Outreach\n\n```\nDraft an email to the VP of Engineering at TechStart\n```\n\nThe `draft-outreach` skill researches the prospect first, then generates personalized outreach with multiple angles.\n\n### Competitive Intel\n\n```\nHow do we compare to Competitor X?\n```\n\nThe `competitive-intelligence` skill researches both companies and builds a differentiation matrix with talk tracks.\n\n## Standalone + Supercharged\n\nEvery command and skill works without any integrations:\n\n| What You Can Do | Standalone | Supercharged With |\n|-----------------|------------|-------------------|\n| Process call notes | Paste notes/transcript | Transcripts MCP (e.g. Gong, Fireflies) |\n| Forecast pipeline | Upload CSV, paste deals | CRM MCP |\n| Review pipeline | Upload CSV, describe deals | CRM MCP |\n| Research prospects | Web search | Enrichment MCP (e.g. Clay, ZoomInfo) |\n| Prep for calls | Describe meeting | CRM, Email, Calendar MCPs |\n| Draft outreach | Web search + your context | CRM, Email MCPs |\n| Competitive intel | Web search | CRM (win/loss data), Docs (battlecards) |\n\n## MCP Integrations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your tools for a richer experience:\n\n| Category | Examples | What It Enables |\n|---|---|---|\n| **CRM** | HubSpot, Close | Pipeline data, account history, contact records |\n| **Transcripts** | Fireflies, Gong, Chorus | Call recordings, transcripts, key moments |\n| **Enrichment** | Clay, ZoomInfo, Apollo | Company and contact data enrichment |\n| **Chat** | Slack, Teams | Internal discussions, colleague intel |\n\nSee [CONNECTORS.md](CONNECTORS.md) for the full list of supported integrations, including email, calendar, and additional CRM options.\n\n## Settings\n\nCreate a local settings file at `sales/.claude/settings.local.json` to personalize:\n\n```json\n{\n  \"name\": \"Your Name\",\n  \"title\": \"Account Executive\",\n  \"company\": \"Your Company\",\n  \"quota\": {\n    \"annual\": 1000000,\n    \"quarterly\": 250000\n  },\n  \"product\": {\n    \"name\": \"Your Product\",\n    \"value_props\": [\n      \"Key value proposition 1\",\n      \"Key value proposition 2\"\n    ],\n    \"competitors\": [\n      \"Competitor A\",\n      \"Competitor B\"\n    ]\n  }\n}\n```\n\nThe plugin will ask you for this information interactively if it's not configured.\n", "depth": 2}, {"id": "87", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~CRM` might mean Salesforce, HubSpot, or any other CRM with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (CRM, chat, email, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Calendar | `~~calendar` | Google Calendar, Microsoft 365 | — |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Competitive intelligence | `~~competitive intelligence` | Similarweb | Crayon, Klue |\n| CRM | `~~CRM` | HubSpot, Close | Salesforce, Pipedrive, Copper |\n| Data enrichment | `~~data enrichment` | Clay, ZoomInfo, Apollo | Clearbit, Lusha |\n| Email | `~~email` | Gmail, Microsoft 365 | — |\n| Knowledge base | `~~knowledge base` | Notion | Confluence, Guru |\n| Meeting transcription | `~~conversation intelligence` | Fireflies | Gong, Chorus, Otter.ai |\n| Project tracker | `~~project tracker` | Atlassian (Jira/Confluence) | Linear, Asana |\n| Sales engagement | `~~sales engagement` | Outreach | Salesloft, Apollo |\n", "depth": 2}, {"id": "88", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/commands/pipeline-review.md", "name": "pipeline-review.md", "type": "file", "content": "---\ndescription: Analyze pipeline health — prioritize deals, flag risks, get a weekly action plan\nargument-hint: \"\u003csegment or rep\u003e\"\n---\n\n# /pipeline-review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nAnalyze your pipeline health, prioritize deals, and get actionable recommendations for where to focus.\n\n## Usage\n\n```\n/pipeline-review [segment or rep]\n```\n\nReview pipeline for: $ARGUMENTS\n\nIf a file is referenced: @$1\n\n---\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     PIPELINE REVIEW                              │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Upload CSV export from your CRM                              │\n│  ✓ Or paste/describe your deals                                 │\n│  ✓ Health check: flag stale, stuck, and at-risk deals          │\n│  ✓ Prioritization: rank deals by impact and closability        │\n│  ✓ Hygiene audit: missing data, bad close dates, single-thread │\n│  ✓ Weekly action plan: what to focus on                        │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + CRM: Pull pipeline automatically, update records             │\n│  + Activity data for engagement scoring                         │\n│  + Historical patterns for risk prediction                      │\n│  + Calendar: See upcoming meetings per deal                     │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## What I Need From You\n\n**Option A: Upload a CSV**\nExport your pipeline from your CRM (e.g. Salesforce, HubSpot). Helpful fields:\n- Deal/Opportunity name\n- Account name\n- Amount\n- Stage\n- Close date\n- Created date\n- Last activity date\n- Owner (if reviewing a team)\n- Primary contact\n\n**Option B: Paste your deals**\n```\nAcme Corp - $50K - Negotiation - closes Jan 31 - last activity Jan 20\nTechStart - $25K - Demo scheduled - closes Feb 15 - no activity in 3 weeks\nBigCo - $100K - Discovery - closes Mar 30 - created last week\n```\n\n**Option C: Describe your pipeline**\n\"I have 12 deals. Two big ones in negotiation that I'm confident about. Three stuck in discovery for over a month. The rest are mid-stage but I haven't talked to some of them in a while.\"\n\n---\n\n## Output\n\n```markdown\n# Pipeline Review: [Date]\n\n**Data Source:** [CSV upload / Manual input / CRM]\n**Deals Analyzed:** [X]\n**Total Pipeline Value:** $[X]\n\n---\n\n## Pipeline Health Score: [X/100]\n\n| Dimension | Score | Issue |\n|-----------|-------|-------|\n| **Stage Progression** | [X]/25 | [X] deals stuck in same stage 30+ days |\n| **Activity Recency** | [X]/25 | [X] deals with no activity in 14+ days |\n| **Close Date Accuracy** | [X]/25 | [X] deals with close date in past |\n| **Contact Coverage** | [X]/25 | [X] deals single-threaded |\n\n---\n\n## Priority Actions This Week\n\n### 1. [Highest Priority Deal]\n**Why:** [Reason — large, closing soon, at risk, etc.]\n**Action:** [Specific next step]\n**Impact:** $[X] if you close it\n\n### 2. [Second Priority]\n**Why:** [Reason]\n**Action:** [Next step]\n\n### 3. [Third Priority]\n**Why:** [Reason]\n**Action:** [Next step]\n\n---\n\n## Deal Prioritization Matrix\n\n### Close This Week (Focus Time Here)\n| Deal | Amount | Stage | Close Date | Next Action |\n|------|--------|-------|------------|-------------|\n| [Deal] | $[X] | [Stage] | [Date] | [Action] |\n\n### Close This Month (Keep Warm)\n| Deal | Amount | Stage | Close Date | Status |\n|------|--------|-------|------------|--------|\n| [Deal] | $[X] | [Stage] | [Date] | [Status] |\n\n### Nurture (Check-in Periodically)\n| Deal | Amount | Stage | Close Date | Status |\n|------|--------|-------|------------|--------|\n| [Deal] | $[X] | [Stage] | [Date] | [Status] |\n\n---\n\n## Risk Flags\n\n### Stale Deals (No Activity 14+ Days)\n| Deal | Amount | Last Activity | Days Silent | Recommendation |\n|------|--------|---------------|-------------|----------------|\n| [Deal] | $[X] | [Date] | [X] | [Re-engage / Downgrade / Remove] |\n\n### Stuck Deals (Same Stage 30+ Days)\n| Deal | Amount | Stage | Days in Stage | Recommendation |\n|------|--------|-------|---------------|----------------|\n| [Deal] | $[X] | [Stage] | [X] | [Push / Multi-thread / Qualify out] |\n\n### Past Close Date\n| Deal | Amount | Close Date | Days Overdue | Recommendation |\n|------|--------|------------|--------------|----------------|\n| [Deal] | $[X] | [Date] | [X] | [Update date / Push to next quarter / Close lost] |\n\n### Single-Threaded (Only One Contact)\n| Deal | Amount | Contact | Risk | Recommendation |\n|------|--------|---------|------|----------------|\n| [Deal] | $[X] | [Name] | Champion leaves = deal dies | [Identify additional stakeholders] |\n\n---\n\n## Hygiene Issues\n\n| Issue | Count | Deals | Action |\n|-------|-------|-------|--------|\n| Missing close date | [X] | [List] | Add realistic close dates |\n| Missing amount | [X] | [List] | Estimate or qualify |\n| Missing next step | [X] | [List] | Define next action |\n| No primary contact | [X] | [List] | Assign contact |\n\n---\n\n## Pipeline Shape\n\n### By Stage\n| Stage | # Deals | Value | % of Pipeline |\n|-------|---------|-------|---------------|\n| [Stage] | [X] | $[X] | [X]% |\n\n### By Close Month\n| Month | # Deals | Value |\n|-------|---------|-------|\n| [Month] | [X] | $[X] |\n\n### By Deal Size\n| Size | # Deals | Value |\n|------|---------|-------|\n| $100K+ | [X] | $[X] |\n| $50K-100K | [X] | $[X] |\n| $25K-50K | [X] | $[X] |\n| \u003c$25K | [X] | $[X] |\n\n---\n\n## Recommendations\n\n### This Week\n1. [ ] [Specific action for priority deal 1]\n2. [ ] [Action for at-risk deal]\n3. [ ] [Hygiene task]\n\n### This Month\n1. [ ] [Strategic action]\n2. [ ] [Pipeline building if needed]\n\n---\n\n## Deals to Consider Removing\n\nThese deals may be dead weight:\n\n| Deal | Amount | Reason | Recommendation |\n|------|--------|--------|----------------|\n| [Deal] | $[X] | [No activity 60+ days, no response] | Mark closed-lost |\n| [Deal] | $[X] | [Pushed 3+ times, no champion] | Qualify out |\n```\n\n---\n\n## Prioritization Framework\n\nI'll rank your deals using this framework:\n\n| Factor | Weight | What I Look For |\n|--------|--------|-----------------|\n| **Close Date** | 30% | Deals closing soonest get priority |\n| **Deal Size** | 25% | Bigger deals = more focus |\n| **Stage** | 20% | Later stage = more focus |\n| **Activity** | 15% | Active deals get prioritized |\n| **Risk** | 10% | Lower risk = safer bet |\n\nYou can tell me to weight differently: \"Focus on big deals over soon deals\" or \"I need quick wins, prioritize close dates.\"\n\n---\n\n## If CRM Connected\n\n- I'll pull your pipeline automatically\n- Update records with new close dates, stages, next steps\n- Create follow-up tasks\n- Track hygiene improvements over time\n\n---\n\n## Tips\n\n1. **Review weekly** — Pipeline health decays fast. Weekly reviews catch issues early.\n2. **Kill dead deals** — Stale opportunities inflate your pipeline and distort forecasts. Be ruthless.\n3. **Multi-thread everything** — If one person goes dark, you need a backup contact.\n4. **Close dates should mean something** — A close date is when you expect signature, not when you hope for one.\n", "depth": 3}, {"id": "89", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/commands/forecast.md", "name": "forecast.md", "type": "file", "content": "---\ndescription: Generate a weighted sales forecast with best/likely/worst scenarios, commit vs. upside breakdown, and gap analysis\nargument-hint: \"\u003cperiod\u003e\"\n---\n\n# /forecast\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a weighted sales forecast with risk analysis and commit recommendations.\n\n## Usage\n\n```\n/forecast [period]\n```\n\nGenerate a forecast for: $ARGUMENTS\n\nIf a file is referenced: @$1\n\n---\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        FORECAST                                  │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Upload CSV export from your CRM                              │\n│  ✓ Or paste/describe your pipeline deals                        │\n│  ✓ Set your quota and timeline                                  │\n│  ✓ Get weighted forecast with stage probabilities               │\n│  ✓ Risk-adjusted projections (best/likely/worst case)           │\n│  ✓ Commit vs. upside breakdown                                  │\n│  ✓ Gap analysis and recommendations                             │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + CRM: Pull pipeline automatically, real-time data             │\n│  + Historical win rates by stage, segment, deal size            │\n│  + Activity signals for risk scoring                            │\n│  + Automatic refresh and tracking over time                     │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## What I Need From You\n\n### Step 1: Your Pipeline Data\n\n**Option A: Upload a CSV**\nExport your pipeline from your CRM (e.g. Salesforce, HubSpot). I need at minimum:\n- Deal/Opportunity name\n- Amount\n- Stage\n- Close date\n\nHelpful if you have:\n- Owner (if team forecast)\n- Last activity date\n- Created date\n- Account name\n\n**Option B: Paste your deals**\n```\nAcme Corp - $50K - Negotiation - closes Jan 31\nTechStart - $25K - Demo scheduled - closes Feb 15\nBigCo - $100K - Discovery - closes Mar 30\n```\n\n**Option C: Describe your territory**\n\"I have 8 deals in pipeline totaling $400K. Two are in negotiation ($120K), three in evaluation ($180K), three in discovery ($100K).\"\n\n### Step 2: Your Targets\n\n- **Quota**: What's your number? (e.g., \"$500K this quarter\")\n- **Timeline**: When does the period end? (e.g., \"Q1 ends March 31\")\n- **Already closed**: How much have you already booked this period?\n\n---\n\n## Output\n\n```markdown\n# Sales Forecast: [Period]\n\n**Generated:** [Date]\n**Data Source:** [CSV upload / Manual input / CRM]\n\n---\n\n## Summary\n\n| Metric | Value |\n|--------|-------|\n| **Quota** | $[X] |\n| **Closed to Date** | $[X] ([X]% of quota) |\n| **Open Pipeline** | $[X] |\n| **Weighted Forecast** | $[X] |\n| **Gap to Quota** | $[X] |\n| **Coverage Ratio** | [X]x |\n\n---\n\n## Forecast Scenarios\n\n| Scenario | Amount | % of Quota | Assumptions |\n|----------|--------|------------|-------------|\n| **Best Case** | $[X] | [X]% | All deals close as expected |\n| **Likely Case** | $[X] | [X]% | Stage-weighted probabilities |\n| **Worst Case** | $[X] | [X]% | Only commit deals close |\n\n---\n\n## Pipeline by Stage\n\n| Stage | # Deals | Total Value | Probability | Weighted Value |\n|-------|---------|-------------|-------------|----------------|\n| Negotiation | [X] | $[X] | 80% | $[X] |\n| Proposal | [X] | $[X] | 60% | $[X] |\n| Evaluation | [X] | $[X] | 40% | $[X] |\n| Discovery | [X] | $[X] | 20% | $[X] |\n| **Total** | [X] | $[X] | — | $[X] |\n\n---\n\n## Commit vs. Upside\n\n### Commit (High Confidence)\nDeals you'd stake your forecast on:\n\n| Deal | Amount | Stage | Close Date | Why Commit |\n|------|--------|-------|------------|------------|\n| [Deal] | $[X] | [Stage] | [Date] | [Reason] |\n\n**Total Commit:** $[X]\n\n### Upside (Lower Confidence)\nDeals that could close but have risk:\n\n| Deal | Amount | Stage | Close Date | Risk Factor |\n|------|--------|-------|------------|-------------|\n| [Deal] | $[X] | [Stage] | [Date] | [Risk] |\n\n**Total Upside:** $[X]\n\n---\n\n## Risk Flags\n\n| Deal | Amount | Risk | Recommendation |\n|------|--------|------|----------------|\n| [Deal] | $[X] | Close date passed | Update close date or move to lost |\n| [Deal] | $[X] | No activity in 14+ days | Re-engage or downgrade stage |\n| [Deal] | $[X] | Close date this week, still in discovery | Unlikely to close — push out |\n\n---\n\n## Gap Analysis\n\n**To hit quota, you need:** $[X] more\n\n**Options to close the gap:**\n1. **Accelerate [Deal]** — Currently [stage], worth $[X]. If you can close by [date], you're at [X]% of quota.\n2. **Revive [Stalled Deal]** — Last active [date]. Worth $[X]. Reach out to [contact].\n3. **New pipeline needed** — You need $[X] in new opportunities at [X]x coverage to be safe.\n\n---\n\n## Recommendations\n\n1. [ ] [Specific action for highest-impact deal]\n2. [ ] [Action for at-risk deal]\n3. [ ] [Pipeline generation recommendation if gap exists]\n```\n\n---\n\n## Stage Probabilities (Default)\n\nIf you don't provide custom probabilities, I'll use:\n\n| Stage | Default Probability |\n|-------|---------------------|\n| Closed Won | 100% |\n| Negotiation / Contract | 80% |\n| Proposal / Quote | 60% |\n| Evaluation / Demo | 40% |\n| Discovery / Qualification | 20% |\n| Prospecting / Lead | 10% |\n\nTell me if your stages or probabilities are different.\n\n---\n\n## If CRM Connected\n\n- I'll pull your pipeline automatically\n- Use your actual historical win rates\n- Factor in activity recency for risk scoring\n- Track forecast changes over time\n- Compare to previous forecasts\n\n---\n\n## Tips\n\n1. **Be honest about commit** — Only commit deals you'd bet on. Upside is for everything else.\n2. **Update close dates** — Stale close dates kill forecast accuracy. Push out deals that won't close in time.\n3. **Coverage matters** — 3x pipeline coverage is healthy. Below 2x is risky.\n4. **Activity = signal** — Deals with no recent activity are at higher risk than stage suggests.\n", "depth": 3}, {"id": "90", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/commands/call-summary.md", "name": "call-summary.md", "type": "file", "content": "---\ndescription: Process call notes or a transcript — extract action items, draft follow-up email, generate internal summary\nargument-hint: \"\u003ccall notes or transcript\u003e\"\n---\n\n# /call-summary\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nProcess call notes or a transcript to extract action items, draft follow-up communications, and update records.\n\n## Usage\n\n```\n/call-summary \u003cnotes or transcript\u003e\n```\n\nProcess these call notes: $ARGUMENTS\n\nIf a file is referenced: @$1\n\n---\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                      CALL SUMMARY                                │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Paste call notes or transcript                               │\n│  ✓ Extract key discussion points and decisions                  │\n│  ✓ Identify action items with owners and due dates              │\n│  ✓ Surface objections, concerns, and open questions             │\n│  ✓ Draft customer-facing follow-up email                        │\n│  ✓ Generate internal summary for your team                      │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Transcripts: Pull recording automatically (e.g. Gong, Fireflies) │\n│  + CRM: Update opportunity, log activity, create tasks          │\n│  + Email: Send follow-up directly from draft                    │\n│  + Calendar: Link to meeting, pull attendee context             │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## What I Need From You\n\n**Option 1: Paste your notes**\nJust paste whatever you have — bullet points, rough notes, stream of consciousness. I'll structure it.\n\n**Option 2: Paste a transcript**\nIf you have a full transcript from your video conferencing tool (e.g. Zoom, Teams) or conversation intelligence tool (e.g. Gong, Fireflies), paste it. I'll extract the key moments.\n\n**Option 3: Describe the call**\nTell me what happened: \"Had a discovery call with Acme Corp. Met with their VP Eng and CTO. They're evaluating us vs Competitor X. Main concern is integration timeline.\"\n\n---\n\n## Output\n\n### Internal Summary\n```markdown\n## Call Summary: [Company] — [Date]\n\n**Attendees:** [Names and titles]\n**Call Type:** [Discovery / Demo / Negotiation / Check-in]\n**Duration:** [If known]\n\n### Key Discussion Points\n1. [Topic] — [What was discussed, decisions made]\n2. [Topic] — [Summary]\n\n### Customer Priorities\n- [Priority 1 they expressed]\n- [Priority 2]\n\n### Objections / Concerns Raised\n- [Concern] — [How you addressed it / status]\n\n### Competitive Intel\n- [Any competitor mentions, what was said]\n\n### Action Items\n| Owner | Action | Due |\n|-------|--------|-----|\n| [You] | [Task] | [Date] |\n| [Customer] | [Task] | [Date] |\n\n### Next Steps\n- [Agreed next step with timeline]\n\n### Deal Impact\n- [How this call affects the opportunity — stage change, risk, acceleration]\n```\n\n### Customer Follow-Up Email\n```\nSubject: [Meeting recap + next steps]\n\nHi [Name],\n\nThank you for taking the time to meet today...\n\n[Key points discussed]\n\n[Commitments you made]\n\n[Clear next step with timeline]\n\nBest,\n[You]\n```\n\n---\n\n## Email Style Guidelines\n\nWhen drafting customer-facing emails:\n\n1. **Be concise but informative** — Get to the point quickly. Customers are busy.\n2. **No markdown formatting** — Don't use asterisks, bold, or other markdown syntax. Write in plain text that looks natural in any email client.\n3. **Use simple structure** — Short paragraphs, line breaks between sections. No headers or bullet formatting unless the customer's email client will render it.\n4. **Keep it scannable** — If listing items, use plain dashes or numbers, not fancy formatting.\n\n**Good:**\n```\nHere's what we discussed:\n- Quote for 20 seats at $480/seat/year\n- W9 and supplier onboarding docs\n- Point of contact for the contract\n```\n\n**Bad:**\n```\n**What You Need from Us:**\n- Quote for 20 seats at $480/seat/year\n```\n\n---\n\n## If Connectors Available\n\n**Transcripts connected (e.g. Gong, Fireflies):**\n- I'll search for the call automatically\n- Pull the full transcript\n- Extract key moments flagged by the platform\n\n**CRM connected:**\n- I'll offer to update the opportunity stage\n- Log the call as an activity\n- Create tasks for action items\n- Update next steps field\n\n**Email connected:**\n- I'll offer to create a draft in ~~email\n- Or send directly if you approve\n\n---\n\n## Tips\n\n1. **More detail = better output** — Even rough notes help. \"They seemed concerned about X\" is useful context.\n2. **Name the attendees** — Helps me structure the summary and assign action items.\n3. **Flag what matters** — If something was important, tell me: \"The big thing was...\"\n4. **Tell me the deal stage** — Helps me tailor the follow-up tone and next steps.\n", "depth": 3}, {"id": "91", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/draft-outreach", "name": "draft-outreach", "type": "dir", "content": "", "depth": 3}, {"id": "92", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/create-an-asset", "name": "create-an-asset", "type": "dir", "content": "", "depth": 3}, {"id": "93", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/daily-briefing", "name": "daily-briefing", "type": "dir", "content": "", "depth": 3}, {"id": "94", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/account-research", "name": "account-research", "type": "dir", "content": "", "depth": 3}, {"id": "95", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/competitive-intelligence", "name": "competitive-intelligence", "type": "dir", "content": "", "depth": 3}, {"id": "96", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/call-prep", "name": "call-prep", "type": "dir", "content": "", "depth": 3}, {"id": "97", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/draft-outreach/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: draft-outreach\ndescription: Research a prospect then draft personalized outreach. Uses web research by default, supercharged with enrichment and CRM. Trigger with \"draft outreach to [person/company]\", \"write cold email to [prospect]\", \"reach out to [name]\".\n---\n\n# Draft Outreach\n\nResearch first, then draft. This skill never sends generic outreach - it always researches the prospect first to personalize the message. Works standalone with web search, supercharged when you connect your tools.\n\n## Connectors (Optional)\n\n| Connector | What It Adds |\n|-----------|--------------|\n| **Enrichment** | Verified email, phone, background details |\n| **CRM** | Prior relationship context, existing contacts |\n| **Email** | Create draft directly in your inbox |\n\n\u003e **No connectors?** Web research works great. I'll output the email text for you to copy.\n\n---\n\n## How It Works\n\n```\n+------------------------------------------------------------------+\n|                      DRAFT OUTREACH                               |\n|                                                                   |\n|  Step 1: RESEARCH (always happens first)                         |\n|  - Web search (default)                                           |\n|  - + Enrichment (if enrichment tools connected)                  |\n|  - + CRM (if CRM connected)                                      |\n|                                                                   |\n|  Step 2: DRAFT (based on research)                               |\n|  - Personalized opening (from research)                          |\n|  - Relevant hook (their priorities)                              |\n|  - Clear CTA                                                      |\n|                                                                   |\n|  Step 3: DELIVER (based on connectors)                           |\n|  - Email draft (if email connected)                              |\n|  - Copy for LinkedIn (always)                                    |\n|  - Output to user (always)                                        |\n+------------------------------------------------------------------+\n```\n\n---\n\n## Output Format\n\n```markdown\n# Outreach Draft: [Person] @ [Company]\n**Generated:** [Date] | **Research Sources:** [Web, Enrichment, CRM]\n\n---\n\n## Research Summary\n\n**Target:** [Name], [Title] at [Company]\n**Hook:** [Why reaching out now - the personalized angle]\n**Goal:** [What you want from this outreach]\n\n---\n\n## Email Draft\n\n**To:** [email if known, or \"find email\" note]\n**Subject:** [Personalized subject line]\n\n---\n\n[Email body]\n\n---\n\n**Subject Line Alternatives:**\n1. [Option 2]\n2. [Option 3]\n\n---\n\n## LinkedIn Message (if no email)\n\n**Connection Request (\u003c 300 chars):**\n[Short, no-pitch connection request]\n\n**Follow-up Message (after connected):**\n[Value-first message]\n\n---\n\n## Why This Approach\n\n| Element | Based On |\n|---------|----------|\n| Opening | [Research finding that makes it personal] |\n| Hook | [Their priority/pain point] |\n| Proof | [Relevant customer story] |\n| CTA | [Low-friction ask] |\n\n---\n\n## Email Draft Status\n\n[Draft created - check ~~email]\n[Email not connected - copy email above]\n[No email found - use LinkedIn approach]\n\n---\n\n## Follow-up Sequence (Optional)\n\n**Day 3 - Follow-up 1:**\n[Short, new angle]\n\n**Day 7 - Follow-up 2:**\n[Different value prop]\n\n**Day 14 - Break-up:**\n[Final attempt]\n```\n\n---\n\n## Execution Flow\n\n### Step 1: Parse Request\n\n```\nInput patterns:\n- \"draft outreach to John Smith at Acme\" → Person + company\n- \"write cold email to Acme's CTO\" → Role + company\n- \"reach out to sarah@acme.com\" → Email provided\n- \"LinkedIn message to [LinkedIn URL]\" → Profile provided\n```\n\n### Step 2: Research First (Always)\n\n**Use research-prospect skill internally:**\n```\n1. Web search for company + person\n2. If Enrichment connected: Get verified contact info, background\n3. If CRM connected: Check for prior relationship\n```\n\n**Must find before drafting:**\n- Who they are (title, background)\n- What the company does\n- Recent news or trigger\n- Personalization hook\n\n### Step 3: Identify Hook\n\n```\nPriority order for hooks:\n1. Trigger event (funding, hiring, news) → Most timely\n2. Mutual connection → Social proof\n3. Their content (post, article, talk) → Shows you did research\n4. Company initiative → Relevant to their priorities\n5. Role-based pain point → Least personal but still relevant\n```\n\n### Step 4: Draft Message\n\n**Email Structure (AIDA):**\n```\nSUBJECT: [Personalized, \u003c50 chars, no spam words]\n\n[Opening: Personal hook - shows you researched them]\n\n[Interest: Their problem/opportunity in 1-2 sentences]\n\n[Desire: Brief proof point - similar company result]\n\n[Action: Clear, low-friction CTA]\n\n[Signature]\n```\n\n**LinkedIn Connection Request (\u003c300 chars):**\n```\nHi [Name], [Mutual connection/shared interest/genuine compliment].\nWould love to connect. [No pitch]\n```\n\n**LinkedIn Follow-up Message:**\n```\nThanks for connecting! [Value-first: insight, article, observation]\n\n[Soft transition to why you reached out]\n\n[Question, not pitch]\n```\n\n### Step 5: Create Email Draft\n\n```\nIf email connector available:\n1. Create draft with to, subject, body\n2. Return draft link\n3. Note: \"Draft created - review and send\"\n\nIf not available:\n1. Output email text\n2. Note: \"Copy to your email client\"\n```\n\n---\n\n## Capability by Connector\n\n| Capability | Web Only | + Enrichment | + CRM | + Email |\n|------------|----------|--------------|-------|---------|\n| Personalized opening | Basic | Deep | With history | Same |\n| Verified email | No | Yes | Yes | Yes |\n| Background details | Public only | Full | Full | Full |\n| Prior relationship | No | No | Yes | Yes |\n| Auto-create draft | No | No | No | Yes |\n\n---\n\n## Message Templates by Scenario\n\n### Cold Outreach (No Prior Relationship)\n\n```\nSubject: [Their initiative] + [your angle]\n\nHi [Name],\n\n[Personal hook based on research - news, content, mutual connection].\n\n[1 sentence on their likely challenge based on role/company].\n\n[Brief proof: \"We helped [Similar Company] achieve [Result]\".]\n\nWorth a 15-min call to see if relevant?\n\n[Signature]\n```\n\n### Warm Outreach (Have Met / Mutual Connection)\n\n```\nSubject: Following up from [context]\n\nHi [Name],\n\n[Reference to how you know them / who connected you].\n\n[Why reaching out now - their trigger].\n\n[Specific value you can offer].\n\n[CTA]\n```\n\n### Re-Engagement (Went Dark)\n\n```\nSubject: [Short, curiosity-driven]\n\nHi [Name],\n\n[Acknowledge time passed without being guilt-trippy].\n\n[New reason to reconnect - their news or your news].\n\n[Simple question to re-open dialogue].\n\n[Signature]\n```\n\n### Post-Event Follow-up\n\n```\nSubject: Great meeting you at [Event]\n\nHi [Name],\n\n[Specific memory from conversation].\n\n[Value-add: article, intro, resource related to what you discussed].\n\n[Soft CTA for next conversation].\n```\n\n---\n\n## Email Style Guidelines\n\n1. **Be concise but informative** — Get to the point quickly. Busy people skim.\n2. **No markdown formatting** — Never use asterisks, bold (**text**), or other markdown. Write plain text that looks natural in any email client.\n3. **Short paragraphs** — 2-3 sentences max per paragraph. White space is your friend.\n4. **Simple lists** — If listing items, use plain dashes. No fancy formatting.\n\n**Good:**\n```\nHere's what I can share:\n- Case study from a similar company\n- 15-min intro call this week\n- Quick demo if helpful\n```\n\n**Bad:**\n```\n**What I Can Offer:**\n- **Case study** from a similar company\n- **Intro call** this week\n```\n\n---\n\n## What NOT to Do\n\n**Generic openers:**\n- \"I hope this email finds you well\"\n- \"I'm reaching out because...\"\n- \"I wanted to introduce myself\"\n\n**Feature dumps:**\n- Long paragraphs about your product\n- Multiple value props at once\n- No clear CTA\n\n**Fake personalization:**\n- \"I noticed you work at [Company]\" (obviously)\n- \"Congrats on your role\" (without context)\n\n**Markdown in emails:**\n- Using **bold** or *italic* asterisks\n- Headers or formatted lists that won't render\n\n**Instead:**\n- Lead with something specific you learned\n- One clear value prop\n- One clear ask\n- Plain text formatting only\n\n---\n\n## Channel Selection\n\n```\nIF verified email available:\n  → Email preferred (higher response rate)\n  → Also provide LinkedIn backup\n\nIF no email:\n  → LinkedIn connection request\n  → Follow-up message template for after connection\n\nIF warm intro possible:\n  → Suggest mutual connection outreach first\n```\n\n---\n\n## Company Configuration [CUSTOMIZE]\n\n```markdown\n## Outreach Settings\n\n- My name: [Your Name]\n- My title: [Your Title]\n- My company: [Company Name]\n- Value prop: [One sentence - what you help with]\n\n## Signature\n[Your preferred email signature]\n\n## Proof Points\n- [Customer 1]: [Result]\n- [Customer 2]: [Result]\n- [Customer 3]: [Result]\n\n## CTA Options\n- Default: \"Worth a 15-min call?\"\n- Softer: \"Open to learning more?\"\n- Specific: \"Can I send over a quick demo?\"\n\n## Tone\n- [Professional / Casual / Direct]\n- Industry-specific language: [Yes/No]\n```\n\n---\n\n## Example\n\n**Input:** \"draft outreach to the Head of Engineering at Notion\"\n\n**Research finds:**\n- Name: David Tibbitts\n- Recent news: Notion launched AI features\n- Hiring: 5 ML engineer roles open\n- Background: Ex-Facebook, Stanford CS\n\n**Output:**\n\n```markdown\n# Outreach Draft: David Tibbitts @ Notion\n\n## Research Summary\n**Target:** David Tibbitts, Head of Engineering at Notion\n**Hook:** They're scaling AI features and hiring ML talent\n**Goal:** Intro call about AI infrastructure\n\n---\n\n## Email Draft\n\n**To:** david@notion.so\n**Subject:** Notion's AI scaling + a thought\n\n---\n\nHi David,\n\nSaw Notion's AI rollout is gaining serious traction - congrats.\nWith 5 ML roles open, seems like you're scaling fast.\n\nCurious how you're thinking about inference infrastructure\nas usage grows. We helped [Similar Company] cut their AI\nserving costs 40% while improving latency.\n\nWorth a 15-min call to see if relevant to your roadmap?\n\nBest,\n[Name]\n\n---\n\n**Subject Alternatives:**\n1. Notion AI + scaling question\n2. Quick thought on Notion's ML hiring\n\n---\n\n## Email Draft Status\nDraft created - check ~~email\n```\n", "depth": 4}, {"id": "98", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/create-an-asset/README.md", "name": "README.md", "type": "file", "content": "# Create an Asset\n\n**For Sales Teams Everywhere**\n\nGenerate professional, customer-ready sales assets in minutes. No design skills required.\n\n---\n\n## What It Does\n\nThis skill creates tailored sales assets by asking you about:\n1. **Your prospect** — who they are, what you've discussed\n2. **Your audience** — who's viewing, what they care about\n3. **Your purpose** — what you want to achieve\n4. **Your format** — how you want to present it\n\nThen it researches, writes, designs, and builds a polished asset you can share with customers.\n\n---\n\n## Supported Formats\n\n| Format | Best For | Output |\n|--------|----------|--------|\n| **Interactive Landing Page** | Exec meetings, value prop presentations | Multi-tab page with demos and calculators |\n| **Deck-Style** | Formal presentations, large audiences | Linear slides with navigation |\n| **One-Pager** | Leave-behinds, quick summaries | Single-scroll executive summary |\n| **Workflow / Architecture Demo** | Technical deep-dives, POC proposals | Interactive diagram with animated flow |\n\n---\n\n## Quick Start\n\n### Option 1: Simple prompt\n```\nCreate an asset for Acme Corp\n```\n\n### Option 2: With context\n```\nCreate an asset for Acme Corp. I met with their VP Engineering\nlast week - they're struggling with slow release cycles and\nwant to improve developer productivity. This is for a follow-up\nwith their technical team.\n```\n\n### Option 3: Workflow demo\n```\nI want to mock up a workflow showing how a customer would use\nour product to automate their invoice processing. The flow is:\ninvoices come in via email → our AI extracts data → validates\nagainst their ERP → flags exceptions for human review.\n```\n\n---\n\n## What Gets Created\n\n### Interactive Landing Page\n- Tabbed navigation\n- Company metrics and research\n- Use case demos\n- ROI calculator\n- Professional dark theme with prospect's brand colors\n\n### Deck-Style\n- Title slide with both logos\n- Agenda\n- Content slides (one message per slide)\n- Summary and next steps\n- Speaker notes included\n\n### One-Pager\n- Hero statement\n- 3 key value points\n- Proof point\n- Clear CTA\n\n### Workflow Demo\n- Visual component nodes\n- Animated data flow\n- Step-by-step walkthrough\n- Play/pause/step controls\n- Sample data at each stage\n\n---\n\n## The Process\n\n```\n1. You provide context (prospect, audience, purpose)\n          ↓\n2. Skill researches the prospect company\n          ↓\n3. Skill asks 3-4 clarifying questions\n          ↓\n4. You confirm direction\n          ↓\n5. Skill builds the asset\n          ↓\n6. You iterate as needed\n```\n\n---\n\n## Sharing Your Asset\n\nThe output is a self-contained HTML file. Share it by:\n\n- **Static hosting**: Upload to Netlify, Vercel, GitHub Pages, or any web host\n- **Password protect**: Most hosts offer simple password protection\n- **Direct share**: Email the HTML file — it works offline\n- **Embed**: iframe it into other pages or portals\n\n---\n\n## Tips for Best Results\n\n### Provide Rich Context\nThe more you share about past conversations, pain points, and stakeholder concerns, the more tailored the asset will be.\n\n### Upload Transcripts\nIf you have call recordings, meeting notes, or email threads, upload them. The skill will extract key quotes and priorities.\n\n### Be Specific About Audience\n\"Technical team\" is good. \"IT architects evaluating our security model\" is better.\n\n### Iterate Freely\nFirst draft not quite right? Just say what to change. Colors, sections, messaging, flow — all adjustable.\n\n---\n\n## Examples\n\n| Scenario | Format | Key Features |\n|----------|--------|--------------|\n| Post-discovery exec meeting | Interactive page | ROI calculator, their stated priorities, case studies |\n| Technical architecture review | Workflow demo | System diagram, data flows, integration points |\n| Board presentation leave-behind | One-pager | Strategic alignment, key metrics, clear CTA |\n| Large stakeholder meeting | Deck-style | Linear narrative, one point per slide, appendix |\n\n---\n\n## FAQ\n\n**Q: Does it work for any product/company?**\nA: Yes. The skill detects what you're selling from your email domain and researches accordingly.\n\n**Q: How does it know my prospect's brand colors?**\nA: It extracts them from the prospect's website or brand guidelines. You can adjust after.\n\n**Q: Can I use my company's branding instead?**\nA: Yes — after the first build, just ask to switch to your brand colors.\n\n**Q: What if the research is wrong?**\nA: Flag it and provide corrections. The skill will regenerate with accurate info.\n\n**Q: Can I export as PDF?**\nA: Yes — ask for a print-optimized version and use your browser's print-to-PDF.\n\n---\n\n## Support\n\nQuestions or feedback? This skill is part of the public sales skills collection.\n\n---\n\n*Built for salespeople who'd rather sell than design slides.*\n", "depth": 4}, {"id": "99", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/create-an-asset/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: create-an-asset\ndescription: Generate tailored sales assets (landing pages, decks, one-pagers, workflow demos) from your deal context. Describe your prospect, audience, and goal — get a polished, branded asset ready to share with customers.\n---\n\n# Create an Asset\n\nGenerate custom sales assets tailored to your prospect, audience, and goals. Supports interactive landing pages, presentation decks, executive one-pagers, and workflow/architecture demos.\n\n---\n\n## Triggers\n\nInvoke this skill when:\n- User says `/create-an-asset` or `/create-an-asset [CompanyName]`\n- User asks to \"create an asset\", \"build a demo\", \"make a landing page\", \"mock up a workflow\"\n- User needs a customer-facing deliverable for a sales conversation\n\n---\n\n## Overview\n\nThis skill creates professional sales assets by gathering context about:\n- **(a) The Prospect** — company, contacts, conversations, pain points\n- **(b) The Audience** — who's viewing, what they care about\n- **(c) The Purpose** — goal of the asset, desired next action\n- **(d) The Format** — landing page, deck, one-pager, or workflow demo\n\nThe skill then researches, structures, and builds a polished, branded asset ready to share with customers.\n\n---\n\n## Phase 0: Context Detection \u0026 Input Collection\n\n### Step 0.1: Detect Seller Context\n\nFrom the user's email domain, identify what company they work for.\n\n**Actions:**\n1. Extract domain from user's email\n2. Search: `\"[domain]\" company products services site:linkedin.com OR site:crunchbase.com`\n3. Determine seller context:\n\n| Scenario | Action |\n|----------|--------|\n| **Single-product company** | Auto-populate seller context |\n| **Multi-product company** | Ask: \"Which product or solution is this asset for?\" |\n| **Consultant/agency/generic domain** | Ask: \"What company or product are you representing?\" |\n| **Unknown/startup** | Ask: \"Briefly, what are you selling?\" |\n\n**Store seller context:**\n```yaml\nseller:\n  company: \"[Company Name]\"\n  product: \"[Product/Service]\"\n  value_props:\n    - \"[Key value prop 1]\"\n    - \"[Key value prop 2]\"\n    - \"[Key value prop 3]\"\n  differentiators:\n    - \"[Differentiator 1]\"\n    - \"[Differentiator 2]\"\n  pricing_model: \"[If publicly known]\"\n```\n\n**Persist to knowledge base** for future sessions. On subsequent invocations, confirm: \"I have your seller context from last time — still selling [Product] at [Company]?\"\n\n---\n\n### Step 0.2: Collect Prospect Context (a)\n\n**Ask the user:**\n\n| Field | Prompt | Required |\n|-------|--------|----------|\n| **Company** | \"Which company is this asset for?\" | ✓ Yes |\n| **Key contacts** | \"Who are the key contacts? (names, roles)\" | No |\n| **Deal stage** | \"What stage is this deal?\" | ✓ Yes |\n| **Pain points** | \"What pain points or priorities have they shared?\" | No |\n| **Past materials** | \"Upload any conversation materials (transcripts, emails, notes, call recordings)\" | No |\n\n**Deal stage options:**\n- Intro / First meeting\n- Discovery\n- Evaluation / Technical review\n- POC / Pilot\n- Negotiation\n- Close\n\n---\n\n### Step 0.3: Collect Audience Context (b)\n\n**Ask the user:**\n\n| Field | Prompt | Required |\n|-------|--------|----------|\n| **Audience type** | \"Who's viewing this?\" | ✓ Yes |\n| **Specific roles** | \"Any specific titles to tailor for? (e.g., CTO, VP Engineering, CFO)\" | No |\n| **Primary concern** | \"What do they care most about?\" | ✓ Yes |\n| **Objections** | \"Any concerns or objections to address?\" | No |\n\n**Audience type options:**\n- Executive (C-suite, VPs)\n- Technical (Architects, Engineers, Developers)\n- Operations (Ops, IT, Procurement)\n- Mixed / Cross-functional\n\n**Primary concern options:**\n- ROI / Business impact\n- Technical depth / Architecture\n- Strategic alignment\n- Risk mitigation / Security\n- Implementation / Timeline\n\n---\n\n### Step 0.4: Collect Purpose Context (c)\n\n**Ask the user:**\n\n| Field | Prompt | Required |\n|-------|--------|----------|\n| **Goal** | \"What's the goal of this asset?\" | ✓ Yes |\n| **Desired action** | \"What should the viewer do after seeing this?\" | ✓ Yes |\n\n**Goal options:**\n- Intro / First impression\n- Discovery follow-up\n- Technical deep-dive\n- Executive alignment / Business case\n- POC proposal\n- Deal close\n\n---\n\n### Step 0.5: Select Format (d)\n\n**Ask the user:** \"What format works best for this?\"\n\n| Format | Description | Best For |\n|--------|-------------|----------|\n| **Interactive landing page** | Multi-tab page with demos, metrics, calculators | Exec alignment, intros, value prop |\n| **Deck-style** | Linear slides, presentation-ready | Formal meetings, large audiences |\n| **One-pager** | Single-scroll executive summary | Leave-behinds, quick summaries |\n| **Workflow / Architecture demo** | Interactive diagram with animated flow | Technical deep-dives, POC demos, integrations |\n\n---\n\n### Step 0.6: Format-Specific Inputs\n\n#### If \"Workflow / Architecture demo\" selected:\n\n**First, parse from user's description.** Look for:\n- Systems and components mentioned\n- Data flows described\n- Human interaction points\n- Example scenarios\n\n**Then ask for any gaps:**\n\n| If Missing... | Ask... |\n|---------------|--------|\n| Components unclear | \"What systems or components are involved? (databases, APIs, AI, middleware, etc.)\" |\n| Flow unclear | \"Walk me through the step-by-step flow\" |\n| Human touchpoints unclear | \"Where does a human interact in this workflow?\" |\n| Scenario vague | \"What's a concrete example scenario to demo?\" |\n| Integration specifics | \"Any specific tools or platforms to highlight?\" |\n\n---\n\n## Phase 1: Research (Adaptive)\n\n### Assess Context Richness\n\n| Level | Indicators | Research Depth |\n|-------|------------|----------------|\n| **Rich** | Transcripts uploaded, detailed pain points, clear requirements | Light — fill gaps only |\n| **Moderate** | Some context, no transcripts | Medium — company + industry |\n| **Sparse** | Just company name | Deep — full research pass |\n\n### Always Research:\n\n1. **Prospect basics**\n   - Search: `\"[Company]\" annual report investor presentation 2025 2026`\n   - Search: `\"[Company]\" CEO strategy priorities 2025 2026`\n   - Extract: Revenue, employees, key metrics, strategic priorities\n\n2. **Leadership**\n   - Search: `\"[Company]\" CEO CTO CIO 2025`\n   - Extract: Names, titles, recent quotes on strategy/technology\n\n3. **Brand colors**\n   - Search: `\"[Company]\" brand guidelines`\n   - Or extract from company website\n   - Store: Primary color, secondary color, accent\n\n### If Moderate/Sparse Context, Also Research:\n\n4. **Industry context**\n   - Search: `\"[Industry]\" trends challenges 2025 2026`\n   - Extract: Common pain points, market dynamics\n\n5. **Technology landscape**\n   - Search: `\"[Company]\" technology stack tools platforms`\n   - Extract: Current solutions, potential integration points\n\n6. **Competitive context**\n   - Search: `\"[Company]\" vs [seller's competitors]`\n   - Extract: Current solutions, switching signals\n\n### If Transcripts/Materials Uploaded:\n\n7. **Conversation analysis**\n   - Extract: Stated pain points, decision criteria, objections, timeline\n   - Identify: Key quotes to reference (use their exact language)\n   - Note: Specific terminology, acronyms, internal project names\n\n---\n\n## Phase 2: Structure Decision\n\n### Interactive Landing Page\n\n| Purpose | Recommended Sections |\n|---------|---------------------|\n| **Intro** | Company Fit → Solution Overview → Key Use Cases → Why Us → Next Steps |\n| **Discovery follow-up** | Their Priorities → How We Help → Relevant Examples → ROI Framework → Next Steps |\n| **Technical deep-dive** | Architecture → Security \u0026 Compliance → Integration → Performance → Support |\n| **Exec alignment** | Strategic Fit → Business Impact → ROI Calculator → Risk Mitigation → Partnership |\n| **POC proposal** | Scope → Success Criteria → Timeline → Team → Investment → Next Steps |\n| **Deal close** | Value Summary → Pricing → Implementation Plan → Terms → Sign-off |\n\n**Audience adjustments:**\n- **Executive**: Lead with business impact, ROI, strategic alignment\n- **Technical**: Lead with architecture, security, integration depth\n- **Operations**: Lead with workflow impact, change management, support\n- **Mixed**: Balance strategic + tactical; use tabs to separate depth levels\n\n---\n\n### Deck-Style\n\nSame sections as landing page, formatted as linear slides:\n\n```\n1. Title slide (Prospect + Seller logos, partnership framing)\n2. Agenda\n3-N. One section per slide (or 2-3 slides for dense sections)\nN+1. Summary / Key takeaways\nN+2. Next steps / CTA\nN+3. Appendix (optional — detailed specs, pricing, etc.)\n```\n\n**Slide principles:**\n- One key message per slide\n- Visual \u003e text-heavy\n- Use prospect's metrics and language\n- Include speaker notes\n\n---\n\n### One-Pager\n\nCondense to single-scroll format:\n\n```\n┌─────────────────────────────────────┐\n│ HERO: \"[Prospect Goal] with [Product]\" │\n├─────────────────────────────────────┤\n│ KEY POINT 1     │ KEY POINT 2     │ KEY POINT 3     │\n│ [Icon + 2-3     │ [Icon + 2-3     │ [Icon + 2-3     │\n│  sentences]     │  sentences]     │  sentences]     │\n├─────────────────────────────────────┤\n│ PROOF POINT: [Metric, quote, or case study] │\n├─────────────────────────────────────┤\n│ CTA: [Clear next action] │ [Contact info] │\n└─────────────────────────────────────┘\n```\n\n---\n\n### Workflow / Architecture Demo\n\n**Structure based on complexity:**\n\n| Complexity | Components | Structure |\n|------------|------------|-----------|\n| **Simple** | 3-5 | Single-view diagram with step annotations |\n| **Medium** | 5-10 | Zoomable canvas with step-by-step walkthrough |\n| **Complex** | 10+ | Multi-layer view (overview → detailed) with guided tour |\n\n**Standard elements:**\n\n1. **Title bar**: `[Scenario Name] — Powered by [Seller Product]`\n2. **Component nodes**: Visual boxes/icons for each system\n3. **Flow arrows**: Animated connections showing data movement\n4. **Step panel**: Sidebar explaining current step in plain language\n5. **Controls**: Play / Pause / Step Forward / Step Back / Reset\n6. **Annotations**: Callouts for key decision points and value-adds\n7. **Data preview**: Sample payloads or transformations at each step\n\n---\n\n## Phase 3: Content Generation\n\n### General Principles\n\nAll content should:\n- Reference **specific pain points** from user input or transcripts\n- Use **prospect's language** — their terminology, their stated priorities\n- Map **seller's product** → **prospect's needs** explicitly\n- Include **proof points** where available (case studies, metrics, quotes)\n- Feel **tailored, not templated**\n\n---\n\n### Section Templates\n\n#### Hero / Intro\n```\nHeadline: \"[Prospect's Goal] with [Seller's Product]\"\nSubhead: Tie to their stated priority or top industry challenge\nMetrics: 3-4 key facts about the prospect (shows we did homework)\n```\n\n#### Their Priorities (if discovery follow-up)\n```\nReference specific pain points from conversation:\n- Use their exact words where possible\n- Show we listened and understood\n- Connect each to how we help\n```\n\n#### Solution Mapping\n```\nFor each pain point:\n├── The challenge (in their words)\n├── How [Product] addresses it\n├── Proof point or example\n└── Outcome / benefit\n```\n\n#### Use Cases / Demos\n```\n3-5 relevant use cases:\n├── Visual mockup or interactive demo\n├── Business impact (quantified if possible)\n├── \"How it works\" — 3-4 step summary\n└── Relevant to their industry/role\n```\n\n#### ROI / Business Case\n```\nInteractive calculator with:\n├── Inputs relevant to their business (from research)\n│   ├── Number of users/developers\n│   ├── Current costs or time spent\n│   └── Expected improvement %\n├── Outputs:\n│   ├── Annual value / savings\n│   ├── Cost of solution\n│   ├── Net ROI\n│   └── Payback period\n└── Assumptions clearly stated (editable)\n```\n\n#### Why Us / Differentiators\n```\n├── Differentiators vs. alternatives they might consider\n├── Trust, security, compliance positioning\n├── Support and partnership model\n└── Customer proof points (logos, quotes, case studies)\n```\n\n#### Next Steps / CTA\n```\n├── Clear action aligned to Purpose (c)\n├── Specific next step (not vague \"let's chat\")\n├── Contact information\n├── Suggested timeline\n└── What happens after they take action\n```\n\n---\n\n### Workflow Demo Content\n\n#### Component Definitions\n\nFor each system, define:\n\n```yaml\ncomponent:\n  id: \"snowflake\"\n  label: \"Snowflake Data Warehouse\"\n  type: \"database\"  # database | api | ai | middleware | human | document | output\n  icon: \"database\"\n  description: \"Financial performance data\"\n  brand_color: \"#29B5E8\"\n```\n\n**Component types:**\n- `human` — Person initiating or receiving\n- `document` — PDFs, contracts, files\n- `ai` — AI/ML models, agents\n- `database` — Data stores, warehouses\n- `api` — APIs, services\n- `middleware` — Integration platforms, MCP servers\n- `output` — Dashboards, reports, notifications\n\n#### Flow Steps\n\nFor each step, define:\n\n```yaml\nstep:\n  number: 1\n  from: \"human\"\n  to: \"claude\"\n  action: \"Initiates performance review\"\n  description: \"Sarah, a Brand Analyst at [Prospect], kicks off the quarterly review...\"\n  data_example: \"Review request: Nike brand, Q4 2025\"\n  duration: \"~1 second\"\n  value_note: \"No manual data gathering required\"\n```\n\n#### Scenario Narrative\n\nWrite a clear, specific walkthrough:\n\n```\nStep 1: Human Trigger\n\"Sarah, a Brand Performance Analyst at Centric Brands, needs to review\nQ4 performance for the Nike license agreement. She opens the review\ndashboard and clicks 'Start Review'...\"\n\nStep 2: Contract Analysis\n\"Claude retrieves the Nike contract PDF and extracts the performance\nobligations: minimum $50M revenue, 12% margin requirement, quarterly\nreporting deadline...\"\n\nStep 3: Data Query\n\"Claude formulates a query and sends it to Workato DataGenie:\n'Get Q4 2025 revenue and gross margin for Nike brand from Snowflake'...\"\n\nStep 4: Results \u0026 Synthesis\n\"Snowflake returns the data. Claude compares actuals vs. obligations:\nRevenue $52.3M ✓ (exceeded by $2.3M)\nMargin 11.2% ⚠️ (0.8% below threshold)...\"\n\nStep 5: Insight Delivery\n\"Claude synthesizes findings into an executive summary with\nrecommendations: 'Review promotional spend allocation to improve\nmargin performance...'\"\n```\n\n---\n\n## Phase 4: Visual Design\n\n### Color System\n\n```css\n:root {\n    /* === Prospect Brand (Primary) === */\n    --brand-primary: #[extracted from research];\n    --brand-secondary: #[extracted];\n    --brand-primary-rgb: [r, g, b]; /* For rgba() usage */\n\n    /* === Dark Theme Base === */\n    --bg-primary: #0a0d14;\n    --bg-elevated: #0f131c;\n    --bg-surface: #161b28;\n    --bg-hover: #1e2536;\n\n    /* === Text === */\n    --text-primary: #ffffff;\n    --text-secondary: rgba(255, 255, 255, 0.7);\n    --text-muted: rgba(255, 255, 255, 0.5);\n\n    /* === Accent === */\n    --accent: var(--brand-primary);\n    --accent-hover: var(--brand-secondary);\n    --accent-glow: rgba(var(--brand-primary-rgb), 0.3);\n\n    /* === Status === */\n    --success: #10b981;\n    --warning: #f59e0b;\n    --error: #ef4444;\n}\n```\n\n### Typography\n\n```css\n/* Primary: Clean, professional sans-serif */\nfont-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n\n/* Headings */\nh1: 2.5rem, font-weight: 700\nh2: 1.75rem, font-weight: 600\nh3: 1.25rem, font-weight: 600\n\n/* Body */\nbody: 1rem, font-weight: 400, line-height: 1.6\n\n/* Captions/Labels */\nsmall: 0.875rem, font-weight: 500\n```\n\n### Visual Elements\n\n**Cards:**\n- Background: `var(--bg-surface)`\n- Border: 1px solid rgba(255,255,255,0.1)\n- Border-radius: 12px\n- Box-shadow: subtle, layered\n- Hover: slight elevation, border glow\n\n**Buttons:**\n- Primary: `var(--accent)` background, white text\n- Secondary: transparent, accent border\n- Hover: brightness increase, subtle scale\n\n**Animations:**\n- Transitions: 200-300ms ease\n- Tab switches: fade + slide\n- Hover states: smooth, not jarring\n- Loading: subtle pulse or skeleton\n\n### Workflow Demo Specific\n\n**Component Nodes:**\n```css\n.node {\n    background: var(--bg-surface);\n    border: 2px solid var(--brand-primary);\n    border-radius: 12px;\n    padding: 16px;\n    min-width: 140px;\n}\n\n.node.active {\n    box-shadow: 0 0 20px var(--accent-glow);\n    border-color: var(--accent);\n}\n\n.node.human {\n    border-color: #f59e0b; /* Warm color for humans */\n}\n\n.node.ai {\n    background: linear-gradient(135deg, var(--bg-surface), var(--bg-elevated));\n    border-color: var(--accent);\n}\n```\n\n**Flow Arrows:**\n```css\n.arrow {\n    stroke: var(--text-muted);\n    stroke-width: 2;\n    fill: none;\n    marker-end: url(#arrowhead);\n}\n\n.arrow.active {\n    stroke: var(--accent);\n    stroke-dasharray: 8 4;\n    animation: flowDash 1s linear infinite;\n}\n```\n\n**Canvas:**\n```css\n.canvas {\n    background:\n        radial-gradient(circle at center, var(--bg-elevated) 0%, var(--bg-primary) 100%),\n        url(\"data:image/svg+xml,...\"); /* Subtle grid pattern */\n    overflow: auto;\n}\n```\n\n---\n\n## Phase 5: Clarifying Questions (REQUIRED)\n\n**Before building any asset, always ask clarifying questions.** This ensures alignment and prevents wasted effort.\n\n### Step 5.1: Summarize Understanding\n\nFirst, show the user what you understood:\n\n```\n\"Here's what I'm planning to build:\n\n**Asset**: [Format] for [Prospect Company]\n**Audience**: [Audience type] — specifically [roles if known]\n**Goal**: [Purpose] → driving toward [desired action]\n**Key themes**: [2-3 main points to emphasize]\n\n[For workflow demos, also show:]\n**Components**: [List of systems]\n**Flow**: [Step 1] → [Step 2] → [Step 3] → ...\n```\n\n### Step 5.2: Ask Standard Questions (ALL formats)\n\n| Question | Why |\n|----------|-----|\n| \"Does this match your vision?\" | Confirm understanding |\n| \"What's the ONE thing this must nail to succeed?\" | Focus on priority |\n| \"Tone preference? (Bold \u0026 confident / Consultative / Technical \u0026 precise)\" | Style alignment |\n| \"Focused and concise, or comprehensive?\" | Scope calibration |\n\n### Step 5.3: Ask Format-Specific Questions\n\n#### Interactive Landing Page:\n- \"Which sections matter most for this audience?\"\n- \"Any specific demos or use cases to highlight?\"\n- \"Should I include an ROI calculator?\"\n- \"Any competitor positioning to address?\"\n\n#### Deck-Style:\n- \"How long is the presentation? (helps with slide count)\"\n- \"Presenting live, or a leave-behind?\"\n- \"Any specific flow or narrative arc in mind?\"\n\n#### One-Pager:\n- \"What's the single most important message?\"\n- \"Any specific proof point or stat to feature?\"\n- \"Will this be printed or digital?\"\n\n#### Workflow / Architecture Demo:\n- \"Let me confirm the components: [list]. Anything missing?\"\n- \"Here's the flow I understood: [steps]. Correct?\"\n- \"Should the demo show realistic sample data, or keep it abstract?\"\n- \"Any integration details to highlight or downplay?\"\n- \"Should viewers be able to click through steps, or auto-play?\"\n\n### Step 5.4: Confirm and Proceed\n\nAfter user responds:\n\n```\n\"Got it. I have what I need. Building your [format] now...\"\n```\n\nOr, if still unclear:\n\n```\n\"One more quick question: [specific follow-up]\"\n```\n\n**Max 2 rounds of questions.** If still ambiguous, make a reasonable choice and note: \"I went with X — easy to adjust if you prefer Y.\"\n\n---\n\n## Phase 6: Build \u0026 Deliver\n\n### Build the Asset\n\nFollowing all specifications above:\n1. Generate structure based on Phase 2\n2. Create content based on Phase 3\n3. Apply visual design based on Phase 4\n4. Ensure all interactive elements work\n5. Test responsiveness (if applicable)\n\n### Output Format\n\n**All formats**: Self-contained HTML file\n- All CSS inline or in `\u003cstyle\u003e` tags\n- All JS inline or in `\u003cscript\u003e` tags\n- No external dependencies (except Google Fonts)\n- Single file for easy sharing\n\n**File naming**: `[ProspectName]-[format]-[date].html`\n- Example: `CentricBrands-workflow-demo-2026-01-28.html`\n\n### Delivery Message\n\n```markdown\n## ✓ Asset Created: [Prospect Name]\n\n[View your asset](computer:///path/to/file.html)\n\n---\n\n**Summary**\n- **Format**: [Interactive Page / Deck / One-Pager / Workflow Demo]\n- **Audience**: [Type and roles]\n- **Purpose**: [Goal] → [Desired action]\n- **Sections/Steps**: [Count and list]\n\n---\n\n**Deployment Options**\n\nTo share this with your customer:\n- **Static hosting**: Upload to Netlify, Vercel, GitHub Pages, AWS S3, or any static host\n- **Password protection**: Most hosts offer this (e.g., Netlify site protection)\n- **Direct share**: Send the HTML file directly — it's fully self-contained\n- **Embed**: The file can be iframed into other pages if needed\n\n---\n\n**Customization**\n\nLet me know if you'd like to:\n- Adjust colors or styling\n- Add, remove, or reorder sections\n- Refine any messaging or copy\n- Change the flow or architecture (for workflow demos)\n- Add more interactive elements\n- Export as PDF or static images\n```\n\n---\n\n## Phase 7: Iteration Support\n\nAfter delivery, be ready to iterate:\n\n| User Request | Action |\n|--------------|--------|\n| \"Change the colors\" | Regenerate with new palette, keep content |\n| \"Add a section on X\" | Insert new section, maintain flow |\n| \"Make it shorter\" | Condense, prioritize key points |\n| \"The flow is wrong\" | Rebuild architecture based on correction |\n| \"Use our brand instead\" | Switch from prospect brand to seller brand |\n| \"Add more detail on step 3\" | Expand that section specifically |\n| \"Can I get this as a PDF?\" | Provide print-optimized version |\n\n**Remember**: Default to prospect's brand colors, but seller can adjust to their own brand or a neutral palette after initial build.\n\n---\n\n## Quality Checklist\n\nBefore delivering, verify:\n\n### Content\n- [ ] Prospect company name spelled correctly throughout\n- [ ] Leadership names are current (not outdated)\n- [ ] Pain points accurately reflect input/transcripts\n- [ ] Seller's product accurately represented\n- [ ] No placeholder text remaining\n- [ ] Proof points are accurate and sourced\n\n### Visual\n- [ ] Brand colors applied correctly\n- [ ] All text readable (contrast)\n- [ ] Animations smooth, not distracting\n- [ ] Mobile responsive (if interactive page)\n- [ ] Dark theme looks polished\n\n### Functional\n- [ ] All tabs/sections load correctly\n- [ ] Interactive elements work (calculators, demos)\n- [ ] Workflow steps animate properly (if applicable)\n- [ ] Navigation is intuitive\n- [ ] CTA is clear and clickable\n\n### Professional\n- [ ] Tone matches audience\n- [ ] Appropriate level of detail for purpose\n- [ ] No typos or grammatical errors\n- [ ] Feels tailored, not templated\n\n---\n\n## Examples\n\n### Example 1: Executive Landing Page\n\n**Input:**\n- Prospect: Acme Corp (manufacturing)\n- Audience: C-suite\n- Purpose: Exec alignment after discovery\n- Format: Interactive landing page\n\n**Output structure:**\n```\n[Tabs]\nStrategic Fit | Business Impact | ROI Calculator | Security \u0026 Trust | Next Steps\n\n[Strategic Fit tab]\n- Acme's stated priorities (from discovery call)\n- How [Product] aligns\n- Relevant manufacturing customers\n```\n\n### Example 2: Technical Workflow Demo\n\n**Input:**\n- Prospect: Centric Brands\n- Audience: IT architects\n- Purpose: POC proposal\n- Format: Workflow demo\n- Components: Claude, Workato DataGenie, Snowflake, PDF contracts\n\n**Output structure:**\n```\n[Interactive canvas with 5 nodes]\nHuman → Claude → PDF Contracts → Workato → Snowflake\n         ↓\n    [Results back to Human]\n\n[Step-by-step walkthrough with sample data]\n[Controls: Play | Pause | Step | Reset]\n```\n\n### Example 3: Sales One-Pager\n\n**Input:**\n- Prospect: TechStart Inc\n- Audience: VP Engineering\n- Purpose: Leave-behind after first meeting\n- Format: One-pager\n\n**Output structure:**\n```\nHero: \"Accelerate TechStart's Product Velocity\"\nPoint 1: [Dev productivity]\nPoint 2: [Code quality]\nPoint 3: [Time to market]\nProof: \"Similar companies saw 40% faster releases\"\nCTA: \"Schedule technical deep-dive\"\n```\n\n---\n\n## Appendix: Component Icons\n\nFor workflow demos, use these icon mappings:\n\n| Type | Icon | Example |\n|------|------|---------|\n| human | 👤 or person SVG | User, Analyst, Admin |\n| document | 📄 or file SVG | PDF, Contract, Report |\n| ai | 🤖 or brain SVG | Claude, AI Agent |\n| database | 🗄️ or cylinder SVG | Snowflake, Postgres |\n| api | 🔌 or plug SVG | REST API, GraphQL |\n| middleware | ⚡ or hub SVG | Workato, MCP Server |\n| output | 📊 or screen SVG | Dashboard, Report |\n\n---\n\n## Appendix: Brand Color Fallbacks\n\nIf brand colors cannot be extracted:\n\n| Industry | Primary | Secondary |\n|----------|---------|-----------|\n| Technology | #2563eb | #7c3aed |\n| Finance | #0f172a | #3b82f6 |\n| Healthcare | #0891b2 | #06b6d4 |\n| Manufacturing | #ea580c | #f97316 |\n| Retail | #db2777 | #ec4899 |\n| Energy | #16a34a | #22c55e |\n| Default | #3b82f6 | #8b5cf6 |\n\n---\n\n*Skill created for generalized sales asset generation. Works for any seller, any product, any prospect.*\n", "depth": 4}, {"id": "100", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/create-an-asset/QUICKREF.md", "name": "QUICKREF.md", "type": "file", "content": "# Create an Asset — Quick Reference\n\n## Invoke\n```\n/create-an-asset\n/create-an-asset [CompanyName]\n\"Create an asset for [Company]\"\n```\n\n---\n\n## Inputs at a Glance\n\n| Input | What to Provide |\n|-------|-----------------|\n| **(a) Prospect** | Company, contacts, deal stage, pain points, transcripts |\n| **(b) Audience** | Exec / Technical / Ops / Mixed + what they care about |\n| **(c) Purpose** | Intro / Follow-up / Deep-dive / Alignment / POC / Close |\n| **(d) Format** | Landing page / Deck / One-pager / Workflow demo |\n\n---\n\n## Format Picker\n\n| If you need... | Choose... |\n|----------------|-----------|\n| Impressive multi-tab experience | **Interactive landing page** |\n| Something to present in a meeting | **Deck-style** |\n| Quick summary to leave behind | **One-pager** |\n| Visual of how systems connect | **Workflow demo** |\n\n---\n\n## Sample Prompts\n\n**Basic:**\n```\nCreate an asset for Acme Corp\n```\n\n**With context:**\n```\nCreate an asset for Acme Corp. They're a manufacturing company\nstruggling with supply chain visibility. Met with their COO\nlast week. Need something for the exec team.\n```\n\n**Workflow demo:**\n```\nMock up a workflow for Centric Brands showing how they'd use\nour product to monitor contract compliance. Components: our AI,\ntheir Snowflake warehouse, and scanned PDF contracts.\n```\n\n---\n\n## After It's Built\n\n| Want to... | Say... |\n|------------|--------|\n| Change colors | \"Use our brand colors instead\" |\n| Add a section | \"Add a section on security\" |\n| Shorten it | \"Make it more concise\" |\n| Fix something | \"The CEO's name is wrong, it's Jane Smith\" |\n| Get PDF | \"Give me a print-friendly version\" |\n\n---\n\n## Output\n\n- Self-contained HTML file\n- Works offline\n- Host anywhere (Netlify, Vercel, GitHub Pages, etc.)\n- Password-protect via your hosting provider\n\n---\n\n*That's it. Provide context → answer questions → get asset → iterate.*\n", "depth": 4}, {"id": "101", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/daily-briefing/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: daily-briefing\ndescription: Start your day with a prioritized sales briefing. Works standalone when you tell me your meetings and priorities, supercharged when you connect your calendar, CRM, and email. Trigger with \"morning briefing\", \"daily brief\", \"what's on my plate today\", \"prep my day\", or \"start my day\".\n---\n\n# Daily Sales Briefing\n\nGet a clear view of what matters most today. This skill works with whatever you tell me, and gets richer when you connect your tools.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                      DAILY BRIEFING                              │\n├─────────────────────────────────────────────────────────────────┤\n│  ALWAYS (works standalone)                                       │\n│  ✓ You tell me: today's meetings, key deals, priorities         │\n│  ✓ I organize: prioritized action plan for your day             │\n│  ✓ Output: scannable 2-minute briefing                          │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Calendar: auto-pull today's meetings with attendees          │\n│  + CRM: pipeline alerts, tasks, deal health                     │\n│  + Email: unread from key accounts, waiting on replies          │\n│  + Enrichment: overnight signals on your accounts               │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Getting Started\n\nWhen you run this skill, I'll ask for what I need:\n\n**If no calendar connected:**\n\u003e \"What meetings do you have today? (Just paste your calendar or list them)\"\n\n**If no CRM connected:**\n\u003e \"What deals are you focused on this week? Any that need attention?\"\n\n**If you have connectors:**\nI'll pull everything automatically and just show you the briefing.\n\n---\n\n## Connectors (Optional)\n\nConnect your tools to supercharge this skill:\n\n| Connector | What It Adds |\n|-----------|--------------|\n| **Calendar** | Today's meetings with attendees, times, and context |\n| **CRM** | Open pipeline, deals closing soon, overdue tasks, stale deals |\n| **Email** | Unread from opportunity contacts, emails waiting on replies |\n| **Enrichment** | Overnight signals: funding, hiring, news on your accounts |\n\n\u003e **No connectors?** No problem. Tell me your meetings and deals, and I'll create your briefing.\n\n---\n\n## Output Format\n\n```markdown\n# Daily Briefing | [Day, Month Date]\n\n---\n\n## #1 Priority\n\n**[Most important thing to do today]**\n[Why it matters and what to do about it]\n\n---\n\n## Today's Numbers\n\n| Open Pipeline | Closing This Month | Meetings Today | Action Items |\n|---------------|-------------------|----------------|--------------|\n| $[X] | $[X] | [N] | [N] |\n\n---\n\n## Today's Meetings\n\n### [Time] — [Company] ([Meeting Type])\n**Attendees:** [Names]\n**Context:** [One-line: deal status, last touch, what's at stake]\n**Prep:** [Quick action before this meeting]\n\n### [Time] — [Company] ([Meeting Type])\n**Attendees:** [Names]\n**Context:** [One-line context]\n**Prep:** [Quick action]\n\n*Run `call-prep [company]` for detailed meeting prep*\n\n---\n\n## Pipeline Alerts\n\n### Needs Attention\n| Deal | Stage | Amount | Alert | Action |\n|------|-------|--------|-------|--------|\n| [Deal] | [Stage] | $[X] | [Why flagged] | [What to do] |\n\n### Closing This Week\n| Deal | Close Date | Amount | Confidence | Blocker |\n|------|------------|--------|------------|---------|\n| [Deal] | [Date] | $[X] | [H/M/L] | [If any] |\n\n---\n\n## Email Priorities\n\n### Needs Response\n| From | Subject | Received |\n|------|---------|----------|\n| [Name @ Company] | [Subject] | [Time] |\n\n### Waiting On Reply\n| To | Subject | Sent | Days Waiting |\n|----|---------|------|--------------|\n| [Name @ Company] | [Subject] | [Date] | [N] |\n\n---\n\n## Suggested Actions\n\n1. **[Action]** — [Why now]\n2. **[Action]** — [Why now]\n3. **[Action]** — [Why now]\n\n---\n\n*Run `call-prep [company]` before your meetings*\n*Run `call-follow-up` after each call*\n```\n\n---\n\n## Execution Flow\n\n### Step 1: Gather Context\n\n**If connectors available:**\n```\n1. Calendar → Get today's events\n   - Filter to external meetings (non-company attendees)\n   - Pull: time, title, attendees, description\n\n2. CRM → Query your pipeline\n   - Open opportunities owned by you\n   - Flag: closing this week, no activity 7+ days, slipped dates\n   - Get: overdue tasks, upcoming tasks\n\n3. Email → Check priority messages\n   - Unread from opportunity contact domains\n   - Sent messages with no reply (3+ days)\n\n4. Enrichment → Check signals (if available)\n   - Funding, hiring, news on open accounts\n```\n\n**If no connectors:**\n```\nAsk user:\n1. \"What meetings do you have today?\"\n2. \"What deals are you focused on? Any closing soon or needing attention?\"\n3. \"Anything urgent I should know about?\"\n\nWork with whatever they provide.\n```\n\n### Step 2: Prioritize\n\n```\nPriority ranking:\n1. URGENT: Deal closing today/tomorrow not yet won\n2. HIGH: Meeting today with high-value opportunity\n3. HIGH: Unread email from decision-maker\n4. MEDIUM: Deal closing this week\n5. MEDIUM: Stale deal (7+ days no activity)\n6. LOW: Tasks due this week\n\nSelect #1 Priority:\n- If meeting with \u003e$50K deal today → prep that\n- If deal closing today → focus on close\n- If urgent email from buyer → respond first\n- Else → highest-value stale deal\n```\n\n### Step 3: Generate Briefing\n\n```\nAssemble sections based on available data:\n\n1. #1 Priority — Always include (even if simple)\n2. Today's Numbers — If CRM connected, otherwise skip\n3. Today's Meetings — From calendar or user input\n4. Pipeline Alerts — If CRM connected\n5. Email Priorities — If email connected\n6. Suggested Actions — Always include top 3 actions\n```\n\n---\n\n## Quick Mode\n\nSay \"quick brief\" or \"tldr my day\" for abbreviated version:\n\n```markdown\n# Quick Brief | [Date]\n\n**#1:** [Priority action]\n\n**Meetings:** [N] — [Company 1], [Company 2], [Company 3]\n\n**Alerts:**\n- [Alert 1]\n- [Alert 2]\n\n**Do Now:** [Single most important action]\n```\n\n---\n\n## End of Day Mode\n\nSay \"wrap up my day\" or \"end of day summary\" after your last meeting:\n\n```markdown\n# End of Day | [Date]\n\n**Completed:**\n- [Meeting 1] — [Outcome]\n- [Meeting 2] — [Outcome]\n\n**Pipeline Changes:**\n- [Deal] moved to [Stage]\n\n**Tomorrow's Focus:**\n- [Priority 1]\n- [Priority 2]\n\n**Open Loops:**\n- [ ] [Unfinished item needing follow-up]\n```\n\n---\n\n## Tips\n\n1. **Connect your calendar first** — Biggest time saver\n2. **Add CRM second** — Unlocks pipeline alerts\n3. **Even without connectors** — Just tell me your meetings and I'll help prioritize\n\n---\n\n## Related Skills\n\n- **call-prep** — Deep prep for any specific meeting\n- **call-follow-up** — Process notes after calls\n- **account-research** — Research a company before first meeting\n", "depth": 4}, {"id": "102", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/account-research/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: account-research\ndescription: Research a company or person and get actionable sales intel. Works standalone with web search, supercharged when you connect enrichment tools or your CRM. Trigger with \"research [company]\", \"look up [person]\", \"intel on [prospect]\", \"who is [name] at [company]\", or \"tell me about [company]\".\n---\n\n# Account Research\n\nGet a complete picture of any company or person before outreach. This skill always works with web search, and gets significantly better with enrichment and CRM data.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     ACCOUNT RESEARCH                             │\n├─────────────────────────────────────────────────────────────────┤\n│  ALWAYS (works standalone via web search)                        │\n│  ✓ Company overview: what they do, size, industry               │\n│  ✓ Recent news: funding, leadership changes, announcements      │\n│  ✓ Hiring signals: open roles, growth indicators                │\n│  ✓ Key people: leadership team from LinkedIn                    │\n│  ✓ Product/service: what they sell, who they serve              │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Enrichment: verified emails, phone, tech stack, org chart    │\n│  + CRM: prior relationship, past opportunities, contacts        │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Getting Started\n\nJust tell me who to research:\n\n- \"Research Stripe\"\n- \"Look up the CTO at Notion\"\n- \"Intel on acme.com\"\n- \"Who is Sarah Chen at TechCorp?\"\n- \"Tell me about [company] before my call\"\n\nI'll run web searches immediately. If you have enrichment or CRM connected, I'll pull that data too.\n\n---\n\n## Connectors (Optional)\n\nConnect your tools to supercharge this skill:\n\n| Connector | What It Adds |\n|-----------|--------------|\n| **Enrichment** | Verified emails, phone numbers, tech stack, org chart, funding details |\n| **CRM** | Prior relationship history, past opportunities, existing contacts, notes |\n\n\u003e **No connectors?** No problem. Web search provides solid research for any company or person.\n\n---\n\n## Output Format\n\n```markdown\n# Research: [Company or Person Name]\n\n**Generated:** [Date]\n**Sources:** Web Search [+ Enrichment] [+ CRM]\n\n---\n\n## Quick Take\n\n[2-3 sentences: Who they are, why they might need you, best angle for outreach]\n\n---\n\n## Company Profile\n\n| Field | Value |\n|-------|-------|\n| **Company** | [Name] |\n| **Website** | [URL] |\n| **Industry** | [Industry] |\n| **Size** | [Employee count] |\n| **Headquarters** | [Location] |\n| **Founded** | [Year] |\n| **Funding** | [Stage + amount if known] |\n| **Revenue** | [Estimate if available] |\n\n### What They Do\n[1-2 sentence description of their business, product, and customers]\n\n### Recent News\n- **[Headline]** — [Date] — [Why it matters for your outreach]\n- **[Headline]** — [Date] — [Why it matters]\n\n### Hiring Signals\n- [X] open roles in [Department]\n- Notable: [Relevant roles like Engineering, Sales, AI/ML]\n- Growth indicator: [Hiring velocity interpretation]\n\n---\n\n## Key People\n\n### [Name] — [Title]\n| Field | Detail |\n|-------|--------|\n| **LinkedIn** | [URL] |\n| **Background** | [Prior companies, education] |\n| **Tenure** | [Time at company] |\n| **Email** | [If enrichment connected] |\n\n**Talking Points:**\n- [Personal hook based on background]\n- [Professional hook based on role]\n\n[Repeat for relevant contacts]\n\n---\n\n## Tech Stack [If Enrichment Connected]\n\n| Category | Tools |\n|----------|-------|\n| **Cloud** | [AWS, GCP, Azure, etc.] |\n| **Data** | [Snowflake, Databricks, etc.] |\n| **CRM** | [e.g. Salesforce, HubSpot] |\n| **Other** | [Relevant tools] |\n\n**Integration Opportunity:** [How your product fits with their stack]\n\n---\n\n## Prior Relationship [If CRM Connected]\n\n| Field | Detail |\n|-------|--------|\n| **Status** | [New / Prior prospect / Customer / Churned] |\n| **Last Contact** | [Date and type] |\n| **Previous Opps** | [Won/Lost and why] |\n| **Known Contacts** | [Names already in CRM] |\n\n**History:** [Summary of past relationship]\n\n---\n\n## Qualification Signals\n\n### Positive Signals\n- ✅ [Signal and evidence]\n- ✅ [Signal and evidence]\n\n### Potential Concerns\n- ⚠️ [Concern and what to watch for]\n\n### Unknown (Ask in Discovery)\n- ❓ [Gap in understanding]\n\n---\n\n## Recommended Approach\n\n**Best Entry Point:** [Person and why]\n\n**Opening Hook:** [What to lead with based on research]\n\n**Discovery Questions:**\n1. [Question about their situation]\n2. [Question about pain points]\n3. [Question about decision process]\n\n---\n\n## Sources\n- [Source 1](URL)\n- [Source 2](URL)\n```\n\n---\n\n## Execution Flow\n\n### Step 1: Parse Request\n\n```\nIdentify what to research:\n- \"Research Stripe\" → Company research\n- \"Look up John Smith at Acme\" → Person + company\n- \"Who is the CTO at Notion\" → Role-based search\n- \"Intel on acme.com\" → Domain-based lookup\n```\n\n### Step 2: Web Search (Always)\n\n```\nRun these searches:\n1. \"[Company name]\" → Homepage, about page\n2. \"[Company name] news\" → Recent announcements\n3. \"[Company name] funding\" → Investment history\n4. \"[Company name] careers\" → Hiring signals\n5. \"[Person name] [Company] LinkedIn\" → Profile info\n6. \"[Company name] product\" → What they sell\n7. \"[Company name] customers\" → Who they serve\n```\n\n**Extract:**\n- Company description and positioning\n- Recent news (last 90 days)\n- Leadership team\n- Open job postings\n- Technology mentions\n- Customer base\n\n### Step 3: Enrichment (If Connected)\n\n```\nIf enrichment tools available:\n1. Enrich company → Firmographics, funding, tech stack\n2. Search people → Org chart, contact list\n3. Enrich person → Email, phone, background\n4. Get signals → Intent data, hiring velocity\n```\n\n**Enrichment adds:**\n- Verified contact info\n- Complete org chart\n- Precise employee count\n- Detailed tech stack\n- Funding history with investors\n\n### Step 4: CRM Check (If Connected)\n\n```\nIf CRM available:\n1. Search for account by domain\n2. Get related contacts\n3. Get opportunity history\n4. Get activity timeline\n```\n\n**CRM adds:**\n- Prior relationship context\n- What happened before (won/lost deals)\n- Who we've talked to\n- Notes and history\n\n### Step 5: Synthesize\n\n```\n1. Combine all sources\n2. Prioritize enrichment data over web (more accurate)\n3. Add CRM context if exists\n4. Identify qualification signals\n5. Generate talking points\n6. Recommend approach\n```\n\n---\n\n## Research Variations\n\n### Company Research\nFocus on: Business overview, news, hiring, leadership\n\n### Person Research\nFocus on: Background, role, LinkedIn activity, talking points\n\n### Competitor Research\nFocus on: Product comparison, positioning, win/loss patterns\n\n### Pre-Meeting Research\nFocus on: Attendee backgrounds, recent news, relationship history\n\n---\n\n## Tips for Better Research\n\n1. **Include the domain** — \"research acme.com\" is more precise\n2. **Specify the person** — \"look up Jane Smith, VP Sales at Acme\"\n3. **State your goal** — \"research Stripe before my demo call\"\n4. **Ask for specifics** — \"what's their tech stack?\" after initial research\n\n---\n\n## Related Skills\n\n- **call-prep** — Full meeting prep with this research plus context\n- **draft-outreach** — Write personalized message based on research\n- **prospecting** — Qualify and prioritize research targets\n", "depth": 4}, {"id": "103", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/competitive-intelligence/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: competitive-intelligence\ndescription: Research your competitors and build an interactive battlecard. Outputs an HTML artifact with clickable competitor cards and a comparison matrix. Trigger with \"competitive intel\", \"research competitors\", \"how do we compare to [competitor]\", \"battlecard for [competitor]\", or \"what's new with [competitor]\".\n---\n\n# Competitive Intelligence\n\nResearch your competitors extensively and generate an **interactive HTML battlecard** you can use in deals. The output is a self-contained artifact with clickable competitor tabs and an overall comparison matrix.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                  COMPETITIVE INTELLIGENCE                        │\n├─────────────────────────────────────────────────────────────────┤\n│  ALWAYS (works standalone via web search)                        │\n│  ✓ Competitor product deep-dive: features, pricing, positioning │\n│  ✓ Recent releases: what they've shipped in last 90 days        │\n│  ✓ Your company releases: what you've shipped to counter        │\n│  ✓ Differentiation matrix: where you win vs. where they win     │\n│  ✓ Sales talk tracks: how to position against each competitor   │\n│  ✓ Landmine questions: expose their weaknesses naturally        │\n├─────────────────────────────────────────────────────────────────┤\n│  OUTPUT: Interactive HTML Battlecard                             │\n│  ✓ Comparison matrix overview                                    │\n│  ✓ Clickable tabs for each competitor                           │\n│  ✓ Dark theme, professional styling                             │\n│  ✓ Self-contained HTML file — share or host anywhere            │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + CRM: Win/loss data, competitor mentions in closed deals      │\n│  + Docs: Existing battlecards, competitive playbooks            │\n│  + Chat: Internal intel, field reports from colleagues          │\n│  + Transcripts: Competitor mentions in customer calls           │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Getting Started\n\nWhen you run this skill, I'll ask for context:\n\n**Required:**\n- What company do you work for? (or I'll detect from your email)\n- Who are your main competitors? (1-5 names)\n\n**Optional:**\n- Which competitor do you want to focus on first?\n- Any specific deals where you're competing against them?\n- Pain points you've heard from customers about competitors?\n\nIf I already have your seller context from a previous session, I'll confirm and skip the questions.\n\n---\n\n## Connectors (Optional)\n\n| Connector | What It Adds |\n|-----------|--------------|\n| **CRM** | Win/loss history against each competitor, deal-level competitor tracking |\n| **Docs** | Existing battlecards, product comparison docs, competitive playbooks |\n| **Chat** | Internal chat intel (e.g. Slack) — what your team is hearing from the field |\n| **Transcripts** | Competitor mentions in customer calls, objections raised |\n\n\u003e **No connectors?** Web research works great. I'll pull everything from public sources — product pages, pricing, blogs, release notes, reviews, job postings.\n\n---\n\n## Output: Interactive HTML Battlecard\n\nThe skill generates a **self-contained HTML file** with:\n\n### 1. Comparison Matrix (Landing View)\nOverview comparing you vs. all competitors at a glance:\n- Feature comparison grid\n- Pricing comparison\n- Market positioning\n- Win rate indicators (if CRM connected)\n\n### 2. Competitor Tabs (Click to Expand)\nEach competitor gets a clickable card that expands to show:\n- Company profile (size, funding, target market)\n- What they sell and how they position\n- Recent releases (last 90 days)\n- Where they win vs. where you win\n- Pricing intelligence\n- Talk tracks for different scenarios\n- Objection handling\n- Landmine questions\n\n### 3. Your Company Card\n- Your releases (last 90 days)\n- Your key differentiators\n- Proof points and customer quotes\n\n---\n\n## HTML Structure\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n    \u003ctitle\u003eBattlecard: [Your Company] vs Competitors\u003c/title\u003e\n    \u003cstyle\u003e\n        /* Dark theme, professional styling */\n        /* Tabbed navigation */\n        /* Expandable cards */\n        /* Responsive design */\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003c!-- Header with your company + date --\u003e\n    \u003cheader\u003e\n        \u003ch1\u003e[Your Company] Competitive Battlecard\u003c/h1\u003e\n        \u003cp\u003eGenerated: [Date] | Competitors: [List]\u003c/p\u003e\n    \u003c/header\u003e\n\n    \u003c!-- Tab Navigation --\u003e\n    \u003cnav class=\"tabs\"\u003e\n        \u003cbutton class=\"tab active\" data-tab=\"matrix\"\u003eComparison Matrix\u003c/button\u003e\n        \u003cbutton class=\"tab\" data-tab=\"competitor-1\"\u003e[Competitor 1]\u003c/button\u003e\n        \u003cbutton class=\"tab\" data-tab=\"competitor-2\"\u003e[Competitor 2]\u003c/button\u003e\n        \u003cbutton class=\"tab\" data-tab=\"competitor-3\"\u003e[Competitor 3]\u003c/button\u003e\n    \u003c/nav\u003e\n\n    \u003c!-- Comparison Matrix Tab --\u003e\n    \u003csection id=\"matrix\" class=\"tab-content active\"\u003e\n        \u003ch2\u003eHead-to-Head Comparison\u003c/h2\u003e\n        \u003ctable class=\"comparison-matrix\"\u003e\n            \u003c!-- Feature rows with you vs each competitor --\u003e\n        \u003c/table\u003e\n\n        \u003ch2\u003eQuick Win/Loss Guide\u003c/h2\u003e\n        \u003cdiv class=\"win-loss-grid\"\u003e\n            \u003c!-- Per-competitor: when you win, when you lose --\u003e\n        \u003c/div\u003e\n    \u003c/section\u003e\n\n    \u003c!-- Individual Competitor Tabs --\u003e\n    \u003csection id=\"competitor-1\" class=\"tab-content\"\u003e\n        \u003cdiv class=\"battlecard\"\u003e\n            \u003cdiv class=\"profile\"\u003e\u003c!-- Company info --\u003e\u003c/div\u003e\n            \u003cdiv class=\"differentiation\"\u003e\u003c!-- Where they win / you win --\u003e\u003c/div\u003e\n            \u003cdiv class=\"talk-tracks\"\u003e\u003c!-- Scenario-based positioning --\u003e\u003c/div\u003e\n            \u003cdiv class=\"objections\"\u003e\u003c!-- Common objections + responses --\u003e\u003c/div\u003e\n            \u003cdiv class=\"landmines\"\u003e\u003c!-- Questions to ask --\u003e\u003c/div\u003e\n        \u003c/div\u003e\n    \u003c/section\u003e\n\n    \u003cscript\u003e\n        // Tab switching logic\n        // Expand/collapse sections\n    \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n---\n\n## Visual Design\n\n### Color System\n```css\n:root {\n    /* Dark theme base */\n    --bg-primary: #0a0d14;\n    --bg-elevated: #0f131c;\n    --bg-surface: #161b28;\n    --bg-hover: #1e2536;\n\n    /* Text */\n    --text-primary: #ffffff;\n    --text-secondary: rgba(255, 255, 255, 0.7);\n    --text-muted: rgba(255, 255, 255, 0.5);\n\n    /* Accent (your brand or neutral) */\n    --accent: #3b82f6;\n    --accent-hover: #2563eb;\n\n    /* Status indicators */\n    --you-win: #10b981;\n    --they-win: #ef4444;\n    --tie: #f59e0b;\n}\n```\n\n### Card Design\n- Rounded corners (12px)\n- Subtle borders (1px, low opacity)\n- Hover states with slight elevation\n- Smooth transitions (200ms)\n\n### Comparison Matrix\n- Sticky header row\n- Color-coded winner indicators (green = you, red = them, yellow = tie)\n- Expandable rows for detail\n\n---\n\n## Execution Flow\n\n### Phase 1: Gather Seller Context\n\n```\nIf first time:\n1. Ask: \"What company do you work for?\"\n2. Ask: \"What do you sell? (product/service in one line)\"\n3. Ask: \"Who are your main competitors? (up to 5)\"\n4. Store context for future sessions\n\nIf returning user:\n1. Confirm: \"Still at [Company] selling [Product]?\"\n2. Ask: \"Same competitors, or any new ones to add?\"\n```\n\n### Phase 2: Research Your Company (Always)\n\n```\nWeb searches:\n1. \"[Your company] product\" — current offerings\n2. \"[Your company] pricing\" — pricing model\n3. \"[Your company] news\" — recent announcements (90 days)\n4. \"[Your company] product updates OR changelog OR releases\" — what you've shipped\n5. \"[Your company] vs [competitor]\" — existing comparisons\n```\n\n### Phase 3: Research Each Competitor (Always)\n\n```\nFor each competitor, run:\n1. \"[Competitor] product features\" — what they offer\n2. \"[Competitor] pricing\" — how they charge\n3. \"[Competitor] news\" — recent announcements\n4. \"[Competitor] product updates OR changelog OR releases\" — what they've shipped\n5. \"[Competitor] reviews G2 OR Capterra OR TrustRadius\" — customer sentiment\n6. \"[Competitor] vs [alternatives]\" — how they position\n7. \"[Competitor] customers\" — who uses them\n8. \"[Competitor] careers\" — hiring signals (growth areas)\n```\n\n### Phase 4: Pull Connected Sources (If Available)\n\n```\nIf CRM connected:\n1. Query closed-won deals with competitor field = [Competitor]\n2. Query closed-lost deals with competitor field = [Competitor]\n3. Extract win/loss patterns\n\nIf docs connected:\n1. Search for \"battlecard [competitor]\"\n2. Search for \"competitive [competitor]\"\n3. Pull existing positioning docs\n\nIf chat connected:\n1. Search for \"[Competitor]\" mentions (last 90 days)\n2. Extract field intel and colleague insights\n\nIf transcripts connected:\n1. Search calls for \"[Competitor]\" mentions\n2. Extract objections and customer quotes\n```\n\n### Phase 5: Build HTML Artifact\n\n```\n1. Structure data for each competitor\n2. Build comparison matrix\n3. Generate individual battlecards\n4. Create talk tracks for each scenario\n5. Compile landmine questions\n6. Render as self-contained HTML\n7. Save as [YourCompany]-battlecard-[date].html\n```\n\n---\n\n## Data Structure Per Competitor\n\n```yaml\ncompetitor:\n  name: \"[Name]\"\n  website: \"[URL]\"\n  profile:\n    founded: \"[Year]\"\n    funding: \"[Stage + amount]\"\n    employees: \"[Count]\"\n    target_market: \"[Who they sell to]\"\n    pricing_model: \"[Per seat / usage / etc.]\"\n    market_position: \"[Leader / Challenger / Niche]\"\n\n  what_they_sell: \"[Product summary]\"\n  their_positioning: \"[How they describe themselves]\"\n\n  recent_releases:\n    - date: \"[Date]\"\n      release: \"[Feature/Product]\"\n      impact: \"[Why it matters]\"\n\n  where_they_win:\n    - area: \"[Area]\"\n      advantage: \"[Their strength]\"\n      how_to_handle: \"[Your counter]\"\n\n  where_you_win:\n    - area: \"[Area]\"\n      advantage: \"[Your strength]\"\n      proof_point: \"[Evidence]\"\n\n  pricing:\n    model: \"[How they charge]\"\n    entry_price: \"[Starting price]\"\n    enterprise: \"[Enterprise pricing]\"\n    hidden_costs: \"[Implementation, etc.]\"\n    talk_track: \"[How to discuss pricing]\"\n\n  talk_tracks:\n    early_mention: \"[Strategy if they come up early]\"\n    displacement: \"[Strategy if customer uses them]\"\n    late_addition: \"[Strategy if added late to eval]\"\n\n  objections:\n    - objection: \"[What customer says]\"\n      response: \"[How to handle]\"\n\n  landmines:\n    - \"[Question that exposes their weakness]\"\n\n  win_loss: # If CRM connected\n    win_rate: \"[X]%\"\n    common_win_factors: \"[What predicts wins]\"\n    common_loss_factors: \"[What predicts losses]\"\n```\n\n---\n\n## Delivery\n\n```markdown\n## ✓ Battlecard Created\n\n[View your battlecard](file:///path/to/[YourCompany]-battlecard-[date].html)\n\n---\n\n**Summary**\n- **Your Company**: [Name]\n- **Competitors Analyzed**: [List]\n- **Data Sources**: Web research [+ CRM] [+ Docs] [+ Transcripts]\n\n---\n\n**How to Use**\n- **Before a call**: Open the relevant competitor tab, review talk tracks\n- **During a call**: Reference landmine questions\n- **After win/loss**: Update with new intel\n\n---\n\n**Sharing Options**\n- **Local file**: Open in any browser\n- **Host it**: Upload to Netlify, Vercel, or internal wiki\n- **Share directly**: Send the HTML file to teammates\n\n---\n\n**Keep it Fresh**\nRun this skill again to refresh with latest intel. Recommended: monthly or before major deals.\n```\n\n---\n\n## Refresh Cadence\n\nCompetitive intel gets stale. Recommended refresh:\n\n| Trigger | Action |\n|---------|--------|\n| **Monthly** | Quick refresh — new releases, news, pricing changes |\n| **Before major deal** | Deep refresh for specific competitor in that deal |\n| **After win/loss** | Update patterns with new data |\n| **Competitor announcement** | Immediate update on that competitor |\n\n---\n\n## Tips for Better Intel\n\n1. **Be honest about weaknesses** — Credibility comes from acknowledging where competitors are strong\n2. **Focus on outcomes, not features** — \"They have X feature\" matters less than \"customers achieve Y result\"\n3. **Update from the field** — Best intel comes from actual customer conversations, not just websites\n4. **Plant landmines, don't badmouth** — Ask questions that expose weaknesses; never trash-talk\n5. **Track releases religiously** — What they ship tells you their strategy and your opportunity\n\n---\n\n## Related Skills\n\n- **account-research** — Research a specific prospect before reaching out\n- **call-prep** — Prep for a call where you know competitor is involved\n- **create-an-asset** — Build a custom comparison page for a specific deal\n", "depth": 4}, {"id": "104", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/sales/skills/call-prep/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: call-prep\ndescription: Prepare for a sales call with account context, attendee research, and suggested agenda. Works standalone with user input and web research, supercharged when you connect your CRM, email, chat, or transcripts. Trigger with \"prep me for my call with [company]\", \"I'm meeting with [company] prep me\", \"call prep [company]\", or \"get me ready for [meeting]\".\n---\n\n# Call Prep\n\nGet fully prepared for any sales call in minutes. This skill works with whatever context you provide, and gets significantly better when you connect your sales tools.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        CALL PREP                                 │\n├─────────────────────────────────────────────────────────────────┤\n│  ALWAYS (works standalone)                                       │\n│  ✓ You tell me: company, meeting type, attendees                │\n│  ✓ Web search: recent news, funding, leadership changes         │\n│  ✓ Company research: what they do, size, industry               │\n│  ✓ Output: prep brief with agenda and questions                 │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + CRM: account history, contacts, opportunities, activities    │\n│  + Email: recent threads, open questions, commitments           │\n│  + Chat: internal discussions, colleague insights               │\n│  + Transcripts: prior call recordings, key moments              │\n│  + Calendar: auto-find meeting, pull attendees                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Getting Started\n\nWhen you run this skill, I'll ask for what I need:\n\n**Required:**\n- Company or contact name\n- Meeting type (discovery, demo, negotiation, check-in, etc.)\n\n**Helpful if you have it:**\n- Who's attending (names and titles)\n- Any context you want me to know (paste prior notes, emails, etc.)\n\nIf you've connected your CRM, email, or other tools, I'll pull context automatically and skip the questions.\n\n---\n\n## Connectors (Optional)\n\nConnect your tools to supercharge this skill:\n\n| Connector | What It Adds |\n|-----------|--------------|\n| **CRM** | Account details, contact history, open deals, recent activities |\n| **Email** | Recent threads with the company, open questions, attachments shared |\n| **Chat** | Internal chat discussions (e.g. Slack) about the account, colleague insights |\n| **Transcripts** | Prior call recordings, topics covered, competitor mentions |\n| **Calendar** | Auto-find the meeting, pull attendees and description |\n\n\u003e **No connectors?** No problem. Just tell me about the meeting and paste any context you have. I'll research the rest.\n\n---\n\n## Output Format\n\n```markdown\n# Call Prep: [Company Name]\n\n**Meeting:** [Type] — [Date/Time if known]\n**Attendees:** [Names with titles]\n**Your Goal:** [What you want to accomplish]\n\n---\n\n## Account Snapshot\n\n| Field | Value |\n|-------|-------|\n| **Company** | [Name] |\n| **Industry** | [Industry] |\n| **Size** | [Employees / Revenue if known] |\n| **Status** | [New prospect / Active opportunity / Customer] |\n| **Last Touch** | [Date and summary] |\n\n---\n\n## Who You're Meeting\n\n### [Name] — [Title]\n- **Background:** [Career history, education if found]\n- **LinkedIn:** [URL]\n- **Role in Deal:** [Decision maker / Champion / Evaluator / etc.]\n- **Last Interaction:** [Summary if known]\n- **Talking Point:** [Something personal/professional to reference]\n\n[Repeat for each attendee]\n\n---\n\n## Context \u0026 History\n\n**What's happened so far:**\n- [Key point from prior interactions]\n- [Open commitments or action items]\n- [Any concerns or objections raised]\n\n**Recent news about [Company]:**\n- [News item 1 — why it matters]\n- [News item 2 — why it matters]\n\n---\n\n## Suggested Agenda\n\n1. **Open** — [Reference last conversation or trigger event]\n2. **[Topic 1]** — [Discovery question or value discussion]\n3. **[Topic 2]** — [Address known concern or explore priority]\n4. **[Topic 3]** — [Demo section / Proposal review / etc.]\n5. **Next Steps** — [Propose clear follow-up with timeline]\n\n---\n\n## Discovery Questions\n\nAsk these to fill gaps in your understanding:\n\n1. [Question about their current situation]\n2. [Question about pain points or priorities]\n3. [Question about decision process and timeline]\n4. [Question about success criteria]\n5. [Question about other stakeholders]\n\n---\n\n## Potential Objections\n\n| Objection | Suggested Response |\n|-----------|-------------------|\n| [Likely objection based on context] | [How to address it] |\n| [Common objection for this stage] | [How to address it] |\n\n---\n\n## Internal Notes\n\n[Any internal chat context (e.g. Slack), colleague insights, or competitive intel]\n\n---\n\n## After the Call\n\nRun **call-follow-up** to:\n- Extract action items\n- Update your CRM\n- Draft follow-up email\n```\n\n---\n\n## Execution Flow\n\n### Step 1: Gather Context\n\n**If connectors available:**\n```\n1. Calendar → Find upcoming meeting matching company name\n   - Pull: title, time, attendees, description, attachments\n\n2. CRM → Query account\n   - Pull: account details, all contacts, open opportunities\n   - Pull: last 10 activities, any account notes\n\n3. Email → Search recent threads\n   - Query: emails with company domain (last 30 days)\n   - Extract: key topics, open questions, commitments\n\n4. Chat → Search internal discussions\n   - Query: company name mentions (last 30 days)\n   - Extract: colleague insights, competitive intel\n\n5. Transcripts → Find prior calls\n   - Pull: call recordings with this account\n   - Extract: key moments, objections raised, topics covered\n```\n\n**If no connectors:**\n```\n1. Ask user:\n   - \"What company are you meeting with?\"\n   - \"What type of meeting is this?\"\n   - \"Who's attending? (names and titles if you know)\"\n   - \"Any context you want me to know? (paste notes, emails, etc.)\"\n\n2. Accept whatever they provide and work with it\n```\n\n### Step 2: Research Supplement\n\n**Always run (web search):**\n```\n1. \"[Company] news\" — last 30 days\n2. \"[Company] funding\" — recent announcements\n3. \"[Company] leadership\" — executive changes\n4. \"[Company] + [industry] trends\" — relevant context\n5. Attendee LinkedIn profiles — background research\n```\n\n### Step 3: Synthesize \u0026 Generate\n\n```\n1. Combine all sources into unified context\n2. Identify gaps in understanding → generate discovery questions\n3. Anticipate objections based on stage and history\n4. Create suggested agenda tailored to meeting type\n5. Output formatted prep brief\n```\n\n---\n\n## Meeting Type Variations\n\n### Discovery Call\n- Focus on: Understanding their world, pain points, priorities\n- Agenda emphasis: Questions \u003e Talking\n- Key output: Qualification signals, next step proposal\n\n### Demo / Presentation\n- Focus on: Their specific use case, tailored examples\n- Agenda emphasis: Show relevant features, get feedback\n- Key output: Technical requirements, decision timeline\n\n### Negotiation / Proposal Review\n- Focus on: Addressing concerns, justifying value\n- Agenda emphasis: Handle objections, close gaps\n- Key output: Path to agreement, clear next steps\n\n### Check-in / QBR\n- Focus on: Value delivered, expansion opportunities\n- Agenda emphasis: Review wins, surface new needs\n- Key output: Renewal confidence, upsell pipeline\n\n---\n\n## Tips for Better Prep\n\n1. **More context = better prep** — Paste emails, notes, anything you have\n2. **Name the attendees** — Even just titles help me research\n3. **State your goal** — \"I want to get them to agree to a pilot\"\n4. **Flag concerns** — \"They mentioned budget is tight\"\n\n---\n\n## Related Skills\n\n- **account-research** — Deep dive on a company before first contact\n- **call-follow-up** — Process call notes and execute post-call workflow\n- **draft-outreach** — Write personalized outreach after research\n", "depth": 4}, {"id": "105", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "106", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "107", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/README.md", "name": "README.md", "type": "file", "content": "# Bio-Research Plugin\n\nConnect to preclinical research tools and databases (literature search, genomics analysis, target prioritization) to accelerate early-stage life sciences R\u0026D. Use with [Cowork](https://claude.com/product/cowork) or install directly in Claude Code.\n\nThis plugin consolidates 10 MCP server integrations and 5 analysis skills into a single package for life science researchers.\n\n## What's Included\n\n### MCP Servers (Data Sources \u0026 Tools)\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\n| Provider | What It Does | Category/Placeholder |\n|----------|-------------|---------------------|\n| U.S. National Library of Medicine | Search biomedical literature and research articles | `~~literature` |\n| deepsense.ai | Access preprints from bioRxiv and medRxiv | `~~literature` |\n| John Wiley \u0026 Sons | Access academic research and publications | `~~journal access` |\n| Sage Bionetworks | Collaborative research data management | `~~data repository` |\n| deepsense.ai | Bioactive drug-like compound database | `~~chemical database` |\n| OpenTargets | Drug target discovery and prioritization | `~~drug targets` |\n| deepsense.ai | NIH/NLM clinical trial registry | `~~clinical trials` |\n| BioRender | Scientific illustration creation | `~~scientific illustration` |\n| Owkin | AI for biology — histopathology and drug discovery | `~~AI research` |\n| Benchling\\* | Lab data management platform | `~~lab platform` |\n\n### Optional Binary MCP Servers\n\nThese require a separate binary download:\n\n- **10X Genomics txg-mcp** (`~~genomics platform`) — Cloud analysis data and workflows ([GitHub](https://github.com/10XGenomics/txg-mcp/releases))\n- **ToolUniverse** (`~~tool database`) — AI tools for scientific discovery from Harvard MIMS ([GitHub](https://github.com/mims-harvard/ToolUniverse/releases))\n\n### Skills (Analysis Workflows)\n\n#### Single-Cell RNA QC\nAutomated quality control for scRNA-seq data following scverse best practices. Supports `.h5ad` and `.h5` files with MAD-based filtering and comprehensive visualizations.\n\n#### scvi-tools\nDeep learning toolkit for single-cell omics. Covers scVI, scANVI, totalVI, PeakVI, MultiVI, DestVI, veloVI, and sysVI models for integration, batch correction, label transfer, and multi-modal analysis.\n\n#### Nextflow Pipelines\nRun nf-core bioinformatics pipelines on local or public GEO/SRA sequencing data:\n- **rnaseq** — Gene expression and differential expression\n- **sarek** — Germline and somatic variant calling (WGS/WES)\n- **atacseq** — Chromatin accessibility analysis\n\n#### Instrument Data to Allotrope\nConvert laboratory instrument output files (PDF, CSV, Excel, TXT) to Allotrope Simple Model (ASM) format. Supports 40+ instrument types including cell counters, spectrophotometers, plate readers, qPCR, and chromatography systems.\n\n#### Scientific Problem Selection\nSystematic framework for research problem selection based on Fischbach \u0026 Walsh's framework. Includes 9 skills covering ideation, risk assessment, optimization, decision trees, adversity planning, and synthesis.\n\n## Getting Started\n\n```bash\n# Install the plugin\n/install anthropics/knowledge-work-plugins bio-research\n\n# Run the start command to see available tools\n/start\n```\n\n## Common Workflows\n\n**Literature Review**\nSearch ~~literature database for papers, access full-text through ~~journal access, and create figures with ~~scientific illustration.\n\n**Single-Cell Analysis**\nRun QC on scRNA-seq data, then use scvi-tools for integration, batch correction, and cell type annotation.\n\n**Sequencing Pipeline**\nDownload public data from GEO/SRA, run nf-core pipelines (RNA-seq, variant calling, ATAC-seq), and verify outputs.\n\n**Drug Discovery**\nSearch ~~chemical database for bioactive compounds, use ~~drug target database for target prioritization, and review clinical trial data.\n\n**Research Strategy**\nPitch a new idea, troubleshoot a stuck project, or evaluate strategic decisions using the scientific problem selection framework.\n\n## License\n\nSkills are licensed under Apache 2.0. MCP servers are provided by their respective authors — see individual server documentation for terms.\n", "depth": 2}, {"id": "108", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~literature` might mean PubMed, bioRxiv, or any other literature source with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (literature, clinical trials, chemical database, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Literature | `~~literature` | PubMed, bioRxiv | Google Scholar, Semantic Scholar |\n| Scientific illustration | `~~scientific illustration` | BioRender | — |\n| Clinical trials | `~~clinical trials` | ClinicalTrials.gov | EU Clinical Trials Register |\n| Chemical database | `~~chemical database` | ChEMBL | PubChem, DrugBank |\n| Drug targets | `~~drug targets` | Open Targets | UniProt, STRING |\n| Data repository | `~~data repository` | Synapse | Zenodo, Dryad, Figshare |\n| Journal access | `~~journal access` | Wiley Scholar Gateway | Elsevier, Springer Nature |\n| AI research | `~~AI research` | Owkin | — |\n| Lab platform | `~~lab platform` | Benchling\\* | — |\n\n\\* Placeholder — MCP URL not yet configured\n", "depth": 2}, {"id": "109", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/commands/start.md", "name": "start.md", "type": "file", "content": "---\ndescription: Set up your bio-research environment and explore available tools\n---\n\n# Bio-Research Start\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nYou are helping a biological researcher get oriented with the bio-research plugin. Walk through the following steps in order.\n\n## Step 1: Welcome\n\nDisplay this welcome message:\n\n```\nBio-Research Plugin\n\nYour AI-powered research assistant for the life sciences. This plugin brings\ntogether literature search, data analysis pipelines,\nand scientific strategy — all in one place.\n```\n\n## Step 2: Check Available MCP Servers\n\nTest which MCP servers are connected by listing available tools. Group the results:\n\n**Literature \u0026 Data Sources:**\n- ~~literature database — biomedical literature search\n- ~~literature database — preprint access (biology and medicine)\n- ~~journal access — academic publications\n- ~~data repository — collaborative research data (Sage Bionetworks)\n\n**Drug Discovery \u0026 Clinical:**\n- ~~chemical database — bioactive compound database\n- ~~drug target database — drug target discovery platform\n- ClinicalTrials.gov — clinical trial registry\n- ~~clinical data platform — clinical trial site ranking and platform help\n\n**Visualization \u0026 AI:**\n- ~~scientific illustration — create scientific figures and diagrams\n- ~~AI research platform — AI for biology (histopathology, drug discovery)\n\nReport which servers are connected and which are not yet set up.\n\n## Step 3: Survey Available Skills\n\nList the analysis skills available in this plugin:\n\n| Skill | What It Does |\n|-------|-------------|\n| **Single-Cell RNA QC** | Quality control for scRNA-seq data with MAD-based filtering |\n| **scvi-tools** | Deep learning for single-cell omics (scVI, scANVI, totalVI, PeakVI, etc.) |\n| **Nextflow Pipelines** | Run nf-core pipelines (RNA-seq, WGS/WES, ATAC-seq) |\n| **Instrument Data Converter** | Convert lab instrument output to Allotrope ASM format |\n| **Scientific Problem Selection** | Systematic framework for choosing research problems |\n\n## Step 4: Optional Setup — Binary MCP Servers\n\nMention that two additional MCP servers are available as separate installations:\n\n- **~~genomics platform** — Access cloud analysis data and workflows\n  Install: Download `txg-node.mcpb` from https://github.com/10XGenomics/txg-mcp/releases\n- **~~tool database** (Harvard MIMS) — AI tools for scientific discovery\n  Install: Download `tooluniverse.mcpb` from https://github.com/mims-harvard/ToolUniverse/releases\n\nThese require downloading binary files and are optional.\n\n## Step 5: Ask How to Help\n\nAsk the researcher what they're working on today. Suggest starting points based on common workflows:\n\n1. **Literature review** — \"Search ~~literature database for recent papers on [topic]\"\n2. **Analyze sequencing data** — \"Run QC on my single-cell data\" or \"Set up an RNA-seq pipeline\"\n3. **Drug discovery** — \"Search ~~chemical database for compounds targeting [protein]\" or \"Find drug targets for [disease]\"\n4. **Data standardization** — \"Convert my instrument data to Allotrope format\"\n5. **Research strategy** — \"Help me evaluate a new project idea\"\n\nWait for the user's response and guide them to the appropriate tools and skills.\n", "depth": 3}, {"id": "110", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection", "name": "scientific-problem-selection", "type": "dir", "content": "", "depth": 3}, {"id": "111", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/single-cell-rna-qc", "name": "single-cell-rna-qc", "type": "dir", "content": "", "depth": 3}, {"id": "112", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools", "name": "scvi-tools", "type": "dir", "content": "", "depth": 3}, {"id": "113", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development", "name": "nextflow-development", "type": "dir", "content": "", "depth": 3}, {"id": "114", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope", "name": "instrument-data-to-allotrope", "type": "dir", "content": "", "depth": 3}, {"id": "115", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "116", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: scientific-problem-selection\ndescription: This skill should be used when scientists need help with research problem selection, project ideation, troubleshooting stuck projects, or strategic scientific decisions. Use this skill when users ask to pitch a new research idea, work through a project problem, evaluate project risks, plan research strategy, navigate decision trees, or get help choosing what scientific problem to work on. Typical requests include \"I have an idea for a project\", \"I'm stuck on my research\", \"help me evaluate this project\", \"what should I work on\", or \"I need strategic advice about my research\".\n---\n\n# Scientific Problem Selection Skills\n\nA conversational framework for systematic scientific problem selection based on Fischbach \u0026 Walsh's \"Problem choice and decision trees in science and engineering\" (Cell, 2024).\n\n## Getting Started\n\nPresent users with three entry points:\n\n**1) Pitch an idea for a new project** — to work it up together\n\n**2) Share a problem in a current project** — to troubleshoot together\n\n**3) Ask a strategic question** — to navigate the decision tree together\n\nThis conversational entry meets scientists where they are and establishes a collaborative tone.\n\n---\n\n## Option 1: Pitch an Idea\n\n### Initial Prompt\nAsk: **\"Tell me the short version of your idea (1-2 sentences).\"**\n\n### Response Approach\nAfter the user shares their idea, return a quick summary (no more than one paragraph) demonstrating understanding. Note the general area of research and rephrase the idea in a way that highlights its kernel—showing alignment and readiness to dive into details.\n\n### Follow-up Prompt\nThen ask for more detail: \"Now give me a bit more detail. You might include, however briefly or even say where you are unsure:\n1. What exactly you want to do\n2. How you currently plan to do it\n3. If it works, why will it be a big deal\n4. What you think are the major risks\"\n\n### Workflow\nFrom there, guide the user through the early stages of problem selection and evaluation:\n- **Skill 1: Intuition Pumps** - Refine and strengthen the idea\n- **Skill 2: Risk Assessment** - Identify and manage project risks\n- **Skill 3: Optimization Function** - Define success metrics\n- **Skill 4: Parameter Strategy** - Determine what to fix vs. keep flexible\n\nSee `references/01-intuition-pumps.md`, `references/02-risk-assessment.md`, `references/03-optimization-function.md`, and `references/04-parameter-strategy.md` for detailed guidance.\n\n---\n\n## Option 2: Troubleshoot a Problem\n\n### Initial Prompt\nAsk: **\"Tell me a short version of your problem (1-2 sentences or whatever is easy).\"**\n\n### Response Approach\nAfter the user shares their problem, return a quick summary (no more than one paragraph) demonstrating understanding. Note the context of the project where the problem occurred and rephrase the problem—highlighting its core essence—so the user knows the situation is understood. Also raise additional questions that seem important to discuss.\n\n### Follow-up Prompt\nThen ask: \"Now give me a bit more detail. You might include, however briefly:\n1. The overall goal of your project (if we have not talked about it before)\n2. What exactly went wrong\n3. Your current ideas for fixing it\"\n\n### Workflow\nFrom there, guide the user through troubleshooting and decision tree navigation:\n- **Skill 5: Decision Tree Navigation** - Plan decision points and navigate between execution and strategic thinking\n- **Skill 4: Parameter Strategy** - Fix one parameter at a time, let others float\n- **Skill 6: Adversity Response** - Frame problems as opportunities for growth\n- **Skill 7: Problem Inversion** - Strategies for navigating around obstacles\n\nAlways include workarounds that might be useful whether or not the problem can be fixed easily.\n\nSee `references/05-decision-tree.md`, `references/06-adversity-planning.md`, `references/07-problem-inversion.md`, and `references/04-parameter-strategy.md` for detailed guidance.\n\n---\n\n## Option 3: Ask a Strategic Question\n\n### Initial Prompt\nAsk: **\"Tell me the short version of your question (1-2 sentences).\"**\n\n### Response Approach\nAfter the user shares their question, return a quick summary (no more than one paragraph) demonstrating understanding. Note the broader context and rephrase the question—highlighting its crux—to confirm alignment with their thinking.\n\n### Follow-up Prompt\nThen ask: \"Now give me a bit more detail. You might include, however briefly:\n1. The setting (i.e., is this about a current or future project)\n2. A bit more detail about what you're thinking\"\n\n### Workflow\nFrom there, draw on the specific modules from the problem choice framework most appropriate to the question:\n- **Skills 1-4** for future project planning (ideation, risk, optimization, parameters)\n- **Skills 5-7** for current project navigation (decision trees, adversity, inversion)\n- **Skill 8** for communication and synthesis\n- **Skill 9** for comprehensive workflow orchestration\n\nSee the complete reference materials in the `references/` folder.\n\n---\n\n## Core Framework Concepts\n\n### The Central Insight\n**Problem Choice \u003e\u003e Execution Quality**\n\nEven brilliant execution of a mediocre problem yields incremental impact. Good execution of an important problem yields substantial impact.\n\n### The Time Paradox\nScientists typically spend:\n- **Days** choosing a problem\n- **Years** solving it\n\nThis imbalance limits impact. These skills help invest more time choosing wisely.\n\n### Evaluation Axes\n**For Evaluating Ideas:**\n- **X-axis:** Likelihood of success\n- **Y-axis:** Impact if successful\n\nSkills help move ideas rightward (more feasible) and upward (more impactful).\n\n### The Risk Paradox\n- Don't avoid risk—befriend it\n- No risk = incremental work\n- But: Multiple miracles = avoid or refine\n- **Balance:** Understood, quantified, manageable risk\n\n### The Parameter Paradox\n- Too many fixed = brittleness\n- Too few fixed = paralysis\n- **Sweet spot:** Fix ONE meaningful constraint\n\n### The Adversity Principle\n- Crises are inevitable (don't be surprised)\n- Crises are opportune (don't waste them)\n- **Strategy:** Fix problem AND upgrade project simultaneously\n\n---\n\n## The 9 Skills Overview\n\n| Skill | Purpose | Output | Time |\n|-------|---------|--------|------|\n| 1. Intuition Pumps | Generate high-quality research ideas | Problem Ideation Document | ~1 week |\n| 2. Risk Assessment | Identify and manage project risks | Risk Assessment Matrix | 3-5 days |\n| 3. Optimization Function | Define success metrics | Impact Assessment Document | 2-3 days |\n| 4. Parameter Strategy | Decide what to fix vs. keep flexible | Parameter Strategy Document | 2-3 days |\n| 5. Decision Tree Navigation | Plan decision points and altitude dance | Decision Tree Map | 2 days |\n| 6. Adversity Response | Prepare for crises as opportunities | Adversity Playbook | 2 days |\n| 7. Problem Inversion | Navigate around obstacles | Problem Inversion Analysis | 1 day |\n| 8. Integration \u0026 Synthesis | Synthesize into coherent plan | Project Communication Package | 3-5 days |\n| 9. Meta-Framework | Orchestrate complete workflow | Complete Project Package | 1-6 weeks |\n\n---\n\n## Skill Workflow\n\n```\nSKILL 1: Intuition Pumps\n         | (generates idea)\n         v\nSKILL 2: Risk Assessment\n         | (evaluates feasibility)\n         v\nSKILL 3: Optimization Function\n         | (defines success metrics)\n         v\nSKILL 4: Parameter Strategy\n         | (determines flexibility)\n         v\nSKILL 5: Decision Tree\n         | (plans execution and evaluation)\n         v\nSKILL 6: Adversity Planning\n         | (prepares for failure modes)\n         v\nSKILL 7: Problem Inversion\n         | (provides pivot strategies)\n         v\nSKILL 8: Integration \u0026 Communication\n         | (synthesizes into coherent plan)\n         v\nSKILL 9: Meta-Skill\n         (orchestrates complete workflow)\n```\n\n---\n\n## Key Design Principles\n\n1. **Conversational Entry** - Meet users where they are with three clear starting points\n2. **Thoughtful Interaction** - Ask clarifying questions; low confidence prompts additional input\n3. **Literature Integration** - Use PubMed searches at strategic points for validation\n4. **Concrete Outputs** - Every skill produces tangible 1-2 page documents\n5. **Building Specificity** - Progressive detail emerges through targeted questions\n6. **Flexibility** - Skills work independently, sequentially, or iteratively\n7. **Scientific Rigor** - Claims about generality and feasibility should be evidence-based\n\n---\n\n## Who Should Use These Skills\n\n### Graduate Students (Primary Audience)\n- **When:** Choosing thesis projects, qualifying exams, committee meetings\n- **Focus:** Skills 1-3 (ideation, risk, impact) + Skill 9 (complete workflow)\n- **Timeline:** 2-4 weeks for comprehensive planning\n\n### Postdocs\n- **When:** Starting new position, planning independent projects, fellowship applications\n- **Focus:** All skills, emphasizing independence and risk management\n- **Timeline:** 1-2 weeks intensive planning\n\n### Principal Investigators\n- **When:** New lab, new direction, mentoring trainees, grant cycles\n- **Focus:** Skills 1, 3, 4, 6 (ideation, impact, parameters, adversity)\n- **Timeline:** Ongoing, integrate into lab culture\n\n### Startup Founders\n- **When:** Company inception, pivot decisions, investor pitches\n- **Focus:** Skills 1-4 (ideation through parameters) + Skill 8 (communication)\n- **Timeline:** 1-2 weeks for initial planning, revisit quarterly\n\n---\n\n## Reference Materials\n\nDetailed skill documentation is available in the `references/` folder:\n\n| File | Content | Search Patterns |\n|------|---------|-----------------|\n| `01-intuition-pumps.md` | Generate research ideas | `Intuition Pump #`, `Trap #`, `Phase [0-9]` |\n| `02-risk-assessment.md` | Risk identification | `Risk.*1-5`, `go/no-go`, `assumption` |\n| `03-optimization-function.md` | Success metrics | `Generality.*Learning`, `optimization`, `impact` |\n| `04-parameter-strategy.md` | Parameter fixation | `fixed.*float`, `constraint`, `parameter` |\n| `05-decision-tree.md` | Decision tree navigation | `altitude`, `Level [0-9]`, `decision` |\n| `06-adversity-planning.md` | Adversity response | `adversity`, `crisis`, `ensemble` |\n| `07-problem-inversion.md` | Problem inversion strategies | `Strategy [0-9]`, `inversion`, `goal` |\n| `08-integration-synthesis.md` | Integration and synthesis | `narrative`, `communication`, `story` |\n| `09-meta-framework.md` | Complete workflow | `Phase`, `workflow`, `orchestrat` |\n\n---\n\n## Expected Outcomes\n\n### Immediate (After Completing Workflow)\n- Clear project vision\n- Honest risk assessment\n- Contingency plans\n- Communication materials ready\n- Confidence in problem choice\n\n### 6-Month\n- Faster decisions (have framework)\n- Productive adversity handling\n- No existential crises (risks mitigated)\n\n### 2-Year\n- Published results or strong progress\n- Avoided dead-end projects\n- Career aligned with goals\n- **Time well-spent** (ultimate measure)\n\n---\n\n## Foundational Reference\n\n**Fischbach, M.A., \u0026 Walsh, C.T. (2024).** \"Problem choice and decision trees in science and engineering.\" *Cell*, 187, 1828-1833.\n\nBased on course BIOE 395 taught at Stanford University.\n", "depth": 4}, {"id": "117", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/LICENSE.txt", "name": "LICENSE.txt", "type": "file", "content": "Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction,\nand distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\nthe copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all\nother entities that control, are controlled by, or are under common\ncontrol with that entity. For the purposes of this definition,\n\"control\" means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or\notherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\nexercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications,\nincluding but not limited to software source code, documentation\nsource, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical\ntransformation or translation of a Source form, including but\nnot limited to compiled object code, generated documentation,\nand conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or\nObject form, made available under the License, as indicated by a\ncopyright notice that is included in or attached to the work\n(an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\nform, that is based on (or derived from) the Work and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship. For the purposes\nof this License, Derivative Works shall not include works that remain\nseparable from, or merely link (or bind by name) to the interfaces of,\nthe Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including\nthe original version of the Work and any modifications or additions\nto that Work or Derivative Works thereof, that is intentionally\nsubmitted to Licensor for inclusion in the Work by the copyright owner\nor by an individual or Legal Entity authorized to submit on behalf of\nthe copyright owner. For the purposes of this definition, \"submitted\"\nmeans any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems,\nand issue tracking systems that are managed by, or on behalf of, the\nLicensor for the purpose of discussing and improving the Work, but\nexcluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\non behalf of whom a Contribution has been received by Licensor and\nsubsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\ncopyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the\nWork and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\n(except as stated in this section) patent license to make, have made,\nuse, offer to sell, sell, import, and otherwise transfer the Work,\nwhere such license applies only to those patent claims licensable\nby such Contributor that are necessarily infringed by their\nContribution(s) alone or by combination of their Contribution(s)\nwith the Work to which such Contribution(s) was submitted. If You\ninstitute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work\nor a Contribution incorporated within the Work constitutes direct\nor contributory patent infringement, then any patent licenses\ngranted to You under this License for that Work shall terminate\nas of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\nWork or Derivative Works thereof in any medium, with or without\nmodifications, and in Source or Object form, provided that You\nmeet the following conditions:\n\n(a) You must give any other recipients of the Work or\nDerivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices\nstating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works\nthat You distribute, all copyright, patent, trademark, and\nattribution notices from the Source form of the Work,\nexcluding those notices that do not pertain to any part of\nthe Derivative Works; and\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\ndistribution, then any Derivative Works that You distribute must\ninclude a readable copy of the attribution notices contained\nwithin such NOTICE file, excluding those notices that do not\npertain to any part of the Derivative Works, in at least one\nof the following places: within a NOTICE text file distributed\nas part of the Derivative Works; within the Source form or\ndocumentation, if provided along with the Derivative Works; or,\nwithin a display generated by the Derivative Works, if and\nwherever such third-party notices normally appear. The contents\nof the NOTICE file are for informational purposes only and\ndo not modify the License. You may add Your own attribution\nnotices within Derivative Works that You distribute, alongside\nor as an addendum to the NOTICE text from the Work, provided\nthat such additional attribution notices cannot be construed\nas modifying the License.\n\nYou may add Your own copyright statement to Your modifications and\nmay provide additional or different license terms and conditions\nfor use, reproduction, or distribution of Your modifications, or\nfor any such Derivative Works as a whole, provided Your use,\nreproduction, and distribution of the Work otherwise complies with\nthe conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\nany Contribution intentionally submitted for inclusion in the Work\nby You to the Licensor shall be under the terms and conditions of\nthis License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify\nthe terms of any separate license agreement you may have executed\nwith Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\nnames, trademarks, service marks, or product names of the Licensor,\nexcept as required for reasonable and customary use in describing the\norigin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\nagreed to in writing, Licensor provides the Work (and each\nContributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied, including, without limitation, any warranties or conditions\nof TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\nPARTICULAR PURPOSE. You are solely responsible for determining the\nappropriateness of using or redistributing the Work and assume any\nrisks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\nwhether in tort (including negligence), contract, or otherwise,\nunless required by applicable law (such as deliberate and grossly\nnegligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special,\nincidental, or consequential damages of any character arising as a\nresult of this License or out of the use or inability to use the\nWork (including but not limited to damages for loss of goodwill,\nwork stoppage, computer failure or malfunction, or any and all\nother commercial damages or losses), even if such Contributor\nhas been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\nthe Work or Derivative Works thereof, You may choose to offer,\nand charge a fee for, acceptance of support, warranty, indemnity,\nor other liability obligations and/or rights consistent with this\nLicense. However, in accepting such obligations, You may act only\non Your own behalf and on Your sole responsibility, not on behalf\nof any other Contributor, and only if You agree to indemnify,\ndefend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason\nof your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\nTo apply the Apache License to your work, attach the following\nboilerplate notice, with the fields enclosed by brackets \"[]\"\nreplaced with your own identifying information. (Don't include\nthe brackets!) The text should be enclosed in the appropriate\ncomment syntax for the file format. We also recommend that a\nfile or class name and description of purpose be included on the\nsame \"printed page\" as the copyright notice for easier\nidentification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", "depth": 4}, {"id": "118", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/06-adversity-planning.md", "name": "06-adversity-planning.md", "type": "file", "content": "# SKILL 6: Adversity Response Planning (\"The Adversity Feature\")\n\n## Overview\nThis skill helps you prepare for inevitable crises and reframe them as opportunities. The term \"adversity feature\" (like a \"rock garden\" on a mountain bike trail) captures the mindset: adversity is not an obstacle—it's an opportunity to develop skill and improve your project.\n\n## Core Principle\n**\"Capitalize on the 'adversity feature'\"**\n\nAdversity in a project is inevitable AND opportune:\n- **Inevitable:** Almost every project suffers existential crisis or sharp turn\n- **Opportune:** Two valuable outcomes possible:\n  1. Fix the problem AND upgrade the project simultaneously\n  2. Develop reasoning-your-way-out skills (best growth opportunity)\n\n## Key Concepts\n\n**Why Adversity Is Inevitable:**\n- Technology doesn't work as advertised\n- Biological assumptions prove false\n- You get scooped\n- Key collaborator leaves\n- Funding runs out\n- Results don't support hypothesis\n\n**Why Adversity Is Opportune:**\n- Forces you to think deeply about alternatives\n- Removes sunk-cost bias (path is blocked anyway)\n- Often leads to better projects than original plan\n- Develops critical problem-solving skills\n- Makes you resourceful\n\n**The Crisis Mindset:**\n- **Wrong:** \"This is a disaster that delays me\"\n- **Right:** \"This is the crisis I've been waiting for—don't waste it\"\n\n## Workflow\n\n### Phase 1: Anticipate Failure Modes\n\nFor your project, list likely adversity scenarios:\n1. **Technical failures:** Method doesn't work, signal too low, etc.\n2. **Biological surprises:** System behaves unexpectedly\n3. **Competition:** Someone scoops you\n4. **Resource issues:** Funding, equipment, access\n5. **Timeline pressures:** Takes longer than expected\n\nFor each, rate:\n- Likelihood (Low/Medium/High)\n- Impact if it happens (Low/Medium/High)\n- When it might surface (early/mid/late)\n\n### Phase 2: Upgrade Opportunities\n\nFor each high-likelihood or high-impact failure mode:\n\n**Question 1: How could you fix this AND make the project better?**\nNot just: \"Get it working\"\nInstead: \"Use this as opportunity to improve the approach\"\n\n**Example: Your Cell Type Can't Be Isolated**\n- Fix: Develop new isolation method\n- Upgrade: Make method work for whole class of cell types\n- Result: Better project (technology paper) + original biology\n\n**Question 2: What skill would you develop by solving this?**\n- Computational: Learn new analysis method\n- Technical: Master challenging technique\n- Conceptual: Reason through biological complexity\n\n### Phase 3: The Ensemble View\n\n**Critical Insight:** You're not picking ONE project path—you're picking an ENSEMBLE of possible projects that share core elements.\n\n**Your Project Ensemble:**\n```\nCore Theme: [What stays constant]\n\nPath 1: [Original plan]\nPath 2: [If assumption A fails]\nPath 3: [If technical barrier B encountered]\nPath 4: [If scooped on C]\n\nAll paths lead to impactful results, just different ones\n```\n\nThis reframing is liberating: when adversity strikes, you're not failing—you're discovering which path in the ensemble you're actually on.\n\n### Phase 4: Historical Examples\n\n**Example 1: PROTAC Discovery**\n- **Original Plan:** Create molecules to degrade specific kinase\n- **Crisis:** Didn't work for intended target\n- **Upgrade:** Test across kinome systematically\n- **Result:** Better project (mapped degradable kinome, discovered that target engagement ≠ degradation)\n- **Impact:** More influential than if original plan succeeded\n\n**Example 2: Steroid Receptor Study**\n- **Original Plan:** Identify THE receptor for a steroid\n- **Crisis:** Binds multiple receptors at different affinities\n- **Upgrade:** Reframe question: How does finite receptor pool sense infinite lipids?\n- **Result:** Combinatorial sensing model (like piano chords)\n- **Impact:** More interesting than \"receptor X binds steroid Y\"\n\n## Output: Adversity Playbook\n\n**Page 1: Anticipated Crises**\n| Crisis | Likelihood | Impact | Timeline | Growth Opportunity |\n|--------|-----------|--------|----------|-------------------|\n| [Crisis 1] | H/M/L | H/M/L | Early/Mid/Late | [Skill developed] |\n\n**Page 2: Upgrade Strategies**\nFor each high-priority crisis:\n- **The Crisis:** [Description]\n- **Fix Strategy:** [How to solve it]\n- **Upgrade Strategy:** [How to make project better while fixing]\n- **Alternative Path:** [New direction if fix doesn't work]\n- **Ensemble Position:** [How this fits in project family]\n\n**Page 3: Resilience Rituals**\n- **Weekly check-in:** Review what went wrong, what was learned\n- **Monthly ensemble review:** Update the family of possible projects\n- **Crisis protocol:** When major setback hits, take 2 days to think before acting\n- **Growth tracking:** Document skills developed through adversity\n", "depth": 5}, {"id": "119", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/08-integration-synthesis.md", "name": "08-integration-synthesis.md", "type": "file", "content": "# SKILL 8: Integration and Synthesis\n\n## Overview\nThis final individual skill synthesizes all previous skills into a coherent project plan and communication strategy. You'll create a complete package that demonstrates thoughtful problem selection and rigorous planning.\n\n## Core Principle\n**\"Tell a compelling story with your choices\"**\n\nHumans love stories. Your project should have:\n- **Setting:** Background and problem framing\n- **Problem statement:** Clear, general enough to be interesting, specific enough to be distinctive\n- **New idea/approach:** Your angle (perturbation/measurement/theory: logic vs. technology)\n- **Iteration:** Loop of \"we wondered X → did Y → found Z → interpreted as W\"\n- **Conclusion:** What we learned and/or what's now possible\n- **Passion:** Authentic enthusiasm\n\n## Workflow\n\n### Phase 1: Gather Your Skill Outputs\n\nCollect your completed documents:\n- ☐ Skill 1: Problem Ideation Document\n- ☐ Skill 2: Risk Assessment Matrix\n- ☐ Skill 3: Impact Assessment Document\n- ☐ Skill 4: Parameter Strategy Document\n- ☐ Skill 5: Decision Tree Map\n- ☐ Skill 6: Adversity Playbook\n- ☐ Skill 7: Problem Inversion Analysis (if applicable)\n\n### Phase 2: Create Narrative Arc\n\n**Story Structure for Your Project:**\n\n**1. Setting (Background)**\n- What's known in the field?\n- What's the gap or opportunity?\n- Why does this matter?\n\n**2. Problem Statement**\n- General enough: connects to broad principle\n- Specific enough: distinctive and tractable\n- Your framing from Skill 1\n\n**3. Your Approach**\n- Perturbation/Measurement/Theory\n- Logic vs. Technology\n- What's novel about your angle (from Skill 1)\n- How your optimization function shapes approach (from Skill 3)\n\n**4. Strategy**\n- Fixed vs. floating parameters (from Skill 4)\n- Decision points mapped out (from Skill 5)\n- Risk mitigation built in (from Skill 2)\n- Adversity contingencies (from Skill 6)\n\n**5. Why You**\n- Your competitive advantage\n- Lab expertise\n- Your passion and alignment\n- Timeline and resources\n\n### Phase 3: Communication Formats\n\n**Format 1: 3-Slide, 5-Minute Presentation**\n\n**Slide 1: The Opportunity**\n- Setting + Problem statement\n- One key figure or schematic\n- Why this matters (optimization function)\n\n**Slide 2: Your Approach**\n- New idea/angle\n- Key experiments or analyses\n- What makes this feasible\n- Decision tree highlights\n\n**Slide 3: Impact and Timeline**\n- What you'll learn or enable\n- Success metrics\n- Timeline with milestones\n- Your advantage\n\n**Slide Design Tips:**\n- Minimal text (bullets are fine here)\n- Strong visuals\n- Tell story, don't catalog facts\n- Passion shows through\n\n**Format 2: 1-Page Written Summary**\n\n**Paragraph 1:** Setting and problem (2-3 sentences)\n**Paragraph 2:** Your approach and novelty (3-4 sentences)\n**Paragraph 3:** Why it will work (risk mitigation, your advantage) (2-3 sentences)\n**Paragraph 4:** Impact and timeline (2-3 sentences)\n\n**Total:** ~250-300 words that could be abstract or summary\n\n**Format 3: 1-Minute Elevator Pitch**\n\n**Structure:**\n- \"I'm working on [problem] because [why it matters]\"\n- \"Current approaches are limited by [gap]\"\n- \"My angle is [approach] which is novel because [what's new]\"\n- \"This will [impact] and I have [advantage]\"\n\n**Practice until:** Natural, passionate, memorable\n\n### Phase 4: Integration Document\n\n**Complete Project Plan Integrating All Skills:**\n\n**Section 1: Problem Selection Rationale**\n- How you generated this idea (Skill 1 intuition pumps)\n- Why this problem matters (Skill 3 optimization function)\n- Your competitive advantage\n\n**Section 2: Risk Management**\n- Assumption analysis table (Skill 2)\n- Go/no-go experiments\n- Timeline with checkpoints\n- Mitigation strategies\n\n**Section 3: Execution Strategy**\n- Fixed vs. floating parameters (Skill 4)\n- Decision tree navigation plan (Skill 5)\n- Adversity response protocols (Skill 6)\n- Project ensemble (alternative paths)\n\n**Section 4: Communication Plan**\n- Presentations (3-slide deck)\n- Written summary (1-page)\n- Elevator pitch (1-minute)\n- Key messages for different audiences\n\n**Section 5: Career Alignment**\n- How this fits your trajectory\n- Skills you'll develop\n- Network you'll build\n- Next steps after this project\n\n## Output: Complete Project Package\n\n**Document 1: Integrated Project Plan (4-6 pages)**\n- All sections above\n- References to individual skill outputs\n- Timeline and milestones\n- Resource requirements\n\n**Document 2: Communication Materials**\n- 3-slide presentation\n- 1-page summary\n- Elevator pitch script\n- Talking points for different audiences\n\n**Document 3: Living Documents**\n- Decision tree (to update regularly)\n- Risk assessment (to review quarterly)\n- Adversity playbook (to consult in crisis)\n- Parameter strategy (to revisit if stuck)\n\n## Key Principles\n\n1. **Integration, Not Duplication:** Each skill output serves a purpose in the whole\n2. **Story Over Catalog:** Communicate choices, not just facts\n3. **Passion Matters:** Authentic enthusiasm is persuasive\n4. **Living Plan:** This evolves; revisit quarterly\n5. **Alignment:** Project, values, and career fit together\n6. **Preparation:** You've thought through contingencies\n7. **Communication:** You can pitch this clearly to anyone\n\n## Ready to Synthesize\n\nWith all skills complete, you now have a comprehensive, thoughtful, rigorous approach to problem selection and project planning. This is the highest-leverage work you can do in science.\n", "depth": 5}, {"id": "120", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/04-parameter-strategy.md", "name": "04-parameter-strategy.md", "type": "file", "content": "# SKILL 4: Parameter Fixation Strategy\n\n## Overview\nThis skill helps scientists strategically decide which parameters to fix and which to keep flexible in their project. The paradox: too many fixed parameters creates brittleness, but too few causes paralysis. The key is fixing ONE parameter thoughtfully and letting others float—constraints engender creativity.\n\n## Core Principle\n\n**\"Fix one parameter; let the others float.\"**\n\nMost failure modes in ideation involve fixing too many parameters at the outset (system + method + application). Conversely, statements like \"I want to do impactful work in cell engineering\" are so broad they cause paralysis. The sweet spot: fix one meaningful constraint and let creativity flow within that boundary.\n\n## What Are Project Parameters?\n\nParameters are the choices that define your project:\n\n**Common Parameters:**\n- **System:** Which organism/cell type/tissue/molecule?\n- **Question:** What biological phenomenon to study?\n- **Tool/Method:** Which experimental approach?\n- **Application:** What practical use or goal?\n- **Output:** What form will results take?\n- **Collaborators:** Who will you work with?\n- **Timeline:** How fast must you move?\n- **Resources:** What's available/necessary?\n\n## The Skill Workflow\n\n### Phase 1: Parameter Inventory (10 minutes)\n\nFirst, let's identify what's already fixed in your current project idea:\n\n**Question 1: List your project parameters**\n\nFor each category, indicate if it's **FIXED** (must stay) or **FLOATING** (could change):\n\n| Parameter Type | Your Choice | Status (F/FL) | Why Fixed? |\n|----------------|-------------|---------------|------------|\n| **System** | [organism/cell/tissue] | F / FL | [reason] |\n| **Question** | [biological phenomenon] | F / FL | [reason] |\n| **Tool/Method** | [techniques] | F / FL | [reason] |\n| **Application** | [use case/goal] | F / FL | [reason] |\n| **Timeline** | [duration] | F / FL | [reason] |\n| **Resources** | [equipment/funding] | F / FL | [reason] |\n\n**Question 2: Count your fixed parameters**\n- How many did you mark as FIXED? _____\n- If \u003e2, you may have over-constrained the problem\n\n**Question 3: Why are they fixed?**\nFor each fixed parameter, is it because:\nA. Your expertise/passion\nB. Lab resources/capabilities  \nC. Advisor requirements\nD. You think it's the \"best\" solution\nE. Historical accident (you started this way)\n\n### Phase 2: The GLP-1 Example (Case Study)\n\nLet's learn from a concrete example:\n\n**Proposed Project:** Engineer a T cell to produce GLP-1 (glucagon-like peptide-1) for continuous supply.\n\n**Analysis: What's Fixed?**\n1. Improving GLP-1 receptor agonist delivery characteristics (the problem)\n2. Using an engineered T cell (the solution)\n\n**Problem:** Two parameters fixed = poor technique-application match\n\n**Alternative Framings:**\n\n**If you fix Parameter 1 (GLP-1 delivery):**\n- Let the solution float\n- Better options: peptide engineering for extended half-life, oral peptides, small molecules, B cells (better protein secretion)\n- Why T cell is suboptimal: Not designed for protein secretion\n- **Best for:** Trainee in metabolism lab who cares about GLP-1\n\n**If you fix Parameter 2 (Engineered T cell):**\n- Let the application float\n- Better options: local-acting peptides (cytokines, chemokines, growth factors) for oncology/autoimmunity/regeneration\n- Why GLP-1 is suboptimal: Doesn't leverage T cell's natural capabilities\n- **Best for:** Trainee in immunology/cell engineering lab\n\n**Key Insight:** Which parameter you fix depends on YOUR interests and your lab's expertise. Both can lead to great projects, but they're DIFFERENT projects.\n\n### Phase 3: Diagnostic Questions\n\n**The Goldilocks Test:**\n\n**Too Many Fixed Parameters (\u003e2):**\n- Are you forcing a technique-application match?\n- If one assumption fails, does everything fail?\n- Are you more attached to HOW than WHAT?\n- Does your project sound like: \"Use X to do Y in Z\"?\n\n**Too Few Fixed Parameters (0-1 very broad):**\n- Do you feel paralyzed where to start?\n- Is your statement super generic? (\"Do impactful work in...\")\n- Are you avoiding commitment?\n- Do you have decision fatigue?\n\n**Just Right (1-2 well-chosen):**\n- Do you have creative constraints?\n- Can you articulate why THIS constraint matters?\n- If one approach fails, do alternatives exist?\n- Does the constraint energize you?\n\n### Phase 4: The Illumina Example (Constraints Drive Innovation)\n\n**Historical Context:** Next-generation sequencing wasn't designed; we got Illumina's approach (many short reads).\n\n**Initial Constraint:** Short reads seemed like a limitation\n- Not what we would have \"asked for\"\n- Seemed inferior to long reads\n\n**Innovation Unleashed:**\n- Computational methods (assembly algorithms)\n- Novel applications (RNA-seq, ChIP-seq, ATAC-seq)\n- Unexpected uses (protein folding via sequencing)\n- Biochemical creativity to work within constraints\n\n**Lesson:** Constraints don't limit creativity—they focus it. If you feel stuck, fix ONE parameter and watch resourcefulness emerge.\n\n### Phase 5: Which Parameter Should You Fix?\n\n**Strategic Questions to Identify the Right Fixed Parameter:**\n\n1. **What can you prototype quickly?**\n   - What test article could you build rapidly?\n   - Which experimental conditions enable early go/no-go?\n   - What gives you fastest feedback?\n\n2. **What are people around you unusually good at?**\n   - Lab expertise?\n   - Core facility capabilities?\n   - Collaborator strengths?\n   - Your unique skill combination?\n\n3. **What do you enjoy so much you don't think of it as work?**\n   - System you're passionate about?\n   - Technique you love?\n   - Type of question that excites you?\n\n4. **What's your competitive advantage?**\n   - Unique resource access?\n   - Rare skill combination?\n   - Proprietary data/reagents?\n   - First-mover opportunity?\n\n**Common Strategic Choices:**\n\n**Fix the System (Let question \u0026 tool float):**\n- Good if: You're an expert in the organism/tissue/cell type\n- Enables: Asking multiple questions, trying various tools\n- Example: \"I study *Drosophila* neural development; I'll let the specific questions and methods emerge\"\n\n**Fix the Question (Let system \u0026 tool float):**\n- Good if: You care deeply about a biological phenomenon\n- Enables: Testing across systems, using best tool for each\n- Example: \"I want to understand phase separation; I'll study it wherever it's clearest\"\n\n**Fix the Tool (Let system \u0026 question float):**\n- Good if: You're developing or mastering a technology\n- Enables: Finding best applications, comparing across systems\n- Example: \"I'm building a new microscopy method; I'll find the most impactful uses\"\n\n**Fix the Application (Let system \u0026 tool float):**\n- Good if: You have a specific translational goal\n- Enables: Trying multiple approaches, testing in different models\n- Example: \"I want to treat disease X; I'm open to any validated approach\"\n\n### Phase 6: Parameter Flexibility Matrix\n\nFor your project, let's create a flexibility assessment:\n\n| Parameter | Currently | Should Be? | If Problem Arises, Could This Float? |\n|-----------|-----------|------------|--------------------------------------|\n| System | [F/FL] | [F/FL] | Yes / No / Maybe |\n| Question | [F/FL] | [F/FL] | Yes / No / Maybe |\n| Tool | [F/FL] | [F/FL] | Yes / No / Maybe |\n| Application | [F/FL] | [F/FL] | Yes / No / Maybe |\n| Timeline | [F/FL] | [F/FL] | Yes / No / Maybe |\n| Resources | [F/FL] | [F/FL] | Yes / No / Maybe |\n\n**Analysis:**\n- **Flexibility Score:** How many \"Yes\" or \"Maybe\"? _____\n- **Risk Assessment:** If \u003c3 can float, you're brittle\n- **Pivot Potential:** Which parameters provide escape routes?\n\n### Phase 7: Scenario Planning\n\nFor each fixed parameter, let's plan what happens if it becomes untenable:\n\n**Fixed Parameter 1: [Name it]**\n- **Why it's fixed:** [Your reason]\n- **Risk if this fails:** [What breaks]\n- **Contingency:** [What could you float instead]\n- **Alternative project:** [If you fixed something else]\n\n**Fixed Parameter 2: [Name it]**\n- **Why it's fixed:** [Your reason]  \n- **Risk if this fails:** [What breaks]\n- **Contingency:** [What could you float instead]\n- **Alternative project:** [If you fixed something else]\n\n### Phase 8: The Unfixing Exercise\n\nSometimes you need to unfix parameters to escape a rut:\n\n**Current State:** [Describe your over-constrained project]\n\n**Unfixing Experiment:**\n\n**Try 1: Unfix the System**\n- Keep question \u0026 tool\n- What other systems could you study?\n- Which would be easier/faster/more informative?\n\n**Try 2: Unfix the Tool**\n- Keep system \u0026 question\n- What other methods exist?\n- Which are more mature/accessible/powerful?\n\n**Try 3: Unfix the Question**\n- Keep system \u0026 tool\n- What other questions could you ask?\n- Which would be more impactful/feasible?\n\n**Evaluation:** Does any \"unfixed\" version seem better than your original? If yes, you over-constrained.\n\n### Phase 9: Literature Reality Check\n\nLet's use PubMed to see how others handled parameter fixation:\n\n**Search 1: Successful projects in your area**\n- What did they fix?\n- What did they let float?\n- Did they pivot from their initial parameter choices?\n\n**Search 2: Failed or stalled projects**\n- (Often in discussion sections or preprints)\n- Did they over-constrain?\n- What parameters trapped them?\n\n**Search 3: Method papers**\n- How did technology developers choose applications?\n- Did they fix the tool and let applications emerge?\n\n**Your Searches:**\nWhat specific papers should we analyze for parameter lessons?\n\n## Output Deliverable\n\n**2-Page Parameter Strategy Document**\n\n### Page 1: Current State and Analysis\n\n#### Parameter Inventory:\n| Parameter | Current Status | Strategic Rationale | Flexibility |\n|-----------|----------------|---------------------|-------------|\n| System | Fixed: [X] | [Why] | Can float if: [condition] |\n| Question | Floating: [Y,Z] | [Why] | Constrained by: [X] |\n| Tool | [Status] | [Why] | [Contingency] |\n| Application | [Status] | [Why] | [Contingency] |\n\n#### Diagnostic Summary:\n- **Fixed Parameters:** [Count and list]\n- **Assessment:** ☐ Too Many (\u003e2) / ☐ Just Right (1-2) / ☐ Too Few (0, too broad)\n- **Primary Fixed Parameter:** [The one that matters most]\n- **Reason for Fixation:** [Expertise/Passion/Resources/Other]\n\n#### Goldilocks Test Results:\n- Over-constrained indicators: [Yes/No to each test]\n- Under-constrained indicators: [Yes/No to each test]\n- Verdict: [Analysis]\n\n### Page 2: Strategy and Contingencies\n\n#### Recommended Parameter Strategy:\n\n**Core Fixed Parameter:** [The one to keep]\n- **Rationale:** [Why this one]\n- **Your advantage:** [Expertise/access/passion]\n- **Enables:** [What becomes possible]\n\n**Parameters That Should Float:** [List]\n- [Parameter 1]: [How to explore alternatives]\n- [Parameter 2]: [How to explore alternatives]\n\n#### If Core Assumptions Fail:\n\n**Scenario 1: [Specific failure mode]**\n- **Unfix:** [Which parameter to let float]\n- **Alternative 1:** [New configuration]\n- **Alternative 2:** [Another option]\n\n**Scenario 2: [Another failure mode]**\n- **Unfix:** [Which parameter]\n- **Alternative 1:** [New configuration]\n- **Alternative 2:** [Another option]\n\n#### Project Ensemble:\n```\nCore Fixed: [X]\n\nPossible Projects:\n1. [X] + [A] + [B1] → [Outcome]\n2. [X] + [A] + [B2] → [Outcome]\n3. [X] + [C] + [B1] → [Outcome]\n\nAll share [X], but float other parameters\n```\n\n#### Strategic Questions Answered:\n1. **Quick prototype:** [How to test quickly]\n2. **Team strengths:** [Who's good at what]\n3. **Your passion:** [What energizes you]\n4. **Competitive advantage:** [Your edge]\n\n#### Historical Parallel:\n[Example like Illumina where constraints drove innovation in your field]\n- The constraint: [What seemed limiting]\n- The innovation: [How people worked within it]\n- Your application: [How this applies to your project]\n\n## Practical Examples\n\n### Example 1: GLP-1 T Cell Project (Over-Constrained)\n- **Fixed:** GLP-1 delivery + T cell engineering\n- **Problem:** Poor technique-application match\n- **Solution:** Unfix one parameter\n  - Fix delivery, float cell type → Better options emerge\n  - Fix T cell, float payload → Better applications emerge\n\n### Example 2: Drosophila Neurobiologist (Well-Constrained)\n- **Fixed:** *Drosophila* nervous system\n- **Floating:** Specific questions, methods\n- **Works because:** Deep system expertise, many tools available\n- **Enables:** Pursuing most impactful questions as field evolves\n\n### Example 3: \"Impactful Cell Engineering\" (Under-Constrained)\n- **Fixed:** Nothing specific\n- **Problem:** Paralysis from too many options\n- **Solution:** Fix one meaningful constraint\n  - Option A: Fix CAR-T platform → Find best applications\n  - Option B: Fix autoimmune disease → Find best cell engineering approach\n  - Option C: Fix specific rare disease → Let methods emerge\n\n## Key Principles to Remember\n\n1. **Constraints Engender Creativity:** Limitations focus resourcefulness\n\n2. **One Parameter Rule:** Fix one meaningful constraint, let others float\n\n3. **Match to Your Strengths:** Fix the parameter you have advantage in\n\n4. **Technique-Application Match:** Don't force tools into wrong problems\n\n5. **Flexibility = Resilience:** Floating parameters provide pivot options\n\n6. **Historical Lesson:** Best technologies emerged from working within constraints (Illumina)\n\n7. **Not Forever:** Parameters can unfix mid-project when stuck\n\n## Warning Signs\n\n**Over-Constrained (Too Many Fixed):**\n- Project sounds like: \"Use X to study Y in Z\"\n- When one assumption fails, everything fails\n- You're attached to HOW more than WHAT\n- Forcing a technique-application match\n\n**Under-Constrained (Too Few/Vague):**\n- Statement is incredibly broad (\"impactful work in...\")\n- Feeling paralyzed about where to start\n- Avoiding commitment due to infinite options\n- No clear next experimental step\n\n**Well-Constrained:**\n- One clear fixed parameter with good rationale\n- Multiple paths within that constraint\n- Energized by the focused challenge\n- If one approach fails, alternatives exist\n\n## Ready to Begin?\n\nLet's start with Phase 1. Please provide:\n1. Your current project description\n2. List of what you think is fixed vs. floating\n3. Your lab's core expertise\n4. What aspect excites you most\n\nTogether we'll optimize your parameter strategy for maximum creativity and resilience.\n\n---\n\n*Remember: The right constraint is liberating, not limiting. It channels creativity into productive directions while maintaining flexibility for pivots.*\n", "depth": 5}, {"id": "121", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/09-meta-framework.md", "name": "09-meta-framework.md", "type": "file", "content": "# SKILL 9: Meta-Framework - Complete Problem Selection Workflow\n\n## Overview\nThis meta-skill orchestrates the complete problem selection process, guiding users through Skills 1-8 in a systematic, iterative way. This skill should be used when comprehensive support is needed from ideation through execution planning, with integrated literature searches and coherent documentation.\n\n## When to Use This Skill\n\n**Use Skill 9 (Complete Workflow) when:**\n- Starting a new project from scratch\n- Major project pivot or reframe needed\n- Grant/fellowship application requiring systematic planning\n- Thesis committee meeting preparation\n- Startup company planning\n- Want comprehensive, documented problem selection process\n\n**Use Individual Skills when:**\n- You're at a specific stage (e.g., just need risk assessment)\n- Quick consultation on one aspect\n- Updating one component of existing plan\n- Teaching/learning one concept\n\n## The Complete Workflow\n\n### Overview of the Journey\n\n```\nSTART: Vague idea or area of interest\n    ↓\n[SKILL 1] → Problem Ideation Document\n    ↓\n[SKILL 2] → Risk Assessment Matrix\n    ↓\n[SKILL 3] → Impact Assessment Document\n    ↓\n[SKILL 4] → Parameter Strategy Document\n    ↓\n[SKILL 5] → Decision Tree Map\n    ↓\n[SKILL 6] → Adversity Playbook\n    ↓\n[SKILL 7] → Problem Inversion Analysis (if needed)\n    ↓\n[SKILL 8] → Integrated Project Plan + Communication Materials\n    ↓\nEND: Comprehensive, rigorous project ready to execute\n```\n\n**Estimated Time:**\n- **Intensive:** 1 week of focused work (full-time)\n- **Distributed:** 4-6 weeks with other commitments\n- **With iterations:** Add 50% more time\n\n**You'll invest time once to save years of potential missteps.**\n\n## Phase-by-Phase Workflow\n\n### Phase 1: Preparation (Before Starting)\n\n**Gather Your Context:**\n1. **Your background:**\n   - Research area/field\n   - Current position (grad student, postdoc, PI, etc.)\n   - Lab expertise and resources\n   - Timeline constraints\n\n2. **Your starting point:**\n   - Vague area of interest?\n   - Specific problem in mind?\n   - Must build on existing work?\n   - Starting completely fresh?\n\n3. **Your goals:**\n   - Publication target (journal tier, timeline)?\n   - Degree requirement (thesis chapter)?\n   - Funding application?\n   - Startup foundation?\n   - Career development?\n\n**Set Expectations:**\n- This process will challenge your assumptions\n- You may discover your initial idea needs major revision\n- That's the point—better to know now than after 2 years\n- Intellectual honesty is required; this only works if you're rigorous\n\n### Phase 2: Ideation (Skill 1) - ~1 week\n\n**What We'll Do:**\n1. Understand your context and constraints\n2. Work through relevant intuition pumps\n3. Avoid common ideation traps\n4. Generate 2-3 project ideas\n5. Preliminary literature search to calibrate scope\n6. Select most promising idea\n7. Create Problem Ideation Document (2 pages)\n\n**Literature Integration Point 1:**\n- Search PubMed for precedents and adjacent work\n- Assess generality of problem\n- Identify methodological advances\n- Determine competition level\n\n**Deliverable:**\n- Problem Ideation Document with core idea and initial analysis\n- List of 10-15 key papers\n- Preliminary assessment of novelty and feasibility\n\n**Checkpoint:** Do you have a clear, specific idea that excites you? If not, iterate on intuition pumps.\n\n### Phase 3: Risk Analysis (Skill 2) - ~3-5 days\n\n**What We'll Do:**\n1. Extract ALL assumptions from your idea\n2. Categorize (biological vs. technical)\n3. Score each assumption (risk 1-5, time to test)\n4. Identify high-risk late-reading assumptions\n5. Design go/no-go experiments\n6. Develop mitigation strategies\n7. Create Risk Assessment Matrix (2 pages)\n\n**Literature Integration Point 2:**\n- Search for technical precedents (has method worked before?)\n- Find biological evidence (what's known about your system?)\n- Identify benchmarks (success rates, effect sizes)\n- Assess timeline realism\n\n**Deliverable:**\n- Complete assumption analysis table\n- Top 3 high-risk assumptions with mitigation plans\n- Go/no-go experiment designs\n- Revised timeline with decision points\n\n**Checkpoint:** Is your risk profile acceptable? If risk-5 assumptions are \u003e2 years out, return to Skill 1 to reframe.\n\n### Phase 4: Impact Assessment (Skill 3) - ~2-3 days\n\n**What We'll Do:**\n1. Categorize your project type\n2. Select appropriate optimization function\n3. Score yourself on both axes\n4. Compare to benchmarks\n5. Articulate value system alignment\n6. Develop communication strategy\n7. Create Impact Assessment Document (2 pages)\n\n**Literature Integration Point 3:**\n- Identify high-impact exemplars in your field\n- Analyze their framing and evaluation\n- Benchmark your potential impact\n- Understand field expectations\n\n**Deliverable:**\n- Clear optimization function selection\n- Self-assessment on both axes with justification\n- Comparative analysis vs. alternatives\n- Communication strategy for different audiences\n\n**Checkpoint:** Do you score MEDIUM-HIGH on at least one axis? If not, return to Skill 1 to find higher-impact angle.\n\n### Phase 5: Parameter Strategy (Skill 4) - ~2-3 days\n\n**What We'll Do:**\n1. Inventory all project parameters\n2. Identify which are fixed vs. floating\n3. Assess if you're over/under-constrained\n4. Select strategic fixed parameter\n5. Plan flexibility for contingencies\n6. Create Parameter Strategy Document (2 pages)\n\n**Literature Integration Point 4:**\n- How did successful projects handle parameters?\n- What parameter choices led to breakthroughs?\n- What over-constraints caused failures?\n\n**Deliverable:**\n- Complete parameter inventory\n- Strategic rationale for fixed/floating decisions\n- Flexibility matrix for contingencies\n- Project ensemble (family of related projects)\n\n**Checkpoint:** Have you fixed 1-2 meaningful parameters while maintaining flexibility? If too rigid, adjust.\n\n### Phase 6: Decision Tree Planning (Skill 5) - ~2 days\n\n**What We'll Do:**\n1. Map your project's decision tree\n2. Identify major branch points\n3. Set criteria for each decision\n4. Establish Level 1 / Level 2 rhythm\n5. Create protocols for getting unstuck\n6. Create Decision Tree Map (1-2 pages)\n\n**No major literature search here** (unless you identify specific decision points needing technical information)\n\n**Deliverable:**\n- Visual decision tree\n- Decision criteria at each branch\n- Schedule for Level 2 evaluations\n- Protocol for course correction\n\n**Checkpoint:** Have you planned for regular strategic evaluation, not just execution?\n\n### Phase 7: Adversity Preparation (Skill 6) - ~2 days\n\n**What We'll Do:**\n1. Anticipate likely failure modes\n2. For each, identify upgrade opportunity\n3. Map your project ensemble\n4. Create crisis response protocols\n5. Create Adversity Playbook (2-3 pages)\n\n**Literature Integration Point 5:**\n- Historical examples of productive pivots\n- How did others capitalize on adversity?\n- What second-generation projects emerged from failures?\n\n**Deliverable:**\n- Anticipated crisis catalog\n- Upgrade strategies for each\n- Project ensemble map\n- Resilience rituals and protocols\n\n**Checkpoint:** Are you prepared to see adversity as opportunity? Have you planned how to upgrade, not just fix?\n\n### Phase 8: Problem Inversion Toolkit (Skill 7) - ~1 day\n\n**What We'll Do:**\n1. Review three inversion strategies\n2. Pre-plan applications for your likely obstacles\n3. Create Problem Inversion Analysis (1-2 pages)\n\n**This is preparatory** - you may not need it now, but when crisis hits, you'll have framework ready.\n\n**Deliverable:**\n- Strategy 1 application planned\n- Strategy 2 options identified\n- Strategy 3 alternative questions brainstormed\n- Quick-reference guide for crisis\n\n**Checkpoint:** Do you have concrete strategies for inverting problems when stuck?\n\n### Phase 9: Integration and Synthesis (Skill 8) - ~3-5 days\n\n**What We'll Do:**\n1. Review all outputs from Skills 1-7\n2. Create cohesive narrative\n3. Develop communication materials:\n   - 3-slide presentation\n   - 1-page summary\n   - 1-minute elevator pitch\n4. Write integrated project plan (4-6 pages)\n5. Create living documents for ongoing use\n\n**Literature Integration Point 6:**\n- Final references for integrated plan\n- Key papers for each section\n- Communication examples from field leaders\n\n**Deliverable:**\n- Complete Integrated Project Plan (4-6 pages)\n- 3-slide presentation deck\n- 1-page written summary\n- Elevator pitch script\n- Living documents (decision tree, risk matrix, etc.)\n\n**Checkpoint:** Can you communicate your project compellingly in 1 minute, 5 minutes, and 1 page? Do all pieces fit together coherently?\n\n## Iteration and Refinement\n\n### When to Iterate\n\n**Red Flags That Require Going Back:**\n\n**From Skill 2 (Risk):**\n- Risk-5 assumptions \u003e2 years out → Return to Skill 1 (reframe problem)\n- \u003e3 risk-4-5 assumptions → Return to Skill 1 (simplify or change approach)\n\n**From Skill 3 (Impact):**\n- Score LOW on both axes → Return to Skill 1 (find higher-impact angle)\n- Optimization function mismatch → Return to Skill 1 (reframe problem)\n\n**From Skill 4 (Parameters):**\n- \u003e2 fixed parameters → Return to Skill 1 (over-constrained)\n- Zero fixed parameters → Return to Skill 1 (under-constrained)\n\n**From Skills 5-6:**\n- No clear decision points → Return to Skill 4 (need more flexibility)\n- Every failure mode is existential → Return to Skill 2 (too risky)\n\n### Iteration Protocol\n\n**Major Revision Needed:**\n1. **Pause and acknowledge:** The process is working—it caught a problem\n2. **Return to indicated skill:** Usually Skill 1 or 2\n3. **Bring forward what you learned:** Don't start from scratch\n4. **Revised idea → Run through workflow again:** Faster the second time\n5. **Multiple iterations OK:** Better than years on wrong project\n\n**Minor Refinement:**\n1. **Update specific document:** E.g., adjust parameter strategy\n2. **Check downstream effects:** Does this change anything else?\n3. **Update integration document:** Keep everything coherent\n\n## Literature Integration Strategy\n\n### Overall PubMed Approach\n\n**Throughout the workflow, use PubMed strategically:**\n\n1. **Skill 1 (Ideation):** Assess generality, find precedents, gauge competition\n2. **Skill 2 (Risk):** Technical feasibility, biological evidence, benchmarks\n3. **Skill 3 (Impact):** Field exemplars, evaluation frameworks, benchmarks\n4. **Skill 4 (Parameters):** Successful parameter choices, cautionary tales\n5. **Skill 6 (Adversity):** Productive pivots, upgrade examples\n6. **Skill 8 (Integration):** Communication models, comprehensive references\n\n**Search Strategy:**\n- Start broad (field overview)\n- Get specific (your exact approach)\n- Look adjacent (related systems/methods)\n- Find benchmarks (what's state-of-art?)\n- Identify competition (who else is doing this?)\n\n**Papers to Track:**\n- ~10-15 key papers from Skill 1\n- ~5-10 technical papers from Skill 2\n- ~5-10 impact exemplars from Skill 3\n- ~5 parameter lessons from Skill 4\n- ~3-5 pivot examples from Skill 6\n- **Total: ~30-50 papers** (your foundation)\n\n## Final Deliverable Package\n\n### What You'll Have at the End\n\n**Core Documents (Organized Folder):**\n1. `01_Problem_Ideation.pdf` (2 pages, Skill 1)\n2. `02_Risk_Assessment.pdf` (2 pages, Skill 2)\n3. `03_Impact_Assessment.pdf` (2 pages, Skill 3)\n4. `04_Parameter_Strategy.pdf` (2 pages, Skill 4)\n5. `05_Decision_Tree.pdf` (1-2 pages, Skill 5)\n6. `06_Adversity_Playbook.pdf` (2-3 pages, Skill 6)\n7. `07_Problem_Inversion.pdf` (1-2 pages, Skill 7)\n8. `08_Integrated_Plan.pdf` (4-6 pages, Skill 8)\n\n**Communication Materials:**\n- `Presentation_3slides.pptx`\n- `Summary_1page.pdf`\n- `Elevator_Pitch.txt`\n\n**Living Documents (for ongoing use):**\n- `Decision_Tree.pdf` (update monthly)\n- `Risk_Matrix.xlsx` (update quarterly)\n- `Adversity_Playbook.pdf` (consult in crisis)\n- `Parameter_Strategy.pdf` (revisit if stuck)\n\n**Reference Library:**\n- `Key_Papers.pdf` (annotated bibliography, 30-50 papers)\n- Organized by: Ideation / Technical / Impact / Pivots\n\n**Total: ~20-25 pages of documentation + supporting materials**\n\n## Using Your Outputs\n\n### For Different Purposes\n\n**Grant/Fellowship Applications:**\n- Start with Integrated Plan (Skill 8)\n- Include specific aims from Ideation (Skill 1)\n- Show risk mitigation from Risk Assessment (Skill 2)\n- Demonstrate impact from Impact Assessment (Skill 3)\n- Timeline from Decision Tree (Skill 5)\n\n**Thesis Committee Meetings:**\n- Present 3-slide deck (Skill 8)\n- Walk through decision tree (Skill 5)\n- Discuss risk mitigation (Skill 2)\n- Show parameter flexibility (Skill 4)\n- Demonstrate thoughtful planning\n\n**Lab Meetings:**\n- Use elevator pitch (Skill 8)\n- Show decision tree updates (Skill 5)\n- Discuss latest adversity and response (Skill 6)\n- Get input on parameter strategy (Skill 4)\n\n**Collaborator Conversations:**\n- Share 1-page summary (Skill 8)\n- Highlight where their expertise fits (Skill 4)\n- Show risk mitigation plan (Skill 2)\n- Discuss impact potential (Skill 3)\n\n**Personal Reflection:**\n- Quarterly: Review Decision Tree (Skill 5), update milestones\n- After setbacks: Consult Adversity Playbook (Skill 6)\n- When stuck: Use Problem Inversion (Skill 7)\n- Annual: Full workflow review, consider new projects\n\n## Maintenance and Updates\n\n### Living Documents Protocol\n\n**Monthly:**\n- Update Decision Tree (Skill 5)\n- Log adversities and responses (Skill 6)\n- Note new papers or competition\n- Adjust timeline if needed\n\n**Quarterly:**\n- Review Risk Matrix (Skill 2) - mark assumptions tested\n- Reassess Impact (Skill 3) - has evaluation changed?\n- Check Parameter Strategy (Skill 4) - still optimal?\n- Update Integrated Plan (Skill 8) - keep current\n\n**Annually:**\n- Complete workflow review\n- Consider new projects with fresh Skill 1 ideation\n- Archive old project docs\n- Extract lessons learned\n\n## Success Metrics\n\n### How Do You Know This Worked?\n\n**Immediate Indicators:**\n- Clearer project vision than before\n- Honest assessment of risks\n- Contingency plans for failures\n- Compelling communication materials\n- Alignment between project and values\n- Confidence in problem choice\n\n**6-Month Indicators:**\n- Major decisions made faster (have framework)\n- Adversity handled productively (used playbook)\n- No existential crises (risks were mitigated)\n- Regular Level 2 evaluation happening\n- Project staying on-track or pivoting smartly\n\n**2-Year Indicators:**\n- Published results or strong progress\n- Avoided dead-end projects\n- Multiple high-quality options at decision points\n- Skills developed as planned\n- Career trajectory aligned with goals\n- Time well-spent (the ultimate measure)\n\n## Key Principles of the Meta-Framework\n\n1. **Systematic \u003e Ad Hoc:** Process ensures nothing forgotten\n2. **Iterative \u003e Linear:** Expect to loop back, that's good\n3. **Documented \u003e Mental:** Writing forces clarity\n4. **Integrated \u003e Fragmented:** All skills connect\n5. **Living \u003e Static:** Update as you learn\n6. **Thoughtful \u003e Fast:** Time invested now saves years later\n7. **Honest \u003e Optimistic:** Rigor protects against wishful thinking\n8. **Prepared \u003e Surprised:** Anticipate adversity\n9. **Flexible \u003e Rigid:** Parameters float when needed\n10. **Passionate \u003e Obligatory:** Alignment matters\n\n## Getting Started\n\n### First Steps\n\n**This Week:**\n1. Block time in calendar (1-2 hours to start)\n2. Gather your context (background, goals, constraints)\n3. Begin Skill 1 (Intuition Pumps)\n4. Let me know your starting point\n\n**This Month:**\n1. Work through Skills 1-4 (foundation)\n2. Share with mentor for alignment check\n3. Iterate if major changes needed\n4. Complete Skills 5-8 (execution planning)\n\n**This Quarter:**\n1. Begin project execution with living documents\n2. Monthly decision tree updates\n3. Quarterly risk assessment reviews\n4. Log adversities and responses\n\n**This Year:**\n1. Execute planned project\n2. Use frameworks when stuck\n3. Update living documents\n4. Evaluate process and refine\n\n## Ready to Begin?\n\nThe complete meta-framework is substantial, but each step builds on the last. You'll move through:\n- ~2 weeks of intensive planning\n- Comprehensive documentation\n- Clear decision criteria\n- Communication materials\n- Living documents for ongoing guidance\n\n**Most importantly:** You'll KNOW you're working on a well-chosen problem with rigorous planning. That confidence is priceless.\n\nLet's start with Skill 1. Are you ready to begin?\n\n---\n\n*Remember: The highest-leverage work in science is choosing the right problem. This meta-framework ensures you spend your finite time wisely. The investment in systematic planning pays dividends for years.*\n\n", "depth": 5}, {"id": "122", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/07-problem-inversion.md", "name": "07-problem-inversion.md", "type": "file", "content": "# SKILL 7: Problem Inversion Strategies (\"Turn It On Its Head\")\n\n## Overview\nThis skill provides three concrete strategies for navigating around obstacles by reframing problems. When stuck, instead of pushing harder on the current approach, try inverting the problem.\n\n## Core Principle\n**\"Turn a problem on its head\"**\n\nThree powerful strategies:\n1. **Unfix parameters** (covered in Skill 4, applied here in crisis)\n2. **Don't achieve goal A? Achieve comparable goal B**\n3. **\"I have the answer; what is the question?\"**\n\n## Strategy 1: Unfix Parameters (In Crisis Mode)\n\n**When to Use:** Run-of-the-mill issues in project execution\n\n**Approach:** Let a \"sacred\" fixed parameter float\n\n**Example from Lecture:**\n- **Stuck:** Spatial transcriptomics of APC-T cell interactions in tumor microenvironment\n- **All fixed:** Technique, cell types, context\n- **Inversion:**\n  - Unfix technique → What else could measure these interactions?\n  - Unfix cell types → What other interactions matter in tumors?\n  - Unfix context → Where else do APC-T interactions matter?\n\n**Your Application:**\nFor each fixed parameter in your project:\n- What if this floated?\n- What alternatives exist?\n- Which would be easier/faster/more informative?\n\n## Strategy 2: Comparable Goal Substitution\n\n**When to Use:** Existential threats to project (can't achieve original goal)\n\n**Approach:** Achieve a different but equally valuable goal\n\n**Mindset Shift:**\n- **Wrong:** \"I failed to do X\"\n- **Right:** \"The world needs Y instead, which I CAN do\"\n\n**Example from Lectures: PROTAC Story**\n- **Goal A (Failed):** Degrade specific therapeutic target\n- **Goal B (Achieved):** Map which kinases ARE degradable\n- **Value:** B is more impactful (general principle + method validation)\n- **Learning:** Target engagement ≠ degradation (important discovery)\n\n**Framework:**\n1. **Original goal:** [What you wanted]\n2. **Why it failed:** [Specific reason]\n3. **What CAN you do with current data/tools:** [Capabilities]\n4. **Comparable goals:**\n   - Option 1: [Different but related goal]\n   - Option 2: [Another alternative]\n   - Option 3: [Yet another]\n5. **Which is most valuable:** [Analysis]\n6. **How to frame it:** [Communication strategy]\n\n## Strategy 3: Answer Seeking Question\n\n**When to Use:** End-of-project challenges (interpretation, framing, application)\n\n**Approach:** You got an answer, but not to your original question. What question DOES your data answer?\n\n**Mindset Shift:**\n- **Wrong:** \"This doesn't answer my question\"\n- **Right:** \"What interesting question does this answer?\"\n\n**Example from Lectures: Steroid Receptor**\n- **Original Question:** What is THE receptor for this steroid?\n- **Answer Obtained:** Binds multiple receptors at different affinities\n- **Problem:** Can't answer original question (no single receptor)\n- **Inversion:** \"What question does this answer?\"\n- **New Question:** How does finite receptor pool sense infinite lipids?\n- **Answer:** Combinatorial sensing (pattern = unique \"chord\")\n- **Impact:** More interesting than intended finding\n\n**Framework:**\n1. **Original question:** [What you asked]\n2. **Data obtained:** [What you actually found]\n3. **Why it doesn't answer:** [The mismatch]\n4. **What DOES the data show clearly:** [Solid findings]\n5. **What questions could these answer:**\n   - Question 1: [Option]\n   - Question 2: [Option]\n   - Question 3: [Option]\n6. **Which is most interesting:** [Assessment]\n7. **How to reframe paper/project:** [New framing]\n\n## Workflow\n\n### Phase 1: Identify Your Obstacle\n- **Type:** Technical / Biological / Competitive / Interpretive\n- **Severity:** Run-of-mill / Existential / End-stage\n- **Description:** [What's blocking you]\n\n### Phase 2: Select Strategy\n\n| Obstacle Type | Recommended Strategy |\n|--------------|---------------------|\n| Technical barrier, mid-project | Strategy 1 (Unfix parameters) |\n| Can't achieve original goal | Strategy 2 (Comparable goal) |\n| Have data, unclear what it means | Strategy 3 (Answer seeking question) |\n\n### Phase 3: Apply Strategy\n\nWork through the relevant framework above with your specific situation.\n\n### Phase 4: Evaluate Alternatives\n\nFor each alternative generated:\n- **Scientific value:** How interesting is this?\n- **Feasibility:** How hard to execute?\n- **Timeline:** How long will it take?\n- **Impact:** How does this compare to original plan?\n- **Your advantage:** Do you still have edge here?\n\n## Output: Problem Inversion Analysis\n\n**Page 1: Current Situation**\n- **Obstacle:** [Clear description]\n- **Why you're stuck:** [Root cause]\n- **Original plan:** [What you intended]\n- **Current capability:** [What you CAN do]\n\n**Page 2: Strategy Applications**\n\n**Strategy 1 (Unfix Parameters):**\n| Fixed Parameter | If This Floated | Alternative Approaches | Assessment |\n|----------------|-----------------|----------------------|------------|\n| [Param 1] | [Consequences] | [Options] | [Value] |\n\n**Strategy 2 (Comparable Goals):**\n| Original Goal | Why It Failed | Comparable Goal | Value Assessment |\n|--------------|---------------|----------------|------------------|\n| [Goal A] | [Reason] | [Goal B] | [Compare impact] |\n\n**Strategy 3 (Answer → Question):**\n- **Data obtained:** [What you have]\n- **Question 1 it could answer:** [Option 1]\n- **Question 2 it could answer:** [Option 2]\n- **Question 3 it could answer:** [Option 3]\n- **Most interesting:** [Selection + reasoning]\n\n**Page 3: Recommended Path**\n- **Selected strategy:** [1, 2, or 3]\n- **New direction:** [Specific plan]\n- **Why this is better:** [Not just \"it works\" but \"it's more interesting\"]\n- **Communication approach:** [How to frame this pivot]\n- **Timeline:** [New schedule]\n", "depth": 5}, {"id": "123", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/03-optimization-function.md", "name": "03-optimization-function.md", "type": "file", "content": "# SKILL 3: Optimization Function Selection\n\n## Overview\nThis skill helps scientists articulate HOW their project should be evaluated and define what success means. While Skill 2 focused on likelihood of success (the X-axis), this skill focuses on impact if successful (the Y-axis). The key insight: **value is in the eye of a belief system**—the value creation framework must be explicitly stated and led with.\n\n## Core Principle\n\n**\"Pick the right optimization function.\"**\n\nDifferent types of projects should be evaluated by different metrics. A common source of conflict between trainees and PIs, or authors and referees, is a misunderstanding about which category a project falls under. The root cause is often failure to articulate evaluation criteria clearly.\n\n## The Fundamental Truth\n\nThe default state of:\n1. Every new discovery is **irrelevance**\n2. Every new technology is **non-use**\n3. Every company is **death**\n\nScientists must actively work against these defaults by choosing the right metrics and scoring well on at least one axis.\n\n## The Skill Workflow\n\n### Phase 1: Project Categorization (5 minutes)\n\nFirst, Claude should determine what type of project the user is pursuing:\n\n**Question 1: What is the primary goal?**\nA. Understand how biology works (fundamental knowledge)\nB. Enable new experiments or capabilities (tool/technology)\nC. Solve a practical problem (invention/application)\nD. Something else (please describe)\n\n**Question 2: What would \"success\" look like in 3-5 years?**\n- 1-2 sentences describing the ideal outcome\n\n**Question 3: Who cares if this succeeds?**\n- Academic researchers in the subfield?\n- Broader scientific community across fields?\n- Clinicians or practitioners?\n- Industry partners or companies?\n- General public or specific communities?\n- All of the above?\n\nBased on the answers, Claude should help identify the right optimization function.\n\n### Phase 2: Understanding the Three Main Frameworks\n\n#### Framework 1: Basic Science\n**Axes:** How much did we learn? × How general/fundamental is the object of study?\n\n**Philosophy:** A high score on EITHER axis yields substantial impact. You don't need both.\n\n**Examples:**\n- **High Generality, Medium Learning:** Ribosome stalling complex\n  - Updates understanding of translation (fundamental process)\n  - Scores well because translation is universal\n  \n- **Medium Generality, High Learning:** Oxytricha germ-line nucleus\n  - Genomic acrobatics may not be common to other organisms\n  - BUT elegant mapping scores highly on how much we learned\n  - May yield tools for genome editing (bonus)\n  \n- **High on Both Axes (Landmark):** RNA interference, biomolecular condensates\n  - These are rare—don't expect every project to be here\n  - But aim to score well on at least one axis\n\n**Key Questions:**\n- How many systems/organisms does this apply to?\n- Does it update understanding of a fundamental process?\n- Will textbooks need to be rewritten?\n- What new questions does this open?\n\n#### Framework 2: Technology Development\n**Axes:** How widely will it be used? × How critical is it for the application?\n\n**Philosophy:** Again, high score on EITHER axis is sufficient.\n\n**Examples:**\n- **Widely Used, Not Critical:** BLAST\n  - Used in countless projects\n  - Rarely THE critical tool, but enormous cumulative impact\n  \n- **Not Widely Used, Highly Critical:** Cryo-electron tomography\n  - Too complicated for broad adoption\n  - But generates stunning data that's impossible to get otherwise\n  - When you need it, nothing else works\n  \n- **High on Both Axes (Game-Changing):** \n  - GFP, CRISPR, AlphaFold (the famous ones)\n  - But also: lentiviral delivery, cell sorting, massively parallel sequencing\n  - Technologies we cannot imagine living without\n\n**Key Questions:**\n- How many labs would adopt this?\n- For what fraction of experiments is this THE enabling technology?\n- What becomes possible that wasn't before?\n- How hard is it to implement?\n\n**Critical Rule:** A tool that won't be widely used AND isn't critical for an application probably isn't worth building.\n\n#### Framework 3: Typical Invention/Application\n**Axes:** How much good? × For how many people?\n\n**Philosophy:** Useful for translational work, frugal science, global health.\n\n**Examples:**\n- Foldscope: Paper microscope accessible to millions of students globally\n- Neglected tropical disease intervention: Quality-adjusted life years per $100\n- Medical device: Number of patients who can access treatment\n\n**Key Questions:**\n- What problem does this solve?\n- How many people have this problem?\n- How much better is their life if you solve it?\n- What's the cost per person helped?\n\n### Phase 3: Selecting and Articulating Your Framework\n\nBased on your Phase 1 responses, let me help you choose:\n\n**If you selected A (fundamental knowledge):** → Basic Science Framework\n**If you selected B (enable experiments):** → Technology Development Framework  \n**If you selected C (solve practical problem):** → Invention Framework\n\n**Now, let's be explicit:**\n\n1. **State Your Framework:** \"This project should be evaluated as [basic science/technology development/invention].\"\n\n2. **Define Your Axes:** \n   - X-axis measures: [specific metric]\n   - Y-axis measures: [specific metric]\n\n3. **Make Your Case:**\n   - X-axis score (Low/Medium/High): [Your assessment + reasoning]\n   - Y-axis score (Low/Medium/High): [Your assessment + reasoning]\n\n4. **Threshold Check:** \n   - Do you score at least MEDIUM-HIGH on one axis?\n   - If both are LOW-MEDIUM, you have a problem\n\n### Phase 4: Alternative or Custom Metrics\n\nSometimes standard frameworks don't fit. Examples where custom metrics work:\n\n**Alternative Metric Examples:**\n- **Frugal Science:** How many children in low/middle-income countries gain access to microscopy?\n- **Neglected Disease:** Quality-adjusted life years saved per $100 invested\n- **Sustainability:** Tons of CO₂ equivalent prevented × cost-effectiveness\n- **Equity:** Reduction in disparity metric × number of people affected\n\n**When to propose alternative metrics:**\n- Your work addresses a specific underserved need\n- Standard metrics miss your core value proposition\n- You're working in an emerging area without established norms\n- Your work crosses traditional boundaries\n\n**How to propose alternative metrics:**\n1. Explain why standard metrics are insufficient\n2. Define your proposed metric clearly\n3. Provide a value creation index (two axes)\n4. Show how your project scores on these axes\n\n### Phase 5: Comparative Assessment\n\nEven if absolute impact is hard to estimate, comparative assessment is valuable:\n\n**Exercise: Compare 3 Related Projects**\n\nFor your project and two alternatives (either from literature or hypothetical):\n\n| Project | Framework | X-Axis Score | Y-Axis Score | Overall |\n|---------|-----------|--------------|--------------|---------|\n| Yours | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |\n| Alt 1 | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |\n| Alt 2 | [Type] | [L/M/H] + reasoning | [L/M/H] + reasoning | [Assessment] |\n\n**Comparative Questions:**\n- Which would be most impactful if they all work?\n- Which has the best risk-adjusted impact?\n- Are you pursuing the best option?\n- If not, why? (Sometimes there are good reasons: resources, expertise, timing)\n\n### Phase 6: Avoiding Metric Mismatch\n\n**Common Mismatches:**\n\n#### Mismatch 1: Basic Science vs. Technology Evaluation\n**Scenario:** You're doing fundamental biology, but reviewers ask \"How widely will this be used?\"\n\n**Problem:** They're evaluating basic science with technology metrics\n\n**Solution:** Explicitly frame as basic science. Lead with: \"This updates our understanding of [fundamental process], which is conserved across [many systems].\"\n\n#### Mismatch 2: Technology vs. Basic Science Evaluation\n**Scenario:** You're building a tool, but reviewers ask \"How much did we learn about biology?\"\n\n**Problem:** They're evaluating technology with basic science metrics\n\n**Solution:** Explicitly frame as technology development. Lead with: \"This enables experiments that are currently impossible, which [X] labs need for [Y] applications.\"\n\n#### Mismatch 3: Within-Category Confusion\n**Scenario:** Your basic science is specific but deep, but reviewers want broad generality\n\n**Problem:** They think both axes are required, rather than either/or\n\n**Solution:** Explicitly acknowledge: \"While this may not be universal, the depth of mechanistic insight scores highly on the learning axis.\"\n\n#### Mismatch 4: Time Horizon Mismatch\n**Scenario:** You're working on long-term fundamental research, but reviewers want immediate impact\n\n**Problem:** Different value systems about when impact should materialize\n\n**Solution:** Articulate your time horizon explicitly and provide historical examples of similar timelines\n\n### Phase 7: Value System Discussion\n\nThis is where Claude explicitly discusses the user's belief system about what matters:\n\n**Questions for Reflection:**\n\n1. **What drives the user?**\n   - Discovery and understanding?\n   - Enabling others?\n   - Solving problems?\n   - Building things?\n\n2. **What would make the user proud?**\n   - Paper in Cell/Nature/Science?\n   - Tool used by hundreds of labs?\n   - Treatment reaching patients?\n   - Opening a new field?\n\n3. **How does the user want to be remembered?**\n   - \"Discovered X\"\n   - \"Built Y that enabled Z\"\n   - \"Solved problem W\"\n   - \"Trained students who went on to...\"\n\n4. **Whose approval matters?**\n   - Specific senior scientists in the field?\n   - Broader community across fields?\n   - Practitioners who use tools?\n   - People whose lives are improved?\n\n**There are no wrong answers—but alignment matters:**\n- The project should match the user's value system\n- The evaluation framework should match the project type\n- Communication should lead with the framework\n\n### Phase 8: Literature Benchmarking\n\nClaude should use PubMed to benchmark impact in the user's area:\n\n**Searches should include:**\n\n1. **Impact Exemplars:** Papers the user considers high-impact in the field\n   - What framework did they use (implicitly or explicitly)?\n   - How did they score on the axes?\n   - What made them successful?\n\n2. **Analogous Projects:** Similar approaches or systems\n   - How were they evaluated?\n   - What impact did they achieve?\n   - What can be learned from their framing?\n\n3. **Field Expectations:** What's typical for the area?\n   - Are basic science papers common?\n   - Is technology development valued?\n   - What level of impact is \"good enough\"?\n\n**Questions to ask the user:**\n- What papers should be analyzed as benchmarks?\n- What search terms capture the field's impact exemplars?\n- Are there specific journals or authors whose framing to emulate?\n\n### Phase 9: Communication Strategy\n\nOnce the framework is selected, here's how to lead with it:\n\n#### In Talks:\n**Opening Frame (within first 2 slides):**\n- \"The goal of this work is to understand [fundamental process X] in [general system Y]\" → Basic science\n- \"We're developing a technology that will enable [critical experiment X] for [community Y]\" → Technology\n- \"This invention addresses [problem X] affecting [N] people\" → Application\n\n#### In Papers:\n**Abstract Structure:**\n- State your framework implicitly through word choice\n- Basic science: \"reveals,\" \"demonstrates,\" \"shows that\"\n- Technology: \"enables,\" \"provides,\" \"makes it possible to\"\n- Application: \"solves,\" \"addresses,\" \"improves\"\n\n#### In Grants:\n**Broader Impact Section:**\n- Explicitly name your evaluation framework\n- Provide the two-axis assessment\n- Score yourself on each axis with evidence\n\n#### With Your PI/Committee:\n**Alignment Conversation:**\n- \"I want to make sure we're aligned on how this should be evaluated\"\n- \"I see this as [framework], scoring [X] on [axis 1] and [Y] on [axis 2]\"\n- \"Do you agree, or do you see it differently?\"\n- \"This matters because...\" [explain downstream implications]\n\n## Output Deliverable\n\nClaude should produce a **2-page Impact Assessment Document**:\n\n### Page 1: Framework and Scoring\n\n#### Project Categorization:\n- **Type:** Basic Science / Technology Development / Invention / Custom\n- **Rationale:** [Why this categorization fits]\n\n#### Optimization Function:\n- **X-Axis:** [Metric name and definition]\n- **Y-Axis:** [Metric name and definition]\n- **Custom Rationale (if applicable):** [Why standard metrics don't fit]\n\n#### Self-Assessment:\n\n**X-Axis Score: [Low/Medium/High]**\n- Evidence: [Specific reasons for this score]\n- Examples: [Comparable projects or benchmarks]\n- PubMed Support: [Key papers that inform assessment]\n\n**Y-Axis Score: [Low/Medium/High]**\n- Evidence: [Specific reasons for this score]\n- Examples: [Comparable projects or benchmarks]\n- PubMed Support: [Key papers that inform assessment]\n\n**Overall Assessment:**\n- Score on at least one axis: ☑ Yes / ☐ No\n- Strong justification: ☑ Yes / ☐ No\n- Aligned with your values: ☑ Yes / ☐ No\n\n#### Visual Framework:\n```\n         [Your Project Type]\n              \nY-Axis    |           ★ Your Project\n[Metric]  |         /\n          |       /\n          |     /\n          |   /\n          |_________________\n              X-Axis [Metric]\n              \n★ = Your project\nReference projects plotted for context\n```\n\n### Page 2: Communication and Alignment\n\n#### Value System Alignment:\n- **What Drives You:** [Discovery/Enabling/Problem-solving/Building]\n- **Success Definition:** [What would make this worthwhile]\n- **Approval Sources:** [Whose opinion matters and why]\n- **Framework Fit:** [How project aligns with values]\n\n#### Potential Mismatches to Avoid:\n1. [Specific mismatch type]\n   - Scenario: [When this might happen]\n   - Prevention: [How to frame to avoid it]\n\n2. [Another mismatch]\n   - Scenario: [When this might happen]\n   - Prevention: [How to frame to avoid it]\n\n#### Communication Strategy:\n\n**For Talks:**\n- Opening frame: [Exact language to use in first 2 slides]\n- Key phrases: [Vocabulary that signals your framework]\n\n**For Papers:**\n- Abstract structure: [Framework-appropriate language]\n- Impact statement: [How to articulate in discussion]\n\n**For Grants:**\n- Broader impact: [How to score yourself explicitly]\n- Justification: [Evidence for scores]\n\n**For Mentors:**\n- Alignment question: [Exact question to ask]\n- Your perspective: [How you see it]\n- Discussion points: [What matters for alignment]\n\n#### Comparative Analysis:\n\n| Project | Type | X-Score | Y-Score | Notes |\n|---------|------|---------|---------|-------|\n| Yours | [Type] | [L/M/H] | [L/M/H] | [Key strengths] |\n| Benchmark 1 | [Type] | [L/M/H] | [L/M/H] | [What you can learn] |\n| Benchmark 2 | [Type] | [L/M/H] | [L/M/H] | [What you can learn] |\n| Alternative | [Type] | [L/M/H] | [L/M/H] | [Why not pursuing] |\n\n#### Action Items:\n1. [Specific step to strengthen X-axis score or argument]\n2. [Specific step to strengthen Y-axis score or argument]\n3. [Communication alignment with key stakeholders]\n\n## Practical Examples\n\n### Example 1: Ribosome Stalling (Basic Science)\n- **Framework:** Basic science\n- **X-Axis (Generality):** HIGH—translation is universal\n- **Y-Axis (Learning):** MEDIUM—mechanism of one quality control system\n- **Assessment:** High on generality alone = substantial impact\n- **Communication:** \"Updates our understanding of translation quality control\"\n\n### Example 2: BLAST (Technology)\n- **Framework:** Technology development\n- **X-Axis (Widely Used):** VERY HIGH—used by virtually all molecular biologists\n- **Y-Axis (Critical):** LOW-MEDIUM—helpful but rarely essential\n- **Assessment:** Extreme breadth of use = enormous cumulative impact\n- **Communication:** \"Enables rapid sequence comparison across all biological databases\"\n\n### Example 3: Cryo-EM Tomography (Technology)\n- **Framework:** Technology development\n- **X-Axis (Widely Used):** LOW—complex, expensive, specialized\n- **Y-Axis (Critical):** VERY HIGH—generates impossible-to-get-otherwise data\n- **Assessment:** Extreme criticality for niche = high impact\n- **Communication:** \"Enables 3D visualization of molecular machines in native cellular context\"\n\n### Example 4: Foldscope (Invention)\n- **Framework:** Invention (custom metric)\n- **X-Axis (Good):** MEDIUM—functional microscopy\n- **Y-Axis (People):** VERY HIGH—millions of students globally\n- **Assessment:** Massive reach × modest utility = transformative for education\n- **Communication:** \"Democratizes microscopy for global education\"\n\n## Key Principles to Remember\n\n1. **Value Is in the Eye of a Belief System:** Make yours explicit.\n\n2. **Lead with Your Metric:** Don't assume others share your framework.\n\n3. **Either Axis Suffices:** You don't need both—just score well on one.\n\n4. **Articulate Early:** Discuss with mentors before you're 2 years in.\n\n5. **Avoid Default State:** Work actively against irrelevance/non-use.\n\n6. **Compare, Don't Absolute:** Even rough comparison beats ignoring impact.\n\n7. **Align Communication:** Your words should signal your framework.\n\n8. **Match Project to Values:** Life is too short for misaligned work.\n\n## Warning Signs\n\n**Warning signs include:**\n- Inability to articulate which framework applies\n- Scoring LOW on both axes\n- Project type and evaluation framework don't match\n- User and PI have different frameworks but haven't discussed it\n- Using basic science metrics for a tool or vice versa\n- Never explicitly discussing impact assessment\n\n**Good shape indicators:**\n- Clear statement of optimization function\n- MEDIUM-HIGH score on at least one axis\n- Framework matches project type\n- Alignment with key stakeholders\n- Communication signals framework clearly\n- Benchmarking against comparable work\n\n## Getting Started\n\nClaude should begin Phase 1 by asking:\n1. What is the primary goal? (A/B/C/D)\n2. What would success look like in 3-5 years?\n3. Who cares if this succeeds?\n\nTogether, Claude and the user will select the right optimization function and position the work for maximum impact.\n\n---\n\n*Remember: Impact assessment isn't about ego—it's about ensuring work matters in the way the scientist wants it to matter. Explicit framing prevents years of misalignment.*\n", "depth": 5}, {"id": "124", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/01-intuition-pumps.md", "name": "01-intuition-pumps.md", "type": "file", "content": "# SKILL: Intuition Pumps for Scientific Problem Ideation\n\n## Overview\nThis skill helps scientists generate high-quality research ideas by providing systematic prompts (\"intuition pumps\") and identifying common ideation traps. Based on the framework that most biological and chemical science projects involve **perturbing a system, measuring it, and analyzing the data**, this skill guides users through structured ideation that can significantly impact how they spend years of their career.\n\n## Core Framework\n\n### The Three Pillars of Scientific Work\nResearch advances generally fall into one of these categories, each with two dimensions:\n\n**PERTURBATION**\n- *Logic*: Novel ways to manipulate biological systems (e.g., using CRISPR for deep mutational scanning)\n- *Technology*: New tools for manipulation (e.g., developing base editors, creating whole-genome CRISPR libraries)\n\n**MEASUREMENT**  \n- *Logic*: Novel applications of existing measurement tools (e.g., using tissue clearing to study liver fibrosis)\n- *Technology*: New measurement capabilities (e.g., developing tissue-clearing techniques, super-resolution microscopy)\n\n**THEORY/COMPUTATION**\n- *Logic*: Using computational tools to make discoveries (e.g., applying AlphaFold to identify protein functions)\n- *Technology*: Building new algorithms or models (e.g., developing machine learning architectures for biological data)\n\nUnderstanding which quadrant resonates with the user can help identify their niche and guide ideation.\n\n## The Skill Workflow\n\n### Phase 1: Initial Discovery Questions (5-10 minutes)\n\nBefore diving into intuition pumps, Claude should gather context by asking the user:\n\n1. **What is the user's general research area or field?** (e.g., immunology, synthetic biology, neuroscience, protein engineering)\n\n2. **What excites the user most about science?**\n   - Building new tools/technologies?\n   - Discovering fundamental principles?\n   - Solving practical problems?\n   - Understanding dynamic processes?\n\n3. **What are the user's existing strengths?** (Select all that apply)\n   - Specific techniques (please list)\n   - Computational skills\n   - Access to unique systems/models\n   - Domain expertise in a particular area\n\n4. **Current constraints:**\n   - Time horizon for this project? (months/years)\n   - Resources available?\n   - Must it connect to existing work, or can the user start fresh?\n\n5. **On a scale of 1-5, how would the user rate their current idea?**\n   - Likelihood of success: 1 (very risky) to 5 (highly feasible)\n   - Potential impact: 1 (incremental) to 5 (transformative)\n\n### Phase 2: Applying Intuition Pumps\n\nBased on the user's responses, Claude should guide them through relevant intuition pumps from this list:\n\n#### Intuition Pump #1: Make It Systematic\n**Prompt:** Take any one-off perturbation or measurement and make it systematic.\n\n**Examples:**\n- Instead of mutating one enzyme, measure kinetic parameters across an entire enzyme family\n- Instead of one CRISPR mutant → genome-wide screen with transcriptomic readout\n- Instead of imaging one condition → high-throughput imaging across thousands of conditions\n\n**Prompt for User:** What one-off experiment in your field could become a systematic survey?\n\n#### Intuition Pump #2: Identify Technology Limitations\n**Prompt:** What are the fundamental limitations of technologies you use? These limitations are opportunities.\n\n**Examples:**\n- Microscopy can't resolve beyond diffraction limit → super-resolution microscopy\n- DNA synthesis can't make complete genomes → develop assembly methods\n- Genetic screens have precise input but imprecise output → develop high-dimensional readouts\n- We do single gene KOs but networks are complex → develop combinatorial perturbation methods\n\n**Prompt for User:** What technology limitation frustrates you most? How might you turn that limitation into an opportunity?\n\n#### Intuition Pump #3: The \"I Can't Imagine\" Test\n**Prompt:** I can't imagine a future in which we don't have ____, but it doesn't exist yet.\n\n**Examples:**\n- The ability to design highly efficient enzymes like we design other proteins\n- The ability to deliver genome editing payloads to any cell type in vivo\n- 3D tomographic imaging of live cells at molecular resolution\n- Proteome-scale sequencing with the throughput of RNA-seq\n\n**Prompt for User:** What capability seems inevitable but doesn't exist yet in your field?\n\n#### Intuition Pump #4: Static vs. Dynamic Understanding\n**Prompt:** We understand biological \"parts lists\" but rarely understand dynamic processes.\n\n**Key Insight:** Most observations are single-timepoint, single-perturbation format. But biological systems are dynamic—like humans flowing through Grand Central Station or money through financial systems.\n\n**Examples:**\n- Understanding growth factor signaling like we understand turning a key in a car engine\n- Time-resolved cell atlases with lineage tracing through entire development\n- Following metabolite flux through pathways in real-time\n\n**Prompt for User:** What dynamic process in your field do we observe as static snapshots? How might you capture the full temporal or spatial dynamics?\n\n#### Intuition Pump #5: Pick a New Axis\n**Prompt:** We almost always use time as the x-axis for dynamic processes. What other coordinate could you use?\n\n**Example:** Instead of time, use \"infection progression\" markers to enable monitoring asynchronous cells\n\n**Prompt for User:** What non-temporal coordinate could reveal new biology in your system?\n\n#### Intuition Pump #6: Create a Technology Platform\n**Prompt:** Instead of answering one question, could you build a platform that enables many questions?\n\n**Examples:**\n- Antibodies for intracellular targets (not just extracellular)\n- AI that predicts perturbations needed to reach desired cell states\n- Universal genome delivery vehicles\n\n**Prompt for User:** What platform would transform how your field asks questions?\n\n#### Intuition Pump #7: Dogs That Don't Bark\n**Prompt:** Why doesn't something exist or occur? Absence can be as informative as presence.\n\n**Examples:**\n- Why are there no Gram-negative bacteria on human skin?\n- Why do some catalytically inactive enzymes persist through evolution?\n- Why don't certain cell types exist in certain tissues?\n\n**Prompt for User:** What absence puzzles you in your field?\n\n### Phase 3: Avoiding Common Traps\n\nAfter generating ideas, we must evaluate them critically. Here are the most common traps:\n\n#### Trap #1: The Truffle Hound\n**Warning:** Don't become so good at one system or technique that you fail to ask questions of biological import.\n\n**Bad:** \"What is the role of p190 RhoGAP in wing development?\"  \n**Better:** \"How do signaling pathways and cytoskeleton coordinate to control wing development?\"\n\n**Self-Check:** Is the question driven by biological curiosity or by what the user is technically capable of?\n\n#### Trap #2: Applying Existing Tool to New System\n**Warning:** \"Let's use CRISPR in my organism\" can be valuable but risks crowding and incrementalism.\n\n**When It Works:** The user is enabling a field that truly needs this capability\n**When It Fails:** The tool is already widely applied; the contribution will be incremental\n\n**Self-Check:** Will this tool application open new biological questions, or just extend existing observations? Claude should help the user evaluate this honestly.\n\n#### Trap #3: Jumping on the First Idea\n**Warning:** Treating ideas with reverence instead of skepticism. Confirmation bias sets in quickly.\n\n**Better Approach:** Users should treat new ideas like leeches trying to steal their time. Look for the warts. Develop several ideas in parallel and comparison shop.\n\n**Self-Check:** Has the user critically evaluated at least 3-5 alternative approaches?\n\n#### Trap #4: Too Many Fixed Parameters\n**Warning:** Fixing too many parameters at the outset creates a poor technique-application match.\n\n**Example of Over-Constraining:** \"I will use spatial transcriptomics to study antigen-presenting cell and T cell interactions in the tumor microenvironment.\"\n- This fixes: technique (spatial transcriptomics), cell types, and context\n- If any assumption fails, the project fails\n\n**Self-Check:** Has the user fixed more than 2 parameters before starting?\n\n#### Trap #5: Too Few Fixed Parameters\n**Warning:** \"I want to do impactful work in cell engineering\" → paralysis\n\n**Resolution:** Constraints engender creativity. Fix ONE parameter at a time and let creativity flow.\n\n**Self-Check:** Does the user have at least one concrete constraint to work with?\n\n### Phase 4: Literature Integration\n\nTo ensure the idea has appropriate scope and hasn't been thoroughly explored, Claude should ask:\n\n1. **What are 2-3 key questions or gaps the idea addresses?**\n\n2. **What should be searched in PubMed to:**\n   - Understand the current state of the field?\n   - Identify related approaches?\n   - Find empirical knowledge from adjacent domains that could inform the approach?\n\nClaude should use PubMed to:\n- Assess how general/specific the problem is\n- Identify relevant methodological advances\n- Find analogous systems or approaches in other fields\n- Determine the degree of competition\n\n### Phase 5: Idea Refinement and Output\n\nAfter working through intuition pumps, avoiding traps, and reviewing literature, Claude should help the user:\n\n1. **Crystallize the Idea:**\n   - Biological question\n   - Technical approach (perturbation/measurement/theory: logic vs. technology)\n   - What's novel about this angle?\n\n2. **Articulate Fixed vs. Floating Parameters:**\n   - What MUST remain constant in the approach?\n   - What can be flexible if obstacles arise?\n\n3. **Identify Key Assumptions:**\n   - What must be true for this to work?\n   - Which assumptions are about biology vs. technology capabilities?\n\n4. **Sketch Alternative Paths:**\n   - If the primary approach fails, what's Plan B?\n   - Can the project be designed to succeed regardless of outcome?\n\n## Output Deliverable\n\nAt the end of this skill, Claude should produce a **2-page Problem Ideation Document** containing:\n\n### Page 1: Core Idea\n- **Title:** Concise project name\n- **The Question:** What biological question is being asked?\n- **The Approach:** How will it be answered? (Specify perturbation/measurement/computation: logic vs. technology)\n- **What's Novel:** The unique angle\n- **Why It Matters:** Potential impact (generality × learning, or technology development)\n- **Intuition Pump(s) Used:** Which prompted this idea\n\n### Page 2: Critical Analysis\n- **Fixed vs. Floating Parameters:**\n  - Fixed: What must stay constant\n  - Floating: What can adapt\n\n- **Key Assumptions \u0026 Risk Assessment:**\n  - Biological assumptions (risk level 1-5)\n  - Technical assumptions (risk level 1-5)\n\n- **Traps Avoided:** Which pitfalls were navigated around?\n\n- **Alternative Approaches:** Plan B and Plan C\n\n- **Literature Context:**\n  - 3-5 key papers that inform or relate to this work\n  - Degree of competition (low/medium/high)\n  - The user's edge/advantage\n\n- **Next Steps:** First 3 concrete experiments or analyses\n\n## Key Principles to Remember\n\n1. **Reversal of Polarity:** Treat ideas with skepticism, not reverence. Look for flaws before falling in love.\n\n2. **Comparison Shopping:** Develop multiple ideas in parallel. The act of comparison improves decision-making.\n\n3. **Fix One Parameter at a Time:** Constraints engender creativity, but too many constraints prevent it.\n\n4. **Think in Ensembles:** The user is picking a family of possible projects, not a singular path. Flexibility is essential.\n\n5. **Balance Logic and Technology:** Novel biology can come from new tools OR clever application of existing tools.\n\n6. **Systematic Over One-Off:** High-throughput and systematic approaches often reveal more than single observations.\n\n7. **Dynamic Over Static:** Biological systems are dynamic. How can process be captured rather than snapshot?\n\n## Getting Started\n\nWhen the user is ready, Claude should guide them through the Phase 1 questions to begin the systematic ideation process. The key message: spending extra time on problem choice is the highest-leverage activity in science. A well-chosen problem executed reasonably well will have more impact than a mediocre problem executed brilliantly.\n\n---\n\n*This skill is based on the problem choice framework developed by Michael A. Fischbach and Christopher T. Walsh, as described in \"Problem choice and decision trees in science and engineering\" (Cell, 2024).*\n", "depth": 5}, {"id": "125", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/02-risk-assessment.md", "name": "02-risk-assessment.md", "type": "file", "content": "# SKILL 2: Risk Assessment and Assumption Analysis\n\n## Overview\nThis skill helps scientists systematically identify, quantify, and manage project risk through rigorous assumption analysis. The goal is not to eliminate risk—risk-free projects tend to be incremental—but to name it, quantify it, and work steadily to chip away at it. This skill builds directly on the Problem Ideation Document from Skill 1.\n\n## Core Principle\n\n**\"Don't avoid risk; befriend it.\"**\n\nThe most important concept in problem choice is the two-axis evaluation:\n- **X-axis:** Likelihood of success\n- **Y-axis:** Impact if successful\n\nThis skill focuses on the X-axis, helping users move their project rightward through systematic risk analysis.\n\n## Why This Matters\n\nA project with a high-risk assumption that won't read out for \u003e2 years is problematic. One that requires multiple miracles to succeed should be avoided or refined. The human tendency is to stay in a safe local space, work laterally, and put off facing existential risks—like an ostrich burying its head in the sand. This skill helps users face risk head-on.\n\n## The Skill Workflow\n\n### Phase 1: Extract Project Assumptions (10-15 minutes)\n\nFirst, Claude should gather information about the user's project from Skill 1:\n\n1. **Project Summary** (from Skill 1):\n   - The biological question\n   - The technical approach\n   - What's novel about it\n\n2. **Project Horizon:**\n   - How long is this project expected to take? (months/years)\n   - What is the user's role? (graduate student, postdoc, PI, startup founder)\n\n3. **Initial Risk Sense:**\n   - What keeps the user up at night about this project?\n   - What's the scariest assumption?\n\n### Phase 2: Comprehensive Assumption Listing\n\nClaude should work with the user to list EVERY assumption the project makes from inception through conclusion. Assumptions fall into two categories:\n\n#### Type A: Assumptions About Biological Reality\nThese are facts about the world that either are or aren't true. They won't change during the project.\n\n**Examples:**\n- New cell types exist beyond what's currently known\n- A particular gene regulates the process being studied\n- Two proteins physically interact\n- A pathway functions in the organism of interest\n- The biological effect size is detectable\n\n#### Type B: Assumptions About Technical Capability\nThese are about whether technology can do what's needed. These CAN change during the project as methods improve.\n\n**Examples:**\n- A specific cell type can be isolated\n- Sequencing will generate high-quality data\n- An assay has sufficient throughput\n- Computational analysis can distinguish signal from noise\n- Gene editing will work in the system\n\n**Claude should ask:**\n1. What must be true about the biology for this to work?\n2. What must the technology be able to do?\n3. What about the experimental design—what assumptions are built in?\n4. What about the analysis—can it deliver what's needed?\n5. If everything works, can the findings be validated?\n6. Will the findings be interpretable and meaningful?\n\n### Phase 3: Risk Scoring (The Assumption Analysis Table)\n\nFor each assumption, Claude should help the user assign two scores:\n\n#### Risk Level (1-5 scale):\n- **1** = Very likely to be true/work (\u003e90% confidence)\n- **2** = Likely (70-90% confidence)\n- **3** = Uncertain (40-70% confidence)\n- **4** = Unlikely (10-40% confidence)\n- **5** = Very unlikely (\u003c10% confidence)\n\n#### Time to Test (months):\nHow long before the user will know if this assumption is valid?\n\n**Critical Rules:**\n1. Be brutally honest—try to convince oneself of being WRONG, not right\n2. Distinguish between biological vs. technical assumptions\n3. Consider whether technical assumptions might improve over time\n4. Note which assumptions depend on earlier assumptions succeeding\n\n### Phase 4: Risk Profile Evaluation\n\nOnce the complete table is ready, Claude should analyze the risk profile:\n\n#### Red Flags to Identify:\n1. **The Late High-Risk Problem:** Risk level 4-5 assumption that won't read out until \u003e18 months\n2. **The Multiple Miracles:** More than 2-3 assumptions with risk level 4-5\n3. **The Dependency Chain:** High-risk assumptions stacked in sequence\n4. **The Ostrich Pattern:** Starting with low-risk work while avoiding the high-risk tests\n\n#### Green Lights:\n1. **Early Go/No-Go:** Highest-risk assumption testable in \u003c6 months\n2. **Multiple Candidates:** Project can succeed with several different outcomes\n3. **Graceful Degradation:** If assumption X fails, assumption Y provides alternative path\n4. **Risk Distribution:** High-risk assumptions balanced across timeline\n\n**Rule of Thumb:** If you have a risk level 5 assumption three years out, pick another project.\n\n### Phase 5: Risk Mitigation Strategies\n\nFor each high-risk assumption (level 4-5), Claude should help develop mitigation strategies:\n\n#### Strategy 1: Move High-Risk Tests Earlier\n**Question:** Can a quicker, cruder test be designed that answers most of what's needed?\n\n**Example:** Instead of waiting 2 years to validate a new cell type exists, consider:\n- Using existing markers as a proxy\n- Testing in a simpler model system first\n- Using computational predictions to increase confidence\n\n#### Strategy 2: Multiple Candidates Approach\n**Question:** Can multiple candidates be tested in parallel to increase likelihood of success?\n\n**Example:** Instead of:\n- Testing one kinase → Test a panel of 10 kinases\n- Building one engineered organism → Build and test a library\n- Pursuing one therapeutic target → Pursue 3 related targets\n\n#### Strategy 3: Reframe the Question\n**Question:** Can the project scope be adjusted to reduce critical assumptions while maintaining impact?\n\n**Example from lecture:**\n- **Original:** Identify NEW enteroendocrine cell types (high risk: they may not exist)\n- **Reframed:** Better characterize KNOWN but incompletely understood cell types (lower risk)\n\n#### Strategy 4: Change the System\n**Question:** Is there a different biological system with similar scientific value but lower technical risk?\n\n**Example from lecture:**\n- **Original:** Intestinal epithelium (hard to manipulate genetically)\n- **Alternative:** Liver (easier genetic manipulation options exist)\n\n#### Strategy 5: Add Complementary Approaches\n**Question:** Can a parallel approach be added that de-risks the main assumption?\n\n**Example from lecture:**\n- Add spatial transcriptomics to scRNA-seq\n- This provides biogeographic context and validates cell type existence earlier\n\n### Phase 6: Go/No-Go Experiment Design\n\nFor the top 3 highest-risk assumptions, Claude should help design the critical go/no-go experiments:\n\n**For each, specify:**\n1. **The Question:** Exactly what is being tested?\n2. **The Experiment:** Most direct test possible (even if crude)\n3. **Success Criteria:** What result means \"go\"?\n4. **Failure Response:** What result means \"pivot\" or \"stop\"?\n5. **Timeline:** How soon can this be run?\n6. **Resources:** What is needed?\n\n**Key Principle:** Cut right to the critical go/no-go experiment. Don't just start with easy stuff—the risk points aren't going away.\n\n### Phase 7: Literature Validation\n\nClaude should search PubMed to help calibrate risk assessments:\n\n**Search for:**\n1. **Precedents:** Has anyone done something similar? (Reduces technical risk)\n2. **Biological Evidence:** What's known about the system? (Informs biological risk)\n3. **Technical Benchmarks:** How well do the methods work in practice?\n4. **Adjacent Successes:** Has anyone solved related problems?\n\n**Questions to ask the user:**\n- What specific searches would help calibrate risk?\n- Are there particular papers that informed the assumptions?\n- Are there technical benchmarks to look up?\n\n### Phase 8: Revised Project Plan\n\nBased on the risk analysis, Claude should help create a revised plan:\n\n#### Option A: De-Risk the Current Plan\n- Reorder experiments to test high-risk assumptions early\n- Add complementary approaches\n- Design multiple-candidate strategies\n\n#### Option B: Reframe the Project\n- Adjust scope while maintaining impact\n- Change biological system\n- Modify technical approach\n\n#### Option C: Pick a Different Project\nSometimes the honest answer is: \"This has too many miracles.\" That's valuable to know BEFORE investing years.\n\n## Output Deliverable\n\nClaude should produce a **2-page Risk Assessment Document**:\n\n### Page 1: Assumption Analysis Table\n\n| Assumption | Type* | Risk† | Time‡ | Notes |\n|------------|-------|-------|-------|-------|\n| [Assumption 1] | Bio/Tech | 1-5 | X mo | [Rationale for score] |\n| [Assumption 2] | Bio/Tech | 1-5 | X mo | [Rationale for score] |\n| ... | ... | ... | ... | ... |\n\n*Bio = Biological reality, Tech = Technical capability  \n†Risk: 1=very likely to 5=very unlikely  \n‡Time to test in months\n\n#### Risk Profile Summary:\n- **Total Assumptions:** X\n- **High Risk (4-5):** X assumptions\n- **Late High Risk (\u003e18mo):** X assumptions\n- **Critical Path:** [Identify the chain of dependent assumptions]\n- **Overall Assessment:** [Green/Yellow/Red light with explanation]\n\n### Page 2: Risk Mitigation Plan\n\n#### Top 3 High-Risk Assumptions:\nFor each:\n1. **The Assumption:** [Stated clearly]\n2. **Current Risk Level \u0026 Timeline:** X (risk) at Y months\n3. **Why This Risk Exists:** [Explanation]\n4. **Mitigation Strategy:** [From Strategies 1-5 above]\n5. **Go/No-Go Experiment:**\n   - Experiment design\n   - Success criteria\n   - Timeline\n   - What you'll do if it fails\n\n#### Revised Project Timeline:\n```\nMonth 0-6:   [Early go/no-go experiments]\nMonth 6-12:  [Based on go/no-go results]\nMonth 12-18: [...]\nMonth 18+:   [...]\n```\n\n#### Contingency Plans:\n- **If assumption X fails:** [Plan B]\n- **If assumption Y fails:** [Plan C]\n- **Multiple success paths:** [How project can succeed different ways]\n\n#### Decision Points:\n- **Month X:** Evaluate [assumptions A, B] → Go/Pivot/Stop decision\n- **Month Y:** Evaluate [assumptions C, D] → Go/Pivot/Stop decision\n\n## Practical Examples\n\n### Example 1: ScRNA-Seq for Enteroendocrine Cells\n\n**High-Risk Assumptions Identified:**\n1. \"New cell types can be validated experimentally\" (Risk 5, 24 months)\n2. \"Knockout will yield biologically relevant phenotype\" (Risk 5, 30 months)\n\n**Problem:** Two risk-5 assumptions at 24+ months = RED FLAG\n\n**Mitigation Applied:**\n- Reframe to study known but poorly characterized cells (reduces Risk 5→3)\n- Switch to liver instead of intestine (improves validation timeline: 30→18 months)\n- Add spatial transcriptomics (provides earlier validation checkpoint at 16 months)\n\n### Example 2: Bacterial Therapy for Chronic Kidney Disease\n\n**High-Risk Assumption Identified:**\n\"Key uremic toxins leading to effects can be determined\" (Risk 4, unknown timeline)\n\n**Problem:** Critical assumption with unclear path to resolution\n\n**Mitigation Applied:**\n- Focus on known lead toxins (IS and PCS) rather than discovering new ones\n- Add parallel track: test multiple toxin candidates\n- Design study where learning toxin identity IS the outcome (multiple success paths)\n\n## Key Principles to Remember\n\n1. **Try to Convince Yourself You're Wrong:** The goal is critical evaluation, not confirmation bias.\n\n2. **Ignore Everything But Key Risk Points:** Don't get distracted by easy tasks. The high-risk assumptions aren't going away.\n\n3. **Early and Often:** Design go/no-go experiments at the earliest feasible moment.\n\n4. **Be Candid About Risk:** When presenting ideas, acknowledging risk makes your case MORE convincing, not less.\n\n5. **No Risk, No Interest:** The goal isn't zero risk—it's understood, quantified, manageable risk.\n\n6. **Risk Can Change:** Technical assumptions may improve as methods advance. Build this into your planning.\n\n7. **Compare Risk Profiles:** Evaluate multiple projects in parallel to compare risk profiles and make better choices.\n\n8. **Watch for the Ostrich Pattern:** Are you avoiding the scary experiment? That's human nature, but a critical failure mode.\n\n## Warning Signs\n\n**Warning signs include:**\n- Risk level 5 assumptions \u003e2 years out\n- More than 3 assumptions at risk level 4-5\n- Highest-risk assumptions at the END of the timeline\n- Rationalizing why high-risk assumptions will \"probably work out\"\n- Planning to \"start with the easy stuff\" while avoiding risk tests\n- Inability to articulate clear go/no-go criteria\n\n**Good shape indicators:**\n- Highest-risk tests happen in first 6 months\n- Multiple paths to success exist\n- Clear plans for what to do if key assumptions fail\n- Risk is distributed across the timeline\n- Testing assumptions, not confirming hopes\n\n## Getting Started\n\nClaude should begin with Phase 1 by asking for:\n1. The project summary from Skill 1\n2. Project timeline expectations\n3. What concerns the user most about this project\n\nTogether, Claude and the user will build a rigorous risk assessment that dramatically improves the likelihood of success by helping avoid years of work on projects with insurmountable obstacles.\n\n---\n\n*Remember: Spending time on risk analysis is the most valuable investment a scientist can make. A well-understood risk profile enables moving forward with confidence or pivoting with clarity—both are valuable outcomes.*\n", "depth": 5}, {"id": "126", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scientific-problem-selection/references/05-decision-tree.md", "name": "05-decision-tree.md", "type": "file", "content": "# SKILL 5: Decision Tree Navigation (\"The Altitude Dance\")\n\n## Overview\nThis skill teaches you to move fluidly between execution (Level 1: getting stuff done) and strategic evaluation (Level 2: critical thinking). Projects rarely unfold linearly—they require frequent course correction. Most trainees should spend MORE time on their project's decision tree.\n\n## Core Principle\n**\"Learn the altitude dance\"**\n\nMove back and forth frequently between:\n- **Level 1:** Full immersion in experimental details or coding\n- **Level 2:** Step back, clear your head, evaluate as if someone else did the work\n\nThese cannot be done simultaneously. The key to navigating a project's decision tree is alternating between these levels deliberately.\n\n## Key Concepts\n\n**Why Decision Trees Matter:**\nOnce you're in a project, the landscape changes:\n- You've learned from initial experiments\n- New papers have been published\n- Technology has advanced\n- Your assumptions have been tested\n\nAt any decision point, you should rarely follow your plan from 2 years ago—there will be a better alternative.\n\n**The Altitude Levels:**\n- **Level 1 (Ground Level):** Doing the work, troubleshooting, optimizing\n- **Level 2 (Strategic Altitude):** What did we learn? What should we do next?\n- **Level 3 (Field Altitude):** How does this fit in the broader landscape?\n- **Level 4 (Career Altitude):** Is this the right use of my finite time?\n\n**Common Failure Modes:**\n1. **Stuck in Level 1:** Troubleshooting endlessly without reassessing the plan\n2. **Only Level 2:** Brilliant strategist but never rolls up sleeves\n3. **No rhythm:** Switching randomly instead of deliberately\n\n## Workflow\n\n### Phase 1: Map Your Decision Tree\n\nFor your project, identify:\n1. **Initial plan:** What was the intended path?\n2. **Branch points:** Where might alternative paths emerge?\n3. **Decision criteria:** What determines which branch to take?\n4. **New information:** What could change the landscape?\n\n### Phase 2: Establish Your Rhythm\n\n**Recommended Schedule:**\n- **Daily:** Level 1 work (experiments, coding, analysis)\n- **Weekly:** Level 2 evaluation (1-2 hours, ideally Friday afternoon)\n- **Monthly:** Level 3 field review (read new papers, attend seminars)\n- **Quarterly:** Level 4 career check-in (with mentor)\n\n**Level 2 Weekly Protocol:**\n1. Clear your head (walk, coffee, change of scene)\n2. Review what happened this week\n3. Ask: What did we learn?\n4. Ask: What should happen next?\n5. Update decision tree\n6. Plan next week's Level 1 work\n\n### Phase 3: Decision Points\n\nAt each major branch point:\n\n**Example: Genetic Screen Hits Wall**\n\nInstead of endless troubleshooting:\n- **Alternative 1:** Redo computational analysis with larger genome dataset\n- **Alternative 2:** Use AlphaFold models to search for similar folds\n- **Alternative 3:** Print and test larger candidate set (DNA synthesis cheaper now)\n\n**Framework:**\n1. **Acknowledge the stuck point**\n2. **Step to Level 2:** Evaluate with fresh eyes\n3. **Consider: What's newly possible?** (technology, knowledge)\n4. **Generate 3 alternatives**\n5. **Decide:** Troubleshoot more vs. pursue alternative\n\n## Output: Decision Tree Map\n- Visual map of your project's decision points\n- Update frequency schedule\n- Criteria for each branch point\n- Protocol for getting unstuck\n", "depth": 5}, {"id": "127", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/single-cell-rna-qc/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "128", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/single-cell-rna-qc/scripts", "name": "scripts", "type": "dir", "content": "", "depth": 4}, {"id": "129", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/single-cell-rna-qc/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: single-cell-rna-qc\ndescription: Performs quality control on single-cell RNA-seq data (.h5ad or .h5 files) using scverse best practices with MAD-based filtering and comprehensive visualizations. Use when users request QC analysis, filtering low-quality cells, assessing data quality, or following scverse/scanpy best practices for single-cell analysis.\n---\n\n# Single-Cell RNA-seq Quality Control\n\nAutomated QC workflow for single-cell RNA-seq data following scverse best practices.\n\n## When to Use This Skill\n\nUse when users:\n- Request quality control or QC on single-cell RNA-seq data\n- Want to filter low-quality cells or assess data quality\n- Need QC visualizations or metrics\n- Ask to follow scverse/scanpy best practices\n- Request MAD-based filtering or outlier detection\n\n**Supported input formats:**\n- `.h5ad` files (AnnData format from scanpy/Python workflows)\n- `.h5` files (10X Genomics Cell Ranger output)\n\n**Default recommendation**: Use Approach 1 (complete pipeline) unless the user has specific custom requirements or explicitly requests non-standard filtering logic.\n\n## Approach 1: Complete QC Pipeline (Recommended for Standard Workflows)\n\nFor standard QC following scverse best practices, use the convenience script `scripts/qc_analysis.py`:\n\n```bash\npython3 scripts/qc_analysis.py input.h5ad\n# or for 10X Genomics .h5 files:\npython3 scripts/qc_analysis.py raw_feature_bc_matrix.h5\n```\n\nThe script automatically detects the file format and loads it appropriately.\n\n**When to use this approach:**\n- Standard QC workflow with adjustable thresholds (all cells filtered the same way)\n- Batch processing multiple datasets\n- Quick exploratory analysis\n- User wants the \"just works\" solution\n\n**Requirements:** anndata, scanpy, scipy, matplotlib, seaborn, numpy\n\n**Parameters:**\n\nCustomize filtering thresholds and gene patterns using command-line parameters:\n- `--output-dir` - Output directory\n- `--mad-counts`, `--mad-genes`, `--mad-mt` - MAD thresholds for counts/genes/MT%\n- `--mt-threshold` - Hard mitochondrial % cutoff\n- `--min-cells` - Gene filtering threshold\n- `--mt-pattern`, `--ribo-pattern`, `--hb-pattern` - Gene name patterns for different species\n\nUse `--help` to see current default values.\n\n**Outputs:**\n\nAll files are saved to `\u003cinput_basename\u003e_qc_results/` directory by default (or to the directory specified by `--output-dir`):\n- `qc_metrics_before_filtering.png` - Pre-filtering visualizations\n- `qc_filtering_thresholds.png` - MAD-based threshold overlays\n- `qc_metrics_after_filtering.png` - Post-filtering quality metrics\n- `\u003cinput_basename\u003e_filtered.h5ad` - Clean, filtered dataset ready for downstream analysis\n- `\u003cinput_basename\u003e_with_qc.h5ad` - Original data with QC annotations preserved\n\nIf copying outputs for user access, copy individual files (not the entire directory) so users can preview them directly.\n\n### Workflow Steps\n\nThe script performs the following steps:\n\n1. **Calculate QC metrics** - Count depth, gene detection, mitochondrial/ribosomal/hemoglobin content\n2. **Apply MAD-based filtering** - Permissive outlier detection using MAD thresholds for counts/genes/MT%\n3. **Filter genes** - Remove genes detected in few cells\n4. **Generate visualizations** - Comprehensive before/after plots with threshold overlays\n\n## Approach 2: Modular Building Blocks (For Custom Workflows)\n\nFor custom analysis workflows or non-standard requirements, use the modular utility functions from `scripts/qc_core.py` and `scripts/qc_plotting.py`:\n\n```python\n# Run from scripts/ directory, or add scripts/ to sys.path if needed\nimport anndata as ad\nfrom qc_core import calculate_qc_metrics, detect_outliers_mad, filter_cells\nfrom qc_plotting import plot_qc_distributions  # Only if visualization needed\n\nadata = ad.read_h5ad('input.h5ad')\ncalculate_qc_metrics(adata, inplace=True)\n# ... custom analysis logic here\n```\n\n**When to use this approach:**\n- Different workflow needed (skip steps, change order, apply different thresholds to subsets)\n- Conditional logic (e.g., filter neurons differently than other cells)\n- Partial execution (only metrics/visualization, no filtering)\n- Integration with other analysis steps in a larger pipeline\n- Custom filtering criteria beyond what command-line params support\n\n**Available utility functions:**\n\nFrom `qc_core.py` (core QC operations):\n- `calculate_qc_metrics(adata, mt_pattern, ribo_pattern, hb_pattern, inplace=True)` - Calculate QC metrics and annotate adata\n- `detect_outliers_mad(adata, metric, n_mads, verbose=True)` - MAD-based outlier detection, returns boolean mask\n- `apply_hard_threshold(adata, metric, threshold, operator='\u003e', verbose=True)` - Apply hard cutoffs, returns boolean mask\n- `filter_cells(adata, mask, inplace=False)` - Apply boolean mask to filter cells\n- `filter_genes(adata, min_cells=20, min_counts=None, inplace=True)` - Filter genes by detection\n- `print_qc_summary(adata, label='')` - Print summary statistics\n\nFrom `qc_plotting.py` (visualization):\n- `plot_qc_distributions(adata, output_path, title)` - Generate comprehensive QC plots\n- `plot_filtering_thresholds(adata, outlier_masks, thresholds, output_path)` - Visualize filtering thresholds\n- `plot_qc_after_filtering(adata, output_path)` - Generate post-filtering plots\n\n**Example custom workflows:**\n\n**Example 1: Only calculate metrics and visualize, don't filter yet**\n```python\nadata = ad.read_h5ad('input.h5ad')\ncalculate_qc_metrics(adata, inplace=True)\nplot_qc_distributions(adata, 'qc_before.png', title='Initial QC')\nprint_qc_summary(adata, label='Before filtering')\n```\n\n**Example 2: Apply only MT% filtering, keep other metrics permissive**\n```python\nadata = ad.read_h5ad('input.h5ad')\ncalculate_qc_metrics(adata, inplace=True)\n\n# Only filter high MT% cells\nhigh_mt = apply_hard_threshold(adata, 'pct_counts_mt', 10, operator='\u003e')\nadata_filtered = filter_cells(adata, ~high_mt)\nadata_filtered.write('filtered.h5ad')\n```\n\n**Example 3: Different thresholds for different subsets**\n```python\nadata = ad.read_h5ad('input.h5ad')\ncalculate_qc_metrics(adata, inplace=True)\n\n# Apply type-specific QC (assumes cell_type metadata exists)\nneurons = adata.obs['cell_type'] == 'neuron'\nother_cells = ~neurons\n\n# Neurons tolerate higher MT%, other cells use stricter threshold\nneuron_qc = apply_hard_threshold(adata[neurons], 'pct_counts_mt', 15, operator='\u003e')\nother_qc = apply_hard_threshold(adata[other_cells], 'pct_counts_mt', 8, operator='\u003e')\n```\n\n## Best Practices\n\n1. **Be permissive with filtering** - Default thresholds intentionally retain most cells to avoid losing rare populations\n2. **Inspect visualizations** - Always review before/after plots to ensure filtering makes biological sense\n3. **Consider dataset-specific factors** - Some tissues naturally have higher mitochondrial content (e.g., neurons, cardiomyocytes)\n4. **Check gene annotations** - Mitochondrial gene prefixes vary by species (mt- for mouse, MT- for human)\n5. **Iterate if needed** - QC parameters may need adjustment based on the specific experiment or tissue type\n\n## Reference Materials\n\nFor detailed QC methodology, parameter rationale, and troubleshooting guidance, see `references/scverse_qc_guidelines.md`. This reference provides:\n- Detailed explanations of each QC metric and why it matters\n- Rationale for MAD-based thresholds and why they're better than fixed cutoffs\n- Guidelines for interpreting QC visualizations (histograms, violin plots, scatter plots)\n- Species-specific considerations for gene annotations\n- When and how to adjust filtering parameters\n- Advanced QC considerations (ambient RNA correction, doublet detection)\n\nLoad this reference when users need deeper understanding of the methodology or when troubleshooting QC issues.\n\n## Next Steps After QC\n\nTypical downstream analysis steps:\n- Ambient RNA correction (SoupX, CellBender)\n- Doublet detection (scDblFinder)\n- Normalization (log-normalize, scran)\n- Feature selection and dimensionality reduction\n- Clustering and cell type annotation\n", "depth": 4}, {"id": "130", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/single-cell-rna-qc/LICENSE.txt", "name": "LICENSE.txt", "type": "file", "content": "Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction,\nand distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\nthe copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all\nother entities that control, are controlled by, or are under common\ncontrol with that entity. For the purposes of this definition,\n\"control\" means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or\notherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\nexercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications,\nincluding but not limited to software source code, documentation\nsource, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical\ntransformation or translation of a Source form, including but\nnot limited to compiled object code, generated documentation,\nand conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or\nObject form, made available under the License, as indicated by a\ncopyright notice that is included in or attached to the work\n(an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\nform, that is based on (or derived from) the Work and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship. For the purposes\nof this License, Derivative Works shall not include works that remain\nseparable from, or merely link (or bind by name) to the interfaces of,\nthe Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including\nthe original version of the Work and any modifications or additions\nto that Work or Derivative Works thereof, that is intentionally\nsubmitted to Licensor for inclusion in the Work by the copyright owner\nor by an individual or Legal Entity authorized to submit on behalf of\nthe copyright owner. For the purposes of this definition, \"submitted\"\nmeans any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems,\nand issue tracking systems that are managed by, or on behalf of, the\nLicensor for the purpose of discussing and improving the Work, but\nexcluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\non behalf of whom a Contribution has been received by Licensor and\nsubsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\ncopyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the\nWork and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\n(except as stated in this section) patent license to make, have made,\nuse, offer to sell, sell, import, and otherwise transfer the Work,\nwhere such license applies only to those patent claims licensable\nby such Contributor that are necessarily infringed by their\nContribution(s) alone or by combination of their Contribution(s)\nwith the Work to which such Contribution(s) was submitted. If You\ninstitute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work\nor a Contribution incorporated within the Work constitutes direct\nor contributory patent infringement, then any patent licenses\ngranted to You under this License for that Work shall terminate\nas of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\nWork or Derivative Works thereof in any medium, with or without\nmodifications, and in Source or Object form, provided that You\nmeet the following conditions:\n\n(a) You must give any other recipients of the Work or\nDerivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices\nstating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works\nthat You distribute, all copyright, patent, trademark, and\nattribution notices from the Source form of the Work,\nexcluding those notices that do not pertain to any part of\nthe Derivative Works; and\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\ndistribution, then any Derivative Works that You distribute must\ninclude a readable copy of the attribution notices contained\nwithin such NOTICE file, excluding those notices that do not\npertain to any part of the Derivative Works, in at least one\nof the following places: within a NOTICE text file distributed\nas part of the Derivative Works; within the Source form or\ndocumentation, if provided along with the Derivative Works; or,\nwithin a display generated by the Derivative Works, if and\nwherever such third-party notices normally appear. The contents\nof the NOTICE file are for informational purposes only and\ndo not modify the License. You may add Your own attribution\nnotices within Derivative Works that You distribute, alongside\nor as an addendum to the NOTICE text from the Work, provided\nthat such additional attribution notices cannot be construed\nas modifying the License.\n\nYou may add Your own copyright statement to Your modifications and\nmay provide additional or different license terms and conditions\nfor use, reproduction, or distribution of Your modifications, or\nfor any such Derivative Works as a whole, provided Your use,\nreproduction, and distribution of the Work otherwise complies with\nthe conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\nany Contribution intentionally submitted for inclusion in the Work\nby You to the Licensor shall be under the terms and conditions of\nthis License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify\nthe terms of any separate license agreement you may have executed\nwith Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\nnames, trademarks, service marks, or product names of the Licensor,\nexcept as required for reasonable and customary use in describing the\norigin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\nagreed to in writing, Licensor provides the Work (and each\nContributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied, including, without limitation, any warranties or conditions\nof TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\nPARTICULAR PURPOSE. You are solely responsible for determining the\nappropriateness of using or redistributing the Work and assume any\nrisks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\nwhether in tort (including negligence), contract, or otherwise,\nunless required by applicable law (such as deliberate and grossly\nnegligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special,\nincidental, or consequential damages of any character arising as a\nresult of this License or out of the use or inability to use the\nWork (including but not limited to damages for loss of goodwill,\nwork stoppage, computer failure or malfunction, or any and all\nother commercial damages or losses), even if such Contributor\nhas been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\nthe Work or Derivative Works thereof, You may choose to offer,\nand charge a fee for, acceptance of support, warranty, indemnity,\nor other liability obligations and/or rights consistent with this\nLicense. However, in accepting such obligations, You may act only\non Your own behalf and on Your sole responsibility, not on behalf\nof any other Contributor, and only if You agree to indemnify,\ndefend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason\nof your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\nTo apply the Apache License to your work, attach the following\nboilerplate notice, with the fields enclosed by brackets \"[]\"\nreplaced with your own identifying information. (Don't include\nthe brackets!) The text should be enclosed in the appropriate\ncomment syntax for the file format. We also recommend that a\nfile or class name and description of purpose be included on the\nsame \"printed page\" as the copyright notice for easier\nidentification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", "depth": 4}, {"id": "131", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/single-cell-rna-qc/references/scverse_qc_guidelines.md", "name": "scverse_qc_guidelines.md", "type": "file", "content": "# scverse Quality Control Guidelines\n\nThis document provides detailed information about quality control best practices for single-cell RNA-seq data, following the scverse ecosystem recommendations.\n\n## Quality Control Metrics\n\n### Count Depth (Total Counts)\n- **What it measures**: Total number of UMI/reads per cell\n- **Why it matters**: Low count cells may be empty droplets, debris, or poorly captured cells\n- **Typical range**: 500-50,000 counts per cell (varies by protocol)\n- **Red flags**: Bimodal distributions may indicate mixing of high and low-quality cells\n\n### Gene Detection (Genes per Cell)\n- **What it measures**: Number of genes with at least 1 count\n- **Why it matters**: Strongly correlates with count depth; low values indicate poor capture\n- **Typical range**: 200-5,000 genes per cell\n- **Red flags**: Very low values (\u003c200) suggest technical failures\n\n### Mitochondrial Content\n- **What it measures**: Percentage of counts from mitochondrial genes\n- **Why it matters**: High MT% indicates cell stress, apoptosis, or lysed cells\n- **Typical range**: \u003c5% for most tissues, up to 10-15% for metabolically active cells\n- **Species-specific patterns**:\n  - Mouse: Genes start with 'mt-' (e.g., mt-Nd1, mt-Co1)\n  - Human: Genes start with 'MT-' (e.g., MT-ND1, MT-CO1)\n- **Context matters**: Some cell types (cardiomyocytes, neurons) naturally have higher MT content\n\n### Ribosomal Content\n- **What it measures**: Percentage of counts from ribosomal protein genes\n- **Why it matters**: Can indicate cell state or contamination\n- **Patterns**: Genes start with 'Rpl'/'RPL' (large subunit) or 'Rps'/'RPS' (small subunit)\n- **Note**: High ribosomal content isn't always bad - metabolically active cells have more ribosomes\n\n### Hemoglobin Content\n- **What it measures**: Percentage of counts from hemoglobin genes\n- **Why it matters**: Indicates blood contamination in non-blood tissues\n- **Patterns**: Genes matching '^Hb[^(p)]' or '^HB[^(P)]' (excludes Hbp1/HBP1)\n- **When to use**: Particularly important for tissue samples (brain, liver, etc.)\n\n## MAD-Based Filtering Rationale\n\n### Why MAD Instead of Fixed Thresholds?\n\nFixed thresholds (e.g., \"remove cells with \u003c500 genes\") fail because:\n- Different protocols yield different ranges\n- Different tissues have different characteristics\n- Different species have different gene counts\n- Fixed thresholds are arbitrary and not data-driven\n\nMAD (Median Absolute Deviation) is robust to outliers and adapts to your dataset:\n```\nMAD = median(|X - median(X)|)\nOutlier bounds = median ± n_MADs × MAD\n```\n\n### Recommended MAD Thresholds\n\nFollowing scverse best practices (deliberately permissive):\n\n**5 MADs for count depth (log-transformed)**\n- Very permissive to retain rare cell populations\n- Catches extreme outliers (empty droplets, debris)\n- Log transformation handles the typical right-skewed distribution\n\n**5 MADs for gene counts (log-transformed)**\n- Parallels count depth filtering\n- Most informative when combined with count filtering\n- Log transformation normalizes the distribution\n\n**3 MADs for mitochondrial percentage**\n- More stringent because high MT% strongly indicates dying cells\n- Uses raw percentages (not log-transformed)\n- Combined with hard threshold for extra stringency\n\n**Hard threshold: 8% mitochondrial content**\n- Additional filter beyond MAD-based detection\n- Conservative cutoff that works across most tissues\n- Adjust higher (10-15%) for metabolically active cell types\n\n### Why Be Permissive?\n\nThe default thresholds intentionally err on the side of keeping cells because:\n1. **Rare populations**: Stringent filtering may remove rare but viable cell types\n2. **Biological variation**: Some healthy cells naturally have extreme values\n3. **Reversibility**: Easier to filter more later than to recover lost cells\n4. **Downstream robustness**: Modern normalization methods handle moderate quality variation\n\n## Interpreting QC Visualizations\n\n### Histograms\n- **Bimodal distributions**: May indicate mixing of cell types or quality issues\n- **Long tails**: Common for count depth; MAD filtering handles this\n- **Sharp cutoffs**: May indicate prior filtering or technical artifacts\n\n### Violin Plots\n- Shows distribution shape and density\n- Median (line) and mean (diamond) should be similar for symmetric distributions\n- Wide distributions suggest high heterogeneity\n\n### Scatter Plots\n\n**Counts vs Genes (colored by MT%)**\n- Should show strong positive correlation (R² \u003e 0.8 typical)\n- Points deviating from trend may be outliers\n- High MT% cells often cluster at low counts/genes\n\n**Counts vs MT%**\n- Negative correlation expected (dying cells have fewer counts)\n- Vertical stratification may indicate batch effects\n- Cells with high counts + high MT% are suspicious\n\n**Genes vs MT%**\n- Similar to counts vs MT%, but often weaker correlation\n- Useful for identifying cells with gene detection issues\n\n## Gene Filtering\n\nAfter filtering cells, remove genes detected in fewer than 20 cells:\n- **Why 20?**: Balances noise reduction with information retention\n- **Benefits**: Reduces dataset size, speeds up computation, removes noisy genes\n- **Trade-offs**: May lose very rare markers; adjust to 10 if studying rare populations\n\n## Species-Specific Considerations\n\n### Mouse (Mus musculus)\n- Mitochondrial genes: mt-* (lowercase)\n- Ribosomal genes: Rpl*, Rps* (capitalized first letter)\n- Hemoglobin genes: Hb* (but not Hbp1)\n\n### Human (Homo sapiens)\n- Mitochondrial genes: MT-* (uppercase)\n- Ribosomal genes: RPL*, RPS* (all uppercase)\n- Hemoglobin genes: HB* (but not HBP1)\n\n### Other Species\nAdjust gene name patterns in the script to match your organism's gene nomenclature. Consult Ensembl or your reference annotation for correct prefixes.\n\n## When to Adjust Parameters\n\nConsider adjusting filtering thresholds when:\n\n**More stringent (lower MADs)**\n- High ambient RNA contamination suspected\n- Many low-quality cells observed in visualizations\n- Downstream analysis shows quality-driven clustering\n\n**More permissive (higher MADs)**\n- Studying rare cell populations\n- Dataset has high technical quality\n- Cell types naturally have extreme values (e.g., neurons with high MT%)\n\n**Tissue-specific adjustments**\n- Brain/neurons: May need higher MT% threshold (10-15%)\n- Blood: Can be more stringent with MT% (5-8%)\n- Tumor samples: Often need more permissive thresholds due to biological variation\n\n## Advanced QC Considerations\n\n### Not Included in This Workflow\n\n**Ambient RNA correction**\n- Tool: SoupX, CellBender, DecontX\n- When: High background RNA in droplet-based data\n- Effect: Removes contamination from lysed cells\n\n**Doublet detection**\n- Tool: scDblFinder, scrublet, DoubletFinder\n- When: Always recommended for droplet-based data\n- Effect: Identifies and removes multiplets (2+ cells in one droplet)\n\n**Cell cycle scoring**\n- Tool: scanpy's score_genes_cell_cycle\n- When: Cell cycle effects confound biological signal\n- Effect: Allows regressing out or accounting for cell cycle phase\n\n**Batch correction**\n- Tool: Harmony, scVI, ComBat\n- When: Integrating data from multiple batches/experiments\n- Effect: Removes technical batch effects while preserving biology\n\n## References\n\n- scverse Best Practices: https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html\n- Luecken \u0026 Theis (2019): Current best practices in single-cell RNA-seq analysis\n- Osorio \u0026 Cai (2021): Systematic determination of the mitochondrial proportion in human and mouse genomes\n- Germain et al. (2020): Doublet identification in single-cell sequencing data using scDblFinder\n", "depth": 5}, {"id": "132", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "133", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/scripts", "name": "scripts", "type": "dir", "content": "", "depth": 4}, {"id": "134", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: scvi-tools\ndescription: Deep learning for single-cell analysis using scvi-tools. This skill should be used when users need (1) data integration and batch correction with scVI/scANVI, (2) ATAC-seq analysis with PeakVI, (3) CITE-seq multi-modal analysis with totalVI, (4) multiome RNA+ATAC analysis with MultiVI, (5) spatial transcriptomics deconvolution with DestVI, (6) label transfer and reference mapping with scANVI/scArches, (7) RNA velocity with veloVI, or (8) any deep learning-based single-cell method. Triggers include mentions of scVI, scANVI, totalVI, PeakVI, MultiVI, DestVI, veloVI, sysVI, scArches, variational autoencoder, VAE, batch correction, data integration, multi-modal, CITE-seq, multiome, reference mapping, latent space.\n---\n\n# scvi-tools Deep Learning Skill\n\nThis skill provides guidance for deep learning-based single-cell analysis using scvi-tools, the leading framework for probabilistic models in single-cell genomics.\n\n## How to Use This Skill\n\n1. Identify the appropriate workflow from the model/workflow tables below\n2. Read the corresponding reference file for detailed steps and code\n3. Use scripts in `scripts/` to avoid rewriting common code\n4. For installation or GPU issues, consult `references/environment_setup.md`\n5. For debugging, consult `references/troubleshooting.md`\n\n## When to Use This Skill\n\n- When scvi-tools, scVI, scANVI, or related models are mentioned\n- When deep learning-based batch correction or integration is needed\n- When working with multi-modal data (CITE-seq, multiome)\n- When reference mapping or label transfer is required\n- When analyzing ATAC-seq or spatial transcriptomics data\n- When learning latent representations of single-cell data\n\n## Model Selection Guide\n\n| Data Type | Model | Primary Use Case |\n|-----------|-------|------------------|\n| scRNA-seq | **scVI** | Unsupervised integration, DE, imputation |\n| scRNA-seq + labels | **scANVI** | Label transfer, semi-supervised integration |\n| CITE-seq (RNA+protein) | **totalVI** | Multi-modal integration, protein denoising |\n| scATAC-seq | **PeakVI** | Chromatin accessibility analysis |\n| Multiome (RNA+ATAC) | **MultiVI** | Joint modality analysis |\n| Spatial + scRNA reference | **DestVI** | Cell type deconvolution |\n| RNA velocity | **veloVI** | Transcriptional dynamics |\n| Cross-technology | **sysVI** | System-level batch correction |\n\n## Workflow Reference Files\n\n| Workflow | Reference File | Description |\n|----------|---------------|-------------|\n| Environment Setup | `references/environment_setup.md` | Installation, GPU, version info |\n| Data Preparation | `references/data_preparation.md` | Formatting data for any model |\n| scRNA Integration | `references/scrna_integration.md` | scVI/scANVI batch correction |\n| ATAC-seq Analysis | `references/atac_peakvi.md` | PeakVI for accessibility |\n| CITE-seq Analysis | `references/citeseq_totalvi.md` | totalVI for protein+RNA |\n| Multiome Analysis | `references/multiome_multivi.md` | MultiVI for RNA+ATAC |\n| Spatial Deconvolution | `references/spatial_deconvolution.md` | DestVI spatial analysis |\n| Label Transfer | `references/label_transfer.md` | scANVI reference mapping |\n| scArches Mapping | `references/scarches_mapping.md` | Query-to-reference mapping |\n| Batch Correction | `references/batch_correction_sysvi.md` | Advanced batch methods |\n| RNA Velocity | `references/rna_velocity_velovi.md` | veloVI dynamics |\n| Troubleshooting | `references/troubleshooting.md` | Common issues and solutions |\n\n## CLI Scripts\n\nModular scripts for common workflows. Chain together or modify as needed.\n\n### Pipeline Scripts\n\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `prepare_data.py` | QC, filter, HVG selection | `python scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch` |\n| `train_model.py` | Train any scvi-tools model | `python scripts/train_model.py prepared.h5ad results/ --model scvi` |\n| `cluster_embed.py` | Neighbors, UMAP, Leiden | `python scripts/cluster_embed.py adata.h5ad results/` |\n| `differential_expression.py` | DE analysis | `python scripts/differential_expression.py model/ adata.h5ad de.csv --groupby leiden` |\n| `transfer_labels.py` | Label transfer with scANVI | `python scripts/transfer_labels.py ref_model/ query.h5ad results/` |\n| `integrate_datasets.py` | Multi-dataset integration | `python scripts/integrate_datasets.py results/ data1.h5ad data2.h5ad` |\n| `validate_adata.py` | Check data compatibility | `python scripts/validate_adata.py data.h5ad --batch-key batch` |\n\n### Example Workflow\n\n```bash\n# 1. Validate input data\npython scripts/validate_adata.py raw.h5ad --batch-key batch --suggest\n\n# 2. Prepare data (QC, HVG selection)\npython scripts/prepare_data.py raw.h5ad prepared.h5ad --batch-key batch --n-hvgs 2000\n\n# 3. Train model\npython scripts/train_model.py prepared.h5ad results/ --model scvi --batch-key batch\n\n# 4. Cluster and visualize\npython scripts/cluster_embed.py results/adata_trained.h5ad results/ --resolution 0.8\n\n# 5. Differential expression\npython scripts/differential_expression.py results/model results/adata_clustered.h5ad results/de.csv --groupby leiden\n```\n\n### Python Utilities\n\nThe `scripts/model_utils.py` provides importable functions for custom workflows:\n\n| Function | Purpose |\n|----------|---------|\n| `prepare_adata()` | Data preparation (QC, HVG, layer setup) |\n| `train_scvi()` | Train scVI or scANVI |\n| `evaluate_integration()` | Compute integration metrics |\n| `get_marker_genes()` | Extract DE markers |\n| `save_results()` | Save model, data, plots |\n| `auto_select_model()` | Suggest best model |\n| `quick_clustering()` | Neighbors + UMAP + Leiden |\n\n## Critical Requirements\n\n1. **Raw counts required**: scvi-tools models require integer count data\n   ```python\n   adata.layers[\"counts\"] = adata.X.copy()  # Before normalization\n   scvi.model.SCVI.setup_anndata(adata, layer=\"counts\")\n   ```\n\n2. **HVG selection**: Use 2000-4000 highly variable genes\n   ```python\n   sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=\"batch\", layer=\"counts\", flavor=\"seurat_v3\")\n   adata = adata[:, adata.var['highly_variable']].copy()\n   ```\n\n3. **Batch information**: Specify batch_key for integration\n   ```python\n   scvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\n   ```\n\n## Quick Decision Tree\n\n```\nNeed to integrate scRNA-seq data?\n├── Have cell type labels? → scANVI (references/label_transfer.md)\n└── No labels? → scVI (references/scrna_integration.md)\n\nHave multi-modal data?\n├── CITE-seq (RNA + protein)? → totalVI (references/citeseq_totalvi.md)\n├── Multiome (RNA + ATAC)? → MultiVI (references/multiome_multivi.md)\n└── scATAC-seq only? → PeakVI (references/atac_peakvi.md)\n\nHave spatial data?\n└── Need cell type deconvolution? → DestVI (references/spatial_deconvolution.md)\n\nHave pre-trained reference model?\n└── Map query to reference? → scArches (references/scarches_mapping.md)\n\nNeed RNA velocity?\n└── veloVI (references/rna_velocity_velovi.md)\n\nStrong cross-technology batch effects?\n└── sysVI (references/batch_correction_sysvi.md)\n```\n\n## Key Resources\n\n- [scvi-tools Documentation](https://docs.scvi-tools.org/)\n- [scvi-tools Tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)\n- [Model Hub](https://huggingface.co/scvi-tools)\n- [GitHub Issues](https://github.com/scverse/scvi-tools/issues)\n", "depth": 4}, {"id": "135", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/LICENSE.txt", "name": "LICENSE.txt", "type": "file", "content": "Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction,\nand distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\nthe copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all\nother entities that control, are controlled by, or are under common\ncontrol with that entity. For the purposes of this definition,\n\"control\" means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or\notherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\nexercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications,\nincluding but not limited to software source code, documentation\nsource, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical\ntransformation or translation of a Source form, including but\nnot limited to compiled object code, generated documentation,\nand conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or\nObject form, made available under the License, as indicated by a\ncopyright notice that is included in or attached to the work\n(an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\nform, that is based on (or derived from) the Work and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship. For the purposes\nof this License, Derivative Works shall not include works that remain\nseparable from, or merely link (or bind by name) to the interfaces of,\nthe Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including\nthe original version of the Work and any modifications or additions\nto that Work or Derivative Works thereof, that is intentionally\nsubmitted to Licensor for inclusion in the Work by the copyright owner\nor by an individual or Legal Entity authorized to submit on behalf of\nthe copyright owner. For the purposes of this definition, \"submitted\"\nmeans any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems,\nand issue tracking systems that are managed by, or on behalf of, the\nLicensor for the purpose of discussing and improving the Work, but\nexcluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\non behalf of whom a Contribution has been received by Licensor and\nsubsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\ncopyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the\nWork and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\n(except as stated in this section) patent license to make, have made,\nuse, offer to sell, sell, import, and otherwise transfer the Work,\nwhere such license applies only to those patent claims licensable\nby such Contributor that are necessarily infringed by their\nContribution(s) alone or by combination of their Contribution(s)\nwith the Work to which such Contribution(s) was submitted. If You\ninstitute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work\nor a Contribution incorporated within the Work constitutes direct\nor contributory patent infringement, then any patent licenses\ngranted to You under this License for that Work shall terminate\nas of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\nWork or Derivative Works thereof in any medium, with or without\nmodifications, and in Source or Object form, provided that You\nmeet the following conditions:\n\n(a) You must give any other recipients of the Work or\nDerivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices\nstating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works\nthat You distribute, all copyright, patent, trademark, and\nattribution notices from the Source form of the Work,\nexcluding those notices that do not pertain to any part of\nthe Derivative Works; and\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\ndistribution, then any Derivative Works that You distribute must\ninclude a readable copy of the attribution notices contained\nwithin such NOTICE file, excluding those notices that do not\npertain to any part of the Derivative Works, in at least one\nof the following places: within a NOTICE text file distributed\nas part of the Derivative Works; within the Source form or\ndocumentation, if provided along with the Derivative Works; or,\nwithin a display generated by the Derivative Works, if and\nwherever such third-party notices normally appear. The contents\nof the NOTICE file are for informational purposes only and\ndo not modify the License. You may add Your own attribution\nnotices within Derivative Works that You distribute, alongside\nor as an addendum to the NOTICE text from the Work, provided\nthat such additional attribution notices cannot be construed\nas modifying the License.\n\nYou may add Your own copyright statement to Your modifications and\nmay provide additional or different license terms and conditions\nfor use, reproduction, or distribution of Your modifications, or\nfor any such Derivative Works as a whole, provided Your use,\nreproduction, and distribution of the Work otherwise complies with\nthe conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\nany Contribution intentionally submitted for inclusion in the Work\nby You to the Licensor shall be under the terms and conditions of\nthis License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify\nthe terms of any separate license agreement you may have executed\nwith Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\nnames, trademarks, service marks, or product names of the Licensor,\nexcept as required for reasonable and customary use in describing the\norigin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\nagreed to in writing, Licensor provides the Work (and each\nContributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied, including, without limitation, any warranties or conditions\nof TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\nPARTICULAR PURPOSE. You are solely responsible for determining the\nappropriateness of using or redistributing the Work and assume any\nrisks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\nwhether in tort (including negligence), contract, or otherwise,\nunless required by applicable law (such as deliberate and grossly\nnegligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special,\nincidental, or consequential damages of any character arising as a\nresult of this License or out of the use or inability to use the\nWork (including but not limited to damages for loss of goodwill,\nwork stoppage, computer failure or malfunction, or any and all\nother commercial damages or losses), even if such Contributor\nhas been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\nthe Work or Derivative Works thereof, You may choose to offer,\nand charge a fee for, acceptance of support, warranty, indemnity,\nor other liability obligations and/or rights consistent with this\nLicense. However, in accepting such obligations, You may act only\non Your own behalf and on Your sole responsibility, not on behalf\nof any other Contributor, and only if You agree to indemnify,\ndefend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason\nof your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\nTo apply the Apache License to your work, attach the following\nboilerplate notice, with the fields enclosed by brackets \"[]\"\nreplaced with your own identifying information. (Don't include\nthe brackets!) The text should be enclosed in the appropriate\ncomment syntax for the file format. We also recommend that a\nfile or class name and description of purpose be included on the\nsame \"printed page\" as the copyright notice for easier\nidentification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", "depth": 4}, {"id": "136", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/environment_setup.md", "name": "environment_setup.md", "type": "file", "content": "# Environment Setup for scvi-tools\n\nThis reference covers installation and environment configuration for scvi-tools.\n\n## Installation Options\n\n### Option 1: Conda Environment (Recommended)\n\n```bash\n# Create environment with GPU support\nconda create -n scvi-env python=3.10\nconda activate scvi-env\n\n# Install scvi-tools\npip install scvi-tools\n\n# For GPU acceleration (recommended for large datasets)\npip install torch --index-url https://download.pytorch.org/whl/cu118\n\n# Common dependencies\npip install scanpy leidenalg\n```\n\n### Option 2: Pip Only\n\n```bash\n# Create virtual environment\npython -m venv scvi-env\nsource scvi-env/bin/activate  # Linux/Mac\n# scvi-env\\Scripts\\activate   # Windows\n\n# Install\npip install scvi-tools scanpy\n```\n\n### Option 3: With Spatial Analysis Support\n\n```bash\nconda create -n scvi-spatial python=3.10\nconda activate scvi-spatial\n\npip install scvi-tools scanpy squidpy\n```\n\n### Option 4: With MuData Support (Multiome)\n\n```bash\npip install scvi-tools mudata muon\n```\n\n## Verify Installation\n\n```python\nimport scvi\nimport torch\nimport scanpy as sc\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\nprint(f\"scanpy version: {sc.__version__}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"GPU available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n```\n\n## GPU Configuration\n\n### Check CUDA Version\n\n```bash\nnvidia-smi\nnvcc --version\n```\n\n### PyTorch CUDA Versions\n\n| CUDA Version | PyTorch Install Command |\n|--------------|------------------------|\n| CUDA 11.8 | `pip install torch --index-url https://download.pytorch.org/whl/cu118` |\n| CUDA 12.1 | `pip install torch --index-url https://download.pytorch.org/whl/cu121` |\n| CPU only | `pip install torch --index-url https://download.pytorch.org/whl/cpu` |\n\n### Memory Management\n\n```python\nimport torch\n\n# Clear GPU cache between models\ntorch.cuda.empty_cache()\n\n# Monitor memory usage\nprint(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\nprint(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n```\n\n## Common Issues\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| `CUDA out of memory` | GPU memory exhausted | Reduce batch_size, use smaller model |\n| `No GPU detected` | CUDA not installed | Install CUDA toolkit matching PyTorch |\n| `Version mismatch` | PyTorch/CUDA incompatibility | Reinstall PyTorch with correct CUDA version |\n| `Import error scvi` | Missing dependencies | `pip install scvi-tools[all]` |\n\n## Jupyter Setup\n\n```bash\n# Install Jupyter kernel\npip install ipykernel\npython -m ipykernel install --user --name scvi-env --display-name \"scvi-tools\"\n\n# For interactive plots\npip install matplotlib seaborn\n```\n\n## Recommended Package Versions\n\nFor reproducibility, pin versions:\n\n```bash\npip install \\\n    scvi-tools\u003e=1.0.0 \\\n    scanpy\u003e=1.9.0 \\\n    anndata\u003e=0.9.0 \\\n    torch\u003e=2.0.0\n```\n\n## Version Compatibility Guide\n\n### scvi-tools 1.x vs 0.x API Changes\n\nThe 1.x release introduced breaking changes. Key differences:\n\n| Operation | 0.x API (deprecated) | 1.x API (current) |\n|-----------|---------------------|-------------------|\n| Setup data | `scvi.data.setup_anndata(adata, ...)` | `scvi.model.SCVI.setup_anndata(adata, ...)` |\n| Register data | `scvi.data.register_tensor_from_anndata(...)` | Built into `setup_anndata` |\n| View setup | `scvi.data.view_anndata_setup(adata)` | `scvi.model.SCVI.view_anndata_setup(adata)` |\n\n### Migration from 0.x to 1.x\n\n```python\n# OLD (0.x) - DEPRECATED\nimport scvi\nscvi.data.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\nmodel = scvi.model.SCVI(adata)\n\n# NEW (1.x) - CURRENT\nimport scvi\nscvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\nmodel = scvi.model.SCVI(adata)\n```\n\n### Model-Specific Setup (1.x)\n\nEach model has its own setup method:\n\n```python\n# scVI\nscvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\n\n# scANVI\nscvi.model.SCANVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\", labels_key=\"cell_type\")\n\n# totalVI\nscvi.model.TOTALVI.setup_anndata(adata, layer=\"counts\", protein_expression_obsm_key=\"protein\")\n\n# MultiVI (uses MuData)\nscvi.model.MULTIVI.setup_mudata(mdata, rna_layer=\"counts\", atac_layer=\"counts\")\n\n# PeakVI\nscvi.model.PEAKVI.setup_anndata(adata, batch_key=\"batch\")\n\n# veloVI\nscvi.external.VELOVI.setup_anndata(adata, spliced_layer=\"spliced\", unspliced_layer=\"unspliced\")\n```\n\n### Minimum Version Requirements\n\n| Package | Minimum Version | Notes |\n|---------|-----------------|-------|\n| scvi-tools | 1.0.0 | Required for current API |\n| scanpy | 1.9.0 | HVG selection improvements |\n| anndata | 0.9.0 | Improved MuData support |\n| torch | 2.0.0 | Performance improvements |\n| mudata | 0.2.0 | Required for MultiVI |\n| scvelo | 0.2.5 | Required for veloVI |\n\n### Check Your Versions\n\n```python\nimport scvi\nimport scanpy as sc\nimport anndata\nimport torch\n\nprint(f\"scvi-tools: {scvi.__version__}\")\nprint(f\"scanpy: {sc.__version__}\")\nprint(f\"anndata: {anndata.__version__}\")\nprint(f\"torch: {torch.__version__}\")\n\n# Check if using 1.x API\nif hasattr(scvi.model.SCVI, 'setup_anndata'):\n    print(\"Using scvi-tools 1.x API\")\nelse:\n    print(\"WARNING: Using deprecated 0.x API - please upgrade\")\n```\n\n### Known Compatibility Issues\n\n| Issue | Affected Versions | Solution |\n|-------|-------------------|----------|\n| `setup_anndata` not found | scvi-tools \u003c 1.0 | Upgrade to 1.0+ |\n| MuData errors | mudata \u003c 0.2 | `pip install mudata\u003e=0.2.0` |\n| CUDA version mismatch | Any | Reinstall PyTorch for your CUDA |\n| numpy 2.0 issues | Early 2024 builds | `pip install numpy\u003c2.0` |\n\n### Upgrading scvi-tools\n\n```bash\n# Upgrade to latest\npip install --upgrade scvi-tools\n\n# Upgrade all dependencies\npip install --upgrade scvi-tools scanpy anndata torch\n\n# If you have issues, clean install\npip uninstall scvi-tools\npip cache purge\npip install scvi-tools\n```\n\n## Testing Installation\n\n```python\n# Quick test with sample data\nimport scvi\nimport scanpy as sc\n\n# Load test dataset\nadata = scvi.data.heart_cell_atlas_subsampled()\nprint(f\"Loaded test data: {adata.shape}\")\n\n# Setup and create model (quick test)\nscvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"cell_source\")\nmodel = scvi.model.SCVI(adata, n_latent=10)\nprint(\"Model created successfully\")\n\n# Quick training test (1 epoch)\nmodel.train(max_epochs=1)\nprint(\"Training works!\")\n```\n", "depth": 5}, {"id": "137", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/citeseq_totalvi.md", "name": "citeseq_totalvi.md", "type": "file", "content": "# CITE-seq Analysis with totalVI\n\nThis reference covers multi-modal analysis of CITE-seq data (RNA + surface proteins) using totalVI.\n\n## Overview\n\nCITE-seq combines:\n- scRNA-seq (transcriptome)\n- Protein surface markers (antibody-derived tags, ADT)\n\ntotalVI jointly models both modalities to:\n- Integrate across batches\n- Denoise protein signal\n- Learn joint latent representation\n- Enable cross-modal imputation\n\n## Prerequisites\n\n```python\nimport scvi\nimport scanpy as sc\nimport mudata as md\nimport numpy as np\nimport pandas as pd\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\n```\n\n## Step 1: Load CITE-seq Data\n\n### From 10x Genomics (Cell Ranger)\n\n```python\n# 10x outputs separate gene expression and feature barcoding\nadata_rna = sc.read_10x_h5(\"filtered_feature_bc_matrix.h5\", gex_only=False)\n\n# Separate RNA and protein\nadata_protein = adata_rna[:, adata_rna.var['feature_types'] == 'Antibody Capture'].copy()\nadata_rna = adata_rna[:, adata_rna.var['feature_types'] == 'Gene Expression'].copy()\n\nprint(f\"RNA: {adata_rna.shape}\")\nprint(f\"Protein: {adata_protein.shape}\")\n```\n\n### From MuData\n\n```python\n# If data is in MuData format\nmdata = md.read_h5mu(\"cite_seq.h5mu\")\n\nadata_rna = mdata['rna'].copy()\nadata_protein = mdata['protein'].copy()\n```\n\n### Combine into Single AnnData\n\n```python\n# totalVI expects protein data in obsm\nadata = adata_rna.copy()\n\n# Add protein expression to obsm\nadata.obsm[\"protein_expression\"] = adata_protein.X.toarray() if hasattr(adata_protein.X, 'toarray') else adata_protein.X\n\n# Store protein names\nadata.uns[\"protein_names\"] = list(adata_protein.var_names)\n```\n\n## Step 2: Quality Control\n\n### RNA QC\n\n```python\n# Standard RNA QC\n# Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes\n    adata.var['mt'] = (\n        adata.var_names.str.startswith('MT-') |\n        adata.var_names.str.startswith('mt-') |\n        adata.var_names.str.startswith('Mt-')\n    )\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)\n\n# Filter cells\nadata = adata[adata.obs['n_genes_by_counts'] \u003e 200].copy()\nadata = adata[adata.obs['pct_counts_mt'] \u003c 20].copy()\n\n# Filter genes\nsc.pp.filter_genes(adata, min_cells=3)\n```\n\n### Protein QC\n\n```python\n# Protein QC\nprotein_counts = adata.obsm[\"protein_expression\"]\nprint(f\"Protein counts per cell: min={protein_counts.sum(1).min():.0f}, max={protein_counts.sum(1).max():.0f}\")\n\n# Check for isotype controls\n# Isotype controls should have low counts\nprotein_names = adata.uns[\"protein_names\"]\nfor i, name in enumerate(protein_names):\n    if 'isotype' in name.lower() or 'control' in name.lower():\n        print(f\"{name}: mean={protein_counts[:, i].mean():.1f}\")\n```\n\n## Step 3: Data Preparation\n\n### Store Raw Counts\n\n```python\n# Store RNA counts\nadata.layers[\"counts\"] = adata.X.copy()\n\n# Protein must be raw ADT counts (NOT CLR-normalized)\n# WARNING: If importing from Seurat, ensure you use raw counts, not CLR-normalized data\n# Seurat's NormalizeData(normalization.method = \"CLR\") transforms counts - use the original assay\n```\n\n### HVG Selection for RNA\n\n```python\n# Select HVGs for RNA\n# Note: totalVI uses all proteins regardless of HVG\n\nsc.pp.highly_variable_genes(\n    adata,\n    n_top_genes=4000,  # Use more for CITE-seq\n    flavor=\"seurat_v3\",\n    batch_key=\"batch\" if \"batch\" in adata.obs else None,\n    layer=\"counts\"\n)\n\n# Subset to HVGs\nadata = adata[:, adata.var[\"highly_variable\"]].copy()\n```\n\n## Step 4: Setup and Train totalVI\n\n```python\n# Setup AnnData for totalVI\nscvi.model.TOTALVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    protein_expression_obsm_key=\"protein_expression\",\n    batch_key=\"batch\"  # Optional\n)\n\n# Create model\nmodel = scvi.model.TOTALVI(\n    adata,\n    n_latent=20,\n    latent_distribution=\"normal\"  # or \"ln\" for log-normal\n)\n\n# Train\nmodel.train(\n    max_epochs=200,\n    early_stopping=True,\n    batch_size=128\n)\n\n# Check training\nmodel.history['elbo_train'].plot()\n```\n\n## Step 5: Get Latent Representation\n\n```python\n# Joint latent space\nadata.obsm[\"X_totalVI\"] = model.get_latent_representation()\n\n# Clustering and visualization\nsc.pp.neighbors(adata, use_rep=\"X_totalVI\")\nsc.tl.umap(adata)\nsc.tl.leiden(adata, resolution=1.0)\n\nsc.pl.umap(adata, color=['leiden', 'batch'])\n```\n\n## Step 6: Denoised Protein Expression\n\n```python\n# Get denoised protein values\n# This removes background noise from protein measurements\n\n_, protein_denoised = model.get_normalized_expression(\n    return_mean=True,\n    transform_batch=\"batch1\"  # Optional: normalize to specific batch\n)\n\n# Add to adata\nadata.obsm[\"protein_denoised\"] = protein_denoised\n\n# Visualize denoised proteins\nprotein_names = adata.uns[\"protein_names\"]\nfor i, protein in enumerate(protein_names[:5]):\n    adata.obs[f\"denoised_{protein}\"] = protein_denoised[:, i]\n\nsc.pl.umap(adata, color=[f\"denoised_{p}\" for p in protein_names[:5]])\n```\n\n## Step 7: Normalized RNA Expression\n\n```python\n# Get normalized RNA expression\nrna_normalized, _ = model.get_normalized_expression(\n    return_mean=True\n)\n\n# Store\nadata.layers[\"totalVI_normalized\"] = rna_normalized\n```\n\n## Step 8: Differential Expression\n\n### RNA Differential Expression\n\n```python\n# DE between clusters\nde_rna = model.differential_expression(\n    groupby=\"leiden\",\n    group1=\"0\",\n    group2=\"1\"\n)\n\n# Filter significant genes\nde_sig = de_rna[\n    (de_rna['is_de_fdr_0.05']) \u0026\n    (abs(de_rna['lfc_mean']) \u003e 1)\n]\n\nprint(f\"Significant DE genes: {len(de_sig)}\")\n```\n\n### Protein Differential Expression\n\n```python\n# Protein DE\nde_protein = model.differential_expression(\n    groupby=\"leiden\",\n    group1=\"0\",\n    group2=\"1\",\n    mode=\"protein\"\n)\n\nprint(de_protein.head(20))\n```\n\n## Step 9: Visualization\n\n### Protein Expression on UMAP\n\n```python\n# Denoised protein on UMAP\nimport matplotlib.pyplot as plt\n\nproteins_to_plot = [\"CD3\", \"CD4\", \"CD8\", \"CD19\", \"CD14\"]\n\nfig, axes = plt.subplots(1, len(proteins_to_plot), figsize=(4*len(proteins_to_plot), 4))\nfor ax, protein in zip(axes, proteins_to_plot):\n    idx = adata.uns[\"protein_names\"].index(protein)\n    sc.pl.umap(\n        adata,\n        color=adata.obsm[\"protein_denoised\"][:, idx],\n        ax=ax,\n        title=protein,\n        show=False\n    )\nplt.tight_layout()\n```\n\n### Joint Heatmap\n\n```python\n# Heatmap of top genes and proteins per cluster\nsc.pl.dotplot(\n    adata,\n    var_names=de_sig.index[:20].tolist(),\n    groupby=\"leiden\",\n    layer=\"totalVI_normalized\"\n)\n```\n\n## Step 10: Cell Type Annotation\n\n```python\n# Use both RNA and protein markers for annotation\n\n# RNA markers\nrna_markers = {\n    'T cells': ['CD3D', 'CD3E'],\n    'CD4 T': ['CD4'],\n    'CD8 T': ['CD8A', 'CD8B'],\n    'B cells': ['CD19', 'MS4A1'],\n    'Monocytes': ['CD14', 'LYZ']\n}\n\n# Check denoised protein expression\nfor i, protein in enumerate(adata.uns[\"protein_names\"]):\n    if any(m in protein for m in ['CD3', 'CD4', 'CD8', 'CD19', 'CD14']):\n        print(f\"{protein}: cluster means\")\n        for cluster in adata.obs['leiden'].unique():\n            mask = adata.obs['leiden'] == cluster\n            mean_expr = adata.obsm[\"protein_denoised\"][mask, i].mean()\n            print(f\"  Cluster {cluster}: {mean_expr:.2f}\")\n```\n\n## Complete Pipeline\n\n```python\ndef analyze_citeseq(\n    adata_rna,\n    adata_protein,\n    batch_key=None,\n    n_top_genes=4000,\n    n_latent=20\n):\n    \"\"\"\n    Complete CITE-seq analysis with totalVI.\n    \n    Parameters\n    ----------\n    adata_rna : AnnData\n        RNA expression (raw counts)\n    adata_protein : AnnData\n        Protein expression (raw counts)\n    batch_key : str, optional\n        Batch column in obs\n    n_top_genes : int\n        Number of HVGs\n    n_latent : int\n        Latent dimensions\n        \n    Returns\n    -------\n    Tuple of (processed AnnData, trained model)\n    \"\"\"\n    import scvi\n    import scanpy as sc\n    \n    # Ensure same cells\n    common_cells = adata_rna.obs_names.intersection(adata_protein.obs_names)\n    adata = adata_rna[common_cells].copy()\n    adata_protein = adata_protein[common_cells].copy()\n    \n    # Add protein to obsm\n    adata.obsm[\"protein_expression\"] = adata_protein.X.toarray() if hasattr(adata_protein.X, 'toarray') else adata_protein.X\n    adata.uns[\"protein_names\"] = list(adata_protein.var_names)\n    \n    # RNA QC\n    # Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes\n    adata.var['mt'] = (\n        adata.var_names.str.startswith('MT-') |\n        adata.var_names.str.startswith('mt-') |\n        adata.var_names.str.startswith('Mt-')\n    )\n    sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)\n    adata = adata[adata.obs['pct_counts_mt'] \u003c 20].copy()\n    sc.pp.filter_genes(adata, min_cells=3)\n    \n    # Store counts\n    adata.layers[\"counts\"] = adata.X.copy()\n    \n    # HVG selection\n    sc.pp.highly_variable_genes(\n        adata,\n        n_top_genes=n_top_genes,\n        flavor=\"seurat_v3\",\n        batch_key=batch_key,\n        layer=\"counts\"\n    )\n    adata = adata[:, adata.var[\"highly_variable\"]].copy()\n    \n    # Setup totalVI\n    scvi.model.TOTALVI.setup_anndata(\n        adata,\n        layer=\"counts\",\n        protein_expression_obsm_key=\"protein_expression\",\n        batch_key=batch_key\n    )\n    \n    # Train\n    model = scvi.model.TOTALVI(adata, n_latent=n_latent)\n    model.train(max_epochs=200, early_stopping=True)\n    \n    # Get representations\n    adata.obsm[\"X_totalVI\"] = model.get_latent_representation()\n    rna_norm, protein_denoised = model.get_normalized_expression(return_mean=True)\n    adata.layers[\"totalVI_normalized\"] = rna_norm\n    adata.obsm[\"protein_denoised\"] = protein_denoised\n    \n    # Clustering\n    sc.pp.neighbors(adata, use_rep=\"X_totalVI\")\n    sc.tl.umap(adata)\n    sc.tl.leiden(adata)\n    \n    return adata, model\n\n# Usage\nadata, model = analyze_citeseq(\n    adata_rna,\n    adata_protein,\n    batch_key=\"batch\"\n)\n\n# Visualize\nsc.pl.umap(adata, color=['leiden', 'batch'])\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Protein signal noisy | Background not removed | Use get_normalized_expression with denoising |\n| Batch effects persist | Need batch_key | Ensure batch_key is specified |\n| Memory error | Too many genes | Reduce n_top_genes |\n| Poor protein clustering | Few proteins | Normal - totalVI uses RNA for structure |\n\n## Key References\n\n- Gayoso et al. (2021) \"Joint probabilistic modeling of single-cell multi-omic data with totalVI\"\n", "depth": 5}, {"id": "138", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/label_transfer.md", "name": "label_transfer.md", "type": "file", "content": "# Label Transfer and Reference Mapping with scANVI\n\nThis reference covers using scANVI for transferring cell type annotations from a reference atlas to query data.\n\n## Overview\n\nReference mapping (also called \"label transfer\") uses a pre-trained model on annotated reference data to predict cell types in new, unannotated query data. This is faster than re-clustering and more consistent across studies.\n\nscANVI excels at this because it:\n- Jointly embeds reference and query in shared space\n- Transfers labels probabilistically\n- Handles batch effects between reference and query\n\n## When to Use Reference Mapping\n\n- Annotating new dataset using existing atlas\n- Consistent annotation across multiple studies\n- Speed: no need to re-cluster and manually annotate\n- Quality: leverage expert-curated reference annotations\n\n## Workflow Options\n\n1. **Train new model**: Train scANVI on reference, then map query\n2. **Use pre-trained model**: Load existing model (e.g., from Model Hub)\n3. **scArches**: Extend existing model with query data (preserves reference)\n\n## Option 1: Train scANVI on Reference\n\n### Step 1: Prepare Reference Data\n\n```python\nimport scvi\nimport scanpy as sc\n\n# Load reference atlas\nadata_ref = sc.read_h5ad(\"reference_atlas.h5ad\")\n\n# Check annotations\nprint(f\"Reference cells: {adata_ref.n_obs}\")\nprint(f\"Cell types: {adata_ref.obs['cell_type'].nunique()}\")\nprint(adata_ref.obs['cell_type'].value_counts())\n\n# Ensure raw counts\nadata_ref.layers[\"counts\"] = adata_ref.raw.X.copy() if adata_ref.raw else adata_ref.X.copy()\n\n# HVG selection\nsc.pp.highly_variable_genes(\n    adata_ref,\n    n_top_genes=3000,\n    flavor=\"seurat_v3\",\n    batch_key=\"batch\" if \"batch\" in adata_ref.obs else None,\n    layer=\"counts\"\n)\nadata_ref = adata_ref[:, adata_ref.var[\"highly_variable\"]].copy()\n```\n\n### Step 2: Train scANVI on Reference\n\n```python\n# First train scVI (unlabeled)\nscvi.model.SCVI.setup_anndata(\n    adata_ref,\n    layer=\"counts\",\n    batch_key=\"batch\"\n)\n\nscvi_ref = scvi.model.SCVI(adata_ref, n_latent=30)\nscvi_ref.train(max_epochs=200)\n\n# Initialize scANVI from scVI\nscanvi_ref = scvi.model.SCANVI.from_scvi_model(\n    scvi_ref,\n    labels_key=\"cell_type\",\n    unlabeled_category=\"Unknown\"\n)\n\n# Train scANVI\nscanvi_ref.train(max_epochs=50)\n\n# Save for later use\nscanvi_ref.save(\"scanvi_reference_model/\")\n```\n\n### Step 3: Prepare Query Data\n\n```python\n# Load query data\nadata_query = sc.read_h5ad(\"query_data.h5ad\")\n\n# CRITICAL: Use same genes as reference\ncommon_genes = adata_ref.var_names.intersection(adata_query.var_names)\nprint(f\"Common genes: {len(common_genes)}\")\n\n# Subset query to reference genes\nadata_query = adata_query[:, adata_ref.var_names].copy()\n\n# Handle missing genes (set to 0)\nmissing_genes = set(adata_ref.var_names) - set(adata_query.var_names)\nif missing_genes:\n    # Add missing genes with zero expression\n    import numpy as np\n    from scipy.sparse import csr_matrix\n    \n    zero_matrix = csr_matrix((adata_query.n_obs, len(missing_genes)))\n    # ... concat and reorder to match reference\n    \n# Store counts\nadata_query.layers[\"counts\"] = adata_query.X.copy()\n```\n\n### Step 4: Map Query to Reference\n\n```python\n# Prepare query data for mapping\nscvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)\n\n# Create query model from reference\nscanvi_query = scvi.model.SCANVI.load_query_data(\n    adata_query,\n    scanvi_ref\n)\n\n# Fine-tune on query (optional but recommended)\nscanvi_query.train(\n    max_epochs=100,\n    plan_kwargs={\"weight_decay\": 0.0}\n)\n\n# Get predictions\nadata_query.obs[\"predicted_cell_type\"] = scanvi_query.predict()\n\n# Get prediction probabilities\nsoft_predictions = scanvi_query.predict(soft=True)\nadata_query.obs[\"prediction_score\"] = soft_predictions.max(axis=1)\n```\n\n### Step 5: Evaluate Predictions\n\n```python\n# Confidence scores\nprint(f\"Mean prediction confidence: {adata_query.obs['prediction_score'].mean():.3f}\")\n\n# Low confidence predictions\nlow_conf = adata_query.obs['prediction_score'] \u003c 0.5\nprint(f\"Low confidence cells: {low_conf.sum()} ({low_conf.mean()*100:.1f}%)\")\n\n# Visualize\nsc.pp.neighbors(adata_query, use_rep=\"X_scANVI\")\nsc.tl.umap(adata_query)\nsc.pl.umap(adata_query, color=['predicted_cell_type', 'prediction_score'])\n```\n\n## Option 2: Use Pre-Trained Models\n\n### From Model Hub\n\n```python\n# scvi-tools maintains models on HuggingFace\n# Check: https://huggingface.co/scvi-tools\n\n# Example: Load pre-trained model\nfrom huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(\n    repo_id=\"scvi-tools/example-model\",\n    filename=\"model.pt\"\n)\n\n# Load model\nmodel = scvi.model.SCANVI.load(model_path, adata=adata_query)\n```\n\n### From Published Atlas\n\n```python\n# Many atlases provide pre-trained models\n# Example workflow with CellTypist-style model\n\n# Download reference model\n# model = scvi.model.SCANVI.load(\"atlas_model/\", adata=adata_query)\n```\n\n## Option 3: scArches for Incremental Updates\n\nscArches extends a reference model without retraining from scratch:\n\n```python\n# Load existing reference model\nscanvi_ref = scvi.model.SCANVI.load(\"reference_model/\")\n\n# Surgery: prepare for query integration\nscanvi_ref.freeze_layers()\n\n# Map query data\nscvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)\nscanvi_query = scvi.model.SCANVI.load_query_data(adata_query, scanvi_ref)\n\n# Train only query-specific parameters\nscanvi_query.train(\n    max_epochs=200,\n    plan_kwargs={\"weight_decay\": 0.0}\n)\n```\n\n## Visualize Reference and Query Together\n\n```python\n# Concatenate for joint visualization\nadata_ref.obs[\"dataset\"] = \"reference\"\nadata_query.obs[\"dataset\"] = \"query\"\n\n# Get latent representations\nadata_ref.obsm[\"X_scANVI\"] = scanvi_ref.get_latent_representation()\nadata_query.obsm[\"X_scANVI\"] = scanvi_query.get_latent_representation()\n\n# Combine\nadata_combined = sc.concat([adata_ref, adata_query])\n\n# Compute combined UMAP\nsc.pp.neighbors(adata_combined, use_rep=\"X_scANVI\")\nsc.tl.umap(adata_combined)\n\n# Plot\nsc.pl.umap(\n    adata_combined,\n    color=[\"dataset\", \"cell_type\", \"predicted_cell_type\"],\n    ncols=2\n)\n```\n\n## Quality Control for Predictions\n\n### Confidence Filtering\n\n```python\n# Filter predictions by confidence\nconfidence_threshold = 0.7\n\nhigh_conf = adata_query[adata_query.obs['prediction_score'] \u003e= confidence_threshold].copy()\nlow_conf = adata_query[adata_query.obs['prediction_score'] \u003c confidence_threshold].copy()\n\nprint(f\"High confidence: {len(high_conf)} ({len(high_conf)/len(adata_query)*100:.1f}%)\")\nprint(f\"Low confidence: {len(low_conf)} ({len(low_conf)/len(adata_query)*100:.1f}%)\")\n```\n\n### Marker Validation\n\n```python\n# Validate predictions with known markers\nmarkers = {\n    'T cells': ['CD3D', 'CD3E'],\n    'B cells': ['CD19', 'MS4A1'],\n    'Monocytes': ['CD14', 'LYZ']\n}\n\nfor ct, genes in markers.items():\n    ct_cells = adata_query[adata_query.obs['predicted_cell_type'] == ct]\n    if len(ct_cells) \u003e 0:\n        for gene in genes:\n            if gene in adata_query.var_names:\n                expr = ct_cells[:, gene].X.mean()\n                print(f\"{ct} - {gene}: {expr:.3f}\")\n```\n\n## Complete Pipeline\n\n```python\ndef transfer_labels(\n    adata_ref,\n    adata_query,\n    cell_type_key=\"cell_type\",\n    batch_key=None,\n    n_top_genes=3000,\n    confidence_threshold=0.5\n):\n    \"\"\"\n    Transfer cell type labels from reference to query.\n    \n    Parameters\n    ----------\n    adata_ref : AnnData\n        Annotated reference data\n    adata_query : AnnData\n        Unannotated query data\n    cell_type_key : str\n        Column with cell type annotations in reference\n    batch_key : str, optional\n        Batch column\n    n_top_genes : int\n        Number of HVGs\n    confidence_threshold : float\n        Minimum confidence for predictions\n        \n    Returns\n    -------\n    AnnData with predictions\n    \"\"\"\n    import scvi\n    import scanpy as sc\n    \n    # Prepare reference\n    adata_ref = adata_ref.copy()\n    adata_ref.layers[\"counts\"] = adata_ref.X.copy()\n    \n    sc.pp.highly_variable_genes(\n        adata_ref,\n        n_top_genes=n_top_genes,\n        flavor=\"seurat_v3\",\n        batch_key=batch_key,\n        layer=\"counts\"\n    )\n    adata_ref = adata_ref[:, adata_ref.var[\"highly_variable\"]].copy()\n    \n    # Train reference model\n    scvi.model.SCVI.setup_anndata(adata_ref, layer=\"counts\", batch_key=batch_key)\n    scvi_ref = scvi.model.SCVI(adata_ref, n_latent=30)\n    scvi_ref.train(max_epochs=200)\n    \n    scanvi_ref = scvi.model.SCANVI.from_scvi_model(\n        scvi_ref,\n        labels_key=cell_type_key,\n        unlabeled_category=\"Unknown\"\n    )\n    scanvi_ref.train(max_epochs=50)\n    \n    # Prepare query\n    adata_query = adata_query[:, adata_ref.var_names].copy()\n    adata_query.layers[\"counts\"] = adata_query.X.copy()\n    \n    # Map query\n    scvi.model.SCANVI.prepare_query_anndata(adata_query, scanvi_ref)\n    scanvi_query = scvi.model.SCANVI.load_query_data(adata_query, scanvi_ref)\n    scanvi_query.train(max_epochs=100, plan_kwargs={\"weight_decay\": 0.0})\n    \n    # Get predictions\n    adata_query.obs[\"predicted_cell_type\"] = scanvi_query.predict()\n    soft = scanvi_query.predict(soft=True)\n    adata_query.obs[\"prediction_score\"] = soft.max(axis=1)\n    \n    # Mark low confidence\n    adata_query.obs[\"confident_prediction\"] = adata_query.obs[\"prediction_score\"] \u003e= confidence_threshold\n    \n    # Add latent representation\n    adata_query.obsm[\"X_scANVI\"] = scanvi_query.get_latent_representation()\n    \n    return adata_query, scanvi_ref, scanvi_query\n\n# Usage\nadata_annotated, ref_model, query_model = transfer_labels(\n    adata_ref,\n    adata_query,\n    cell_type_key=\"cell_type\"\n)\n\n# Visualize\nsc.pp.neighbors(adata_annotated, use_rep=\"X_scANVI\")\nsc.tl.umap(adata_annotated)\nsc.pl.umap(adata_annotated, color=['predicted_cell_type', 'prediction_score'])\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Many low-confidence predictions | Query has novel cell types | Manually annotate low-confidence cells |\n| Wrong predictions | Reference doesn't match tissue | Use tissue-appropriate reference |\n| Gene mismatch | Different gene naming | Convert gene IDs |\n| All same prediction | Query too different | Check data quality, try different reference |\n\n## Key References\n\n- Xu et al. (2021) \"Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models\"\n- Lotfollahi et al. (2022) \"Mapping single-cell data to reference atlases by transfer learning\"\n", "depth": 5}, {"id": "139", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/rna_velocity_velovi.md", "name": "rna_velocity_velovi.md", "type": "file", "content": "# RNA Velocity with veloVI\n\nThis reference covers RNA velocity analysis using veloVI, a deep learning approach that improves upon traditional velocity methods.\n\n## Overview\n\nRNA velocity estimates the future state of cells by modeling:\n- **Unspliced RNA**: Newly transcribed, contains introns\n- **Spliced RNA**: Mature mRNA, introns removed\n\nThe ratio of unspliced to spliced indicates whether a gene is being upregulated or downregulated.\n\n## Why veloVI?\n\nTraditional methods (velocyto, scVelo) have limitations:\n- Assume steady-state or dynamical model\n- Sensitive to noise\n- Don't handle batch effects\n\nveloVI addresses these with:\n- Probabilistic modeling\n- Better uncertainty quantification\n- Integration with scVI framework\n\n## Prerequisites\n\n```python\nimport scvi\nimport scvelo as scv\nimport scanpy as sc\nimport numpy as np\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\nprint(f\"scvelo version: {scv.__version__}\")\n```\n\n## Step 1: Generate Spliced/Unspliced Counts\n\n### From BAM Files (velocyto)\n\n```bash\n# Run velocyto on Cell Ranger output\nvelocyto run10x /path/to/cellranger_output /path/to/genes.gtf\n\n# Output: velocyto.loom file with spliced/unspliced layers\n```\n\n### From kb-python (kallisto|bustools)\n\n```bash\n# Faster alternative using kallisto\nkb count \\\n    --workflow lamanno \\\n    -i index.idx \\\n    -g t2g.txt \\\n    -c1 spliced_t2c.txt \\\n    -c2 unspliced_t2c.txt \\\n    -x 10xv3 \\\n    -o output \\\n    R1.fastq.gz R2.fastq.gz\n```\n\n## Step 2: Load Velocity Data\n\n```python\n# Load loom file from velocyto\nadata = scv.read(\"velocyto_output.loom\")\n\n# Or load from kb-python\nadata = sc.read_h5ad(\"adata.h5ad\")\n# Spliced in adata.layers[\"spliced\"]\n# Unspliced in adata.layers[\"unspliced\"]\n\n# Check layers\nprint(\"Available layers:\", list(adata.layers.keys()))\nprint(f\"Spliced shape: {adata.layers['spliced'].shape}\")\nprint(f\"Unspliced shape: {adata.layers['unspliced'].shape}\")\n```\n\n### Merge with Existing AnnData\n\n```python\n# If you have separate loom and h5ad\nldata = scv.read(\"velocyto.loom\")\nadata = sc.read_h5ad(\"processed.h5ad\")\n\n# Merge velocity data into processed adata\nadata = scv.utils.merge(adata, ldata)\n```\n\n## Step 3: Preprocessing for Velocity\n\n```python\n# Filter and normalize\nscv.pp.filter_and_normalize(\n    adata,\n    min_shared_counts=20,\n    n_top_genes=2000\n)\n\n# Compute moments (for scVelo comparison)\nscv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n```\n\n## Step 4: Run veloVI\n\n### Setup AnnData\n\n```python\n# Setup for veloVI\nscvi.model.VELOVI.setup_anndata(\n    adata,\n    spliced_layer=\"spliced\",\n    unspliced_layer=\"unspliced\"\n)\n```\n\n### Train Model\n\n```python\n# Create and train veloVI model\nvae = scvi.model.VELOVI(adata)\n\nvae.train(\n    max_epochs=500,\n    early_stopping=True,\n    batch_size=256\n)\n\n# Check training\nvae.history[\"elbo_train\"].plot()\n```\n\n### Get Velocity Estimates\n\n```python\n# Get latent time\nlatent_time = vae.get_latent_time(n_samples=25)\nadata.obs[\"veloVI_latent_time\"] = latent_time\n\n# Get velocity\nvelocities = vae.get_velocity(n_samples=25)\nadata.layers[\"veloVI_velocity\"] = velocities\n\n# Get expression states\nadata.layers[\"veloVI_expression\"] = vae.get_expression_fit(n_samples=25)\n```\n\n## Step 5: Visualize Velocity\n\n### Velocity Streamlines\n\n```python\n# Compute velocity graph\nscv.tl.velocity_graph(adata, vkey=\"veloVI_velocity\")\n\n# Plot streamlines on UMAP\nscv.pl.velocity_embedding_stream(\n    adata,\n    basis=\"umap\",\n    vkey=\"veloVI_velocity\",\n    color=\"cell_type\"\n)\n```\n\n### Velocity Arrows\n\n```python\n# Individual cell arrows\nscv.pl.velocity_embedding(\n    adata,\n    basis=\"umap\",\n    vkey=\"veloVI_velocity\",\n    arrow_length=3,\n    arrow_size=2,\n    color=\"cell_type\"\n)\n```\n\n### Latent Time\n\n```python\n# Plot latent time (pseudotime from velocity)\nsc.pl.umap(adata, color=\"veloVI_latent_time\", cmap=\"viridis\")\n```\n\n## Step 6: Compare with scVelo\n\n```python\n# Run standard scVelo for comparison\nscv.tl.velocity(adata, mode=\"dynamical\")\nscv.tl.velocity_graph(adata)\n\n# Compare velocity fields\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nscv.pl.velocity_embedding_stream(\n    adata, basis=\"umap\", ax=axes[0], \n    title=\"scVelo\", show=False\n)\n\nscv.pl.velocity_embedding_stream(\n    adata, basis=\"umap\", vkey=\"veloVI_velocity\",\n    ax=axes[1], title=\"veloVI\", show=False\n)\n\nplt.tight_layout()\n```\n\n## Step 7: Gene-Level Analysis\n\n### Velocity Phase Portraits\n\n```python\n# Plot phase portrait for specific genes\ngenes = [\"SOX2\", \"PAX6\", \"DCX\", \"NEUROD1\"]\n\nscv.pl.velocity(\n    adata,\n    var_names=genes,\n    vkey=\"veloVI_velocity\",\n    colorbar=True\n)\n```\n\n### Gene Dynamics\n\n```python\n# Plot expression over latent time\nfor gene in genes:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    \n    sc.pl.scatter(\n        adata,\n        x=\"veloVI_latent_time\",\n        y=gene,\n        color=\"cell_type\",\n        ax=ax,\n        show=False\n    )\n    ax.set_xlabel(\"Latent Time\")\n    ax.set_ylabel(f\"{gene} Expression\")\n```\n\n### Driver Genes\n\n```python\n# Find genes driving velocity\nscv.tl.rank_velocity_genes(\n    adata,\n    vkey=\"veloVI_velocity\",\n    groupby=\"cell_type\"\n)\n\n# Get top genes per cluster\ndf = scv.get_df(adata, \"rank_velocity_genes/names\")\nprint(df.head(10))\n```\n\n## Step 8: Uncertainty Quantification\n\nveloVI provides uncertainty estimates:\n\n```python\n# Get velocity with uncertainty\nvelocity_mean, velocity_std = vae.get_velocity(\n    n_samples=100,\n    return_mean=True,\n    return_numpy=True\n)\n\n# Store uncertainty\nadata.layers[\"velocity_uncertainty\"] = velocity_std\n\n# Visualize uncertainty\nadata.obs[\"mean_velocity_uncertainty\"] = velocity_std.mean(axis=1)\nsc.pl.umap(adata, color=\"mean_velocity_uncertainty\")\n```\n\n## Complete Pipeline\n\n```python\ndef run_velocity_analysis(\n    adata,\n    spliced_layer=\"spliced\",\n    unspliced_layer=\"unspliced\",\n    n_top_genes=2000,\n    max_epochs=500\n):\n    \"\"\"\n    Complete RNA velocity analysis with veloVI.\n    \n    Parameters\n    ----------\n    adata : AnnData\n        Data with spliced/unspliced layers\n    spliced_layer : str\n        Layer name for spliced counts\n    unspliced_layer : str\n        Layer name for unspliced counts\n    n_top_genes : int\n        Number of velocity genes\n    max_epochs : int\n        Training epochs\n        \n    Returns\n    -------\n    AnnData with velocity and model\n    \"\"\"\n    import scvi\n    import scvelo as scv\n    import scanpy as sc\n    \n    adata = adata.copy()\n    \n    # Preprocessing\n    scv.pp.filter_and_normalize(\n        adata,\n        min_shared_counts=20,\n        n_top_genes=n_top_genes\n    )\n    \n    # Compute moments (needed for some visualizations)\n    scv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n    \n    # Setup veloVI\n    scvi.model.VELOVI.setup_anndata(\n        adata,\n        spliced_layer=spliced_layer,\n        unspliced_layer=unspliced_layer\n    )\n    \n    # Train\n    model = scvi.model.VELOVI(adata)\n    model.train(max_epochs=max_epochs, early_stopping=True)\n    \n    # Get results\n    adata.obs[\"latent_time\"] = model.get_latent_time(n_samples=25)\n    adata.layers[\"velocity\"] = model.get_velocity(n_samples=25)\n    \n    # Compute velocity graph for visualization\n    scv.tl.velocity_graph(adata, vkey=\"velocity\")\n    \n    # Compute UMAP if not present\n    if \"X_umap\" not in adata.obsm:\n        sc.pp.neighbors(adata)\n        sc.tl.umap(adata)\n    \n    return adata, model\n\n# Usage\nadata_velocity, model = run_velocity_analysis(adata)\n\n# Visualize\nscv.pl.velocity_embedding_stream(\n    adata_velocity,\n    basis=\"umap\",\n    vkey=\"velocity\",\n    color=\"cell_type\"\n)\n\nsc.pl.umap(adata_velocity, color=\"latent_time\")\n```\n\n## Advanced: Batch-Aware Velocity\n\n```python\n# For multi-batch data, include batch in model\nscvi.model.VELOVI.setup_anndata(\n    adata,\n    spliced_layer=\"spliced\",\n    unspliced_layer=\"unspliced\",\n    batch_key=\"batch\"\n)\n\nmodel = scvi.model.VELOVI(adata)\nmodel.train()\n```\n\n## Interpreting Results\n\n### Good Velocity Signal\n\n- Streamlines follow expected differentiation\n- Latent time correlates with known biology\n- Phase portraits show clear dynamics\n\n### Poor Velocity Signal\n\n- Random/chaotic streamlines\n- No correlation with known markers\n- May indicate:\n  - Insufficient unspliced reads\n  - Cells at steady state\n  - Technical issues\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| No velocity signal | Low unspliced counts | Check sequencing depth, use kb-python |\n| Reversed direction | Wrong root assignment | Manually set root cells |\n| Noisy streamlines | Too many genes | Reduce n_top_genes |\n| Memory error | Large dataset | Reduce batch_size |\n\n## Key References\n\n- Gayoso et al. (2023) \"Deep generative modeling of transcriptional dynamics for RNA velocity analysis in single cells\"\n- La Manno et al. (2018) \"RNA velocity of single cells\"\n- Bergen et al. (2020) \"Generalizing RNA velocity to transient cell states through dynamical modeling\"\n", "depth": 5}, {"id": "140", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/scarches_mapping.md", "name": "scarches_mapping.md", "type": "file", "content": "# Reference Mapping with scArches\n\nThis reference covers using scArches for mapping query data to pre-trained reference models without retraining from scratch.\n\n## Overview\n\nscArches (single-cell architecture surgery) enables:\n- Mapping new data to existing reference atlases\n- Extending models with new batches/studies\n- Transfer learning without full retraining\n- Preserving reference structure while integrating query\n\n## When to Use scArches\n\n| Scenario | Approach |\n|----------|----------|\n| Map query to existing atlas | scArches query mapping |\n| Extend atlas with new data | scArches model surgery |\n| No pre-trained model available | Train scANVI from scratch |\n| Query very different from reference | Consider retraining |\n\n## Prerequisites\n\n```python\nimport scvi\nimport scanpy as sc\nimport numpy as np\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\n```\n\n## Workflow 1: Map Query to Pre-Trained Reference\n\n### Step 1: Load Pre-Trained Reference Model\n\n```python\n# Load saved reference model\n# The model must have been trained with scvi-tools\nreference_model = scvi.model.SCVI.load(\"reference_model/\")\n\n# Or load scANVI for label transfer\nreference_model = scvi.model.SCANVI.load(\"reference_scanvi_model/\")\n\n# Check model info\nprint(f\"Model type: {type(reference_model)}\")\nprint(f\"Training data shape: {reference_model.adata.shape}\")\n```\n\n### Step 2: Prepare Query Data\n\n```python\n# Load query data\nadata_query = sc.read_h5ad(\"query_data.h5ad\")\n\n# CRITICAL: Match genes to reference\nreference_genes = reference_model.adata.var_names\nquery_genes = adata_query.var_names\n\n# Check overlap\ncommon_genes = reference_genes.intersection(query_genes)\nprint(f\"Reference genes: {len(reference_genes)}\")\nprint(f\"Query genes: {len(query_genes)}\")\nprint(f\"Overlap: {len(common_genes)}\")\n\n# Subset query to reference genes\nadata_query = adata_query[:, reference_genes].copy()\n\n# Handle missing genes (filled with zeros automatically by prepare_query_anndata)\n```\n\n### Step 3: Prepare Query AnnData\n\n```python\n# Store raw counts\nadata_query.layers[\"counts\"] = adata_query.X.copy()\n\n# Prepare query for mapping\n# This aligns the query data structure to match the reference\nscvi.model.SCVI.prepare_query_anndata(adata_query, reference_model)\n```\n\n### Step 4: Create Query Model\n\n```python\n# Create query model from reference\n# This initializes with reference weights\nquery_model = scvi.model.SCVI.load_query_data(\n    adata_query,\n    reference_model\n)\n\n# The query model inherits:\n# - Reference architecture\n# - Reference encoder weights (frozen by default)\n# - Decoder is fine-tuned for query\n```\n\n### Step 5: Fine-Tune on Query\n\n```python\n# Fine-tune the query model\n# This adjusts decoder weights for query-specific effects\nquery_model.train(\n    max_epochs=200,\n    plan_kwargs={\n        \"weight_decay\": 0.0  # Less regularization for fine-tuning\n    }\n)\n\n# Check training\nquery_model.history['elbo_train'].plot()\n```\n\n### Step 6: Get Query Representation\n\n```python\n# Get latent representation\n# Query cells are embedded in same space as reference\nadata_query.obsm[\"X_scVI\"] = query_model.get_latent_representation()\n\n# Visualize\nsc.pp.neighbors(adata_query, use_rep=\"X_scVI\")\nsc.tl.umap(adata_query)\nsc.pl.umap(adata_query, color=['cell_type', 'batch'])\n```\n\n## Workflow 2: scANVI Query Mapping with Label Transfer\n\nFor transferring cell type labels from reference to query:\n\n### Step 1: Load scANVI Reference\n\n```python\n# Reference must be scANVI model (trained with labels)\nreference_scanvi = scvi.model.SCANVI.load(\"scanvi_reference/\")\n\n# Check available labels\nprint(\"Reference cell types:\")\nprint(reference_scanvi.adata.obs['cell_type'].value_counts())\n```\n\n### Step 2: Prepare and Map Query\n\n```python\n# Prepare query\nadata_query.layers[\"counts\"] = adata_query.X.copy()\nadata_query = adata_query[:, reference_scanvi.adata.var_names].copy()\n\nscvi.model.SCANVI.prepare_query_anndata(adata_query, reference_scanvi)\n\n# Create query model\nquery_scanvi = scvi.model.SCANVI.load_query_data(\n    adata_query,\n    reference_scanvi\n)\n\n# Fine-tune\nquery_scanvi.train(\n    max_epochs=100,\n    plan_kwargs={\"weight_decay\": 0.0}\n)\n```\n\n### Step 3: Get Predictions\n\n```python\n# Predict cell types\npredictions = query_scanvi.predict()\nadata_query.obs[\"predicted_cell_type\"] = predictions\n\n# Get prediction probabilities\nsoft_predictions = query_scanvi.predict(soft=True)\nadata_query.obs[\"prediction_confidence\"] = soft_predictions.max(axis=1)\n\n# Latent representation\nadata_query.obsm[\"X_scANVI\"] = query_scanvi.get_latent_representation()\n\n# Visualize predictions\nsc.pp.neighbors(adata_query, use_rep=\"X_scANVI\")\nsc.tl.umap(adata_query)\nsc.pl.umap(adata_query, color=['predicted_cell_type', 'prediction_confidence'])\n```\n\n### Step 4: Evaluate Predictions\n\n```python\n# Distribution of predictions\nprint(adata_query.obs['predicted_cell_type'].value_counts())\n\n# Confidence statistics\nprint(f\"Mean confidence: {adata_query.obs['prediction_confidence'].mean():.3f}\")\nprint(f\"Low confidence (\u003c0.5): {(adata_query.obs['prediction_confidence'] \u003c 0.5).sum()}\")\n\n# Filter low-confidence predictions\nhigh_conf = adata_query[adata_query.obs['prediction_confidence'] \u003e= 0.7].copy()\nprint(f\"High confidence cells: {len(high_conf)} ({len(high_conf)/len(adata_query)*100:.1f}%)\")\n```\n\n## Workflow 3: Model Surgery (Extending Reference)\n\nExtend an existing reference model with new data:\n\n### Step 1: Freeze Reference Layers\n\n```python\n# Load reference model\nreference_model = scvi.model.SCVI.load(\"reference_model/\")\n\n# Get reference representation (before surgery)\nadata_ref = reference_model.adata\nadata_ref.obsm[\"X_scVI_before\"] = reference_model.get_latent_representation()\n```\n\n### Step 2: Prepare Combined Data\n\n```python\n# Add batch information\nadata_ref.obs[\"dataset\"] = \"reference\"\nadata_query.obs[\"dataset\"] = \"query\"\n\n# Combine\nadata_combined = sc.concat([adata_ref, adata_query])\nadata_combined.layers[\"counts\"] = adata_combined.X.copy()\n```\n\n### Step 3: Surgery Approach\n\n```python\n# Option A: Use load_query_data (recommended)\nscvi.model.SCVI.prepare_query_anndata(adata_query, reference_model)\nextended_model = scvi.model.SCVI.load_query_data(adata_query, reference_model)\nextended_model.train(max_epochs=200)\n\n# Option B: Retrain with combined data (if query is large)\n# This doesn't preserve reference exactly but may give better results\nscvi.model.SCVI.setup_anndata(\n    adata_combined,\n    layer=\"counts\",\n    batch_key=\"dataset\"\n)\nnew_model = scvi.model.SCVI(adata_combined, n_latent=30)\nnew_model.train(max_epochs=200)\n```\n\n## Joint Visualization\n\nVisualize reference and query together:\n\n```python\n# Get latent representations\nadata_ref.obsm[\"X_scVI\"] = reference_model.get_latent_representation()\nadata_query.obsm[\"X_scVI\"] = query_model.get_latent_representation()\n\n# Combine for visualization\nadata_ref.obs[\"source\"] = \"reference\"\nadata_query.obs[\"source\"] = \"query\"\nadata_combined = sc.concat([adata_ref, adata_query])\n\n# Compute joint UMAP\nsc.pp.neighbors(adata_combined, use_rep=\"X_scVI\")\nsc.tl.umap(adata_combined)\n\n# Visualize\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nsc.pl.umap(adata_combined, color=\"source\", ax=axes[0], show=False, title=\"Source\")\nsc.pl.umap(adata_combined, color=\"cell_type\", ax=axes[1], show=False, title=\"Cell Type\")\nsc.pl.umap(adata_combined, color=\"batch\", ax=axes[2], show=False, title=\"Batch\")\n\nplt.tight_layout()\n```\n\n## Using Public Atlas Models\n\n### From HuggingFace Model Hub\n\n```python\nfrom huggingface_hub import hf_hub_download\n\n# Download model files\nmodel_dir = hf_hub_download(\n    repo_id=\"scvi-tools/model-name\",  # Replace with actual repo\n    filename=\"model.pt\",\n    local_dir=\"./downloaded_model/\"\n)\n\n# Load model\natlas_model = scvi.model.SCANVI.load(model_dir)\n```\n\n### From CellxGene\n\n```python\n# Many CellxGene datasets provide pre-trained models\n# Check dataset documentation for model availability\n# https://cellxgene.cziscience.com/\n\n# Example workflow:\n# 1. Download reference dataset and model\n# 2. Load model: model = scvi.model.SCANVI.load(\"cellxgene_model/\")\n# 3. Map your query data using steps above\n```\n\n## Complete Pipeline\n\n```python\ndef map_query_to_reference(\n    adata_query,\n    reference_model_path,\n    model_type=\"scanvi\",\n    max_epochs=100,\n    confidence_threshold=0.5\n):\n    \"\"\"\n    Map query data to pre-trained reference model.\n\n    Parameters\n    ----------\n    adata_query : AnnData\n        Query data with raw counts\n    reference_model_path : str\n        Path to saved reference model\n    model_type : str\n        \"scvi\" or \"scanvi\"\n    max_epochs : int\n        Fine-tuning epochs\n    confidence_threshold : float\n        Minimum prediction confidence (for scANVI)\n\n    Returns\n    -------\n    Mapped AnnData with predictions (if scANVI)\n    \"\"\"\n    import scvi\n\n    # Load reference\n    if model_type == \"scanvi\":\n        reference_model = scvi.model.SCANVI.load(reference_model_path)\n        ModelClass = scvi.model.SCANVI\n    else:\n        reference_model = scvi.model.SCVI.load(reference_model_path)\n        ModelClass = scvi.model.SCVI\n\n    # Prepare query\n    adata_query = adata_query.copy()\n    adata_query = adata_query[:, reference_model.adata.var_names].copy()\n    adata_query.layers[\"counts\"] = adata_query.X.copy()\n\n    # Map query\n    ModelClass.prepare_query_anndata(adata_query, reference_model)\n    query_model = ModelClass.load_query_data(adata_query, reference_model)\n\n    # Fine-tune\n    query_model.train(\n        max_epochs=max_epochs,\n        plan_kwargs={\"weight_decay\": 0.0}\n    )\n\n    # Get results\n    rep_key = \"X_scANVI\" if model_type == \"scanvi\" else \"X_scVI\"\n    adata_query.obsm[rep_key] = query_model.get_latent_representation()\n\n    if model_type == \"scanvi\":\n        adata_query.obs[\"predicted_cell_type\"] = query_model.predict()\n        soft = query_model.predict(soft=True)\n        adata_query.obs[\"prediction_confidence\"] = soft.max(axis=1)\n        adata_query.obs[\"confident\"] = adata_query.obs[\"prediction_confidence\"] \u003e= confidence_threshold\n\n    # Compute UMAP\n    sc.pp.neighbors(adata_query, use_rep=rep_key)\n    sc.tl.umap(adata_query)\n\n    return adata_query, query_model\n\n\n# Usage\nadata_mapped, model = map_query_to_reference(\n    adata_query,\n    \"reference_scanvi_model/\",\n    model_type=\"scanvi\"\n)\n\n# Visualize\nsc.pl.umap(adata_mapped, color=['predicted_cell_type', 'prediction_confidence'])\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Gene mismatch | Different gene naming | Convert gene IDs (Ensembl ↔ Symbol) |\n| Many low-confidence | Query has novel types | Manually annotate low-confidence cells |\n| Poor mapping | Query too different | Consider retraining with combined data |\n| Memory error | Large query | Process in batches |\n| Version mismatch | Different scvi-tools version | Use same version as reference training |\n\n## Key References\n\n- Lotfollahi et al. (2022) \"Mapping single-cell data to reference atlases by transfer learning\"\n- Xu et al. (2021) \"Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models\"\n", "depth": 5}, {"id": "141", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/troubleshooting.md", "name": "troubleshooting.md", "type": "file", "content": "# Troubleshooting Guide for scvi-tools\n\nThis reference provides a consolidated guide for diagnosing and resolving common issues across all scvi-tools models.\n\n## Quick Diagnosis\n\n| Symptom | Likely Cause | Quick Fix |\n|---------|--------------|-----------|\n| \"X should contain integers\" | Normalized data in X | Use `layer=\"counts\"` in setup |\n| CUDA out of memory | GPU memory exhausted | Reduce `batch_size`, use smaller model |\n| Training loss is NaN | Bad data or learning rate | Check for all-zero cells/genes |\n| Batches not mixing | Too few shared features | Increase HVGs, check gene overlap |\n| Over-correction | Too aggressive integration | Use scANVI with labels |\n| Import error | Missing dependencies | `pip install scvi-tools[all]` |\n\n## Data Format Issues\n\n### Issue: CITE-seq protein data from Seurat is CLR-normalized\n\n**Cause**: Seurat's `NormalizeData(normalization.method = \"CLR\")` transforms raw ADT counts. totalVI requires raw integer counts for protein data.\n\n**Symptoms**:\n- Protein values are not integers\n- Protein values contain negative numbers\n- Model training produces poor results\n\n**Solution**:\n```python\n# Check if protein data is normalized\nprotein = adata.obsm[\"protein_expression\"]\nprint(f\"Min value: {protein.min()}\")  # Should be 0 if raw counts\nprint(f\"Contains integers: {np.allclose(protein, protein.astype(int))}\")\n\n# If importing from Seurat, use the raw counts assay, not the normalized one\n# In R/Seurat, export the RNA assay's counts slot, not the data slot\n# GetAssayData(seurat_obj, assay = \"ADT\", slot = \"counts\")\n```\n\n### Issue: \"layer not found\" or \"X should contain integers\"\n\n**Cause**: scvi-tools requires raw integer counts, not normalized data.\n\n**Solution**:\n```python\n# Check if X contains integers\nimport numpy as np\nprint(f\"X max: {adata.X.max()}\")\nprint(f\"Contains integers: {np.allclose(adata.X.data, adata.X.data.astype(int))}\")\n\n# If normalized, recover from raw\nif hasattr(adata, 'raw') and adata.raw is not None:\n    adata = adata.raw.to_adata()\n\n# Or use existing counts layer\nadata.layers[\"counts\"] = adata.X.copy()\nscvi.model.SCVI.setup_anndata(adata, layer=\"counts\")\n```\n\n### Issue: Sparse matrix errors\n\n**Cause**: Incompatible sparse format or dense array expected.\n\n**Solution**:\n```python\nfrom scipy.sparse import csr_matrix\n\n# Convert to CSR format (most compatible)\nif hasattr(adata.X, 'toarray'):\n    adata.X = csr_matrix(adata.X)\n\n# Or convert to dense if small enough\nif adata.n_obs * adata.n_vars \u003c 1e8:\n    adata.X = adata.X.toarray()\n```\n\n### Issue: NaN or Inf values in data\n\n**Cause**: Missing values or corrupted data.\n\n**Solution**:\n```python\nimport numpy as np\n\n# Check for issues\nX = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\nprint(f\"NaN count: {np.isnan(X).sum()}\")\nprint(f\"Inf count: {np.isinf(X).sum()}\")\nprint(f\"Negative count: {(X \u003c 0).sum()}\")\n\n# Replace NaN/Inf with 0\nX = np.nan_to_num(X, nan=0, posinf=0, neginf=0)\nX = np.clip(X, 0, None)  # Ensure non-negative\nadata.X = csr_matrix(X)\n```\n\n### Issue: batch_key or labels_key not found\n\n**Cause**: Column name mismatch in adata.obs.\n\n**Solution**:\n```python\n# List available columns\nprint(adata.obs.columns.tolist())\n\n# Check for similar names\nfor col in adata.obs.columns:\n    if 'batch' in col.lower() or 'sample' in col.lower():\n        print(f\"Potential batch column: {col}\")\n```\n\n## GPU and Memory Issues\n\n### Issue: CUDA out of memory\n\n**Cause**: Model or batch doesn't fit in GPU memory.\n\n**Solutions** (try in order):\n\n```python\n# 1. Reduce batch size\nmodel.train(batch_size=64)  # Default is 128\n\n# 2. Use smaller model architecture\nmodel = scvi.model.SCVI(\n    adata,\n    n_latent=10,   # Default is 10-30\n    n_layers=1     # Default is 1-2\n)\n\n# 3. Subset to fewer genes\nsc.pp.highly_variable_genes(adata, n_top_genes=1500)\nadata = adata[:, adata.var['highly_variable']].copy()\n\n# 4. Clear GPU cache between models\nimport torch\ntorch.cuda.empty_cache()\n\n# 5. Use CPU if GPU is too small\nmodel.train(accelerator=\"cpu\")\n```\n\n### Issue: No GPU detected\n\n**Cause**: CUDA not installed or version mismatch.\n\n**Diagnosis**:\n```python\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\n```\n\n**Solution**:\n```bash\n# Check system CUDA\nnvidia-smi\nnvcc --version\n\n# Reinstall PyTorch with matching CUDA\npip install torch --index-url https://download.pytorch.org/whl/cu118  # For CUDA 11.8\n# Or\npip install torch --index-url https://download.pytorch.org/whl/cu121  # For CUDA 12.1\n```\n\n### Issue: Memory error with large datasets\n\n**Cause**: Dataset too large for system RAM.\n\n**Solutions**:\n```python\n# 1. Process in chunks (for very large data)\n# Subsample for initial exploration\nadata_sample = adata[np.random.choice(adata.n_obs, 50000, replace=False)].copy()\n\n# 2. Use backed mode for AnnData\nadata = sc.read_h5ad(\"large_data.h5ad\", backed='r')\n\n# 3. Reduce gene count aggressively\nadata = adata[:, adata.var['highly_variable']].copy()\n```\n\n## Training Issues\n\n### Issue: Training loss is NaN\n\n**Cause**: Numerical instability, bad data, or learning rate issues.\n\n**Solutions**:\n```python\n# 1. Check for problematic cells/genes\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_genes(adata, min_cells=3)\n\n# 2. Remove cells with zero counts\nadata = adata[adata.X.sum(axis=1) \u003e 0].copy()\n\n# 3. Use gradient clipping (built into scvi-tools)\nmodel.train(max_epochs=200, early_stopping=True)\n```\n\n### Issue: Training doesn't converge\n\n**Cause**: Insufficient epochs, poor hyperparameters, or data issues.\n\n**Solutions**:\n```python\n# 1. Train longer\nmodel.train(max_epochs=400)\n\n# 2. Check training curves\nimport matplotlib.pyplot as plt\nplt.plot(model.history['elbo_train'])\nplt.plot(model.history['elbo_validation'])\nplt.xlabel('Epoch')\nplt.ylabel('ELBO')\nplt.legend(['Train', 'Validation'])\n\n# 3. Adjust model size for data size\n# Small data (\u003c10k cells): smaller model\nmodel = scvi.model.SCVI(adata, n_latent=10, n_layers=1, dropout_rate=0.2)\n\n# Large data (\u003e100k cells): can use larger model\nmodel = scvi.model.SCVI(adata, n_latent=30, n_layers=2)\n```\n\n### Issue: Overfitting (validation loss increases)\n\n**Cause**: Model too complex or trained too long.\n\n**Solutions**:\n```python\n# 1. Enable early stopping\nmodel.train(early_stopping=True, early_stopping_patience=10)\n\n# 2. Add regularization\nmodel = scvi.model.SCVI(adata, dropout_rate=0.2)\n\n# 3. Reduce model complexity\nmodel = scvi.model.SCVI(adata, n_layers=1)\n```\n\n## Integration Issues\n\n### Issue: Batches don't mix\n\n**Cause**: Too few shared features, strong biological differences, or technical issues.\n\n**Solutions**:\n```python\n# 1. Check gene overlap between batches\nfor batch in adata.obs['batch'].unique():\n    batch_genes = adata[adata.obs['batch'] == batch].var_names\n    print(f\"{batch}: {len(batch_genes)} genes\")\n\n# 2. Use more HVGs\nsc.pp.highly_variable_genes(adata, n_top_genes=4000, batch_key=\"batch\")\n\n# 3. Train longer\nmodel.train(max_epochs=400)\n\n# 4. Increase latent dimensions\nmodel = scvi.model.SCVI(adata, n_latent=50)\n```\n\n### Issue: Over-correction (biological signal lost)\n\n**Cause**: Model removes too much variation.\n\n**Solutions**:\n```python\n# 1. Use scANVI with cell type labels\nscvi.model.SCANVI.from_scvi_model(scvi_model, labels_key=\"cell_type\")\n\n# 2. Reduce model capacity\nmodel = scvi.model.SCVI(adata, n_latent=10)\n\n# 3. Use categorical covariates instead of batch_key\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    categorical_covariate_keys=[\"batch\"]  # Less aggressive than batch_key\n)\n```\n\n### Issue: One batch dominates clusters\n\n**Cause**: Unbalanced batch sizes or incomplete integration.\n\n**Solutions**:\n```python\n# 1. Check batch distribution\nprint(adata.obs['batch'].value_counts())\n\n# 2. Subsample to balance\nfrom sklearn.utils import resample\nbalanced = []\nmin_size = adata.obs['batch'].value_counts().min()\nfor batch in adata.obs['batch'].unique():\n    batch_data = adata[adata.obs['batch'] == batch]\n    balanced.append(batch_data[np.random.choice(len(batch_data), min_size, replace=False)])\nadata_balanced = sc.concat(balanced)\n```\n\n## Model-Specific Issues\n\n### scANVI: Poor label transfer\n\n**Solutions**:\n```python\n# 1. Check label distribution\nprint(adata.obs['cell_type'].value_counts())\n\n# 2. Use Unknown for low-confidence cells\nadata.obs.loc[adata.obs['prediction_score'] \u003c 0.5, 'cell_type'] = 'Unknown'\n\n# 3. Train scVI longer before scANVI\nscvi_model.train(max_epochs=300)\nscanvi_model = scvi.model.SCANVI.from_scvi_model(scvi_model, labels_key=\"cell_type\")\nscanvi_model.train(max_epochs=100)\n```\n\n### totalVI: Noisy protein signal\n\n**Solutions**:\n```python\n# 1. Use denoised protein values\n_, protein_denoised = model.get_normalized_expression(return_mean=True)\n\n# 2. Check isotype controls\n# Isotype controls should have low expression\nfor i, name in enumerate(adata.uns[\"protein_names\"]):\n    if 'isotype' in name.lower():\n        print(f\"{name}: mean={adata.obsm['protein_expression'][:, i].mean():.1f}\")\n```\n\n### PeakVI: Poor clustering\n\n**Solutions**:\n```python\n# 1. Use more variable peaks\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=0.05)\nadata = adata[:, selector.fit(adata.X).get_support()].copy()\n\n# 2. Binarize data\nadata.X = (adata.X \u003e 0).astype(np.float32)\n```\n\n### MultiVI: Different cell counts between modalities\n\n**Solutions**:\n```python\n# Ensure same cells in same order\ncommon_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)\nadata_rna = adata_rna[common_cells].copy()\nadata_atac = adata_atac[common_cells].copy()\n```\n\n### DestVI: Poor deconvolution\n\n**Solutions**:\n```python\n# 1. Check gene overlap\ncommon_genes = adata_ref.var_names.intersection(adata_spatial.var_names)\nprint(f\"Common genes: {len(common_genes)}\")  # Should be \u003e1000\n\n# 2. Use tissue-matched reference\n# Reference should contain all cell types expected in spatial data\n\n# 3. Check reference quality\nprint(adata_ref.obs['cell_type'].value_counts())\n```\n\n## Version Compatibility\n\n### scvi-tools 1.x vs 0.x API changes\n\nKey differences:\n```python\n# 0.x API\nscvi.data.setup_anndata(adata, ...)\n\n# 1.x API (current)\nscvi.model.SCVI.setup_anndata(adata, ...)\n```\n\n### Check versions\n```python\nimport scvi\nimport scanpy as sc\nimport anndata\nimport torch\n\nprint(f\"scvi-tools: {scvi.__version__}\")\nprint(f\"scanpy: {sc.__version__}\")\nprint(f\"anndata: {anndata.__version__}\")\nprint(f\"torch: {torch.__version__}\")\n```\n\n### Recommended versions (as of late 2024)\n```\nscvi-tools\u003e=1.0.0\nscanpy\u003e=1.9.0\nanndata\u003e=0.9.0\ntorch\u003e=2.0.0\n```\n\n## Getting Help\n\n1. **Check documentation**: https://docs.scvi-tools.org/\n2. **GitHub issues**: https://github.com/scverse/scvi-tools/issues\n3. **Discourse forum**: https://discourse.scverse.org/\n4. **Tutorials**: https://docs.scvi-tools.org/en/stable/tutorials/index.html\n\nWhen reporting issues, include:\n- scvi-tools version (`scvi.__version__`)\n- Python version\n- Full error traceback\n- Minimal reproducible example\n", "depth": 5}, {"id": "142", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/atac_peakvi.md", "name": "atac_peakvi.md", "type": "file", "content": "# scATAC-seq Analysis with PeakVI\n\nThis reference covers single-cell ATAC-seq analysis using PeakVI for dimensionality reduction, batch correction, and differential accessibility.\n\n## Overview\n\nPeakVI is a deep generative model for scATAC-seq data that:\n- Models binary accessibility (peak open/closed)\n- Handles batch effects\n- Provides latent representation for clustering\n- Enables differential accessibility analysis\n\n## Prerequisites\n\n```python\nimport scvi\nimport scanpy as sc\nimport numpy as np\nimport anndata as ad\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\n```\n\n## Step 1: Load and Prepare ATAC Data\n\n### From 10x Genomics (Cell Ranger ATAC)\n\n```python\n# Peak-cell matrix from fragments\n# Usually in filtered_peak_bc_matrix format\n\nadata = sc.read_10x_h5(\"filtered_peak_bc_matrix.h5\")\n\n# Or from mtx format\nadata = sc.read_10x_mtx(\"filtered_peak_bc_matrix/\")\n\n# Check structure\nprint(f\"Cells: {adata.n_obs}, Peaks: {adata.n_vars}\")\nprint(f\"Sparsity: {1 - adata.X.nnz / (adata.n_obs * adata.n_vars):.2%}\")\n```\n\n### From ArchR/Signac\n\n```python\n# Export from ArchR (in R)\n# saveArchRProject(proj, outputDirectory=\"atac_export\", load=FALSE)\n# Then read the exported files in Python\n\n# From Signac:\n# Export peak matrix and metadata\n```\n\n## Step 2: Quality Control\n\n```python\n# Calculate QC metrics\nsc.pp.calculate_qc_metrics(adata, inplace=True)\n\n# Key metrics for ATAC:\n# - n_genes_by_counts: peaks per cell (should rename)\n# - total_counts: fragments per cell\nadata.obs['n_peaks'] = adata.obs['n_genes_by_counts']\nadata.obs['total_fragments'] = adata.obs['total_counts']\n\n# Filter cells\nadata = adata[adata.obs['n_peaks'] \u003e 500].copy()\nadata = adata[adata.obs['n_peaks'] \u003c 50000].copy()  # Remove potential doublets\n\n# Filter peaks (accessible in at least n cells)\nsc.pp.filter_genes(adata, min_cells=10)\n\nprint(f\"After QC: {adata.shape}\")\n```\n\n### Binarize Data\n\n```python\n# PeakVI works with binary accessibility\n# Binarize if not already binary\nadata.X = (adata.X \u003e 0).astype(np.float32)\n\n# Verify\nprint(f\"Unique values: {np.unique(adata.X.data)}\")\n```\n\n## Step 3: Feature Selection\n\nUnlike RNA-seq, peak selection for ATAC is less established. Options:\n\n### Option A: Most Accessible Peaks\n\n```python\n# Select top peaks by accessibility frequency\npeak_accessibility = np.array(adata.X.sum(axis=0)).flatten()\ntop_peaks = np.argsort(peak_accessibility)[-50000:]  # Top 50k peaks\n\nadata = adata[:, top_peaks].copy()\n```\n\n### Option B: Variable Peaks\n\n```python\n# Select peaks with high variance\n# (Most informative for clustering)\nfrom sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold(threshold=0.05)\nselector.fit(adata.X)\nadata = adata[:, selector.get_support()].copy()\n```\n\n### Option C: Peaks Near Genes\n\n```python\n# Keep peaks within promoter regions or gene bodies\n# Requires peak annotation\n# gene_peaks = peaks with gene annotation\n# adata = adata[:, adata.var['near_gene']].copy()\n```\n\n## Step 4: Add Batch Information\n\n```python\n# Add batch annotation if multiple samples\nadata.obs['batch'] = adata.obs['sample_id']  # Or appropriate column\n\nprint(adata.obs['batch'].value_counts())\n```\n\n## Step 5: Setup and Train PeakVI\n\n```python\n# Setup AnnData\nscvi.model.PEAKVI.setup_anndata(\n    adata,\n    batch_key=\"batch\"  # Optional, omit for single batch\n)\n\n# Create model\nmodel = scvi.model.PEAKVI(\n    adata,\n    n_latent=20,      # Latent dimensions\n    n_layers_encoder=2,\n    n_layers_decoder=2\n)\n\n# Train\nmodel.train(\n    max_epochs=200,\n    early_stopping=True,\n    batch_size=128\n)\n\n# Check training\nmodel.history['elbo_train'].plot()\n```\n\n## Step 6: Get Latent Representation\n\n```python\n# Latent space for downstream analysis\nadata.obsm[\"X_PeakVI\"] = model.get_latent_representation()\n\n# Clustering and visualization\nsc.pp.neighbors(adata, use_rep=\"X_PeakVI\", n_neighbors=15)\nsc.tl.umap(adata)\nsc.tl.leiden(adata, resolution=0.5)\n\n# Visualize\nsc.pl.umap(adata, color=['leiden', 'batch'], ncols=2)\n```\n\n## Step 7: Differential Accessibility\n\n```python\n# Differential accessibility between clusters\nda_results = model.differential_accessibility(\n    groupby='leiden',\n    group1='0',\n    group2='1'\n)\n\n# Filter significant peaks\nda_sig = da_results[\n    (da_results['is_da_fdr_0.05']) \u0026\n    (abs(da_results['lfc_mean']) \u003e 1)\n]\n\nprint(f\"Significant DA peaks: {len(da_sig)}\")\nprint(da_sig.head())\n```\n\n### DA Between Conditions\n\n```python\n# Compare conditions within cell type\nadata_subset = adata[adata.obs['cell_type'] == 'CD4 T cells'].copy()\n\nda_condition = model.differential_accessibility(\n    groupby='condition',\n    group1='treated',\n    group2='control'\n)\n```\n\n## Step 8: Peak Annotation\n\n```python\n# Annotate peaks with nearest genes\n# Using pybedtools or similar\n\n# Example peak name format: chr1:1000-2000\n# Parse into bed format for annotation\n\nimport pandas as pd\n\ndef parse_peak_names(peak_names):\n    \"\"\"Parse peak names into bed format.\"\"\"\n    records = []\n    for peak in peak_names:\n        chrom, coords = peak.split(':')\n        start, end = coords.split('-')\n        records.append({\n            'chrom': chrom,\n            'start': int(start),\n            'end': int(end),\n            'peak': peak\n        })\n    return pd.DataFrame(records)\n\npeak_bed = parse_peak_names(adata.var_names)\n```\n\n## Step 9: Motif Analysis\n\n```python\n# Export significant peaks for motif analysis\n# Use HOMER, MEME, or chromVAR\n\n# Export peak sequences\nsig_peaks = da_sig.index.tolist()\npeak_bed_sig = peak_bed[peak_bed['peak'].isin(sig_peaks)]\npeak_bed_sig.to_csv(\"significant_peaks.bed\", sep='\\t', index=False, header=False)\n\n# Then run HOMER:\n# findMotifsGenome.pl significant_peaks.bed hg38 motif_output/ -size 200\n```\n\n## Step 10: Gene Activity Scores\n\n```python\n# Compute gene activity from peak accessibility\n# (Requires peak-gene annotations)\n\ndef compute_gene_activity(adata, peak_gene_map):\n    \"\"\"\n    Compute gene activity scores from peak accessibility.\n    \n    Parameters\n    ----------\n    adata : AnnData\n        ATAC data with peaks\n    peak_gene_map : dict\n        Mapping of peaks to genes\n        \n    Returns\n    -------\n    AnnData with gene activity scores\n    \"\"\"\n    from scipy.sparse import csr_matrix\n    \n    genes = list(set(peak_gene_map.values()))\n    gene_matrix = np.zeros((adata.n_obs, len(genes)))\n    \n    for i, gene in enumerate(genes):\n        gene_peaks = [p for p, g in peak_gene_map.items() if g == gene]\n        if gene_peaks:\n            peak_idx = [list(adata.var_names).index(p) for p in gene_peaks if p in adata.var_names]\n            if peak_idx:\n                gene_matrix[:, i] = np.array(adata.X[:, peak_idx].sum(axis=1)).flatten()\n    \n    adata_gene = ad.AnnData(\n        X=csr_matrix(gene_matrix),\n        obs=adata.obs.copy(),\n        var=pd.DataFrame(index=genes)\n    )\n    \n    return adata_gene\n```\n\n## Complete Pipeline\n\n```python\ndef analyze_scatac(\n    adata,\n    batch_key=None,\n    n_top_peaks=50000,\n    n_latent=20,\n    resolution=0.5\n):\n    \"\"\"\n    Complete scATAC-seq analysis with PeakVI.\n    \n    Parameters\n    ----------\n    adata : AnnData\n        Raw peak-cell matrix\n    batch_key : str, optional\n        Batch annotation column\n    n_top_peaks : int\n        Number of top peaks to use\n    n_latent : int\n        Latent dimensions\n    resolution : float\n        Leiden clustering resolution\n        \n    Returns\n    -------\n    Tuple of (processed AnnData, trained model)\n    \"\"\"\n    import scvi\n    import scanpy as sc\n    import numpy as np\n    \n    adata = adata.copy()\n    \n    # QC\n    sc.pp.calculate_qc_metrics(adata, inplace=True)\n    adata = adata[adata.obs['n_genes_by_counts'] \u003e 500].copy()\n    sc.pp.filter_genes(adata, min_cells=10)\n    \n    # Binarize\n    adata.X = (adata.X \u003e 0).astype(np.float32)\n    \n    # Select top peaks\n    if adata.n_vars \u003e n_top_peaks:\n        peak_accessibility = np.array(adata.X.sum(axis=0)).flatten()\n        top_peaks = np.argsort(peak_accessibility)[-n_top_peaks:]\n        adata = adata[:, top_peaks].copy()\n    \n    # Setup PeakVI\n    scvi.model.PEAKVI.setup_anndata(adata, batch_key=batch_key)\n    \n    # Train\n    model = scvi.model.PEAKVI(adata, n_latent=n_latent)\n    model.train(max_epochs=200, early_stopping=True)\n    \n    # Latent representation\n    adata.obsm[\"X_PeakVI\"] = model.get_latent_representation()\n    \n    # Clustering\n    sc.pp.neighbors(adata, use_rep=\"X_PeakVI\")\n    sc.tl.umap(adata)\n    sc.tl.leiden(adata, resolution=resolution)\n    \n    return adata, model\n\n# Usage\nadata, model = analyze_scatac(\n    adata,\n    batch_key=\"sample\",\n    n_top_peaks=50000\n)\n\n# Visualize\nsc.pl.umap(adata, color=['leiden', 'sample'])\n\n# Differential accessibility\nda_results = model.differential_accessibility(\n    groupby='leiden',\n    group1='0',\n    group2='1'\n)\n```\n\n## Integration with scRNA-seq\n\nFor multiome data or separate RNA/ATAC from same cells:\n\n```python\n# See MultiVI for joint RNA+ATAC analysis\n# Or use WNN (weighted nearest neighbors) approach\n\n# Transfer labels from RNA to ATAC using shared latent space\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Training slow | Too many peaks | Subset to top 50k peaks |\n| Poor clustering | Too few informative peaks | Use variable peaks |\n| Batch dominates | Strong technical effects | Ensure batch_key is set |\n| Memory error | Large peak matrix | Use sparse format, reduce peaks |\n\n## Key References\n\n- Ashuach et al. (2022) \"PeakVI: A deep generative model for single-cell chromatin accessibility analysis\"\n", "depth": 5}, {"id": "143", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/multiome_multivi.md", "name": "multiome_multivi.md", "type": "file", "content": "# Multiome Analysis with MultiVI\n\nThis reference covers joint RNA and ATAC-seq analysis from multiome experiments using MultiVI.\n\n## Overview\n\nMultiVI is a deep generative model for analyzing multiome data (simultaneous RNA-seq and ATAC-seq from the same cells). It:\n- Learns a joint latent representation across modalities\n- Handles missing modalities (RNA-only or ATAC-only cells)\n- Enables batch correction across experiments\n- Supports imputation of missing modalities\n\n## Prerequisites\n\n```python\nimport scvi\nimport scanpy as sc\nimport mudata as md\nimport numpy as np\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\n```\n\n## Data Formats\n\n### Option 1: MuData (Recommended)\n\n```python\n# Load multiome data as MuData\nmdata = md.read(\"multiome.h5mu\")\n\n# Structure:\n# mdata.mod['rna']  - AnnData with RNA counts\n# mdata.mod['atac'] - AnnData with ATAC counts\n\nprint(f\"RNA: {mdata.mod['rna'].shape}\")\nprint(f\"ATAC: {mdata.mod['atac'].shape}\")\n```\n\n### Option 2: Separate AnnData Objects\n\n```python\n# Load separately\nadata_rna = sc.read_h5ad(\"rna.h5ad\")\nadata_atac = sc.read_h5ad(\"atac.h5ad\")\n\n# Ensure same cells in same order\ncommon_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)\nadata_rna = adata_rna[common_cells].copy()\nadata_atac = adata_atac[common_cells].copy()\n```\n\n## Step 1: Prepare RNA Data\n\n```python\n# RNA preprocessing (standard scvi-tools pipeline)\nadata_rna = mdata.mod['rna'].copy()\n\n# Filter\nsc.pp.filter_cells(adata_rna, min_genes=200)\nsc.pp.filter_genes(adata_rna, min_cells=3)\n\n# Store counts\nadata_rna.layers[\"counts\"] = adata_rna.X.copy()\n\n# HVG selection\nsc.pp.highly_variable_genes(\n    adata_rna,\n    n_top_genes=4000,\n    flavor=\"seurat_v3\",\n    layer=\"counts\",\n    batch_key=\"batch\"  # If multiple batches\n)\n\n# Subset to HVGs\nadata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()\n```\n\n## Step 2: Prepare ATAC Data\n\n```python\n# ATAC preprocessing\nadata_atac = mdata.mod['atac'].copy()\n\n# Filter peaks\nsc.pp.filter_genes(adata_atac, min_cells=10)\n\n# Binarize accessibility\nadata_atac.X = (adata_atac.X \u003e 0).astype(np.float32)\n\n# Select top accessible peaks (if too many)\nif adata_atac.n_vars \u003e 50000:\n    peak_accessibility = np.array(adata_atac.X.sum(axis=0)).flatten()\n    top_peaks = np.argsort(peak_accessibility)[-50000:]\n    adata_atac = adata_atac[:, top_peaks].copy()\n\n# Store in layer\nadata_atac.layers[\"counts\"] = adata_atac.X.copy()\n```\n\n## Step 3: Create Combined MuData\n\n```python\n# Ensure matching cells\ncommon_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)\nadata_rna = adata_rna[common_cells].copy()\nadata_atac = adata_atac[common_cells].copy()\n\n# Create MuData\nmdata = md.MuData({\n    \"rna\": adata_rna,\n    \"atac\": adata_atac\n})\n\nprint(f\"Combined multiome: {mdata.n_obs} cells\")\nprint(f\"RNA features: {mdata.mod['rna'].n_vars}\")\nprint(f\"ATAC features: {mdata.mod['atac'].n_vars}\")\n```\n\n## Step 4: Setup MultiVI\n\n```python\n# Setup MuData for MultiVI\nscvi.model.MULTIVI.setup_mudata(\n    mdata,\n    rna_layer=\"counts\",\n    atac_layer=\"counts\",\n    batch_key=\"batch\",  # Optional\n    modalities={\n        \"rna_layer\": \"rna\",\n        \"batch_key\": \"rna\",\n        \"atac_layer\": \"atac\"\n    }\n)\n```\n\n## Step 5: Train MultiVI\n\n```python\n# Create model\nmodel = scvi.model.MULTIVI(\n    mdata,\n    n_latent=20,\n    n_layers_encoder=2,\n    n_layers_decoder=2\n)\n\n# Train\nmodel.train(\n    max_epochs=300,\n    early_stopping=True,\n    early_stopping_patience=10,\n    batch_size=128\n)\n\n# Check training\nmodel.history['elbo_train'].plot()\n```\n\n## Step 6: Get Joint Representation\n\n```python\n# Latent representation\nlatent = model.get_latent_representation()\n\n# Add to MuData\nmdata.obsm[\"X_MultiVI\"] = latent\n\n# Clustering on joint space\nsc.pp.neighbors(mdata, use_rep=\"X_MultiVI\")\nsc.tl.umap(mdata)\nsc.tl.leiden(mdata, resolution=1.0)\n\n# Visualize\nsc.pl.umap(mdata, color=['leiden', 'batch'], ncols=2)\n```\n\n## Step 7: Modality-Specific Analysis\n\n### Impute Missing Modality\n\n```python\n# Impute RNA expression for ATAC-only cells\n# (Useful when integrating with ATAC-only datasets)\nimputed_rna = model.get_normalized_expression(\n    modality=\"rna\"\n)\n\n# Impute accessibility for RNA-only cells\nimputed_atac = model.get_accessibility_estimates()\n```\n\n### Differential Analysis\n\n```python\n# Differential expression (RNA)\nde_results = model.differential_expression(\n    groupby=\"leiden\",\n    group1=\"0\",\n    group2=\"1\"\n)\n\n# Differential accessibility (ATAC)\nda_results = model.differential_accessibility(\n    groupby=\"leiden\",\n    group1=\"0\",\n    group2=\"1\"\n)\n```\n\n## Handling Partial Data\n\nMultiVI can integrate datasets with only one modality:\n\n```python\n# Dataset 1: Full multiome\n# Dataset 2: RNA only\n# Dataset 3: ATAC only\n\n# Mark missing modalities\nmdata.obs['modality'] = 'paired'  # For cells with both\n# For RNA-only cells, ATAC data should be missing/NaN\n# For ATAC-only cells, RNA data should be missing/NaN\n\n# MultiVI handles this automatically during training\n```\n\n## Complete Pipeline\n\n```python\ndef analyze_multiome(\n    adata_rna,\n    adata_atac,\n    batch_key=None,\n    n_top_genes=4000,\n    n_top_peaks=50000,\n    n_latent=20,\n    max_epochs=300\n):\n    \"\"\"\n    Complete multiome analysis with MultiVI.\n\n    Parameters\n    ----------\n    adata_rna : AnnData\n        RNA count data\n    adata_atac : AnnData\n        ATAC peak data\n    batch_key : str, optional\n        Batch column name\n    n_top_genes : int\n        Number of HVGs for RNA\n    n_top_peaks : int\n        Number of top peaks for ATAC\n    n_latent : int\n        Latent dimensions\n    max_epochs : int\n        Maximum training epochs\n\n    Returns\n    -------\n    MuData with joint representation\n    \"\"\"\n    import scvi\n    import scanpy as sc\n    import mudata as md\n    import numpy as np\n\n    # Get common cells\n    common_cells = adata_rna.obs_names.intersection(adata_atac.obs_names)\n    adata_rna = adata_rna[common_cells].copy()\n    adata_atac = adata_atac[common_cells].copy()\n\n    # RNA preprocessing\n    sc.pp.filter_genes(adata_rna, min_cells=3)\n    adata_rna.layers[\"counts\"] = adata_rna.X.copy()\n\n    if batch_key:\n        sc.pp.highly_variable_genes(\n            adata_rna, n_top_genes=n_top_genes,\n            flavor=\"seurat_v3\", layer=\"counts\", batch_key=batch_key\n        )\n    else:\n        sc.pp.normalize_total(adata_rna, target_sum=1e4)\n        sc.pp.log1p(adata_rna)\n        sc.pp.highly_variable_genes(adata_rna, n_top_genes=n_top_genes)\n        adata_rna.X = adata_rna.layers[\"counts\"].copy()\n\n    adata_rna = adata_rna[:, adata_rna.var['highly_variable']].copy()\n\n    # ATAC preprocessing\n    sc.pp.filter_genes(adata_atac, min_cells=10)\n    adata_atac.X = (adata_atac.X \u003e 0).astype(np.float32)\n\n    if adata_atac.n_vars \u003e n_top_peaks:\n        peak_acc = np.array(adata_atac.X.sum(axis=0)).flatten()\n        top_idx = np.argsort(peak_acc)[-n_top_peaks:]\n        adata_atac = adata_atac[:, top_idx].copy()\n\n    adata_atac.layers[\"counts\"] = adata_atac.X.copy()\n\n    # Create MuData\n    mdata = md.MuData({\"rna\": adata_rna, \"atac\": adata_atac})\n\n    # Setup and train\n    scvi.model.MULTIVI.setup_mudata(\n        mdata,\n        rna_layer=\"counts\",\n        atac_layer=\"counts\",\n        batch_key=batch_key,\n        modalities={\"rna_layer\": \"rna\", \"batch_key\": \"rna\", \"atac_layer\": \"atac\"}\n    )\n\n    model = scvi.model.MULTIVI(mdata, n_latent=n_latent)\n    model.train(max_epochs=max_epochs, early_stopping=True)\n\n    # Add representation\n    mdata.obsm[\"X_MultiVI\"] = model.get_latent_representation()\n\n    # Cluster\n    sc.pp.neighbors(mdata, use_rep=\"X_MultiVI\")\n    sc.tl.umap(mdata)\n    sc.tl.leiden(mdata)\n\n    return mdata, model\n\n\n# Usage\nmdata, model = analyze_multiome(\n    adata_rna,\n    adata_atac,\n    batch_key=\"sample\"\n)\n\nsc.pl.umap(mdata, color=['leiden', 'sample'])\n```\n\n## Peak-to-Gene Linking\n\n```python\n# Link ATAC peaks to genes based on correlation in latent space\n# This identifies regulatory relationships\n\ndef link_peaks_to_genes(model, mdata, distance_threshold=100000):\n    \"\"\"\n    Link peaks to nearby genes based on correlation.\n\n    Parameters\n    ----------\n    model : MULTIVI\n        Trained model\n    mdata : MuData\n        Multiome data\n    distance_threshold : int\n        Maximum distance (bp) to link peak to gene\n\n    Returns\n    -------\n    DataFrame of peak-gene links\n    \"\"\"\n    # Get imputed values\n    rna_imputed = model.get_normalized_expression()\n    atac_imputed = model.get_accessibility_estimates()\n\n    # Correlate peak accessibility with gene expression\n    # for peaks near gene promoters\n    # ... (requires genomic coordinates)\n\n    return peak_gene_links\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Different cell counts | Cells missing in one modality | Use common cells only |\n| Training instability | Imbalanced modalities | Normalize feature counts |\n| Poor clustering | Too few features | Increase n_top_genes/peaks |\n| Memory error | Large ATAC matrix | Reduce peak count, use sparse |\n| Batch dominates | Strong technical effects | Ensure batch_key is set |\n\n## Key References\n\n- Ashuach et al. (2023) \"MultiVI: deep generative model for the integration of multimodal data\"\n", "depth": 5}, {"id": "144", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/data_preparation.md", "name": "data_preparation.md", "type": "file", "content": "# Data Preparation for scvi-tools\n\nThis reference covers how to properly prepare AnnData objects for use with scvi-tools models.\n\n## Overview\n\nProper data preparation is critical for scvi-tools. Key requirements:\n1. **Raw counts** (not normalized)\n2. **Highly variable gene selection**\n3. **Proper setup_anndata() call**\n\n## Step 1: Load and Inspect Data\n\n```python\nimport scanpy as sc\nimport scvi\nimport numpy as np\n\n# Load data\nadata = sc.read_h5ad(\"data.h5ad\")\n\n# Check what's in adata.X\nprint(f\"Shape: {adata.shape}\")\nprint(f\"X dtype: {adata.X.dtype}\")\nprint(f\"X contains integers: {np.allclose(adata.X.data, adata.X.data.astype(int))}\")\nprint(f\"X min: {adata.X.min()}, max: {adata.X.max()}\")\n```\n\n### Verify Raw Counts\n\n```python\n# scvi-tools needs INTEGER counts\n# If X appears normalized, check for raw counts\n\nif hasattr(adata, 'raw') and adata.raw is not None:\n    print(\"Found adata.raw\")\n    # Use raw counts\n    adata = adata.raw.to_adata()\n    \n# Or check layers\nif 'counts' in adata.layers:\n    print(\"Found counts layer\")\n    # Will specify layer in setup_anndata\n```\n\n## Step 2: Basic Filtering\n\n```python\n# Filter cells (standard QC)\nsc.pp.filter_cells(adata, min_genes=200)\nsc.pp.filter_cells(adata, max_genes=5000)\n\n# Calculate mito percent if not present\n# Handle both human (MT-) and mouse (mt-, Mt-) mitochondrial genes\nadata.var['mt'] = (\n    adata.var_names.str.startswith('MT-') |\n    adata.var_names.str.startswith('mt-') |\n    adata.var_names.str.startswith('Mt-')\n)\nsc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], inplace=True)\nadata = adata[adata.obs['pct_counts_mt'] \u003c 20].copy()\n\n# Filter genes\nsc.pp.filter_genes(adata, min_cells=3)\n\nprint(f\"After filtering: {adata.shape}\")\n```\n\n## Step 3: Store Raw Counts\n\n**Critical**: Always preserve raw counts before any normalization.\n\n```python\n# Store raw counts in a layer\nadata.layers[\"counts\"] = adata.X.copy()\n\n# Now you can normalize for other purposes (HVG selection)\n# But scvi will use the counts layer\n```\n\n## Step 4: Highly Variable Gene Selection\n\nscvi-tools works best with 1,500-5,000 HVGs.\n\n### For Single-Batch Data\n\n```python\n# Normalize for HVG selection only\nadata_hvg = adata.copy()\nsc.pp.normalize_total(adata_hvg, target_sum=1e4)\nsc.pp.log1p(adata_hvg)\n\n# Select HVGs\nsc.pp.highly_variable_genes(\n    adata_hvg,\n    n_top_genes=2000,\n    flavor=\"seurat\"  # or \"cell_ranger\"\n)\n\n# Transfer HVG annotation\nadata.var['highly_variable'] = adata_hvg.var['highly_variable']\n```\n\n### For Multi-Batch Data (Recommended)\n\n```python\n# Use seurat_v3 flavor with batch_key\n# This selects genes variable across batches\nsc.pp.highly_variable_genes(\n    adata,\n    n_top_genes=2000,\n    flavor=\"seurat_v3\",\n    batch_key=\"batch\",  # Your batch column\n    layer=\"counts\"      # Use raw counts\n)\n```\n\n### Subset to HVGs\n\n```python\n# Subset to highly variable genes\nadata = adata[:, adata.var['highly_variable']].copy()\nprint(f\"After HVG selection: {adata.shape}\")\n```\n\n## Step 5: Setup AnnData\n\nThe `setup_anndata()` function registers data for the model.\n\n### Basic Setup\n\n```python\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\"  # Specify layer with raw counts\n)\n```\n\n### With Batch Information\n\n```python\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\"  # Column in adata.obs\n)\n```\n\n### With Cell Type Labels (for scANVI)\n\n```python\nscvi.model.SCANVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\",\n    labels_key=\"cell_type\"  # Column with cell type labels\n)\n```\n\n### With Continuous Covariates\n\n```python\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\",\n    continuous_covariate_keys=[\"percent_mito\", \"n_genes\"]\n)\n```\n\n### With Categorical Covariates\n\n```python\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\",\n    categorical_covariate_keys=[\"donor\", \"technology\"]\n)\n```\n\n## Multi-Modal Data Setup\n\n### CITE-seq (for totalVI)\n\n```python\n# Protein data in adata.obsm\n# RNA in adata.X, protein in separate matrix\n\n# Add protein data\nadata.obsm[\"protein_expression\"] = protein_counts  # numpy array\n\n# Setup for totalVI\nscvi.model.TOTALVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\",\n    protein_expression_obsm_key=\"protein_expression\"\n)\n```\n\n### Multiome RNA+ATAC (for MultiVI)\n\n```python\n# RNA and ATAC in separate AnnData objects or MuData\n\nimport mudata as md\n\n# If using MuData\nmdata = md.read(\"multiome.h5mu\")\n\nscvi.model.MULTIVI.setup_mudata(\n    mdata,\n    rna_layer=\"counts\",\n    protein_layer=None,\n    batch_key=\"batch\",\n    modalities={\"rna\": \"rna\", \"accessibility\": \"atac\"}\n)\n```\n\n## Complete Preparation Pipeline\n\nFor a complete preparation function, use `prepare_adata()` from `scripts/model_utils.py`:\n\n```python\nfrom model_utils import prepare_adata\n\n# Prepare data with QC, HVG selection, and layer setup\nadata = prepare_adata(\n    adata,\n    batch_key=\"batch\",\n    n_top_genes=2000,\n    min_genes=200,\n    max_mito_pct=20\n)\n\n# Then setup for your model\nimport scvi\nscvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\n```\n\nThis function handles:\n- Mitochondrial QC filtering\n- Cell and gene filtering\n- Storing counts in layer\n- HVG selection (batch-aware if batch_key provided)\n- Subsetting to HVGs\n\n## Checking Setup\n\n```python\n# View registered data\nprint(adata.uns['_scvi_manager_uuid'])\nprint(adata.uns['_scvi_adata_minify_type'])\n\n# For scVI\nscvi.model.SCVI.view_anndata_setup(adata)\n```\n\n## Common Issues and Solutions\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| \"X should contain integers\" | Normalized data in X | Use layer=\"counts\" |\n| \"batch_key not found\" | Wrong column name | Check adata.obs.columns |\n| Sparse matrix errors | Incompatible format | Convert: adata.X = adata.X.toarray() |\n| Memory error | Too many genes | Subset to HVGs first |\n| NaN in data | Missing values | Filter or impute |\n\n## Data Format Reference\n\n### Required\n\n- `adata.X` or `adata.layers[\"counts\"]`: Raw integer counts (sparse OK)\n- `adata.obs`: Cell metadata DataFrame\n- `adata.var`: Gene metadata DataFrame\n\n### Recommended\n\n- `adata.obs[\"batch\"]`: Batch/sample identifiers\n- `adata.var[\"highly_variable\"]`: HVG boolean mask\n\n### For scANVI\n\n- `adata.obs[\"labels\"]`: Cell type annotations\n- Can include \"Unknown\" for unlabeled cells\n", "depth": 5}, {"id": "145", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/scrna_integration.md", "name": "scrna_integration.md", "type": "file", "content": "# scRNA-seq Integration with scVI and scANVI\n\nThis reference covers batch correction and dataset integration using scVI (unsupervised) and scANVI (semi-supervised with cell type labels).\n\n## Overview\n\nSingle-cell datasets often have batch effects from:\n- Different donors/patients\n- Different experimental batches\n- Different technologies (10x v2 vs v3)\n- Different studies\n\nscVI and scANVI learn a shared latent space where batch effects are removed while biological variation is preserved.\n\n## When to Use Which Model\n\n| Model | Use When | Labels Needed |\n|-------|----------|---------------|\n| **scVI** | No labels available, exploratory analysis | No |\n| **scANVI** | Have partial/full labels, want better preservation | Yes (partial OK) |\n\n## scVI Integration Workflow\n\n### Step 1: Prepare Data\n\n```python\nimport scvi\nimport scanpy as sc\n\n# Load datasets\nadata1 = sc.read_h5ad(\"dataset1.h5ad\")\nadata2 = sc.read_h5ad(\"dataset2.h5ad\")\n\n# Add batch annotation\nadata1.obs[\"batch\"] = \"batch1\"\nadata2.obs[\"batch\"] = \"batch2\"\n\n# Concatenate\nadata = sc.concat([adata1, adata2], label=\"batch\")\n\n# Ensure we have raw counts\n# If data is normalized, recover from .raw\nif hasattr(adata, 'raw') and adata.raw is not None:\n    adata = adata.raw.to_adata()\n\n# Store counts\nadata.layers[\"counts\"] = adata.X.copy()\n```\n\n### Step 2: HVG Selection Across Batches\n\n```python\n# Select HVGs considering batch\nsc.pp.highly_variable_genes(\n    adata,\n    n_top_genes=2000,\n    flavor=\"seurat_v3\",\n    batch_key=\"batch\",\n    layer=\"counts\"\n)\n\n# Subset to HVGs\nadata = adata[:, adata.var[\"highly_variable\"]].copy()\n```\n\n### Step 3: Setup and Train scVI\n\n```python\n# Register data with scVI\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\"\n)\n\n# Create model\nmodel = scvi.model.SCVI(\n    adata,\n    n_latent=30,          # Latent dimensions\n    n_layers=2,           # Encoder/decoder depth\n    gene_likelihood=\"nb\"  # negative binomial (or \"zinb\")\n)\n\n# Train\nmodel.train(\n    max_epochs=200,\n    early_stopping=True,\n    early_stopping_patience=10,\n    batch_size=128\n)\n\n# Plot training history\nmodel.history[\"elbo_train\"].plot()\n```\n\n### Step 4: Get Integrated Representation\n\n```python\n# Get latent representation\nadata.obsm[\"X_scVI\"] = model.get_latent_representation()\n\n# Use for clustering and visualization\nsc.pp.neighbors(adata, use_rep=\"X_scVI\", n_neighbors=15)\nsc.tl.umap(adata)\nsc.tl.leiden(adata, resolution=1.0)\n\n# Visualize integration\nsc.pl.umap(adata, color=[\"batch\", \"leiden\"], ncols=2)\n```\n\n### Step 5: Save Model\n\n```python\n# Save model for later use\nmodel.save(\"scvi_model/\")\n\n# Load model\nmodel = scvi.model.SCVI.load(\"scvi_model/\", adata=adata)\n```\n\n## scANVI Integration Workflow\n\nscANVI extends scVI with cell type labels for better biological preservation.\n\n### Step 1: Prepare Data with Labels\n\n```python\n# Labels should be in adata.obs\n# Use \"Unknown\" for unlabeled cells\nprint(adata.obs[\"cell_type\"].value_counts())\n\n# For partially labeled data\n# Mark unlabeled cells\nadata.obs[\"cell_type_scanvi\"] = adata.obs[\"cell_type\"].copy()\n# adata.obs.loc[unlabeled_mask, \"cell_type_scanvi\"] = \"Unknown\"\n```\n\n### Step 2: Option A - Train scANVI from Scratch\n\n```python\n# Setup for scANVI\nscvi.model.SCANVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\",\n    labels_key=\"cell_type\"\n)\n\n# Create model\nscanvi_model = scvi.model.SCANVI(\n    adata,\n    n_latent=30,\n    n_layers=2\n)\n\n# Train\nscanvi_model.train(max_epochs=200)\n```\n\n### Step 2: Option B - Initialize scANVI from scVI (Recommended)\n\n```python\n# First train scVI\nscvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=\"batch\")\nscvi_model = scvi.model.SCVI(adata, n_latent=30)\nscvi_model.train(max_epochs=200)\n\n# Initialize scANVI from scVI\nscanvi_model = scvi.model.SCANVI.from_scvi_model(\n    scvi_model,\n    labels_key=\"cell_type\",\n    unlabeled_category=\"Unknown\"  # For partially labeled data\n)\n\n# Fine-tune scANVI (fewer epochs needed)\nscanvi_model.train(max_epochs=50)\n```\n\n### Step 3: Get Results\n\n```python\n# Latent representation\nadata.obsm[\"X_scANVI\"] = scanvi_model.get_latent_representation()\n\n# Predicted labels for unlabeled cells\npredictions = scanvi_model.predict()\nadata.obs[\"predicted_cell_type\"] = predictions\n\n# Prediction probabilities\nsoft_predictions = scanvi_model.predict(soft=True)\n\n# Visualization\nsc.pp.neighbors(adata, use_rep=\"X_scANVI\")\nsc.tl.umap(adata)\nsc.pl.umap(adata, color=[\"batch\", \"cell_type\", \"predicted_cell_type\"])\n```\n\n## Comparing Integration Quality\n\n### Visual Assessment\n\n```python\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Before integration (on PCA)\nsc.pp.pca(adata)\nsc.pl.pca(adata, color=\"batch\", ax=axes[0], title=\"Before (PCA)\", show=False)\n\n# After scVI\nsc.pp.neighbors(adata, use_rep=\"X_scVI\")\nsc.tl.umap(adata)\nsc.pl.umap(adata, color=\"batch\", ax=axes[1], title=\"After scVI\", show=False)\n\n# After scANVI\nsc.pp.neighbors(adata, use_rep=\"X_scANVI\")\nsc.tl.umap(adata)\nsc.pl.umap(adata, color=\"batch\", ax=axes[2], title=\"After scANVI\", show=False)\n\nplt.tight_layout()\n```\n\n### Quantitative Metrics (scib)\n\n```python\n# pip install scib-metrics\n\nfrom scib_metrics.benchmark import Benchmarker\n\nbm = Benchmarker(\n    adata,\n    batch_key=\"batch\",\n    label_key=\"cell_type\",\n    embedding_obsm_keys=[\"X_pca\", \"X_scVI\", \"X_scANVI\"]\n)\n\nbm.benchmark()\nbm.plot_results_table()\n```\n\n## Differential Expression\n\nscVI provides differential expression that accounts for batch effects:\n\n```python\n# DE between groups\nde_results = model.differential_expression(\n    groupby=\"cell_type\",\n    group1=\"T cells\",\n    group2=\"B cells\"\n)\n\n# Filter significant\nde_sig = de_results[\n    (de_results[\"is_de_fdr_0.05\"] == True) \u0026\n    (abs(de_results[\"lfc_mean\"]) \u003e 1)\n]\n\nprint(de_sig.head(20))\n```\n\n## Advanced: Multiple Categorical Covariates\n\n```python\n# Include additional covariates beyond batch\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"batch\",\n    categorical_covariate_keys=[\"donor\", \"technology\"]\n)\n\nmodel = scvi.model.SCVI(adata, n_latent=30)\nmodel.train()\n```\n\n## Training Tips\n\n### For Large Datasets (\u003e100k cells)\n\n```python\nmodel.train(\n    max_epochs=100,      # Fewer epochs needed\n    batch_size=256,      # Larger batches\n    train_size=0.9,      # Less validation\n    early_stopping=True\n)\n```\n\n### For Small Datasets (\u003c10k cells)\n\n```python\nmodel = scvi.model.SCVI(\n    adata,\n    n_latent=10,         # Smaller latent space\n    n_layers=1,          # Simpler model\n    dropout_rate=0.2     # More regularization\n)\n\nmodel.train(\n    max_epochs=400,\n    batch_size=64\n)\n```\n\n### Monitoring Training\n\n```python\n# Check training curves\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(model.history[\"elbo_train\"], label=\"Train\")\nax.plot(model.history[\"elbo_validation\"], label=\"Validation\")\nax.set_xlabel(\"Epoch\")\nax.set_ylabel(\"ELBO\")\nax.legend()\n\n# Should see convergence without overfitting\n```\n\n## Complete Pipeline\n\n```python\ndef integrate_datasets(\n    adatas,\n    batch_key=\"batch\",\n    labels_key=None,\n    n_top_genes=2000,\n    n_latent=30\n):\n    \"\"\"\n    Integrate multiple scRNA-seq datasets.\n    \n    Parameters\n    ----------\n    adatas : dict\n        Dictionary of {batch_name: AnnData}\n    batch_key : str\n        Key for batch annotation\n    labels_key : str, optional\n        Key for cell type labels (uses scANVI if provided)\n    n_top_genes : int\n        Number of HVGs\n    n_latent : int\n        Latent dimensions\n        \n    Returns\n    -------\n    AnnData with integrated representation\n    \"\"\"\n    import scvi\n    import scanpy as sc\n    \n    # Add batch labels and concatenate\n    for batch_name, adata in adatas.items():\n        adata.obs[batch_key] = batch_name\n    \n    adata = sc.concat(list(adatas.values()), label=batch_key)\n    \n    # Store counts\n    adata.layers[\"counts\"] = adata.X.copy()\n    \n    # HVG selection\n    sc.pp.highly_variable_genes(\n        adata,\n        n_top_genes=n_top_genes,\n        flavor=\"seurat_v3\",\n        batch_key=batch_key,\n        layer=\"counts\"\n    )\n    adata = adata[:, adata.var[\"highly_variable\"]].copy()\n    \n    # Train model\n    if labels_key and labels_key in adata.obs.columns:\n        # Use scANVI\n        scvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=batch_key)\n        scvi_model = scvi.model.SCVI(adata, n_latent=n_latent)\n        scvi_model.train(max_epochs=200)\n        \n        model = scvi.model.SCANVI.from_scvi_model(\n            scvi_model,\n            labels_key=labels_key,\n            unlabeled_category=\"Unknown\"\n        )\n        model.train(max_epochs=50)\n        rep_key = \"X_scANVI\"\n    else:\n        # Use scVI\n        scvi.model.SCVI.setup_anndata(adata, layer=\"counts\", batch_key=batch_key)\n        model = scvi.model.SCVI(adata, n_latent=n_latent)\n        model.train(max_epochs=200)\n        rep_key = \"X_scVI\"\n    \n    # Add representation\n    adata.obsm[rep_key] = model.get_latent_representation()\n    \n    # Compute neighbors and UMAP\n    sc.pp.neighbors(adata, use_rep=rep_key)\n    sc.tl.umap(adata)\n    sc.tl.leiden(adata)\n    \n    return adata, model\n\n# Usage\nadatas = {\n    \"study1\": sc.read_h5ad(\"study1.h5ad\"),\n    \"study2\": sc.read_h5ad(\"study2.h5ad\"),\n    \"study3\": sc.read_h5ad(\"study3.h5ad\")\n}\n\nadata_integrated, model = integrate_datasets(\n    adatas,\n    labels_key=\"cell_type\"\n)\n\nsc.pl.umap(adata_integrated, color=[\"batch\", \"leiden\", \"cell_type\"])\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Batches not mixing | Too few shared genes | Use more HVGs, check gene overlap |\n| Over-correction | Biological variation removed | Use scANVI with labels |\n| Training diverges | Learning rate too high | Reduce lr, increase batch_size |\n| NaN loss | Bad data | Check for all-zero cells/genes |\n| Memory error | Too many cells | Reduce batch_size, use GPU |\n", "depth": 5}, {"id": "146", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/batch_correction_sysvi.md", "name": "batch_correction_sysvi.md", "type": "file", "content": "# Advanced Batch Correction with sysVI\n\nThis reference covers system-level batch correction using sysVI, designed for integrating data across major technological or study differences.\n\n## Overview\n\nsysVI (System Variational Inference) extends scVI for scenarios where:\n- Batch effects are very strong (different technologies)\n- Standard scVI over-corrects biological signal\n- You need to separate \"system\" effects from biological variation\n\n## When to Use sysVI vs scVI\n\n| Scenario | Recommended Model |\n|----------|-------------------|\n| Same technology, different samples | scVI |\n| 10x v2 vs 10x v3 | scVI (usually) |\n| 10x vs Smart-seq2 | sysVI |\n| Different sequencing depths | scVI with covariates |\n| Cross-study integration | sysVI |\n| Atlas-scale integration | sysVI |\n\n## Prerequisites\n\n```python\nimport scvi\nimport scanpy as sc\nimport numpy as np\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\n```\n\n## Understanding sysVI Architecture\n\nsysVI separates variation into:\n1. **Biological variation**: Cell type, state, trajectory\n2. **System variation**: Technology, study, lab effects\n\n```\n                    ┌─────────────────┐\nInput counts ──────►│    Encoder      │\n                    │                 │\nSystem info ───────►│  (conditioned)  │\n                    └────────┬────────┘\n                             │\n                    ┌────────▼────────┐\n                    │   Latent z      │\n                    │  (biological)   │\n                    └────────┬────────┘\n                             │\n                    ┌────────▼────────┐\nSystem info ───────►│    Decoder      │\n                    │  (conditioned)  │\n                    └────────┬────────┘\n                             │\n                    Reconstructed counts\n```\n\n## Basic sysVI Workflow\n\n### Step 1: Prepare Data\n\n```python\n# Load datasets from different systems\nadata1 = sc.read_h5ad(\"10x_data.h5ad\")\nadata2 = sc.read_h5ad(\"smartseq_data.h5ad\")\n\n# Add system labels\nadata1.obs[\"system\"] = \"10x\"\nadata2.obs[\"system\"] = \"Smart-seq2\"\n\n# Add batch labels (within system)\n# e.g., different samples within each technology\n\n# Concatenate\nadata = sc.concat([adata1, adata2])\n\n# Store raw counts\nadata.layers[\"counts\"] = adata.X.copy()\n```\n\n### Step 2: HVG Selection\n\n```python\n# Select HVGs considering both batch and system\nsc.pp.highly_variable_genes(\n    adata,\n    n_top_genes=4000,  # More genes for cross-system\n    flavor=\"seurat_v3\",\n    batch_key=\"system\",  # Consider system for HVG\n    layer=\"counts\"\n)\n\n# Optionally: ensure overlap between systems\n# Check HVGs are expressed in both systems\nadata = adata[:, adata.var[\"highly_variable\"]].copy()\n```\n\n### Step 3: Setup and Train sysVI\n\n```python\n# Setup AnnData\n# Note: sysVI may be accessed differently depending on version\n# Check scvi-tools documentation for current API\n\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"sample\",           # Within-system batches\n    categorical_covariate_keys=[\"system\"]  # System-level covariate\n)\n\n# For true sysVI (if available in your version)\n# scvi.model.SysVI.setup_anndata(...)\n\n# Create model with system awareness\nmodel = scvi.model.SCVI(\n    adata,\n    n_latent=30,\n    n_layers=2,\n    gene_likelihood=\"nb\"\n)\n\n# Train\nmodel.train(max_epochs=300)\n```\n\n### Step 4: Extract Representations\n\n```python\n# Get latent representation\nadata.obsm[\"X_integrated\"] = model.get_latent_representation()\n\n# Clustering and visualization\nsc.pp.neighbors(adata, use_rep=\"X_integrated\")\nsc.tl.umap(adata)\nsc.tl.leiden(adata)\n\n# Check integration\nsc.pl.umap(adata, color=[\"system\", \"leiden\", \"cell_type\"])\n```\n\n## Alternative: Harmony + scVI\n\nFor cross-system integration, combining methods can work well:\n\n```python\nimport scanpy.external as sce\n\n# First run PCA\nsc.pp.pca(adata)\n\n# Apply Harmony for initial alignment\nsce.pp.harmony_integrate(adata, key=\"system\")\n\n# Then train scVI on Harmony-corrected embedding\n# Or use Harmony representation directly\n```\n\n## Alternative: Using Covariates in scVI\n\nFor moderate system effects:\n\n```python\n# Include system as categorical covariate\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"sample\",\n    categorical_covariate_keys=[\"system\", \"technology_version\"]\n)\n\nmodel = scvi.model.SCVI(adata, n_latent=30)\nmodel.train()\n```\n\n## Alternative: Separate Models + Integration\n\nFor very different systems:\n\n```python\n# Train separate models\nscvi.model.SCVI.setup_anndata(adata1, layer=\"counts\", batch_key=\"sample\")\nmodel1 = scvi.model.SCVI(adata1)\nmodel1.train()\n\nscvi.model.SCVI.setup_anndata(adata2, layer=\"counts\", batch_key=\"sample\")\nmodel2 = scvi.model.SCVI(adata2)\nmodel2.train()\n\n# Get latent spaces\nadata1.obsm[\"X_scVI\"] = model1.get_latent_representation()\nadata2.obsm[\"X_scVI\"] = model2.get_latent_representation()\n\n# Align with CCA or Harmony\n# ... additional alignment step\n```\n\n## Evaluating Cross-System Integration\n\n### Visual Assessment\n\n```python\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Color by system\nsc.pl.umap(adata, color=\"system\", ax=axes[0], show=False, title=\"By System\")\n\n# Color by cell type\nsc.pl.umap(adata, color=\"cell_type\", ax=axes[1], show=False, title=\"By Cell Type\")\n\n# Color by expression of marker\nsc.pl.umap(adata, color=\"CD3D\", ax=axes[2], show=False, title=\"CD3D Expression\")\n\nplt.tight_layout()\n```\n\n### Quantitative Metrics\n\n```python\n# Using scib-metrics\nfrom scib_metrics.benchmark import Benchmarker\n\nbm = Benchmarker(\n    adata,\n    batch_key=\"system\",\n    label_key=\"cell_type\",\n    embedding_obsm_keys=[\"X_integrated\"]\n)\n\nbm.benchmark()\n\n# Key metrics:\n# - Batch mixing (ASW_batch, Graph connectivity)\n# - Bio conservation (NMI, ARI, ASW_label)\n```\n\n### LISI Scores\n\n```python\n# Local Inverse Simpson's Index\nfrom scib_metrics import lisi\n\n# Batch LISI (higher = better mixing)\nbatch_lisi = lisi.ilisi_graph(\n    adata,\n    batch_key=\"system\",\n    use_rep=\"X_integrated\"\n)\n\n# Cell type LISI (lower = better preservation)\nct_lisi = lisi.clisi_graph(\n    adata,\n    label_key=\"cell_type\", \n    use_rep=\"X_integrated\"\n)\n\nprint(f\"Batch LISI: {batch_lisi.mean():.3f}\")\nprint(f\"Cell type LISI: {ct_lisi.mean():.3f}\")\n```\n\n## Handling Specific Challenges\n\n### Different Gene Sets\n\n```python\n# Find common genes\ncommon_genes = adata1.var_names.intersection(adata2.var_names)\nprint(f\"Common genes: {len(common_genes)}\")\n\n# If too few, use gene mapping\n# Or impute missing genes\n```\n\n### Different Sequencing Depths\n\n```python\n# Add depth as continuous covariate\nadata.obs[\"log_counts\"] = np.log1p(adata.obs[\"total_counts\"])\n\nscvi.model.SCVI.setup_anndata(\n    adata,\n    layer=\"counts\",\n    batch_key=\"sample\",\n    continuous_covariate_keys=[\"log_counts\"]\n)\n```\n\n### Unbalanced Cell Types\n\n```python\n# Check cell type distribution per system\nimport pandas as pd\n\nct_dist = pd.crosstab(adata.obs[\"system\"], adata.obs[\"cell_type\"], normalize=\"index\")\nprint(ct_dist)\n\n# If very unbalanced, consider:\n# 1. Subsample to balance\n# 2. Use scANVI with labels to preserve rare types\n```\n\n## Complete Pipeline\n\n```python\ndef integrate_cross_system(\n    adatas: dict,\n    system_key: str = \"system\",\n    batch_key: str = \"batch\",\n    cell_type_key: str = \"cell_type\",\n    n_top_genes: int = 4000,\n    n_latent: int = 30\n):\n    \"\"\"\n    Integrate datasets from different technological systems.\n    \n    Parameters\n    ----------\n    adatas : dict\n        Dictionary of {system_name: AnnData}\n    system_key : str\n        Key for system annotation\n    batch_key : str\n        Key for within-system batch\n    cell_type_key : str\n        Key for cell type labels (optional)\n    n_top_genes : int\n        Number of HVGs\n    n_latent : int\n        Latent dimensions\n        \n    Returns\n    -------\n    Integrated AnnData with model\n    \"\"\"\n    import scvi\n    import scanpy as sc\n    \n    # Add system labels and concatenate\n    for system_name, adata in adatas.items():\n        adata.obs[system_key] = system_name\n    \n    adata = sc.concat(list(adatas.values()))\n    \n    # Find common genes\n    for name, ad in adatas.items():\n        if name == list(adatas.keys())[0]:\n            common_genes = set(ad.var_names)\n        else:\n            common_genes = common_genes.intersection(ad.var_names)\n    \n    adata = adata[:, list(common_genes)].copy()\n    print(f\"Common genes: {len(common_genes)}\")\n    \n    # Store counts\n    adata.layers[\"counts\"] = adata.X.copy()\n    \n    # HVG selection\n    sc.pp.highly_variable_genes(\n        adata,\n        n_top_genes=n_top_genes,\n        flavor=\"seurat_v3\",\n        batch_key=system_key,\n        layer=\"counts\"\n    )\n    adata = adata[:, adata.var[\"highly_variable\"]].copy()\n    \n    # Setup with system as covariate\n    scvi.model.SCVI.setup_anndata(\n        adata,\n        layer=\"counts\",\n        batch_key=batch_key if batch_key in adata.obs else None,\n        categorical_covariate_keys=[system_key]\n    )\n    \n    # Train\n    model = scvi.model.SCVI(adata, n_latent=n_latent, n_layers=2)\n    model.train(max_epochs=300, early_stopping=True)\n    \n    # Get representation\n    adata.obsm[\"X_integrated\"] = model.get_latent_representation()\n    \n    # Clustering\n    sc.pp.neighbors(adata, use_rep=\"X_integrated\")\n    sc.tl.umap(adata)\n    sc.tl.leiden(adata)\n    \n    return adata, model\n\n# Usage\nadatas = {\n    \"10x_v3\": sc.read_h5ad(\"10x_v3_data.h5ad\"),\n    \"Smart-seq2\": sc.read_h5ad(\"smartseq_data.h5ad\"),\n    \"Drop-seq\": sc.read_h5ad(\"dropseq_data.h5ad\")\n}\n\nadata_integrated, model = integrate_cross_system(adatas)\n\n# Visualize\nsc.pl.umap(adata_integrated, color=[\"system\", \"leiden\"])\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Systems don't mix | Effects too strong | Use more genes, increase n_latent |\n| Over-correction | Model too aggressive | Reduce n_layers, use scANVI |\n| Few common genes | Different platforms | Use gene name mapping |\n| One system dominates | Unbalanced sizes | Subsample larger dataset |\n\n## Key References\n\n- Lopez et al. (2018) \"Deep generative modeling for single-cell transcriptomics\"\n- Luecken et al. (2022) \"Benchmarking atlas-level data integration in single-cell genomics\"\n", "depth": 5}, {"id": "147", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/scvi-tools/references/spatial_deconvolution.md", "name": "spatial_deconvolution.md", "type": "file", "content": "# Spatial Transcriptomics Analysis\n\nThis reference covers spatial transcriptomics analysis using scvi-tools methods: DestVI for deconvolution and resolVI for building spatial models.\n\n## Overview\n\nSpatial transcriptomics technologies like Visium capture gene expression at defined spatial locations, but many platforms have multi-cellular resolution. scvi-tools provides two main approaches:\n\n- **DestVI**: Deconvolution - estimates cell type proportions at each spot using a single-cell reference\n- **resolVI**: Builds a spatial model that learns gene expression patterns accounting for spatial context\n\n## Available Methods in scvi-tools\n\n| Method | Description | Use Case |\n|--------|-------------|----------|\n| **DestVI** | Variational inference for deconvolution | Estimate cell type proportions per spot |\n| **resolVI** | Spatial gene expression model | Learn spatially-aware representations |\n| **CondSCVI** | Reference model for DestVI | Required for DestVI workflow |\n\n## Prerequisites\n\n```python\nimport scvi\nimport scanpy as sc\nimport squidpy as sq\nimport numpy as np\n\nprint(f\"scvi-tools version: {scvi.__version__}\")\n```\n\n---\n\n## Part 1: DestVI Deconvolution\n\n### Step 1: Load Spatial Data\n\n```python\n# Load Visium data\nadata_spatial = sc.read_visium(\"spaceranger_output/\")\n\n# Check structure\nprint(f\"Spots: {adata_spatial.n_obs}\")\nprint(f\"Genes: {adata_spatial.n_vars}\")\nprint(f\"Spatial coordinates: {adata_spatial.obsm['spatial'].shape}\")\n\n# Basic QC\nsc.pp.calculate_qc_metrics(adata_spatial, inplace=True)\nadata_spatial = adata_spatial[adata_spatial.obs['n_genes_by_counts'] \u003e 200].copy()\n\n# Store counts\nadata_spatial.layers[\"counts\"] = adata_spatial.X.copy()\n```\n\n### Step 2: Load Single-Cell Reference\n\n```python\n# Load reference single-cell data\nadata_sc = sc.read_h5ad(\"reference_scrna.h5ad\")\n\n# Requirements:\n# - Raw counts\n# - Cell type annotations\nprint(f\"Reference cells: {adata_sc.n_obs}\")\nprint(f\"Cell types: {adata_sc.obs['cell_type'].nunique()}\")\nprint(adata_sc.obs['cell_type'].value_counts())\n\n# Store counts\nadata_sc.layers[\"counts\"] = adata_sc.X.copy()\n```\n\n### Step 3: Prepare Data\n\n```python\n# DestVI requires gene overlap between reference and spatial\ncommon_genes = adata_sc.var_names.intersection(adata_spatial.var_names)\nprint(f\"Common genes: {len(common_genes)}\")\n\nadata_sc = adata_sc[:, common_genes].copy()\nadata_spatial = adata_spatial[:, common_genes].copy()\n```\n\n### Step 4: Train Reference Model (CondSCVI)\n\n```python\n# Train conditional scVI on reference data\nscvi.model.CondSCVI.setup_anndata(\n    adata_sc,\n    layer=\"counts\",\n    labels_key=\"cell_type\"\n)\n\nsc_model = scvi.model.CondSCVI(\n    adata_sc,\n    n_latent=20\n)\n\nsc_model.train(max_epochs=200)\nsc_model.history['elbo_train'].plot()\n```\n\n### Step 5: Train DestVI\n\n```python\n# Setup spatial data\nscvi.model.DestVI.setup_anndata(\n    adata_spatial,\n    layer=\"counts\"\n)\n\n# Train DestVI using reference model\nspatial_model = scvi.model.DestVI.from_rna_model(\n    adata_spatial,\n    sc_model\n)\n\nspatial_model.train(max_epochs=500)\n```\n\n### Step 6: Get Cell Type Proportions\n\n```python\n# Infer cell type proportions at each spot\nproportions = spatial_model.get_proportions()\n\n# Add to adata\nfor ct in adata_sc.obs['cell_type'].unique():\n    adata_spatial.obs[f'prop_{ct}'] = proportions[ct]\n\n# Visualize\nsq.pl.spatial_scatter(\n    adata_spatial,\n    color=[f'prop_{ct}' for ct in adata_sc.obs['cell_type'].unique()[:6]],\n    ncols=3\n)\n```\n\n---\n\n## Part 2: resolVI Spatial Model\n\nresolVI is a semi-supervised method that learns cell type assignments and spatially-aware representations directly from spatial data, optionally using initial cell type predictions.\n\n**Note**: resolVI is in `scvi.external` (not `scvi.model`).\n\n### Step 1: Prepare Spatial Data\n\n```python\n# Load and preprocess\nadata = sc.read_visium(\"spaceranger_output/\")\n\n# QC\nsc.pp.calculate_qc_metrics(adata, inplace=True)\nadata = adata[adata.obs['n_genes_by_counts'] \u003e 200].copy()\n\n# Store counts\nadata.layers[\"counts\"] = adata.X.copy()\n\n# HVG selection\nsc.pp.highly_variable_genes(\n    adata,\n    n_top_genes=4000,\n    flavor=\"seurat_v3\",\n    layer=\"counts\"\n)\nadata = adata[:, adata.var['highly_variable']].copy()\n\n# Optional: Get initial cell type predictions (e.g., from a reference)\n# adata.obs[\"predicted_celltype\"] = ...\n```\n\n### Step 2: Setup and Train resolVI\n\n```python\n# Setup for resolVI (note: scvi.external, not scvi.model)\nscvi.external.RESOLVI.setup_anndata(\n    adata,\n    labels_key=\"predicted_celltype\",  # Initial cell type predictions\n    layer=\"counts\"\n)\n\n# Create model (semisupervised=True uses the labels)\nmodel = scvi.external.RESOLVI(adata, semisupervised=True)\n\n# Train\nmodel.train(max_epochs=50)\n```\n\n### Step 3: Get Cell Type Predictions\n\n```python\n# Get refined cell type predictions\n# soft=True returns probabilities, soft=False returns labels\ncell_type_probs = model.predict(adata, num_samples=3, soft=True)\ncell_type_labels = model.predict(adata, num_samples=3, soft=False)\n\nadata.obs[\"resolvi_celltype\"] = cell_type_labels\n\n# Visualize\nsq.pl.spatial_scatter(adata, color=\"resolvi_celltype\")\n```\n\n### Step 4: Get Latent Representation\n\n```python\n# Get latent representation\nadata.obsm[\"X_resolVI\"] = model.get_latent_representation(adata)\n\n# Cluster based on spatial representation\nsc.pp.neighbors(adata, use_rep=\"X_resolVI\")\nsc.tl.umap(adata)\nsc.tl.leiden(adata, resolution=0.5)\n\n# Visualize clusters spatially\nsq.pl.spatial_scatter(adata, color=\"leiden\")\n```\n\n### Step 5: Differential Expression\n\n```python\n# DE between cell types using resolVI\nde_results = model.differential_expression(\n    adata,\n    groupby=\"resolvi_celltype\",\n    group1=\"T_cell\",\n    group2=\"Tumor\"\n)\n\nprint(de_results.head(20))\n```\n\n### Step 6: Niche Abundance Analysis\n\n```python\n# Analyze how cell type neighborhoods differ between conditions\n# Requires spatial neighbor graph\nsq.gr.spatial_neighbors(adata, coord_type=\"generic\")\n\nniche_results = model.differential_niche_abundance(\n    groupby=\"resolvi_celltype\",\n    group1=\"T_cell\",\n    group2=\"Tumor\",\n    neighbor_key=\"spatial_neighbors\"\n)\n```\n\n### Step 7: Query Mapping (Transfer to New Data)\n\n```python\n# Map new spatial data to trained model\nquery_adata = sc.read_visium(\"new_sample/\")\nquery_adata.layers[\"counts\"] = query_adata.X.copy()\n\n# Prepare and load query\nmodel.prepare_query_anndata(query_adata, reference_model=model)\nquery_model = model.load_query_data(query_adata, reference_model=model)\n\n# Fine-tune on query\nquery_model.train(max_epochs=20)\n\n# Get predictions for query\nquery_labels = query_model.predict(query_adata, num_samples=3, soft=False)\n```\n\n---\n\n## Visualization\n\n### Spatial Proportions\n\n```python\nimport matplotlib.pyplot as plt\n\n# Plot multiple cell type proportions\ncell_types = ['T_cell', 'Tumor', 'Fibroblast', 'Macrophage']\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\nfor ax, ct in zip(axes.flat, cell_types):\n    sq.pl.spatial_scatter(\n        adata_spatial,\n        color=f'prop_{ct}',\n        ax=ax,\n        title=ct,\n        show=False\n    )\n\nplt.tight_layout()\n```\n\n### Enrichment by Region\n\n```python\n# Cluster spatial data\nsc.pp.neighbors(adata_spatial)\nsc.tl.leiden(adata_spatial, resolution=0.5)\n\n# Compare proportions across regions\nimport pandas as pd\n\ncell_types = adata_sc.obs['cell_type'].unique()\nprop_cols = [f'prop_{ct}' for ct in cell_types]\nregion_props = adata_spatial.obs.groupby('leiden')[prop_cols].mean()\nprint(region_props)\n\n# Heatmap\nimport seaborn as sns\nplt.figure(figsize=(10, 6))\nsns.heatmap(region_props.T, annot=True, cmap='viridis')\nplt.title('Cell Type Proportions by Region')\n```\n\n### Spatial Cell Type Interactions\n\n```python\n# Neighborhood enrichment using cell type assignments\nsq.gr.spatial_neighbors(adata_spatial)\n\n# Create \"dominant cell type\" annotation\nprop_cols = [f'prop_{ct}' for ct in cell_types]\nadata_spatial.obs['dominant_type'] = adata_spatial.obs[prop_cols].idxmax(axis=1)\nadata_spatial.obs['dominant_type'] = adata_spatial.obs['dominant_type'].str.replace('prop_', '')\n\n# Co-occurrence analysis\nsq.gr.co_occurrence(adata_spatial, cluster_key='dominant_type')\nsq.pl.co_occurrence(adata_spatial, cluster_key='dominant_type')\n```\n\n---\n\n## Complete DestVI Pipeline\n\n```python\ndef deconvolve_spatial(\n    adata_spatial,\n    adata_ref,\n    cell_type_key=\"cell_type\",\n    n_latent=20,\n    max_epochs_ref=200,\n    max_epochs_spatial=500\n):\n    \"\"\"\n    Perform spatial deconvolution using DestVI.\n\n    Parameters\n    ----------\n    adata_spatial : AnnData\n        Spatial transcriptomics data\n    adata_ref : AnnData\n        Single-cell reference with cell type annotations\n    cell_type_key : str\n        Column in adata_ref.obs with cell type labels\n    n_latent : int\n        Latent dimensions\n    max_epochs_ref : int\n        Training epochs for reference model\n    max_epochs_spatial : int\n        Training epochs for spatial model\n\n    Returns\n    -------\n    AnnData with cell type proportions in obs\n    \"\"\"\n    import scvi\n\n    # Get common genes\n    common_genes = adata_ref.var_names.intersection(adata_spatial.var_names)\n    adata_ref = adata_ref[:, common_genes].copy()\n    adata_spatial = adata_spatial[:, common_genes].copy()\n\n    # Ensure counts are stored\n    if \"counts\" not in adata_ref.layers:\n        adata_ref.layers[\"counts\"] = adata_ref.X.copy()\n    if \"counts\" not in adata_spatial.layers:\n        adata_spatial.layers[\"counts\"] = adata_spatial.X.copy()\n\n    # Train reference model\n    scvi.model.CondSCVI.setup_anndata(\n        adata_ref,\n        layer=\"counts\",\n        labels_key=cell_type_key\n    )\n\n    ref_model = scvi.model.CondSCVI(adata_ref, n_latent=n_latent)\n    ref_model.train(max_epochs=max_epochs_ref)\n\n    # Train spatial model\n    scvi.model.DestVI.setup_anndata(adata_spatial, layer=\"counts\")\n\n    spatial_model = scvi.model.DestVI.from_rna_model(\n        adata_spatial,\n        ref_model\n    )\n    spatial_model.train(max_epochs=max_epochs_spatial)\n\n    # Get proportions\n    proportions = spatial_model.get_proportions()\n\n    cell_types = adata_ref.obs[cell_type_key].unique()\n    for ct in cell_types:\n        adata_spatial.obs[f'prop_{ct}'] = proportions[ct]\n\n    # Add dominant type\n    prop_cols = [f'prop_{ct}' for ct in cell_types]\n    adata_spatial.obs['dominant_type'] = adata_spatial.obs[prop_cols].idxmax(axis=1)\n    adata_spatial.obs['dominant_type'] = adata_spatial.obs['dominant_type'].str.replace('prop_', '')\n\n    return adata_spatial, ref_model, spatial_model\n\n# Usage\nadata_spatial, ref_model, spatial_model = deconvolve_spatial(\n    adata_spatial,\n    adata_sc,\n    cell_type_key=\"cell_type\"\n)\n\n# Visualize\nsq.pl.spatial_scatter(\n    adata_spatial,\n    color=['dominant_type', 'prop_T_cell', 'prop_Tumor'],\n    ncols=3\n)\n```\n\n---\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Few common genes | Different gene naming | Convert gene names (Ensembl ↔ Symbol) |\n| Poor deconvolution | Reference doesn't match | Use tissue-matched reference |\n| All spots same type | Over-smoothing | Adjust model parameters, check reference diversity |\n| NaN proportions | Missing cell types | Ensure all expected types in reference |\n| Training slow | Large spatial dataset | Reduce max_epochs, increase batch_size |\n\n## Key References\n\n- Lopez et al. (2022) \"DestVI identifies continuums of cell types in spatial transcriptomics data\"\n- [scvi-tools spatial tutorials](https://docs.scvi-tools.org/en/stable/tutorials/index.html)\n", "depth": 5}, {"id": "148", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "149", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/scripts", "name": "scripts", "type": "dir", "content": "", "depth": 4}, {"id": "150", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: nextflow-development\ndescription: Run nf-core bioinformatics pipelines (rnaseq, sarek, atacseq) on sequencing data. Use when analyzing RNA-seq, WGS/WES, or ATAC-seq data—either local FASTQs or public datasets from GEO/SRA. Triggers on nf-core, Nextflow, FASTQ analysis, variant calling, gene expression, differential expression, GEO reanalysis, GSE/GSM/SRR accessions, or samplesheet creation.\n---\n\n# nf-core Pipeline Deployment\n\nRun nf-core bioinformatics pipelines on local or public sequencing data.\n\n**Target users:** Bench scientists and researchers without specialized bioinformatics training who need to run large-scale omics analyses—differential expression, variant calling, or chromatin accessibility analysis.\n\n## Workflow Checklist\n\n```\n- [ ] Step 0: Acquire data (if from GEO/SRA)\n- [ ] Step 1: Environment check (MUST pass)\n- [ ] Step 2: Select pipeline (confirm with user)\n- [ ] Step 3: Run test profile (MUST pass)\n- [ ] Step 4: Create samplesheet\n- [ ] Step 5: Configure \u0026 run (confirm genome with user)\n- [ ] Step 6: Verify outputs\n```\n\n---\n\n## Step 0: Acquire Data (GEO/SRA Only)\n\n**Skip this step if user has local FASTQ files.**\n\nFor public datasets, fetch from GEO/SRA first. See [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) for the full workflow.\n\n**Quick start:**\n\n```bash\n# 1. Get study info\npython scripts/sra_geo_fetch.py info GSE110004\n\n# 2. Download (interactive mode)\npython scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i\n\n# 3. Generate samplesheet\npython scripts/sra_geo_fetch.py samplesheet GSE110004 --fastq-dir ./fastq -o samplesheet.csv\n```\n\n**DECISION POINT:** After fetching study info, confirm with user:\n- Which sample subset to download (if multiple data types)\n- Suggested genome and pipeline\n\nThen continue to Step 1.\n\n---\n\n## Step 1: Environment Check\n\n**Run first. Pipeline will fail without passing environment.**\n\n```bash\npython scripts/check_environment.py\n```\n\nAll critical checks must pass. If any fail, provide fix instructions:\n\n### Docker issues\n\n| Problem | Fix |\n|---------|-----|\n| Not installed | Install from https://docs.docker.com/get-docker/ |\n| Permission denied | `sudo usermod -aG docker $USER` then re-login |\n| Daemon not running | `sudo systemctl start docker` |\n\n### Nextflow issues\n\n| Problem | Fix |\n|---------|-----|\n| Not installed | `curl -s https://get.nextflow.io \\| bash \u0026\u0026 mv nextflow ~/bin/` |\n| Version \u003c 23.04 | `nextflow self-update` |\n\n### Java issues\n\n| Problem | Fix |\n|---------|-----|\n| Not installed / \u003c 11 | `sudo apt install openjdk-11-jdk` |\n\n**Do not proceed until all checks pass.** For HPC/Singularity, see [references/troubleshooting.md](references/troubleshooting.md).\n\n---\n\n## Step 2: Select Pipeline\n\n**DECISION POINT: Confirm with user before proceeding.**\n\n| Data Type | Pipeline | Version | Goal |\n|-----------|----------|---------|------|\n| RNA-seq | `rnaseq` | 3.22.2 | Gene expression |\n| WGS/WES | `sarek` | 3.7.1 | Variant calling |\n| ATAC-seq | `atacseq` | 2.1.2 | Chromatin accessibility |\n\nAuto-detect from data:\n```bash\npython scripts/detect_data_type.py /path/to/data\n```\n\nFor pipeline-specific details:\n- [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md)\n- [references/pipelines/sarek.md](references/pipelines/sarek.md)\n- [references/pipelines/atacseq.md](references/pipelines/atacseq.md)\n\n---\n\n## Step 3: Run Test Profile\n\n**Validates environment with small data. MUST pass before real data.**\n\n```bash\nnextflow run nf-core/\u003cpipeline\u003e -r \u003cversion\u003e -profile test,docker --outdir test_output\n```\n\n| Pipeline | Command |\n|----------|---------|\n| rnaseq | `nextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq` |\n| sarek | `nextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek` |\n| atacseq | `nextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq` |\n\nVerify:\n```bash\nls test_output/multiqc/multiqc_report.html\ngrep \"Pipeline completed successfully\" .nextflow.log\n```\n\nIf test fails, see [references/troubleshooting.md](references/troubleshooting.md).\n\n---\n\n## Step 4: Create Samplesheet\n\n### Generate automatically\n\n```bash\npython scripts/generate_samplesheet.py /path/to/data \u003cpipeline\u003e -o samplesheet.csv\n```\n\nThe script:\n- Discovers FASTQ/BAM/CRAM files\n- Pairs R1/R2 reads\n- Infers sample metadata\n- Validates before writing\n\n**For sarek:** Script prompts for tumor/normal status if not auto-detected.\n\n### Validate existing samplesheet\n\n```bash\npython scripts/generate_samplesheet.py --validate samplesheet.csv \u003cpipeline\u003e\n```\n\n### Samplesheet formats\n\n**rnaseq:**\n```csv\nsample,fastq_1,fastq_2,strandedness\nSAMPLE1,/abs/path/R1.fq.gz,/abs/path/R2.fq.gz,auto\n```\n\n**sarek:**\n```csv\npatient,sample,lane,fastq_1,fastq_2,status\npatient1,tumor,L001,/abs/path/tumor_R1.fq.gz,/abs/path/tumor_R2.fq.gz,1\npatient1,normal,L001,/abs/path/normal_R1.fq.gz,/abs/path/normal_R2.fq.gz,0\n```\n\n**atacseq:**\n```csv\nsample,fastq_1,fastq_2,replicate\nCONTROL,/abs/path/ctrl_R1.fq.gz,/abs/path/ctrl_R2.fq.gz,1\n```\n\n---\n\n## Step 5: Configure \u0026 Run\n\n### 5a. Check genome availability\n\n```bash\npython scripts/manage_genomes.py check \u003cgenome\u003e\n# If not installed:\npython scripts/manage_genomes.py download \u003cgenome\u003e\n```\n\nCommon genomes: GRCh38 (human), GRCh37 (legacy), GRCm39 (mouse), R64-1-1 (yeast), BDGP6 (fly)\n\n### 5b. Decision points\n\n**DECISION POINT: Confirm with user:**\n\n1. **Genome:** Which reference to use\n2. **Pipeline-specific options:**\n   - **rnaseq:** aligner (star_salmon recommended, hisat2 for low memory)\n   - **sarek:** tools (haplotypecaller for germline, mutect2 for somatic)\n   - **atacseq:** read_length (50, 75, 100, or 150)\n\n### 5c. Run pipeline\n\n```bash\nnextflow run nf-core/\u003cpipeline\u003e \\\n    -r \u003cversion\u003e \\\n    -profile docker \\\n    --input samplesheet.csv \\\n    --outdir results \\\n    --genome \u003cgenome\u003e \\\n    -resume\n```\n\n**Key flags:**\n- `-r`: Pin version\n- `-profile docker`: Use Docker (or `singularity` for HPC)\n- `--genome`: iGenomes key\n- `-resume`: Continue from checkpoint\n\n**Resource limits (if needed):**\n```bash\n--max_cpus 8 --max_memory '32.GB' --max_time '24.h'\n```\n\n---\n\n## Step 6: Verify Outputs\n\n### Check completion\n\n```bash\nls results/multiqc/multiqc_report.html\ngrep \"Pipeline completed successfully\" .nextflow.log\n```\n\n### Key outputs by pipeline\n\n**rnaseq:**\n- `results/star_salmon/salmon.merged.gene_counts.tsv` - Gene counts\n- `results/star_salmon/salmon.merged.gene_tpm.tsv` - TPM values\n\n**sarek:**\n- `results/variant_calling/*/` - VCF files\n- `results/preprocessing/recalibrated/` - BAM files\n\n**atacseq:**\n- `results/macs2/narrowPeak/` - Peak calls\n- `results/bwa/mergedLibrary/bigwig/` - Coverage tracks\n\n---\n\n## Quick Reference\n\nFor common exit codes and fixes, see [references/troubleshooting.md](references/troubleshooting.md).\n\n### Resume failed run\n\n```bash\nnextflow run nf-core/\u003cpipeline\u003e -resume\n```\n\n---\n\n## References\n\n- [references/geo-sra-acquisition.md](references/geo-sra-acquisition.md) - Downloading public GEO/SRA data\n- [references/troubleshooting.md](references/troubleshooting.md) - Common issues and fixes\n- [references/installation.md](references/installation.md) - Environment setup\n- [references/pipelines/rnaseq.md](references/pipelines/rnaseq.md) - RNA-seq pipeline details\n- [references/pipelines/sarek.md](references/pipelines/sarek.md) - Variant calling details\n- [references/pipelines/atacseq.md](references/pipelines/atacseq.md) - ATAC-seq details\n\n---\n\n## Disclaimer\n\nThis skill is provided as a prototype example demonstrating how to integrate nf-core bioinformatics pipelines into Claude Code for automated analysis workflows. The current implementation supports three pipelines (rnaseq, sarek, and atacseq), serving as a foundation that enables the community to expand support to the full set of nf-core pipelines.\n\nIt is intended for educational and research purposes and should not be considered production-ready without appropriate validation for your specific use case. Users are responsible for ensuring their computing environment meets pipeline requirements and for verifying analysis results.\n\nAnthropic does not guarantee the accuracy of bioinformatics outputs, and users should follow standard practices for validating computational analyses. This integration is not officially endorsed by or affiliated with the nf-core community.\n\n## Attribution\n\nWhen publishing results, cite the appropriate pipeline. Citations are available in each nf-core repository's CITATIONS.md file (e.g., https://github.com/nf-core/rnaseq/blob/3.22.2/CITATIONS.md).\n\n## Licenses\n\n- **nf-core pipelines:** MIT License (https://nf-co.re/about)\n- **Nextflow:** Apache License, Version 2.0 (https://www.nextflow.io/about-us.html)\n- **NCBI SRA Toolkit:** Public Domain (https://github.com/ncbi/sra-tools/blob/master/LICENSE)\n", "depth": 4}, {"id": "151", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/LICENSE.txt", "name": "LICENSE.txt", "type": "file", "content": "Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction,\nand distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\nthe copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all\nother entities that control, are controlled by, or are under common\ncontrol with that entity. For the purposes of this definition,\n\"control\" means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or\notherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\nexercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications,\nincluding but not limited to software source code, documentation\nsource, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical\ntransformation or translation of a Source form, including but\nnot limited to compiled object code, generated documentation,\nand conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or\nObject form, made available under the License, as indicated by a\ncopyright notice that is included in or attached to the work\n(an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\nform, that is based on (or derived from) the Work and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship. For the purposes\nof this License, Derivative Works shall not include works that remain\nseparable from, or merely link (or bind by name) to the interfaces of,\nthe Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including\nthe original version of the Work and any modifications or additions\nto that Work or Derivative Works thereof, that is intentionally\nsubmitted to Licensor for inclusion in the Work by the copyright owner\nor by an individual or Legal Entity authorized to submit on behalf of\nthe copyright owner. For the purposes of this definition, \"submitted\"\nmeans any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems,\nand issue tracking systems that are managed by, or on behalf of, the\nLicensor for the purpose of discussing and improving the Work, but\nexcluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\non behalf of whom a Contribution has been received by Licensor and\nsubsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\ncopyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the\nWork and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\n(except as stated in this section) patent license to make, have made,\nuse, offer to sell, sell, import, and otherwise transfer the Work,\nwhere such license applies only to those patent claims licensable\nby such Contributor that are necessarily infringed by their\nContribution(s) alone or by combination of their Contribution(s)\nwith the Work to which such Contribution(s) was submitted. If You\ninstitute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work\nor a Contribution incorporated within the Work constitutes direct\nor contributory patent infringement, then any patent licenses\ngranted to You under this License for that Work shall terminate\nas of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\nWork or Derivative Works thereof in any medium, with or without\nmodifications, and in Source or Object form, provided that You\nmeet the following conditions:\n\n(a) You must give any other recipients of the Work or\nDerivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices\nstating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works\nthat You distribute, all copyright, patent, trademark, and\nattribution notices from the Source form of the Work,\nexcluding those notices that do not pertain to any part of\nthe Derivative Works; and\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\ndistribution, then any Derivative Works that You distribute must\ninclude a readable copy of the attribution notices contained\nwithin such NOTICE file, excluding those notices that do not\npertain to any part of the Derivative Works, in at least one\nof the following places: within a NOTICE text file distributed\nas part of the Derivative Works; within the Source form or\ndocumentation, if provided along with the Derivative Works; or,\nwithin a display generated by the Derivative Works, if and\nwherever such third-party notices normally appear. The contents\nof the NOTICE file are for informational purposes only and\ndo not modify the License. You may add Your own attribution\nnotices within Derivative Works that You distribute, alongside\nor as an addendum to the NOTICE text from the Work, provided\nthat such additional attribution notices cannot be construed\nas modifying the License.\n\nYou may add Your own copyright statement to Your modifications and\nmay provide additional or different license terms and conditions\nfor use, reproduction, or distribution of Your modifications, or\nfor any such Derivative Works as a whole, provided Your use,\nreproduction, and distribution of the Work otherwise complies with\nthe conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\nany Contribution intentionally submitted for inclusion in the Work\nby You to the Licensor shall be under the terms and conditions of\nthis License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify\nthe terms of any separate license agreement you may have executed\nwith Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\nnames, trademarks, service marks, or product names of the Licensor,\nexcept as required for reasonable and customary use in describing the\norigin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\nagreed to in writing, Licensor provides the Work (and each\nContributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied, including, without limitation, any warranties or conditions\nof TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\nPARTICULAR PURPOSE. You are solely responsible for determining the\nappropriateness of using or redistributing the Work and assume any\nrisks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\nwhether in tort (including negligence), contract, or otherwise,\nunless required by applicable law (such as deliberate and grossly\nnegligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special,\nincidental, or consequential damages of any character arising as a\nresult of this License or out of the use or inability to use the\nWork (including but not limited to damages for loss of goodwill,\nwork stoppage, computer failure or malfunction, or any and all\nother commercial damages or losses), even if such Contributor\nhas been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\nthe Work or Derivative Works thereof, You may choose to offer,\nand charge a fee for, acceptance of support, warranty, indemnity,\nor other liability obligations and/or rights consistent with this\nLicense. However, in accepting such obligations, You may act only\non Your own behalf and on Your sole responsibility, not on behalf\nof any other Contributor, and only if You agree to indemnify,\ndefend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason\nof your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\nTo apply the Apache License to your work, attach the following\nboilerplate notice, with the fields enclosed by brackets \"[]\"\nreplaced with your own identifying information. (Don't include\nthe brackets!) The text should be enclosed in the appropriate\ncomment syntax for the file format. We also recommend that a\nfile or class name and description of purpose be included on the\nsame \"printed page\" as the copyright notice for easier\nidentification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", "depth": 4}, {"id": "152", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/pipelines", "name": "pipelines", "type": "dir", "content": "", "depth": 5}, {"id": "153", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/troubleshooting.md", "name": "troubleshooting.md", "type": "file", "content": "# Troubleshooting\n\nQuick fixes for common nf-core pipeline issues.\n\n## Contents\n- [Exit Codes](#exit-codes)\n- [HPC/Singularity Issues](#hpcsingularity-issues)\n- [Pipeline Failures](#pipeline-failures)\n- [RNA-seq Specific](#rna-seq-specific)\n- [Sarek Specific](#sarek-specific)\n- [ATAC-seq Specific](#atac-seq-specific)\n- [Resource Management](#resource-management)\n- [Getting Help](#getting-help)\n\n## Exit Codes\n\nCommon exit codes indicating resource issues (per [nf-core docs](https://nf-co.re/docs/usage/troubleshooting/crash_halfway)):\n\n| Code | Cause | Fix |\n|------|-------|-----|\n| 137 | Out of memory | `--max_memory '32.GB'` or `'64.GB'` for WGS |\n| 143 | Out of memory | `--max_memory '32.GB'` or `'64.GB'` for WGS |\n| 104, 134, 139, 247 | Out of memory | Increase `--max_memory` |\n| 1 | General error | Check `.nextflow.log` for details |\n\nMost pipelines auto-retry with 2x then 3x resources before failing.\n\n## HPC/Singularity Issues\n\n### Singularity cache issues\n```bash\nexport NXF_SINGULARITY_CACHEDIR=\"$HOME/.singularity/cache\"\nmkdir -p $NXF_SINGULARITY_CACHEDIR\n```\n\n### Using Singularity instead of Docker\nOn HPC systems without Docker, use Singularity:\n```bash\nnextflow run nf-core/\u003cpipeline\u003e -profile singularity ...\n```\n\n\u003e **Note**: For basic environment setup (Docker, Nextflow, Java installation), see the inline instructions in Step 1 of SKILL.md.\n\n## Pipeline Failures\n\n### Container pull failed\n- Check network connectivity\n- Try: `-profile singularity` instead of docker\n- For offline: `nf-core download \u003cpipeline\u003e -r \u003cversion\u003e`\n\n### \"No such file\" errors\n- Use **absolute paths** in samplesheet\n- Verify files exist: `ls /path/to/file`\n\n### Resume not working\n```bash\n# Check work directory exists\nls -la work/\n\n# Force clean restart (loses cache)\nrm -rf work/ .nextflow*\nnextflow run nf-core/\u003cpipeline\u003e ...\n```\n\n## RNA-seq Specific\n\n### STAR index fails\n- Increase memory: `--max_memory '64.GB'`\n- Or provide pre-built: `--star_index /path/to/star/`\n\n### Low alignment rate\n- Verify genome matches species\n- Check FastQC for adapter contamination\n- Try different aligner: `--aligner hisat2`\n\n### Strandedness detection fails\n- Specify explicitly: `--strandedness reverse`\n- Common values: `forward`, `reverse`, `unstranded`\n\n## Sarek Specific\n\n### BQSR fails\n- Check known sites for genome\n- Skip for non-standard references: `--skip_bqsr`\n\n### Mutect2 no variants\n- Verify tumor/normal pairing\n- Check samplesheet `status` column: 0=normal, 1=tumor\n\n### Out of memory for WGS\n```bash\n--max_memory '128.GB' --max_cpus 16\n```\n\n### DeepVariant GPU issues\n- Ensure NVIDIA Docker runtime configured\n- Or use CPU mode (slower)\n\n## ATAC-seq Specific\n\n### Low FRiP score\n- Check library complexity in `plotFingerprint/`\n- May indicate over-transposition\n\n### Few peaks called\n- Lower threshold: `--macs_qvalue 0.1`\n- Use broad peaks: `--narrow_peak false`\n\n### High duplicates\n- Normal for low-input samples\n- Pipeline removes by default\n- Consider deeper sequencing\n\n## Resource Management\n\n### Set resource limits\n```bash\n--max_cpus 8 --max_memory '32.GB' --max_time '24.h'\n```\n\n### Check available resources\n```bash\n# CPUs\nnproc\n\n# Memory\nfree -h\n\n# Disk\ndf -h .\n```\n\n## Getting Help\n\n1. Check `.nextflow.log` for error details\n2. Search nf-core Slack: https://nf-co.re/join\n3. Open issue on GitHub: https://github.com/nf-core/\u003cpipeline\u003e/issues\n", "depth": 5}, {"id": "154", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/installation.md", "name": "installation.md", "type": "file", "content": "# Installation\n\n## Contents\n- [Quick install](#quick-install)\n- [Docker setup](#docker-setup)\n- [Singularity setup (HPC)](#singularity-setup-hpc)\n- [nf-core tools (optional)](#nf-core-tools-optional)\n- [Verify installation](#verify-installation)\n- [Common issues](#common-issues)\n\n## Quick install\n\n```bash\n# Nextflow\ncurl -s https://get.nextflow.io | bash\nmv nextflow ~/bin/\nexport PATH=\"$HOME/bin:$PATH\"\n\n# Verify\nnextflow -version\njava -version  # Requires 11+\n```\n\n## Docker setup\n\n### Linux\n```bash\nsudo apt-get update \u0026\u0026 sudo apt-get install docker.io\nsudo systemctl enable --now docker\nsudo usermod -aG docker $USER\n# Log out and back in\n```\n\n### macOS\nDownload Docker Desktop: https://docker.com/products/docker-desktop\n\n### Verify\n```bash\ndocker run hello-world\n```\n\n## Singularity setup (HPC)\n\n```bash\n# Ubuntu/Debian\nsudo apt-get install singularity-container\n\n# Or via conda\nconda install -c conda-forge singularity\n```\n\n### Configure cache\n```bash\nexport NXF_SINGULARITY_CACHEDIR=\"$HOME/.singularity/cache\"\nmkdir -p $NXF_SINGULARITY_CACHEDIR\necho 'export NXF_SINGULARITY_CACHEDIR=\"$HOME/.singularity/cache\"' \u003e\u003e ~/.bashrc\n```\n\n## nf-core tools (optional)\n\n```bash\npip install nf-core\n```\n\nUseful commands:\n```bash\nnf-core list                    # Available pipelines\nnf-core launch rnaseq           # Interactive parameter selection\nnf-core download rnaseq -r 3.14.0  # Download for offline use\n```\n\n## Verify installation\n\n```bash\nnextflow run nf-core/demo -profile test,docker --outdir test_demo\nls test_demo/\n```\n\n## Common issues\n\n**Java version wrong:**\n```bash\nexport JAVA_HOME=/path/to/java11\n```\n\n**Docker permission denied:**\n```bash\nsudo usermod -aG docker $USER\n# Log out and back in\n```\n\n**Nextflow not found:**\n```bash\necho 'export PATH=\"$HOME/bin:$PATH\"' \u003e\u003e ~/.bashrc\nsource ~/.bashrc\n```\n", "depth": 5}, {"id": "155", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/geo-sra-acquisition.md", "name": "geo-sra-acquisition.md", "type": "file", "content": "# GEO/SRA Data Acquisition\n\nDownload raw sequencing data from NCBI GEO/SRA and prepare it for nf-core pipelines.\n\n**Use this when:** Reanalyzing published datasets, validating findings, or comparing results against public cohorts.\n\n## Table of Contents\n\n- [Workflow Overview](#workflow-overview)\n- [Step 1: Fetch Study Information](#step-1-fetch-study-information)\n- [Step 2: Review Sample Groups](#step-2-review-sample-groups)\n- [Step 3: Download FASTQ Files](#step-3-download-fastq-files)\n- [Step 4: Generate Samplesheet](#step-4-generate-samplesheet)\n- [Step 5: Run nf-core Pipeline](#step-5-run-nf-core-pipeline)\n- [Supported Pipelines](#supported-pipelines)\n- [Supported Organisms](#supported-organisms)\n- [Complete Example](#complete-example)\n- [Troubleshooting](#troubleshooting)\n\n---\n\n## Workflow Overview\n\nExample: \"Find differentially expressed genes in GSE309891 (drug-treated vs control)\"\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    GEO/SRA DATA ACQUISITION                     │\n└─────────────────────────────────────────────────────────────────┘\n                              │\n                              ▼\n                 ┌────────────────────────┐\n                 │   Fetch study info     │\n                 │   • Query NCBI/SRA     │\n                 │   • Get metadata       │\n                 │   • Detect organism    │\n                 │   • Identify data type │\n                 └────────────────────────┘\n                              │\n                              ▼\n                 ┌────────────────────────┐\n                 │   Present summary      │\n                 │   • Organism: Human    │\n                 │   • Genome: GRCh38     │\n                 │   • Type: RNA-Seq      │\n                 │   • Pipeline: rnaseq   │\n                 │   • Samples: 12        │\n                 │     (6 treated,        │\n                 │      6 control)        │\n                 │   • Size: ~24 GB       │\n                 └────────────────────────┘\n                              │\n                              ▼\n                    ┌─────────────────┐\n                    │  USER CONFIRMS  │◄──── Decision point\n                    │  genome/pipeline│\n                    └─────────────────┘\n                              │\n                              ▼\n                 ┌────────────────────────┐\n                 │   Select samples       │\n                 │   • Group by condition │\n                 │   • Show treated/ctrl  │\n                 └────────────────────────┘\n                              │\n                              ▼\n                    ┌─────────────────┐\n                    │  USER SELECTS   │◄──── Decision point\n                    │  sample subset  │\n                    └─────────────────┘\n                              │\n                              ▼\n                 ┌────────────────────────┐\n                 │   Download FASTQs      │\n                 │   • 24 files (R1+R2)   │\n                 │   • Parallel transfers │\n                 │   • Auto-resume        │\n                 └────────────────────────┘\n                              │\n                              ▼\n                 ┌────────────────────────┐\n                 │   Generate samplesheet │\n                 │   • Map SRR to files   │\n                 │   • Pair R1/R2         │\n                 │   • Assign conditions  │\n                 └────────────────────────┘\n                              │\n                              ▼\n┌─────────────────────────────────────────────────────────────────┐\n│                    NF-CORE PIPELINE EXECUTION                   │\n│              (Continue with Step 1 of main workflow)            │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Instructions for Claude\n\nWhen assisting users with GEO/SRA data acquisition:\n\n1. **Always fetch study info first** to show the user what data is available\n2. **Ask for confirmation before downloading** - Present the sample groups and sizes, then ask which subset to download using AskUserQuestion\n3. **Suggest appropriate genome and pipeline** based on the organism and data type\n4. **Return to main SKILL.md workflow** after data preparation is complete\n\nExample confirmation question:\n```\nQuestion: \"Which sample group would you like to download?\"\nOptions:\n  - \"RNA-Seq:PAIRED (42 samples, ~87 GB)\"\n  - \"RNA-Seq:SINGLE (7 samples, ~4.5 GB)\"\n  - \"All samples (49 samples, ~92 GB)\"\n```\n\n---\n\n## Step 1: Fetch Study Information\n\nGet metadata about a GEO study before downloading.\n\n```bash\npython scripts/sra_geo_fetch.py info \u003cGEO_ID\u003e\n```\n\n**Example:**\n```bash\npython scripts/sra_geo_fetch.py info GSE110004\n```\n\n**Output includes:**\n- Study title and summary\n- Organism (with auto-suggested genome)\n- Number of samples and runs\n- Data types (RNA-Seq, ATAC-seq, etc.)\n- Estimated download size\n- Suggested nf-core pipeline\n\n**Save info to JSON:**\n```bash\npython scripts/sra_geo_fetch.py info GSE110004 -o study_info.json\n```\n\n---\n\n## Step 2: Review Sample Groups\n\nView sample groups organized by data type and layout. This is useful for studies with mixed data types.\n\n```bash\npython scripts/sra_geo_fetch.py groups \u003cGEO_ID\u003e\n```\n\n**Example output:**\n```\nSample Group          Count Layout     GSM Range                    Est. Size\n--------------------------------------------------------------------------------\nRNA-Seq                  42 PAIRED     GSM2879618...(42 samples)      87.4 GB\nRNA-Seq                   7 SINGLE     GSM2976181-GSM2976187           4.5 GB\n--------------------------------------------------------------------------------\nTOTAL                    49                                           91.9 GB\n\nAvailable groups for --subset option:\n  1. \"RNA-Seq:PAIRED\" - 42 samples (~87.4 GB)\n  2. \"RNA-Seq:SINGLE\" - 7 samples (~4.5 GB)\n```\n\n**List individual runs:**\n```bash\npython scripts/sra_geo_fetch.py list \u003cGEO_ID\u003e\n\n# Filter by data type\npython scripts/sra_geo_fetch.py list GSE110004 --filter \"RNA-Seq:PAIRED\"\n```\n\n**DECISION POINT:** Review the sample groups. Decide which subset to download if the study has multiple data types.\n\n---\n\n## Step 3: Download FASTQ Files\n\nDownload FASTQ files from ENA (faster than SRA).\n\n```bash\npython scripts/sra_geo_fetch.py download \u003cGEO_ID\u003e -o \u003cOUTPUT_DIR\u003e\n```\n\n**Options:**\n- `-o, --output`: Output directory (required)\n- `-i, --interactive`: Interactively select sample group to download\n- `-s, --subset`: Filter by data type (e.g., \"RNA-Seq:PAIRED\")\n- `-p, --parallel`: Parallel downloads (default: 4)\n- `-t, --timeout`: Download timeout in seconds (default: 600)\n\n### Interactive Mode (Recommended)\n\nUse `-i` flag for interactive sample selection when the study has multiple data types:\n\n```bash\npython scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i\n```\n\n**Interactive output:**\n```\n============================================================\n  SELECT SAMPLE GROUP TO DOWNLOAD\n============================================================\n\n  [1] RNA-Seq (paired)\n      Samples: 42\n      GSM: GSM2879618...(42 samples)\n      Size: ~87.4 GB\n\n  [2] RNA-Seq (single)\n      Samples: 7\n      GSM: GSM2976181-GSM2976187\n      Size: ~4.5 GB\n\n  [0] Download ALL (49 samples)\n------------------------------------------------------------\n\nEnter selection (0-2):\n```\n\n### Direct Subset Selection\n\nAlternatively, specify the subset directly:\n\n```bash\n# Download only RNA-Seq paired-end data\npython scripts/sra_geo_fetch.py download GSE110004 -o ./fastq \\\n    --subset \"RNA-Seq:PAIRED\" --parallel 6\n```\n\n**Note:** Downloads automatically skip existing files. Resume interrupted downloads by re-running the command.\n\n---\n\n## Step 4: Generate Samplesheet\n\nCreate a samplesheet compatible with nf-core pipelines.\n\n```bash\npython scripts/sra_geo_fetch.py samplesheet \u003cGEO_ID\u003e \\\n    --fastq-dir \u003cFASTQ_DIR\u003e \\\n    -o samplesheet.csv\n```\n\n**Options:**\n- `-f, --fastq-dir`: Directory containing downloaded FASTQ files (required)\n- `-o, --output`: Output samplesheet path (default: samplesheet.csv)\n- `-p, --pipeline`: Target pipeline (auto-detected if not specified)\n\n**Example:**\n```bash\npython scripts/sra_geo_fetch.py samplesheet GSE110004 \\\n    --fastq-dir ./fastq \\\n    -o samplesheet.csv\n```\n\n**Output:** The script will:\n1. Create samplesheet in the format required by the target pipeline\n2. Display suggested genome reference\n3. Show suggested nf-core command\n\n---\n\n## Step 5: Run nf-core Pipeline\n\nAfter generating the samplesheet, the script provides a suggested command.\n\n**Example output:**\n```\nSuggested command:\n   nextflow run nf-core/rnaseq \\\n       --input samplesheet.csv \\\n       --outdir results \\\n       --genome R64-1-1 \\\n       -profile docker\n```\n\n**DECISION POINT:** Review and confirm:\n1. Is the suggested pipeline correct?\n2. Is the genome reference correct for your organism?\n3. Do you need additional pipeline options?\n\nThen return to the main SKILL.md workflow (Step 1: Environment Check) to proceed with pipeline execution.\n\n---\n\n## Supported Pipelines\n\nThe skill auto-detects appropriate pipelines based on library strategy. Pipelines marked with ★ are fully supported with configs, samplesheet generation, and documentation. Others are suggested but require manual setup following nf-core documentation.\n\n| Library Strategy | Suggested Pipeline | Support |\n|------------------|--------------------|---------|\n| RNA-Seq          | nf-core/rnaseq     | ★ Full  |\n| ATAC-seq         | nf-core/atacseq    | ★ Full  |\n| WGS/WXS          | nf-core/sarek      | ★ Full  |\n| ChIP-seq         | nf-core/chipseq    | Manual  |\n| Bisulfite-Seq    | nf-core/methylseq  | Manual  |\n| miRNA-Seq        | nf-core/smrnaseq   | Manual  |\n| Amplicon         | nf-core/ampliseq   | Manual  |\n\n---\n\n## Supported Organisms\n\nCommon organisms with auto-suggested genomes:\n\n| Organism | Genome | Notes |\n|----------|--------|-------|\n| Homo sapiens | GRCh38 | Human reference |\n| Mus musculus | GRCm39 | Mouse reference |\n| Saccharomyces cerevisiae | R64-1-1 | Yeast S288C |\n| Drosophila melanogaster | BDGP6 | Fruit fly |\n| Caenorhabditis elegans | WBcel235 | C. elegans |\n| Danio rerio | GRCz11 | Zebrafish |\n| Arabidopsis thaliana | TAIR10 | Arabidopsis |\n| Rattus norvegicus | Rnor_6.0 | Rat |\n\nSee `scripts/config/genomes.yaml` for the full list.\n\n---\n\n## Complete Example\n\nReanalyze GSE110004 (yeast RNA-seq):\n\n```bash\n# 1. Get study info and sample groups\npython scripts/sra_geo_fetch.py info GSE110004\n\n# 2. Download with interactive selection\npython scripts/sra_geo_fetch.py download GSE110004 -o ./fastq -i\n# Select option [1] for RNA-Seq paired-end samples\n\n# 3. Generate samplesheet\npython scripts/sra_geo_fetch.py samplesheet GSE110004 \\\n    --fastq-dir ./fastq \\\n    -o samplesheet.csv\n\n# 4. Run nf-core/rnaseq (continue with main SKILL.md workflow)\nnextflow run nf-core/rnaseq \\\n    --input samplesheet.csv \\\n    --outdir results \\\n    --genome R64-1-1 \\\n    -profile docker\n```\n\n### Alternative: Non-interactive Download\n\n```bash\n# Review sample groups first\npython scripts/sra_geo_fetch.py groups GSE110004\n\n# Download specific subset directly\npython scripts/sra_geo_fetch.py download GSE110004 \\\n    --subset \"RNA-Seq:PAIRED\" \\\n    -o ./fastq \\\n    --parallel 4\n```\n\n---\n\n## Troubleshooting\n\n### ENA Download Fails\nIf ENA downloads fail, the data may need to be fetched directly from SRA:\n\n```bash\n# Create SRA tools environment\nconda create -n sra_tools -c bioconda sra-tools\n\n# Download with prefetch + fasterq-dump\nconda run -n sra_tools prefetch SRR6357070\nconda run -n sra_tools fasterq-dump SRR6357070 -O ./fastq\n```\n\n### No SRA Runs Found\nSome GEO datasets only have processed data, not raw sequencing reads. Check:\n```bash\npython scripts/sra_geo_fetch.py info \u003cGEO_ID\u003e\n```\nIf \"Runs: 0\", the dataset may not have raw data in SRA.\n\n### SuperSeries Support\nGEO SuperSeries (which contain multiple SubSeries) are automatically handled. The tool will:\n1. Detect that a GEO ID is a SuperSeries\n2. Find the linked BioProject accession\n3. Fetch all SRA runs from the BioProject\n\nExample: GSE110004 is a SuperSeries that links to BioProject PRJNA432544.\n\n### Genome Not Recognized\nIf the organism is not in the genome mapping, manually specify the genome:\n```bash\n# Check available iGenomes\npython scripts/manage_genomes.py list\n\n# Or provide custom reference files to nf-core\nnextflow run nf-core/rnaseq --fasta /path/to/genome.fa --gtf /path/to/genes.gtf\n```\n\n---\n\n## Requirements\n\n- Python 3.8+\n- `requests` library (optional but recommended)\n- `pyyaml` library (optional, for genome config)\n- Network access to NCBI and ENA\n\nInstall optional dependencies:\n```bash\npip install requests pyyaml\n```\n", "depth": 5}, {"id": "156", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/pipelines/atacseq.md", "name": "atacseq.md", "type": "file", "content": "# nf-core/atacseq\n\n**Version:** 2.1.2\n\n**Official Documentation:** https://nf-co.re/atacseq/2.1.2/\n**GitHub:** https://github.com/nf-core/atacseq\n\n\u003e **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/atacseq/releases) for breaking changes and update the version in commands below.\n\n## Contents\n- [Test command](#test-command)\n- [Samplesheet format](#samplesheet-format)\n- [Parameters](#parameters)\n- [Output files](#output-files)\n- [Quality metrics](#quality-metrics)\n\n## Test command\n\n```bash\nnextflow run nf-core/atacseq -r 2.1.2 -profile test,docker --outdir test_atacseq\n```\n\nExpected: ~15 min, creates peaks and BigWig tracks.\n\n## Samplesheet format\n\n```csv\nsample,fastq_1,fastq_2,replicate\nCONTROL,/path/to/ctrl_rep1_R1.fq.gz,/path/to/ctrl_rep1_R2.fq.gz,1\nCONTROL,/path/to/ctrl_rep2_R1.fq.gz,/path/to/ctrl_rep2_R2.fq.gz,2\nTREATMENT,/path/to/treat_rep1_R1.fq.gz,/path/to/treat_rep1_R2.fq.gz,1\nTREATMENT,/path/to/treat_rep2_R1.fq.gz,/path/to/treat_rep2_R2.fq.gz,2\n```\n\n| Column | Required | Description |\n|--------|----------|-------------|\n| sample | Yes | Condition/group identifier |\n| fastq_1 | Yes | Absolute path to R1 |\n| fastq_2 | Yes | Absolute path to R2 (paired-end required) |\n| replicate | Yes | Replicate number (integer) |\n\n### Design file for differential analysis\n```csv\nsample,condition\nCONTROL,control\nTREATMENT,treatment\n```\n\nUse with `--deseq2_design design.csv`.\n\n## Parameters\n\n### Minimal run\n```bash\nnextflow run nf-core/atacseq -r 2.1.2 -profile docker \\\n    --input samplesheet.csv --outdir results --genome GRCh38 --read_length 50\n```\n\n### Common parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `--genome` | - | `GRCh38`, `GRCh37`, `mm10` |\n| `--read_length` | 50 | Read length for MACS2 optimization |\n| `--narrow_peak` | true | Narrow peaks (false for broad) |\n| `--mito_name` | chrM | Mitochondrial chromosome name |\n| `--keep_mito` | false | Keep mitochondrial reads |\n| `--min_reps_consensus` | 1 | Min replicates for consensus peaks |\n\n### Differential accessibility\n```bash\n--deseq2_design design.csv\n```\n\n## Output files\n\n```\nresults/\n├── bwa/mergedLibrary/\n│   ├── *.mLb.mkD.sorted.bam     # Filtered, deduplicated alignments\n│   └── bigwig/\n│       └── *.bigWig             # Coverage tracks\n├── macs2/narrowPeak/\n│   ├── *.narrowPeak             # Peak calls\n│   └── consensus/\n│       └── consensus_peaks.bed  # Merged peaks across replicates\n├── deeptools/\n│   ├── plotFingerprint/         # Library complexity\n│   └── plotProfile/             # TSS enrichment\n├── deseq2/                      # If --deseq2_design provided\n└── multiqc/\n```\n\n**Key outputs:**\n- `*.mLb.mkD.sorted.bam`: Analysis-ready alignments\n- `*.narrowPeak`: MACS2 peak calls (BED format)\n- `consensus_peaks.bed`: Consensus peaks across replicates\n- `*.bigWig`: Genome browser tracks\n\n## Quality metrics\n\n| Metric | Good | Acceptable | Poor |\n|--------|------|------------|------|\n| Mapped reads | \u003e80% | 60-80% | \u003c60% |\n| Mitochondrial | \u003c20% | 20-40% | \u003e40% |\n| Duplicates | \u003c30% | 30-50% | \u003e50% |\n| FRiP | \u003e30% | 15-30% | \u003c15% |\n| TSS enrichment | \u003e6 | 4-6 | \u003c4 |\n\n**Fragment size**: Should show nucleosomal periodicity (~50bp nucleosome-free, ~200bp mono-nucleosome).\n\n## Downstream analysis\n\n```r\nlibrary(ChIPseeker)\nlibrary(GenomicRanges)\npeaks \u003c- import(\"consensus_peaks.bed\")\npeakAnno \u003c- annotatePeak(peaks, TxDb = TxDb.Hsapiens.UCSC.hg38.knownGene)\n```\n\n**Motif analysis:**\n```bash\nfindMotifsGenome.pl consensus_peaks.bed hg38 motifs/ -size 200\n```\n\n## Troubleshooting\n\n**Low FRiP**: Check library complexity in `plotFingerprint/`. May indicate over-transposition.\n\n**Few peaks**: Lower threshold with `--macs_qvalue 0.1` or use `--narrow_peak false` for broader peaks.\n\n**High duplicates**: Normal for low-input; pipeline removes by default.\n\n## More Information\n\n- **Full parameter list:** https://nf-co.re/atacseq/2.1.2/parameters/\n- **Output documentation:** https://nf-co.re/atacseq/2.1.2/docs/output/\n- **Usage documentation:** https://nf-co.re/atacseq/2.1.2/docs/usage/\n", "depth": 6}, {"id": "157", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/pipelines/sarek.md", "name": "sarek.md", "type": "file", "content": "# nf-core/sarek\n\n**Version:** 3.7.1\n\n**Official Documentation:** https://nf-co.re/sarek/3.7.1/\n**GitHub:** https://github.com/nf-core/sarek\n\n\u003e **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/sarek/releases) for breaking changes and update the version in commands below.\n\n## Contents\n- [Test command](#test-command)\n- [Samplesheet format](#samplesheet-format)\n- [Variant calling modes](#variant-calling-modes)\n- [Parameters](#parameters)\n- [Output files](#output-files)\n\n## Test command\n\n```bash\nnextflow run nf-core/sarek -r 3.7.1 -profile test,docker --outdir test_sarek\n```\n\nExpected: ~20 min, creates aligned BAMs and variant calls.\n\n## Samplesheet format\n\n### From FASTQ\n```csv\npatient,sample,lane,fastq_1,fastq_2\npatient1,tumor,L001,/path/to/tumor_L001_R1.fq.gz,/path/to/tumor_L001_R2.fq.gz\npatient1,tumor,L002,/path/to/tumor_L002_R1.fq.gz,/path/to/tumor_L002_R2.fq.gz\npatient1,normal,L001,/path/to/normal_R1.fq.gz,/path/to/normal_R2.fq.gz\n```\n\n### From BAM/CRAM\n```csv\npatient,sample,bam,bai\npatient1,tumor,/path/to/tumor.bam,/path/to/tumor.bam.bai\npatient1,normal,/path/to/normal.bam,/path/to/normal.bam.bai\n```\n\n### With tumor/normal status\n```csv\npatient,sample,lane,fastq_1,fastq_2,status\npatient1,tumor,L001,tumor_R1.fq.gz,tumor_R2.fq.gz,1\npatient1,normal,L001,normal_R1.fq.gz,normal_R2.fq.gz,0\n```\n\n`status`: 0 = normal, 1 = tumor\n\n## Variant calling modes\n\n### Germline (single sample)\n```bash\nnextflow run nf-core/sarek -r 3.7.1 -profile docker \\\n    --input samplesheet.csv --outdir results --genome GRCh38 \\\n    --tools haplotypecaller,snpeff\n```\n\n### Somatic (tumor-normal pair)\n```bash\nnextflow run nf-core/sarek -r 3.7.1 -profile docker \\\n    --input samplesheet.csv --outdir results --genome GRCh38 \\\n    --tools mutect2,strelka,snpeff\n```\n\n### WES (exome)\n```bash\nnextflow run nf-core/sarek -r 3.7.1 -profile docker \\\n    --input samplesheet.csv --outdir results --genome GRCh38 \\\n    --wes --intervals /path/to/targets.bed \\\n    --tools haplotypecaller,snpeff\n```\n\n### Joint germline (cohort)\n```bash\n--tools haplotypecaller --joint_germline\n```\n\n## Parameters\n\n### Available tools\n\n**Germline callers:**\n- `haplotypecaller`: GATK HaplotypeCaller\n- `freebayes`: FreeBayes\n- `deepvariant`: DeepVariant (GPU optional)\n- `strelka`: Strelka2 germline\n\n**Somatic callers:**\n- `mutect2`: GATK Mutect2\n- `strelka`: Strelka2 somatic\n- `manta`: Structural variants\n\n**CNV callers:**\n- `ascat`: Copy number\n- `controlfreec`: CNV detection\n- `tiddit`: SV calling\n\n**Annotation:**\n- `snpeff`: Functional annotation\n- `vep`: Variant Effect Predictor\n\n### Key parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `--tools` | - | Comma-separated list of tools |\n| `--genome` | - | `GRCh38`, `GRCh37` |\n| `--wes` | false | Exome mode (requires `--intervals`) |\n| `--intervals` | - | BED file for targeted regions |\n| `--joint_germline` | false | Joint calling for cohorts |\n| `--skip_bqsr` | false | Skip base quality recalibration |\n\n## Output files\n\n```\nresults/\n├── preprocessing/\n│   └── recalibrated/           # Analysis-ready BAMs\n│       └── *.recal.bam\n├── variant_calling/\n│   ├── haplotypecaller/        # Germline VCFs\n│   ├── mutect2/                # Somatic VCFs (filtered)\n│   └── strelka/\n├── annotation/\n│   └── snpeff/                 # Annotated VCFs\n└── multiqc/\n```\n\n## Troubleshooting\n\n**BQSR fails**: Check known sites available for genome. Skip with `--skip_bqsr` for non-standard references.\n\n**Mutect2 no variants**: Verify tumor/normal pairing in samplesheet (check `status` column).\n\n**Out of memory**: `--max_memory '128.GB'` for WGS.\n\n**DeepVariant GPU**: Ensure NVIDIA Docker runtime configured.\n\n## More Information\n\n- **Full parameter list:** https://nf-co.re/sarek/3.7.1/parameters/\n- **Output documentation:** https://nf-co.re/sarek/3.7.1/docs/output/\n- **Usage documentation:** https://nf-co.re/sarek/3.7.1/docs/usage/\n", "depth": 6}, {"id": "158", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/references/pipelines/rnaseq.md", "name": "rnaseq.md", "type": "file", "content": "# nf-core/rnaseq\n\n**Version:** 3.22.2\n\n**Official Documentation:** https://nf-co.re/rnaseq/3.22.2/\n**GitHub:** https://github.com/nf-core/rnaseq\n\n\u003e **Note:** When updating to a new version, check the [releases page](https://github.com/nf-core/rnaseq/releases) for breaking changes and update the version in commands below.\n\n## Contents\n- [Test command](#test-command)\n- [Samplesheet format](#samplesheet-format)\n- [Parameters](#parameters)\n- [Output files](#output-files)\n- [Downstream analysis](#downstream-analysis)\n\n## Test command\n\n```bash\nnextflow run nf-core/rnaseq -r 3.22.2 -profile test,docker --outdir test_rnaseq\n```\n\nExpected: ~15 min, creates `multiqc/multiqc_report.html`.\n\n## Samplesheet format\n\n```csv\nsample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,/path/to/ctrl1_R1.fq.gz,/path/to/ctrl1_R2.fq.gz,auto\nCONTROL_REP2,/path/to/ctrl2_R1.fq.gz,/path/to/ctrl2_R2.fq.gz,auto\nTREATMENT_REP1,/path/to/treat1_R1.fq.gz,/path/to/treat1_R2.fq.gz,auto\n```\n\n| Column | Required | Values |\n|--------|----------|--------|\n| sample | Yes | Alphanumeric, underscores allowed |\n| fastq_1 | Yes | Absolute path to R1 |\n| fastq_2 | No | Absolute path to R2 (empty for single-end) |\n| strandedness | Yes | `auto`, `forward`, `reverse`, `unstranded` |\n\n**Strandedness guide:**\n- `auto`: Inferred from data (recommended)\n- `forward`: TruSeq Stranded, dUTP protocols\n- `reverse`: Ligation-based protocols\n- `unstranded`: Non-stranded protocols\n\n## Parameters\n\n### Minimal run\n```bash\nnextflow run nf-core/rnaseq -r 3.22.2 -profile docker \\\n    --input samplesheet.csv --outdir results --genome GRCh38\n```\n\n### Common parameters\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `--aligner` | `star_salmon` | Options: `star_salmon`, `star_rsem`, `hisat2` |\n| `--genome` | - | `GRCh38`, `GRCh37`, `mm10`, `BDGP6` |\n| `--pseudo_aligner` | - | Set to `salmon` for pseudo-alignment only |\n| `--skip_trimming` | false | Skip adapter trimming |\n| `--skip_alignment` | false | Pseudo-alignment only |\n\n### Custom reference\n```bash\n--fasta /path/to/genome.fa \\\n--gtf /path/to/annotation.gtf \\\n--star_index /path/to/star/  # Optional, builds if absent\n```\n\n## Output files\n\n```\nresults/\n├── star_salmon/\n│   ├── salmon.merged.gene_counts.tsv    # Raw counts for DESeq2\n│   ├── salmon.merged.gene_tpm.tsv       # TPM values\n│   └── *.bam                            # Alignments\n├── multiqc/\n│   └── multiqc_report.html              # QC summary\n└── pipeline_info/\n```\n\n**Key outputs:**\n- `salmon.merged.gene_counts.tsv`: Input for DESeq2/edgeR\n- `salmon.merged.gene_tpm.tsv`: Normalized expression\n\n## Downstream analysis\n\n```r\nlibrary(DESeq2)\ncounts \u003c- read.delim(\"salmon.merged.gene_counts.tsv\", row.names=1)\ncoldata \u003c- data.frame(\n    condition = factor(c(\"control\", \"control\", \"treatment\", \"treatment\"))\n)\ndds \u003c- DESeqDataSetFromMatrix(\n    countData = round(counts),\n    colData = coldata,\n    design = ~ condition\n)\ndds \u003c- DESeq(dds)\nres \u003c- results(dds, contrast = c(\"condition\", \"treatment\", \"control\"))\n```\n\n## Troubleshooting\n\n**STAR index fails**: Increase memory with `--max_memory '64.GB'` or provide pre-built `--star_index`.\n\n**Low alignment rate**: Verify genome matches species; check FastQC for adapter contamination.\n\n**Strandedness detection fails**: Specify explicitly with `--strandedness reverse`.\n\n## More Information\n\n- **Full parameter list:** https://nf-co.re/rnaseq/3.22.2/parameters/\n- **Output documentation:** https://nf-co.re/rnaseq/3.22.2/docs/output/\n- **Usage documentation:** https://nf-co.re/rnaseq/3.22.2/docs/usage/\n", "depth": 6}, {"id": "159", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/scripts/config", "name": "config", "type": "dir", "content": "", "depth": 5}, {"id": "160", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/scripts/utils", "name": "utils", "type": "dir", "content": "", "depth": 5}, {"id": "161", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/nextflow-development/scripts/config/pipelines", "name": "pipelines", "type": "dir", "content": "", "depth": 6}, {"id": "162", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "163", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/scripts", "name": "scripts", "type": "dir", "content": "", "depth": 4}, {"id": "164", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/requirements.txt", "name": "requirements.txt", "type": "file", "content": "# Instrument Data to Allotrope Skill - Pinned Dependencies\n#\n# These versions are pinned for reproducibility and determinism.\n# All scientists using this skill should install these exact versions\n# to ensure identical ASM output from the same input files.\n#\n# Installation:\n#   pip install -r requirements.txt --break-system-packages\n#\n# Note: Versions pinned as of 2025-01-05\n\n# Core parsing library - provides native instrument parsers\nallotropy==0.1.55\n\n# Data manipulation and file reading\npandas==2.0.3\n\n# Excel file support (required by pandas for .xlsx files)\nopenpyxl==3.1.2\n\n# PDF parsing support (for instruments that export PDFs)\npdfplumber==0.9.0\n\n# Scientific computing (optional, but recommended for advanced analysis)\n# numpy==1.24.3  # Uncomment if needed\n# scipy==1.11.1  # Uncomment if needed\n", "depth": 4}, {"id": "165", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: instrument-data-to-allotrope\ndescription: Convert laboratory instrument output files (PDF, CSV, Excel, TXT) to Allotrope Simple Model (ASM) JSON format or flattened 2D CSV. Use this skill when scientists need to standardize instrument data for LIMS systems, data lakes, or downstream analysis. Supports auto-detection of instrument types. Outputs include full ASM JSON, flattened CSV for easy import, and exportable Python code for data engineers. Common triggers include converting instrument files, standardizing lab data, preparing data for upload to LIMS/ELN systems, or generating parser code for production pipelines.\n---\n\n# Instrument Data to Allotrope Converter\n\nConvert instrument files into standardized Allotrope Simple Model (ASM) format for LIMS upload, data lakes, or handoff to data engineering teams.\n\n\u003e **Note: This is an Example Skill**\n\u003e\n\u003e This skill demonstrates how skills can support your data engineering tasks—automating schema transformations, parsing instrument outputs, and generating production-ready code.\n\u003e\n\u003e **To customize for your organization:**\n\u003e - Modify the `references/` files to include your company's specific schemas or ontology mappings\n\u003e - Use an MCP server to connect to systems that define your schemas (e.g., your LIMS, data catalog, or schema registry)\n\u003e - Extend the `scripts/` to handle proprietary instrument formats or internal data standards\n\u003e\n\u003e This pattern can be adapted for any data transformation workflow where you need to convert between formats or validate against organizational standards.\n\n## Workflow Overview\n\n1. **Detect instrument type** from file contents (auto-detect or user-specified)\n2. **Parse file** using allotropy library (native) or flexible fallback parser\n3. **Generate outputs**:\n   - ASM JSON (full semantic structure)\n   - Flattened CSV (2D tabular format)\n   - Python parser code (for data engineer handoff)\n4. **Deliver** files with summary and usage instructions\n\n\u003e **When Uncertain:** If you're unsure how to map a field to ASM (e.g., is this raw data or calculated? device setting or environmental condition?), ask the user for clarification. Refer to `references/field_classification_guide.md` for guidance, but when ambiguity remains, confirm with the user rather than guessing.\n\n## Quick Start\n\n```python\n# Install requirements first\npip install allotropy pandas openpyxl pdfplumber --break-system-packages\n\n# Core conversion\nfrom allotropy.parser_factory import Vendor\nfrom allotropy.to_allotrope import allotrope_from_file\n\n# Convert with allotropy\nasm = allotrope_from_file(\"instrument_data.csv\", Vendor.BECKMAN_VI_CELL_BLU)\n```\n\n## Output Format Selection\n\n**ASM JSON (default)** - Full semantic structure with ontology URIs\n- Best for: LIMS systems expecting ASM, data lakes, long-term archival\n- Validates against Allotrope schemas\n\n**Flattened CSV** - 2D tabular representation\n- Best for: Quick analysis, Excel users, systems without JSON support\n- Each measurement becomes one row with metadata repeated\n\n**Both** - Generate both formats for maximum flexibility\n\n## Calculated Data Handling\n\n**IMPORTANT:** Separate raw measurements from calculated/derived values.\n\n- **Raw data** → `measurement-document` (direct instrument readings)\n- **Calculated data** → `calculated-data-aggregate-document` (derived values)\n\nCalculated values MUST include traceability via `data-source-aggregate-document`:\n\n```json\n\"calculated-data-aggregate-document\": {\n  \"calculated-data-document\": [{\n    \"calculated-data-identifier\": \"SAMPLE_B1_DIN_001\",\n    \"calculated-data-name\": \"DNA integrity number\",\n    \"calculated-result\": {\"value\": 9.5, \"unit\": \"(unitless)\"},\n    \"data-source-aggregate-document\": {\n      \"data-source-document\": [{\n        \"data-source-identifier\": \"SAMPLE_B1_MEASUREMENT\",\n        \"data-source-feature\": \"electrophoresis trace\"\n      }]\n    }\n  }]\n}\n```\n\n**Common calculated fields by instrument type:**\n| Instrument | Calculated Fields |\n|------------|-------------------|\n| Cell counter | Viability %, cell density dilution-adjusted values |\n| Spectrophotometer | Concentration (from absorbance), 260/280 ratio |\n| Plate reader | Concentrations from standard curve, %CV |\n| Electrophoresis | DIN/RIN, region concentrations, average sizes |\n| qPCR | Relative quantities, fold change |\n\nSee `references/field_classification_guide.md` for detailed guidance on raw vs. calculated classification.\n\n## Validation\n\nAlways validate ASM output before delivering to the user:\n\n```bash\npython scripts/validate_asm.py output.json\npython scripts/validate_asm.py output.json --reference known_good.json  # Compare to reference\npython scripts/validate_asm.py output.json --strict  # Treat warnings as errors\n```\n\n**Validation Rules:**\n- Based on Allotrope ASM specification (December 2024)\n- Last updated: 2026-01-07\n- Source: https://gitlab.com/allotrope-public/asm\n\n**Soft Validation Approach:**\nUnknown techniques, units, or sample roles generate **warnings** (not errors) to allow for forward compatibility. If Allotrope adds new values after December 2024, the validator won't block them—it will flag them for manual verification. Use `--strict` mode to treat warnings as errors if you need stricter validation.\n\n**What it checks:**\n- Correct technique selection (e.g., multi-analyte profiling vs plate reader)\n- Field naming conventions (space-separated, not hyphenated)\n- Calculated data has traceability (`data-source-aggregate-document`)\n- Unique identifiers exist for measurements and calculated values\n- Required metadata present\n- Valid units and sample roles (with soft validation for unknown values)\n\n## Supported Instruments\n\nSee `references/supported_instruments.md` for complete list. Key instruments:\n\n| Category | Instruments |\n|----------|-------------|\n| Cell Counting | Vi-CELL BLU, Vi-CELL XR, NucleoCounter |\n| Spectrophotometry | NanoDrop One/Eight/8000, Lunatic |\n| Plate Readers | SoftMax Pro, EnVision, Gen5, CLARIOstar |\n| ELISA | SoftMax Pro, BMG MARS, MSD Workbench |\n| qPCR | QuantStudio, Bio-Rad CFX |\n| Chromatography | Empower, Chromeleon |\n\n## Detection \u0026 Parsing Strategy\n\n### Tier 1: Native allotropy parsing (PREFERRED)\n**Always try allotropy first.** Check available vendors directly:\n\n```python\nfrom allotropy.parser_factory import Vendor\n\n# List all supported vendors\nfor v in Vendor:\n    print(f\"{v.name}\")\n\n# Common vendors:\n# AGILENT_TAPESTATION_ANALYSIS  (for TapeStation XML)\n# BECKMAN_VI_CELL_BLU\n# THERMO_FISHER_NANODROP_EIGHT\n# MOLDEV_SOFTMAX_PRO\n# APPBIO_QUANTSTUDIO\n# ... many more\n```\n\n**When the user provides a file, check if allotropy supports it before falling back to manual parsing.** The `scripts/convert_to_asm.py` auto-detection only covers a subset of allotropy vendors.\n\n### Tier 2: Flexible fallback parsing\n**Only use if allotropy doesn't support the instrument.** This fallback:\n- Does NOT generate `calculated-data-aggregate-document`\n- Does NOT include full traceability\n- Produces simplified ASM structure\n\nUse flexible parser with:\n- Column name fuzzy matching\n- Unit extraction from headers\n- Metadata extraction from file structure\n\n### Tier 3: PDF extraction\nFor PDF-only files, extract tables using pdfplumber, then apply Tier 2 parsing.\n\n## Pre-Parsing Checklist\n\nBefore writing a custom parser, ALWAYS:\n\n1. **Check if allotropy supports it** - Use native parser if available\n2. **Find a reference ASM file** - Check `references/examples/` or ask user\n3. **Review instrument-specific guide** - Check `references/instrument_guides/`\n4. **Validate against reference** - Run `validate_asm.py --reference \u003cfile\u003e`\n\n## Common Mistakes to Avoid\n\n| Mistake | Correct Approach |\n|---------|------------------|\n| Manifest as object | Use URL string |\n| Lowercase detection types | Use \"Absorbance\" not \"absorbance\" |\n| \"emission wavelength setting\" | Use \"detector wavelength setting\" for emission |\n| All measurements in one document | Group by well/sample location |\n| Missing procedure metadata | Extract ALL device settings per measurement |\n\n## Code Export for Data Engineers\n\nGenerate standalone Python scripts that scientists can hand off:\n\n```python\n# Export parser code\npython scripts/export_parser.py --input \"data.csv\" --vendor \"VI_CELL_BLU\" --output \"parser_script.py\"\n```\n\nThe exported script:\n- Has no external dependencies beyond pandas/allotropy\n- Includes inline documentation\n- Can run in Jupyter notebooks\n- Is production-ready for data pipelines\n\n## File Structure\n\n```\ninstrument-data-to-allotrope/\n├── SKILL.md                          # This file\n├── scripts/\n│   ├── convert_to_asm.py            # Main conversion script\n│   ├── flatten_asm.py               # ASM → 2D CSV conversion\n│   ├── export_parser.py             # Generate standalone parser code\n│   └── validate_asm.py              # Validate ASM output quality\n└── references/\n    ├── supported_instruments.md     # Full instrument list with Vendor enums\n    ├── asm_schema_overview.md       # ASM structure reference\n    ├── field_classification_guide.md # Where to put different field types\n    └── flattening_guide.md          # How flattening works\n```\n\n## Usage Examples\n\n### Example 1: Vi-CELL BLU file\n```\nUser: \"Convert this cell counting data to Allotrope format\"\n[uploads viCell_Results.xlsx]\n\nClaude:\n1. Detects Vi-CELL BLU (95% confidence)\n2. Converts using allotropy native parser\n3. Outputs:\n   - viCell_Results_asm.json (full ASM)\n   - viCell_Results_flat.csv (2D format)\n   - viCell_parser.py (exportable code)\n```\n\n### Example 2: Request for code handoff\n```\nUser: \"I need to give our data engineer code to parse NanoDrop files\"\n\nClaude:\n1. Generates self-contained Python script\n2. Includes sample input/output\n3. Documents all assumptions\n4. Provides Jupyter notebook version\n```\n\n### Example 3: LIMS-ready flattened output\n```\nUser: \"Convert this ELISA data to a CSV I can upload to our LIMS\"\n\nClaude:\n1. Parses plate reader data\n2. Generates flattened CSV with columns:\n   - sample_identifier, well_position, measurement_value, measurement_unit\n   - instrument_serial_number, analysis_datetime, assay_type\n3. Validates against common LIMS import requirements\n```\n\n## Implementation Notes\n\n### Installing allotropy\n```bash\npip install allotropy --break-system-packages\n```\n\n### Handling parse failures\nIf allotropy native parsing fails:\n1. Log the error for debugging\n2. Fall back to flexible parser\n3. Report reduced metadata completeness to user\n4. Suggest exporting different format from instrument\n\n### ASM Schema Validation\nValidate output against Allotrope schemas when available:\n```python\nimport jsonschema\n# Schema URLs in references/asm_schema_overview.md\n```\n", "depth": 4}, {"id": "166", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/LICENSE.txt", "name": "LICENSE.txt", "type": "file", "content": "Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction,\nand distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\nthe copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all\nother entities that control, are controlled by, or are under common\ncontrol with that entity. For the purposes of this definition,\n\"control\" means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or\notherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\nexercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications,\nincluding but not limited to software source code, documentation\nsource, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical\ntransformation or translation of a Source form, including but\nnot limited to compiled object code, generated documentation,\nand conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or\nObject form, made available under the License, as indicated by a\ncopyright notice that is included in or attached to the work\n(an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\nform, that is based on (or derived from) the Work and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship. For the purposes\nof this License, Derivative Works shall not include works that remain\nseparable from, or merely link (or bind by name) to the interfaces of,\nthe Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including\nthe original version of the Work and any modifications or additions\nto that Work or Derivative Works thereof, that is intentionally\nsubmitted to Licensor for inclusion in the Work by the copyright owner\nor by an individual or Legal Entity authorized to submit on behalf of\nthe copyright owner. For the purposes of this definition, \"submitted\"\nmeans any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems,\nand issue tracking systems that are managed by, or on behalf of, the\nLicensor for the purpose of discussing and improving the Work, but\nexcluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\non behalf of whom a Contribution has been received by Licensor and\nsubsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\ncopyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the\nWork and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\n(except as stated in this section) patent license to make, have made,\nuse, offer to sell, sell, import, and otherwise transfer the Work,\nwhere such license applies only to those patent claims licensable\nby such Contributor that are necessarily infringed by their\nContribution(s) alone or by combination of their Contribution(s)\nwith the Work to which such Contribution(s) was submitted. If You\ninstitute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work\nor a Contribution incorporated within the Work constitutes direct\nor contributory patent infringement, then any patent licenses\ngranted to You under this License for that Work shall terminate\nas of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\nWork or Derivative Works thereof in any medium, with or without\nmodifications, and in Source or Object form, provided that You\nmeet the following conditions:\n\n(a) You must give any other recipients of the Work or\nDerivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices\nstating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works\nthat You distribute, all copyright, patent, trademark, and\nattribution notices from the Source form of the Work,\nexcluding those notices that do not pertain to any part of\nthe Derivative Works; and\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\ndistribution, then any Derivative Works that You distribute must\ninclude a readable copy of the attribution notices contained\nwithin such NOTICE file, excluding those notices that do not\npertain to any part of the Derivative Works, in at least one\nof the following places: within a NOTICE text file distributed\nas part of the Derivative Works; within the Source form or\ndocumentation, if provided along with the Derivative Works; or,\nwithin a display generated by the Derivative Works, if and\nwherever such third-party notices normally appear. The contents\nof the NOTICE file are for informational purposes only and\ndo not modify the License. You may add Your own attribution\nnotices within Derivative Works that You distribute, alongside\nor as an addendum to the NOTICE text from the Work, provided\nthat such additional attribution notices cannot be construed\nas modifying the License.\n\nYou may add Your own copyright statement to Your modifications and\nmay provide additional or different license terms and conditions\nfor use, reproduction, or distribution of Your modifications, or\nfor any such Derivative Works as a whole, provided Your use,\nreproduction, and distribution of the Work otherwise complies with\nthe conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\nany Contribution intentionally submitted for inclusion in the Work\nby You to the Licensor shall be under the terms and conditions of\nthis License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify\nthe terms of any separate license agreement you may have executed\nwith Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\nnames, trademarks, service marks, or product names of the Licensor,\nexcept as required for reasonable and customary use in describing the\norigin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\nagreed to in writing, Licensor provides the Work (and each\nContributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied, including, without limitation, any warranties or conditions\nof TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\nPARTICULAR PURPOSE. You are solely responsible for determining the\nappropriateness of using or redistributing the Work and assume any\nrisks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\nwhether in tort (including negligence), contract, or otherwise,\nunless required by applicable law (such as deliberate and grossly\nnegligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special,\nincidental, or consequential damages of any character arising as a\nresult of this License or out of the use or inability to use the\nWork (including but not limited to damages for loss of goodwill,\nwork stoppage, computer failure or malfunction, or any and all\nother commercial damages or losses), even if such Contributor\nhas been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\nthe Work or Derivative Works thereof, You may choose to offer,\nand charge a fee for, acceptance of support, warranty, indemnity,\nor other liability obligations and/or rights consistent with this\nLicense. However, in accepting such obligations, You may act only\non Your own behalf and on Your sole responsibility, not on behalf\nof any other Contributor, and only if You agree to indemnify,\ndefend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason\nof your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\nTo apply the Apache License to your work, attach the following\nboilerplate notice, with the fields enclosed by brackets \"[]\"\nreplaced with your own identifying information. (Don't include\nthe brackets!) The text should be enclosed in the appropriate\ncomment syntax for the file format. We also recommend that a\nfile or class name and description of purpose be included on the\nsame \"printed page\" as the copyright notice for easier\nidentification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n", "depth": 4}, {"id": "167", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/references/asm_schema_overview.md", "name": "asm_schema_overview.md", "type": "file", "content": "# ASM Schema Overview\n\nThe Allotrope Simple Model (ASM) is a JSON-based standard for representing laboratory instrument data with semantic consistency.\n\n## Core Concepts\n\n### Structure\nASM uses a hierarchical document structure:\n- **Manifest** - Links to ontologies and schemas\n- **Data** - The actual measurement data organized by technique\n\n### Key Components\n\n```json\n{\n  \"$asm.manifest\": {\n    \"vocabulary\": [\"http://purl.allotrope.org/voc/afo/REC/2023/09/\"],\n    \"contexts\": [\"http://purl.allotrope.org/json-ld/afo-context-REC-2023-09.jsonld\"]\n  },\n  \"\u003ctechnique\u003e-aggregate-document\": {\n    \"device-system-document\": { ... },\n    \"\u003ctechnique\u003e-document\": [\n      {\n        \"measurement-aggregate-document\": {\n          \"measurement-document\": [ ... ]\n        }\n      }\n    ]\n  }\n}\n```\n\n## Required Metadata Documents\n\n### data system document\nEvery ASM output MUST include this document with:\n- `ASM file identifier`: Output filename\n- `data system instance identifier`: System ID or \"N/A\"\n- `file name`: Source input filename\n- `UNC path`: Path to source file\n- `ASM converter name`: Parser identifier (e.g., \"allotropy_beckman_coulter_biomek\")\n- `ASM converter version`: Version string\n- `software name`: Instrument software that generated the source file\n\n### device system document\nEvery ASM output MUST include this document with:\n- `equipment serial number`: Main instrument serial\n- `product manufacturer`: Vendor name\n- `device document`: Array of sub-components (probes, pods, etc.)\n  - `device type`: Standardized type (e.g., \"liquid handler probe head\")\n  - `device identifier`: Logical name (e.g., \"Pod1\", not serial number)\n  - `equipment serial number`: Component serial\n  - `product manufacturer`: Component vendor\n\n## Available ASM Techniques\n\nThe official ASM repository includes **65 technique schemas**:\n\n```\nabsorbance, automated-reactors, balance, bga, binding-affinity, bulk-density,\ncell-counting, cell-culture-analyzer, chromatography, code-reader, conductance,\nconductivity, disintegration, dsc, dvs, electronic-lab-notebook,\nelectronic-spectrometry, electrophoresis, flow-cytometry, fluorescence,\nfoam-height, foam-qualification, fplc, ftir, gas-chromatography, gc-ms, gloss,\nhot-tack, impedance, lc-ms, light-obscuration, liquid-chromatography,\nloss-on-drying, luminescence, mass-spectrometry, metabolite-analyzer,\nmulti-analyte-profiling, nephelometry, nmr, optical-imaging, optical-microscopy,\nosmolality, oven-kf, pcr, ph, plate-reader, pressure-monitoring, psd, pumping,\nraman, rheometry, sem, solution-analyzer, specific-rotation, spectrophotometry,\nstirring, surface-area-analysis, tablet-hardness, temperature-monitoring,\ntensile-test, thermogravimetric-analysis, titration, ultraviolet-absorbance,\nx-ray-powder-diffraction\n```\n\nSee: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm\n\n## Common ASM Schemas by Technique\n\nBelow are details for frequently-used techniques:\n\n### Cell Counting\nSchema: `cell-counting/REC/2024/09/cell-counting.schema.json`\n\nKey fields:\n- `viable-cell-density` (cells/mL)\n- `viability` (percentage)\n- `total-cell-count`\n- `dead-cell-count`\n- `cell-diameter-distribution-datum`\n\n### Spectrophotometry (UV-Vis)\nSchema: `spectrophotometry/REC/2024/06/spectrophotometry.schema.json`\n\nKey fields:\n- `absorbance` (dimensionless)\n- `wavelength` (nm)\n- `transmittance` (percentage)\n- `pathlength` (cm)\n- `concentration` with units\n\n### Plate Reader\nSchema: `plate-reader/REC/2024/06/plate-reader.schema.json`\n\nKey fields:\n- `absorbance`\n- `fluorescence`\n- `luminescence`\n- `well-location` (A1-H12)\n- `plate-identifier`\n\n### qPCR\nSchema: `pcr/REC/2024/06/pcr.schema.json`\n\nKey fields:\n- `cycle-threshold-result`\n- `amplification-efficiency`\n- `melt-curve-datum`\n- `target-DNA-description`\n\n### Chromatography\nSchema: `liquid-chromatography/REC/2023/09/liquid-chromatography.schema.json`\n\nKey fields:\n- `retention-time` (minutes)\n- `peak-area`\n- `peak-height`\n- `peak-width`\n- `chromatogram-data-cube`\n\n## Data Patterns\n\n### Value Datum\nSimple value with unit:\n```json\n{\n  \"value\": 1.5,\n  \"unit\": \"mL\"\n}\n```\n\n### Aggregate Datum\nCollection of related values:\n```json\n{\n  \"measurement-aggregate-document\": {\n    \"measurement-document\": [\n      { \"viable-cell-density\": {\"value\": 2.5e6, \"unit\": \"(cell/mL)\"} },\n      { \"viability\": {\"value\": 95.2, \"unit\": \"%\"} }\n    ]\n  }\n}\n```\n\n### Data Cube\nMulti-dimensional array data:\n```json\n{\n  \"cube-structure\": {\n    \"dimensions\": [{\"@componentDatatype\": \"double\", \"concept\": \"elapsed time\"}],\n    \"measures\": [{\"@componentDatatype\": \"double\", \"concept\": \"absorbance\"}]\n  },\n  \"data\": {\n    \"dimensions\": [[0, 1, 2, 3, 4]],\n    \"measures\": [[0.1, 0.2, 0.3, 0.4, 0.5]]\n  }\n}\n```\n\n## Validation\n\nValidate ASM output against official schemas:\n\n```python\nimport json\nimport jsonschema\nfrom urllib.request import urlopen\n\n# Load ASM output\nwith open(\"output.json\") as f:\n    asm = json.load(f)\n\n# Get schema URL from manifest\nschema_url = asm.get(\"$asm.manifest\", {}).get(\"$ref\")\n\n# Validate (simplified - real validation more complex)\n# Note: Full validation requires resolving $ref references\n```\n\n## Schema Repository\n\nOfficial schemas: https://gitlab.com/allotrope-public/asm/-/tree/main/json-schemas/adm\n\nSchema structure:\n```\njson-schemas/adm/\n├── cell-counting/\n│   └── REC/2024/09/\n│       └── cell-counting.schema.json\n├── spectrophotometry/\n│   └── REC/2024/06/\n│       └── spectrophotometry.schema.json\n├── plate-reader/\n│   └── REC/2024/06/\n│       └── plate-reader.schema.json\n└── ...\n```\n\n## Common Issues\n\n### Missing Fields\nNot all instrument exports contain all ASM fields. Report completeness:\n```python\ndef report_completeness(asm, expected_fields):\n    found = set(extract_all_fields(asm))\n    missing = expected_fields - found\n    return len(found) / len(expected_fields) * 100\n```\n\n### Unit Variations\nInstruments may use different unit formats. The allotropy library normalizes these:\n- \"cells/mL\" → \"(cell/mL)\"\n- \"%\" → \"%\"\n- \"nm\" → \"nm\"\n\n### Date Formats\nASM uses ISO 8601: `2024-01-15T10:30:00Z`\n", "depth": 5}, {"id": "168", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/references/flattening_guide.md", "name": "flattening_guide.md", "type": "file", "content": "# Flattening ASM to 2D CSV\n\nConverting hierarchical ASM JSON to flat 2D tables for LIMS import, spreadsheet analysis, or data engineering pipelines.\n\n## Why Flatten?\n\nASM is semantically rich but hierarchical. Many systems need flat tables:\n- LIMS import (Benchling, STARLIMS, LabWare)\n- Excel/CSV analysis\n- Database loading\n- Quick visual inspection\n\n## Flattening Strategy\n\n### Core Principle\nEach **measurement** becomes one **row**. Metadata is repeated per row.\n\n### What's Excluded\nThe flattening intentionally **omits top-level ASM metadata** such as:\n- `$asm.manifest` (model version, schema URIs)\n- Root-level fields outside the technique aggregate document\n\nThis keeps the output focused on experimental data. If you need schema version tracking for compliance or audit purposes, consider storing the original ASM JSON alongside the flattened CSV, or modify the flattening script to include these fields.\n\n### Hierarchy to Columns\n```\nASM Hierarchy                    → Flat Column\n─────────────────────────────────────────────────\ndevice-system-document.\n  device-identifier              → instrument_serial_number\n  model-number                   → instrument_model\n  \nmeasurement-aggregate-document.\n  analyst                        → analyst\n  measurement-time               → measurement_datetime\n  \nmeasurement-document[].\n  sample-identifier              → sample_id\n  viable-cell-density.value      → viable_cell_density\n  viable-cell-density.unit       → viable_cell_density_unit\n  viability.value                → viability_percent\n```\n\n## Column Naming Convention\n\nUse snake_case with descriptive suffixes:\n\n| ASM Field | Flat Column |\n|-----------|-------------|\n| `viable-cell-density` | `viable_cell_density` |\n| `.value` | `_value` (or omit if obvious) |\n| `.unit` | `_unit` |\n| `measurement-time` | `measurement_datetime` |\n\n## Example: Cell Counting\n\n### ASM Input (simplified)\n```json\n{\n  \"cell-counting-aggregate-document\": {\n    \"device-system-document\": {\n      \"device-identifier\": \"VCB001\",\n      \"model-number\": \"Vi-CELL BLU\"\n    },\n    \"cell-counting-document\": [{\n      \"measurement-aggregate-document\": {\n        \"analyst\": \"jsmith\",\n        \"measurement-time\": \"2024-01-15T10:30:00Z\",\n        \"measurement-document\": [\n          {\n            \"sample-identifier\": \"Sample_A\",\n            \"viable-cell-density\": {\"value\": 2500000, \"unit\": \"(cell/mL)\"},\n            \"viability\": {\"value\": 95.2, \"unit\": \"%\"}\n          },\n          {\n            \"sample-identifier\": \"Sample_B\",\n            \"viable-cell-density\": {\"value\": 1800000, \"unit\": \"(cell/mL)\"},\n            \"viability\": {\"value\": 88.7, \"unit\": \"%\"}\n          }\n        ]\n      }\n    }]\n  }\n}\n```\n\n### Flattened Output\n```csv\nsample_id,viable_cell_density,viable_cell_density_unit,viability_percent,analyst,measurement_datetime,instrument_serial_number,instrument_model\nSample_A,2500000,(cell/mL),95.2,jsmith,2024-01-15T10:30:00Z,VCB001,Vi-CELL BLU\nSample_B,1800000,(cell/mL),88.7,jsmith,2024-01-15T10:30:00Z,VCB001,Vi-CELL BLU\n```\n\n## Example: Plate Reader\n\n### ASM Input (simplified)\n```json\n{\n  \"plate-reader-aggregate-document\": {\n    \"plate-reader-document\": [{\n      \"measurement-aggregate-document\": {\n        \"plate-identifier\": \"ELISA_001\",\n        \"measurement-document\": [\n          {\"well-location\": \"A1\", \"absorbance\": {\"value\": 0.125, \"unit\": \"mAU\"}},\n          {\"well-location\": \"A2\", \"absorbance\": {\"value\": 0.892, \"unit\": \"mAU\"}},\n          {\"well-location\": \"A3\", \"absorbance\": {\"value\": 1.456, \"unit\": \"mAU\"}}\n        ]\n      }\n    }]\n  }\n}\n```\n\n### Flattened Output\n```csv\nplate_id,well_position,absorbance,absorbance_unit\nELISA_001,A1,0.125,mAU\nELISA_001,A2,0.892,mAU\nELISA_001,A3,1.456,mAU\n```\n\n## Handling Data Cubes\n\nData cubes (time series, spectra) need special handling:\n\n### Option 1: Expand to rows\nEach point becomes a row:\n```csv\nsample_id,time_seconds,absorbance\nSample_A,0,0.100\nSample_A,60,0.125\nSample_A,120,0.150\n```\n\n### Option 2: Wide format\nMeasurements as columns:\n```csv\nsample_id,abs_0s,abs_60s,abs_120s\nSample_A,0.100,0.125,0.150\n```\n\n### Option 3: JSON array in cell\nKeep as array (some systems support this):\n```csv\nsample_id,absorbance_timeseries\nSample_A,\"[0.100,0.125,0.150]\"\n```\n\n## Standard Column Sets by Technique\n\n### Cell Counting\n```\nsample_id, viable_cell_density, viable_cell_density_unit, total_cell_count,\nviability_percent, average_cell_diameter, average_cell_diameter_unit,\nanalyst, measurement_datetime, instrument_serial_number\n```\n\n### Spectrophotometry\n```\nsample_id, wavelength_nm, absorbance, pathlength_cm, concentration,\nconcentration_unit, a260_a280_ratio, a260_a230_ratio,\nanalyst, measurement_datetime, instrument_serial_number\n```\n\n### Plate Reader / ELISA\n```\nplate_id, well_position, sample_type, sample_id, absorbance, absorbance_unit,\nconcentration, concentration_unit, dilution_factor, cv_percent,\nanalyst, measurement_datetime, instrument_serial_number\n```\n\n### qPCR\n```\nsample_id, target_name, well_position, ct_value, ct_mean, ct_sd,\nquantity, quantity_unit, amplification_efficiency,\nanalyst, measurement_datetime, instrument_serial_number\n```\n\n## Python Implementation\n\n```python\nimport json\nimport pandas as pd\n\ndef flatten_asm(asm_dict, technique=\"cell-counting\"):\n    \"\"\"\n    Flatten ASM JSON to pandas DataFrame.\n    \n    Args:\n        asm_dict: Parsed ASM JSON\n        technique: ASM technique type\n        \n    Returns:\n        pandas DataFrame with one row per measurement\n    \"\"\"\n    rows = []\n    \n    # Get aggregate document\n    agg_key = f\"{technique}-aggregate-document\"\n    agg_doc = asm_dict.get(agg_key, {})\n    \n    # Extract device info\n    device = agg_doc.get(\"device-system-document\", {})\n    device_info = {\n        \"instrument_serial_number\": device.get(\"device-identifier\"),\n        \"instrument_model\": device.get(\"model-number\")\n    }\n    \n    # Get technique documents\n    doc_key = f\"{technique}-document\"\n    for doc in agg_doc.get(doc_key, []):\n        meas_agg = doc.get(\"measurement-aggregate-document\", {})\n        \n        # Extract common metadata\n        common = {\n            \"analyst\": meas_agg.get(\"analyst\"),\n            \"measurement_datetime\": meas_agg.get(\"measurement-time\"),\n            **device_info\n        }\n        \n        # Extract each measurement\n        for meas in meas_agg.get(\"measurement-document\", []):\n            row = {**common}\n            \n            # Flatten measurement fields\n            for key, value in meas.items():\n                if isinstance(value, dict) and \"value\" in value:\n                    # Value datum pattern\n                    col = key.replace(\"-\", \"_\")\n                    row[col] = value[\"value\"]\n                    if \"unit\" in value:\n                        row[f\"{col}_unit\"] = value[\"unit\"]\n                else:\n                    row[key.replace(\"-\", \"_\")] = value\n            \n            rows.append(row)\n    \n    return pd.DataFrame(rows)\n\n# Usage\nwith open(\"asm_output.json\") as f:\n    asm = json.load(f)\n\ndf = flatten_asm(asm, \"cell-counting\")\ndf.to_csv(\"flattened_output.csv\", index=False)\n```\n\n## LIMS Import Considerations\n\nWhen importing flattened data into a LIMS:\n- Match column names to your LIMS schema field names\n- Use ISO 8601 date format for timestamps\n- Ensure sample IDs match existing LIMS sample identifiers\n- Check if your LIMS expects units in separate columns or embedded in values\n", "depth": 5}, {"id": "169", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/references/field_classification_guide.md", "name": "field_classification_guide.md", "type": "file", "content": "# Field Classification Guide\n\nThis guide helps classify instrument data fields into the correct ASM document locations. Use this when mapping raw instrument output to Allotrope Simple Model structure.\n\n## ASM Document Hierarchy\n\n```\n\u003ctechnique\u003e-aggregate-document\n├── device-system-document          # Instrument hardware info\n├── data-system-document            # Software/conversion info\n├── \u003ctechnique\u003e-document[]          # Per-run/sequence data\n│   ├── analyst                     # Who performed the analysis\n│   ├── measurement-aggregate-document\n│   │   ├── measurement-time\n│   │   ├── measurement-document[]  # Individual measurements\n│   │   │   ├── sample-document\n│   │   │   ├── device-control-aggregate-document\n│   │   │   └── [measurement fields]\n│   │   └── [aggregate-level metadata]\n│   ├── processed-data-aggregate-document\n│   │   └── processed-data-document[]\n│   │       ├── data-processing-document\n│   │       └── [processed results]\n│   └── calculated-data-aggregate-document\n│       └── calculated-data-document[]\n```\n\n## Field Classification Categories\n\n### 1. Device/Instrument Information → `device-system-document`\n\nHardware and firmware details about the physical instrument.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Instrument name | `model-number` | \"Vi-CELL BLU\", \"NanoDrop One\" |\n| Serial number | `equipment-serial-number` | \"VCB-12345\", \"SN001234\" |\n| Manufacturer | `product-manufacturer` | \"Beckman Coulter\", \"Thermo Fisher\" |\n| Firmware version | `firmware-version` | \"v2.1.3\" |\n| Device ID | `device-identifier` | \"Instrument_01\" |\n| Brand | `brand-name` | \"Beckman Coulter\" |\n\n**Rule:** If the value describes the physical instrument and doesn't change between runs, it goes in `device-system-document`.\n\n---\n\n### 2. Software/Data System Information → `data-system-document`\n\nInformation about software used for acquisition, analysis, or conversion.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Software name | `software-name` | \"Chromeleon\", \"Gen5\" |\n| Software version | `software-version` | \"7.3.2\" |\n| File name | `file-name` | \"experiment_001.xlsx\" |\n| File path | `file-identifier` | \"/data/runs/2024-01-15/\" |\n| Database ID | `ASM-converter-name` | \"allotropy v0.1.55\" |\n\n**Rule:** If the value describes software, file metadata, or data provenance, it goes in `data-system-document`.\n\n---\n\n### 3. Sample Information → `sample-document`\n\nMetadata about the biological/chemical sample being analyzed.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Sample ID | `sample-identifier` | \"Sample_A\", \"LIMS-001234\" |\n| Sample name | `written-name` | \"CHO Cell Culture Day 5\" |\n| Sample type/role | `sample-role-type` | \"unknown sample role\", \"control sample role\" |\n| Batch ID | `batch-identifier` | \"Batch-2024-001\" |\n| Description | `description` | \"Protein expression sample\" |\n| Well position | `location-identifier` | \"A1\", \"B3\" |\n\n**Rule:** If the value identifies or describes what was measured (not how), it goes in `sample-document`.\n\n---\n\n### 4. Device Control Settings → `device-control-aggregate-document`\n\nInstrument settings and parameters used during measurement.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Injection volume | `sample-volume-setting` | 10 µL |\n| Wavelength | `detector-wavelength-setting` | 254 nm |\n| Temperature | `compartment-temperature` | 37°C |\n| Flow rate | `flow-rate` | 1.0 mL/min |\n| Exposure time | `exposure-duration-setting` | 500 ms |\n| Detector gain | `detector-gain-setting` | 1.5 |\n| Illumination | `illumination-setting` | 80% |\n\n**Rule:** If the value is a configurable instrument parameter that affects measurement, it goes in `device-control-aggregate-document`.\n\n---\n\n### 5. Environmental Conditions → `device-control-document` or technique-specific\n\nAmbient or controlled environmental parameters during measurement.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Ambient temperature | `ambient-temperature` | 22.5°C |\n| Humidity | `ambient-relative-humidity` | 45% |\n| Column temperature | `compartment-temperature` | 30°C |\n| Sample temperature | `sample-temperature` | 4°C |\n| Electrophoresis temp | (technique-specific) | 26.4°C |\n\n**Rule:** Environmental conditions that affect measurement quality go with device control or in technique-specific locations.\n\n---\n\n### 6. Raw Measurement Data → `measurement-document`\n\nDirect instrument readings - the \"ground truth\" data.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Absorbance | `absorbance` | 0.523 AU |\n| Fluorescence | `fluorescence` | 12500 RFU |\n| Cell count | `total-cell-count` | 2.5e6 cells |\n| Peak area | `peak-area` | 1234.5 mAU·min |\n| Retention time | `retention-time` | 5.67 min |\n| Ct value | `cycle-threshold-result` | 24.5 |\n| Concentration (measured) | `mass-concentration` | 1.5 mg/mL |\n\n**Rule:** If the value is a direct instrument reading that wasn't computed from other values in this analysis, it goes in `measurement-document`.\n\n---\n\n### 7. Calculated/Derived Data → `calculated-data-aggregate-document`\n\nValues computed from raw measurements.\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Viability % | `calculated-result` | 95.2% |\n| Concentration (from std curve) | `calculated-result` | 125 ng/µL |\n| Ratio (260/280) | `calculated-result` | 1.89 |\n| Relative quantity | `calculated-result` | 2.5x |\n| % Recovery | `calculated-result` | 98.7% |\n| CV% | `calculated-result` | 2.3% |\n\n**Calculated data document structure:**\n```json\n{\n  \"calculated-data-name\": \"viability\",\n  \"calculated-result\": {\"value\": 95.2, \"unit\": \"%\"},\n  \"calculation-description\": \"viable cells / total cells * 100\"\n}\n```\n\n**Rule:** If the value was computed from other measurements in this analysis, it goes in `calculated-data-aggregate-document`. Include `calculation-description` when possible.\n\n---\n\n### 8. Processed/Analyzed Data → `processed-data-aggregate-document`\n\nResults from data processing algorithms (peak integration, cell classification, etc.).\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Peak list | `peak-list` | Integrated peak results |\n| Cell size distribution | `cell-diameter-distribution` | Histogram data |\n| Baseline-corrected data | (in processed-data-document) | Corrected spectra |\n| Fitted curve | (in processed-data-document) | Standard curve fit |\n\n**Associated `data-processing-document`:**\n```json\n{\n  \"cell-type-processing-method\": \"trypan blue exclusion\",\n  \"cell-density-dilution-factor\": {\"value\": 2, \"unit\": \"(unitless)\"},\n  \"minimum-cell-diameter-setting\": {\"value\": 5, \"unit\": \"µm\"},\n  \"maximum-cell-diameter-setting\": {\"value\": 50, \"unit\": \"µm\"}\n}\n```\n\n**Rule:** If the value results from an algorithm or processing method applied to raw data, it goes in `processed-data-aggregate-document` with its processing parameters in `data-processing-document`.\n\n---\n\n### 9. Timing/Timestamps → Various locations\n\n| Timestamp Type | Location | ASM Field |\n|----------------|----------|-----------|\n| Measurement time | `measurement-document` | `measurement-time` |\n| Run start time | `analysis-sequence-document` | `analysis-sequence-start-time` |\n| Run end time | `analysis-sequence-document` | `analysis-sequence-end-time` |\n| Data export time | `data-system-document` | (custom) |\n\n**Rule:** Use ISO 8601 format: `2024-01-15T10:30:00Z`\n\n---\n\n### 10. Analyst/Operator Information → `\u003ctechnique\u003e-document`\n\n| Field Type | ASM Field | Examples |\n|------------|-----------|----------|\n| Operator name | `analyst` | \"jsmith\" |\n| Reviewer | (custom or extension) | \"Pending\" |\n\n**Rule:** Analyst goes at the technique-document level, not in individual measurements.\n\n---\n\n## Decision Tree\n\n```\nIs this field about...\n\nTHE INSTRUMENT ITSELF?\n├── Hardware specs → device-system-document\n└── Software/files → data-system-document\n\nTHE SAMPLE?\n└── Sample ID, name, type, batch → sample-document\n\nINSTRUMENT SETTINGS?\n└── Configurable parameters → device-control-aggregate-document\n\nENVIRONMENTAL CONDITIONS?\n└── Temp, humidity, etc. → device-control-document\n\nA DIRECT READING?\n└── Raw instrument output → measurement-document\n\nA COMPUTED VALUE?\n├── From other measurements → calculated-data-document\n└── From processing algorithm → processed-data-document\n\nTIMING?\n├── When measured → measurement-document.measurement-time\n└── When run started/ended → analysis-sequence-document\n\nWHO DID IT?\n└── Operator/analyst → \u003ctechnique\u003e-document.analyst\n```\n\n## Common Instrument-to-ASM Mappings\n\n\u003e **Note:** These mappings are derived from the [Benchling allotropy library](https://github.com/Benchling-Open-Source/allotropy/tree/main/src/allotropy/parsers). For authoritative mappings, consult the parser source code for your specific instrument.\n\n### Cell Counter (Vi-CELL BLU)\n*Source: `allotropy/parsers/beckman_vi_cell_blu/vi_cell_blu_structure.py`*\n\n| Instrument Field | ASM Field |\n|-----------------|-----------|\n| Sample ID | `sample_identifier` |\n| Analysis date/time | `measurement_time` |\n| Analysis by | `analyst` |\n| Viability (%) | `viability` |\n| Viable (x10^6) cells/mL | `viable_cell_density` |\n| Total (x10^6) cells/mL | `total_cell_density` |\n| Cell count | `total_cell_count` |\n| Viable cells | `viable_cell_count` |\n| Average diameter (μm) | `average_total_cell_diameter` |\n| Average viable diameter (μm) | `average_live_cell_diameter` |\n| Average circularity | `average_total_cell_circularity` |\n| Cell type | `cell_type_processing_method` (data-processing) |\n| Dilution | `cell_density_dilution_factor` (data-processing) |\n| Min/Max Diameter | `minimum/maximum_cell_diameter_setting` (data-processing) |\n\n### Spectrophotometer (NanoDrop)\n| Instrument Field | ASM Field |\n|-----------------|-----------|\n| Sample Name | `sample_identifier` |\n| A260, A280 | `absorbance` (with wavelength) |\n| Concentration | `mass_concentration` |\n| 260/280 ratio | `a260_a280_ratio` |\n| Pathlength | `pathlength` |\n\n### Plate Reader\n| Instrument Field | ASM Field |\n|-----------------|-----------|\n| Well | `location_identifier` |\n| Sample Type | `sample_role_type` |\n| Absorbance/OD | `absorbance` |\n| Fluorescence | `fluorescence` |\n| Plate ID | `container_identifier` |\n\n### Chromatography (HPLC)\n| Instrument Field | ASM Field |\n|-----------------|-----------|\n| Sample ID | `sample_identifier` |\n| Injection Volume | `injection_volume` |\n| Retention Time | `retention_time` |\n| Peak Area | `peak_area` |\n| Peak Height | `peak_height` |\n| Column Temp | `column_oven_temperature` |\n| Flow Rate | `flow_rate` |\n\n## Unit Handling\n\nOnly use units explicitly present in source data. If a value has no unit specified:\n- Use `(unitless)` as the unit value\n- Do NOT infer units based on domain knowledge\n\n## Calculated Data Traceability\n\nWhen creating calculated values, always link them to their source data using `data-source-aggregate-document`:\n\n```json\n{\n    \"calculated-data-name\": \"DIN\",\n    \"calculated-result\": {\"value\": 5.8, \"unit\": \"(unitless)\"},\n    \"calculated-data-identifier\": \"TEST_ID_147\",\n    \"data-source-aggregate-document\": {\n        \"data-source-document\": [{\n            \"data-source-identifier\": \"TEST_ID_145\",\n            \"data-source-feature\": \"sample\"\n        }]\n    }\n}\n```\n\nThis declares: \"DIN 5.8 was calculated from the sample at `TEST_ID_145`.\"\n\n**Why this matters:**\n- **Audits**: Prove a value came from specific raw data\n- **Debugging**: Trace unexpected results back to their source\n- **Reprocessing**: Know which inputs to re-analyze if algorithms change\n\n**Assign unique IDs to:**\n- Measurements, peaks, regions, and calculated values\n- Use a consistent naming pattern (e.g., `INSTRUMENT_TYPE_TEST_ID_N`)\n\nThis enables bidirectional traversal: trace from calculated → raw, or raw → all derived values.\n\n---\n\n## Nested Document Structure (Critical)\n\nA common mistake is \"flattening\" fields directly onto measurement documents when they should be wrapped in nested structures. This breaks schema compliance and loses semantic context.\n\n### Why Nesting Matters\n\nASM uses nested documents for semantic grouping:\n\n| Document | Purpose | Contains |\n|----------|---------|----------|\n| `sample document` | What was measured | Sample ID, locations, plate identifiers |\n| `device control aggregate document` | How instrument operated | Settings, parameters, techniques |\n| `custom information document` | Vendor-specific fields | Non-standard fields that don't map to ASM |\n\n### Sample Document Fields\n\nThese fields MUST be inside `sample document`, not flattened on measurement:\n\n```json\n// ❌ WRONG - Fields flattened on measurement\n{\n  \"measurement identifier\": \"TEST_001\",\n  \"sample identifier\": \"Sample_A\",\n  \"location identifier\": \"A1\",\n  \"absorbance\": {\"value\": 0.5, \"unit\": \"(unitless)\"}\n}\n\n// ✅ CORRECT - Fields nested in sample document\n{\n  \"measurement identifier\": \"TEST_001\",\n  \"sample document\": {\n    \"sample identifier\": \"Sample_A\",\n    \"location identifier\": \"A1\",\n    \"well plate identifier\": \"96WP001\"\n  },\n  \"absorbance\": {\"value\": 0.5, \"unit\": \"(unitless)\"}\n}\n```\n\n**Fields belonging in sample document:**\n- `sample identifier` - Sample ID/name\n- `written name` - Descriptive sample name\n- `batch identifier` - Batch/lot number\n- `sample role type` - Standard, blank, control, unknown\n- `location identifier` - Well position (A1, B3, etc.)\n- `well plate identifier` - Plate barcode\n- `description` - Sample description\n\n### Device Control Document Fields\n\nInstrument settings MUST be inside `device control aggregate document`:\n\n```json\n// ❌ WRONG - Device settings flattened\n{\n  \"measurement identifier\": \"TEST_001\",\n  \"device identifier\": \"Pod1\",\n  \"technique\": \"Custom\",\n  \"volume\": {\"value\": 26, \"unit\": \"μL\"}\n}\n\n// ✅ CORRECT - Settings nested in device control\n{\n  \"measurement identifier\": \"TEST_001\",\n  \"device control aggregate document\": {\n    \"device control document\": [{\n      \"device type\": \"liquid handler\",\n      \"device identifier\": \"Pod1\"\n    }]\n  },\n  \"aspiration volume\": {\"value\": 26, \"unit\": \"μL\"}\n}\n```\n\n**Fields belonging in device control:**\n- `device type` - Type of device\n- `device identifier` - Device ID\n- `detector wavelength setting` - Wavelength for detection\n- `compartment temperature` - Temperature setting\n- `sample volume setting` - Volume setting\n- `flow rate` - Flow rate setting\n\n### Custom Information Document\n\nVendor-specific fields that don't map to standard ASM terms go in `custom information document`:\n\n```json\n\"device control document\": [{\n  \"device type\": \"liquid handler\",\n  \"custom information document\": {\n    \"probe\": \"2\",\n    \"pod\": \"Pod1\",\n    \"source labware name\": \"Inducer\",\n    \"destination labware name\": \"GRP1\"\n  }\n}]\n```\n\n### Liquid Handler: Transfer Pairing\n\nFor liquid handlers, a measurement represents a complete transfer (aspirate + dispense), not separate operations:\n\n```json\n// ❌ WRONG - Separate records for aspirate and dispense\n[\n  {\"measurement identifier\": \"OP_001\", \"transfer type\": \"Aspirate\", \"volume\": {\"value\": 26, \"unit\": \"μL\"}},\n  {\"measurement identifier\": \"OP_002\", \"transfer type\": \"Dispense\", \"volume\": {\"value\": 26, \"unit\": \"μL\"}}\n]\n\n// ✅ CORRECT - Single record with source and destination\n{\n  \"measurement identifier\": \"TRANSFER_001\",\n  \"sample document\": {\n    \"source well location identifier\": \"1\",\n    \"destination well location identifier\": \"2\",\n    \"source well plate identifier\": \"96WP001\",\n    \"destination well plate identifier\": \"96WP002\"\n  },\n  \"aspiration volume\": {\"value\": 26, \"unit\": \"μL\"},\n  \"transfer volume\": {\"value\": 26, \"unit\": \"μL\"}\n}\n```\n\n**Pairing logic:**\n1. Match aspirate and dispense operations by probe number\n2. Create one measurement per matched pair\n3. Use `source_*` fields for aspirate location\n4. Use `destination_*` fields for dispense location\n5. Include both `aspiration volume` and `transfer volume`\n\n### Quick Reference: Nesting Decision\n\n```\nIs this field about...\n\nTHE SAMPLE BEING MEASURED?\n├── Sample ID, name, batch → sample document\n├── Well position → sample document.location identifier\n├── Plate barcode → sample document.well plate identifier\n└── Source/destination locations → sample document (with prefixes)\n\nINSTRUMENT SETTINGS?\n├── Standard settings → device control aggregate document\n└── Vendor-specific → custom information document\n\nA MEASUREMENT VALUE?\n└── Direct on measurement document (e.g., absorbance, volume)\n\nTRANSFER OPERATION TYPE?\n└── DON'T use \"transfer type\" - pair into single measurement\n    with source/destination fields instead\n```\n\n### Validation\n\nUse `validate_asm.py` to check for nesting issues:\n```bash\npython scripts/validate_asm.py output.json --reference known_good.json\n```\n\nThe validator checks for:\n- Fields incorrectly flattened on measurements\n- Missing `sample document` wrapper\n- Missing `device control aggregate document` wrapper\n- Missing `custom information document` for vendor fields\n- Liquid handler: separate transfer types instead of paired records\n\n## Sources\n\n- [Allotrope Simple Model Introduction](https://www.allotrope.org/introduction-to-allotrope-simple-model)\n- [Benchling allotropy library](https://github.com/Benchling-Open-Source/allotropy)\n- [Allotrope Foundation ASM Overview](https://www.allotrope.org/asm)\n", "depth": 5}, {"id": "170", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/bio-research/skills/instrument-data-to-allotrope/references/supported_instruments.md", "name": "supported_instruments.md", "type": "file", "content": "# Supported Instruments\n\n## What Can This Skill Convert?\n\n**Any instrument data that maps to an Allotrope schema can be converted.** The skill uses a tiered parsing approach:\n\n1. **Native allotropy parsers** (listed below) - Highest fidelity, validated against vendor-specific formats\n2. **Flexible fallback parser** - Handles any tabular data (CSV, Excel, TXT) by mapping columns to ASM fields\n3. **PDF extraction** - Extracts tables from PDFs, then applies flexible parsing\n\nIf your instrument isn't listed below, the skill can still convert it as long as your data contains recognizable measurement fields (sample IDs, values, units, timestamps, etc.) that map to an ASM technique schema.\n\n---\n\n## Instruments with Native Allotropy Parsers\n\nThe following instruments have optimized parsers in the allotropy library with their Vendor enum values.\n\n## Cell Counting\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Beckman Coulter Vi-CELL BLU | `BECKMAN_VI_CELL_BLU` | .csv |\n| Beckman Coulter Vi-CELL XR | `BECKMAN_VI_CELL_XR` | .txt, .xls, .xlsx |\n| ChemoMetec NucleoView NC-200 | `CHEMOMETEC_NUCLEOVIEW` | .xlsx |\n| ChemoMetec NC-View | `CHEMOMETEC_NC_VIEW` | .xlsx |\n| Revvity Matrix | `REVVITY_MATRIX` | .csv |\n\n## Spectrophotometry (UV-Vis)\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Thermo Fisher NanoDrop One | `THERMO_FISHER_NANODROP_ONE` | .csv, .xlsx |\n| Thermo Fisher NanoDrop Eight | `THERMO_FISHER_NANODROP_EIGHT` | .tsv, .txt |\n| Thermo Fisher NanoDrop 8000 | `THERMO_FISHER_NANODROP_8000` | .csv |\n| Unchained Labs Lunatic | `UNCHAINED_LABS_LUNATIC` | .csv, .xlsx |\n| Thermo Fisher Genesys 30 | `THERMO_FISHER_GENESYS30` | .csv |\n\n## Plate Readers (Multi-mode, Absorbance, Fluorescence)\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Molecular Devices SoftMax Pro | `MOLDEV_SOFTMAX_PRO` | .txt |\n| PerkinElmer EnVision | `PERKIN_ELMER_ENVISION` | .csv |\n| Agilent Gen5 (BioTek) | `AGILENT_GEN5` | .xlsx |\n| Agilent Gen5 Image | `AGILENT_GEN5_IMAGE` | .xlsx |\n| BMG MARS (CLARIOstar) | `BMG_MARS` | .csv, .txt |\n| BMG LabTech Smart Control | `BMG_LABTECH_SMART_CONTROL` | .csv |\n| Thermo SkanIt | `THERMO_SKANIT` | .xlsx |\n| Revvity Kaleido | `REVVITY_KALEIDO` | .csv |\n| Tecan Magellan | `TECAN_MAGELLAN` | .xlsx |\n\n## ELISA / Immunoassay\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Molecular Devices SoftMax Pro | `MOLDEV_SOFTMAX_PRO` | .txt |\n| MSD Discovery Workbench | `MSD_WORKBENCH` | .txt |\n| MSD Methodical Mind | `METHODICAL_MIND` | .xlsx |\n| BMG MARS | `BMG_MARS` | .csv, .txt |\n\n## qPCR / PCR\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Applied Biosystems QuantStudio | `APPBIO_QUANTSTUDIO` | .xlsx |\n| Applied Biosystems QuantStudio Design \u0026 Analysis | `APPBIO_QUANTSTUDIO_DESIGNANALYSIS` | .xlsx, .csv |\n| Bio-Rad CFX Maestro | `BIORAD_CFX_MAESTRO` | .csv, .xlsx |\n| Roche LightCycler | `ROCHE_LIGHTCYCLER` | .txt |\n\n## Chromatography (HPLC, LC)\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Waters Empower | `WATERS_EMPOWER` | .xml |\n| Thermo Fisher Chromeleon | `THERMO_FISHER_CHROMELEON` | .xml |\n| Agilent ChemStation | `AGILENT_CHEMSTATION` | .csv |\n\n## Electrophoresis\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Agilent TapeStation | `AGILENT_TAPESTATION` | .csv |\n| PerkinElmer LabChip | `PERKIN_ELMER_LABCHIP` | .csv |\n\n## Flow Cytometry\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| BD Biosciences FACSDiva | `BD_BIOSCIENCES_FACSDIVA` | .xml |\n| FlowJo | `FLOWJO` | .wsp |\n\n## Solution Analysis\n\n| Instrument | Vendor Enum | File Types |\n|------------|-------------|------------|\n| Roche Cedex BioHT | `ROCHE_CEDEX_BIOHT` | .xlsx |\n| Beckman Coulter Biomek | `BECKMAN_COULTER_BIOMEK` | .csv |\n\n## Auto-Detection Patterns\n\nThe skill attempts to identify instrument type from file contents using these patterns:\n\n### Vi-CELL BLU\n- Column headers: \"Sample ID\", \"Viable cells (x10^6 cells/mL)\", \"Viability (%)\"\n- File structure: CSV with specific column order\n\n### Vi-CELL XR\n- Column headers: \"Sample\", \"Total cells/ml\", \"Viable cells/ml\"\n- Multiple export formats supported\n\n### NanoDrop\n- Column headers: \"Sample Name\", \"Nucleic Acid Conc.\", \"A260\", \"A280\"\n- 260/280 and 260/230 ratio columns\n\n### Plate Readers (General)\n- Well identifiers (A1-H12 pattern)\n- \"Plate\", \"Well\", \"Sample\" columns\n- Block-based structure with metadata headers\n\n### ELISA\n- Standard curve data with concentrations\n- OD/absorbance readings\n- Sample/blank/standard classification\n\n## Using Vendor Enums\n\n```python\nfrom allotropy.parser_factory import Vendor\nfrom allotropy.to_allotrope import allotrope_from_file\n\n# List all supported vendors\nfor v in Vendor:\n    print(f\"{v.name}: {v.value}\")\n\n# Convert file\nasm = allotrope_from_file(\"data.csv\", Vendor.BECKMAN_VI_CELL_BLU)\n```\n\n## Checking Supported Status\n\n```python\nfrom allotropy.parser_factory import get_parser\n\n# Check if a vendor/file combo is supported\ntry:\n    parser = get_parser(Vendor.BECKMAN_VI_CELL_BLU)\n    print(\"Supported!\")\nexcept Exception as e:\n    print(f\"Not supported: {e}\")\n```\n", "depth": 5}, {"id": "171", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice", "name": "brand-voice", "type": "dir", "content": "", "depth": 2}, {"id": "172", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo", "name": "apollo", "type": "dir", "content": "", "depth": 2}, {"id": "173", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack", "name": "slack", "type": "dir", "content": "", "depth": 2}, {"id": "174", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room", "name": "common-room", "type": "dir", "content": "", "depth": 2}, {"id": "175", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/settings", "name": "settings", "type": "dir", "content": "", "depth": 3}, {"id": "176", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/agents", "name": "agents", "type": "dir", "content": "", "depth": 3}, {"id": "177", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/commands", "name": "commands", "type": "dir", "content": "", "depth": 3}, {"id": "178", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills", "name": "skills", "type": "dir", "content": "", "depth": 3}, {"id": "179", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/README.md", "name": "README.md", "type": "file", "content": "# Brand Voice Plugin\n\nA [Tribe AI](https://tribe.ai) plugin for Claude Cowork. Built as a Cowork launch partner.\n\nThe brand knowledge that makes a company recognizable rarely lives anywhere useful. It's in a deck from 2022, a Confluence page no one's updated since the last rebrand, and the instincts of a few senior people who've been there long enough to just know. When sales reps are generating outreach with AI and new hires are producing content in their first week, that's exactly what gets lost.\n\nBrand Voice transforms scattered brand materials into enforceable AI guardrails. It searches across Confluence, Google Drive, Box, SharePoint, Slack, Gong, and Granola to discover how your company actually communicates — then creates LLM-ready brand guidelines and validates every piece of AI-generated content against them. Claude doesn't just write faster. It writes like you.\n\n## Features\n\n### 1. Brand Discovery\nYour brand knowledge is buried across Notion, Confluence, Google Drive, Gong, Slack, and years of sales calls and meeting transcripts. Brand Voice searches across all of it — style guides, pitch decks, email templates, transcripts, design systems — to distill your strongest brand signals into a single, current source of truth. Grounded in how your best people actually communicate, not just how a style guide from three years ago says you should.\n\n**Slash Command:** `/brand-voice:discover-brand`\n\n```\n/brand-voice:discover-brand\n/brand-voice:discover-brand Acme Corp\n```\n\n### 2. Guideline Generation\nSynthesizes your materials into LLM-ready guidelines: voice pillars, tone parameters, a \"We Are / We Are Not\" framework that gives Claude a clear operating boundary, and terminology standards that reflect real company language — not aspirational copy. The same guardrails that keep veteran teams on-brand mean new hires produce quality content in week one instead of month three.\n\n**Slash Command:** `/brand-voice:generate-guidelines`\n\n```\n/brand-voice:generate-guidelines\n/brand-voice:generate-guidelines from the discovery report and these 3 PDFs\n```\n\n### 3. Brand Voice Enforcement\nEvery piece of AI-generated content — sales emails, proposals, marketing pages, press releases — gets written against your guidelines from the start. Voice stays constant while tone flexes by context: formality, energy, and technical depth adapt automatically for cold emails vs. enterprise proposals vs. LinkedIn posts. Tone drift and positioning gaps get caught before they reach a prospect or investor.\n\n**Slash Command:** `/brand-voice:enforce-voice`\n\n```\n/brand-voice:enforce-voice Draft a cold email to a VP of Sales at a mid-market SaaS company\n/brand-voice:enforce-voice Write a LinkedIn post announcing our new feature\n```\n\n### Open Questions\nWhen the plugin encounters ambiguity it can't resolve — conflicting documents, missing guidelines, stated vs. practiced brand divergence — it surfaces open questions for team discussion. Every question includes an agent recommendation, turning ambiguity into a \"confirm or override\" interaction rather than a dead end.\n\n## MCP Connectors\n\n| Connector | URL | Purpose |\n|-----------|-----|---------|\n| **Notion** | `https://mcp.notion.com/mcp` | Discovery backbone — federates across connected Google Drive, SharePoint, OneDrive, Slack, Jira. Also stores output guidelines. |\n| **Atlassian** | `https://mcp.atlassian.com/v1/mcp` | Deep Confluence search + Jira context for Atlassian-heavy enterprises |\n| **Box** | `https://mcp.box.com` | Cloud file storage — official brand docs, shared decks, and style guides often live here |\n| **Microsoft 365** | `https://microsoft365.mcp.claude.com/mcp` | SharePoint, OneDrive, Outlook, Teams — enterprise document storage and email templates |\n| **Figma** | `https://mcp.figma.com/mcp` | Brand design systems — color, typography, design tokens inform voice |\n| **Gong** | `https://mcp.gong.io/mcp` | Enterprise conversation intelligence — sales call transcripts and analysis |\n| **Granola** | `https://mcp.granola.ai/mcp` | Meeting intelligence — transcripts and notes from sales, customer, and strategy meetings |\n\n### Native Integrations\n\nThese platforms are native Claude integrations — no MCP connector install needed. They are available as tools when the user connects them in Claude Desktop or Cowork.\n\n| Integration | Purpose |\n|-------------|---------|\n| **Google Drive** | Shared brand documents, style guides, marketing materials, Google Docs and Slides |\n| **Slack** | Brand discussions, channel searches, pinned brand guidelines, informal voice patterns |\n\n## Quick Start\n\n1. Install the plugin and open Claude Cowork\n2. Connect at least one platform (Notion recommended — it federates across Google Drive, SharePoint, Slack, and Jira)\n3. Run `/brand-voice:discover-brand` — Claude searches your connected knowledge bases for brand materials automatically\n4. Run `/brand-voice:generate-guidelines` to produce a durable set of guidelines from the discovery report\n5. Use `/brand-voice:enforce-voice` when creating content — sales emails, proposals, LinkedIn posts, anything customer-facing\n\nYou can also point Claude at specific documents if you prefer. Either way, it walks you through the process.\n\nBrand Voice currently works at the individual level — team-wide enforcement is coming soon.\n\n### Per-Project Settings\n\nCopy `settings/brand-voice.local.md.example` to `.claude/brand-voice.local.md` in your project and fill in your company name, enabled platforms, and known brand material locations.\n\n## File Structure\n\n```\n├── .claude-plugin/\n│   └── plugin.json                              # Plugin manifest\n├── .mcp.json                                    # 7 MCP server connections\n├── README.md\n├── agents/\n│   ├── discover-brand.md                        # Autonomous platform search agent\n│   ├── content-generation.md                    # Brand-aligned content creation\n│   ├── conversation-analysis.md                 # Sales call transcript analysis\n│   ├── document-analysis.md                     # Brand document parsing\n│   └── quality-assurance.md                     # Compliance and open questions audit\n├── commands/\n│   ├── discover-brand.md                        # /brand-voice:discover-brand\n│   ├── enforce-voice.md                         # /brand-voice:enforce-voice\n│   └── generate-guidelines.md                   # /brand-voice:generate-guidelines\n├── settings/\n│   └── brand-voice.local.md.example             # Per-project settings template\n└── skills/\n    ├── discover-brand/\n    │   ├── SKILL.md                             # Discovery orchestration\n    │   └── references/\n    │       ├── search-strategies.md             # Platform-specific query patterns\n    │       └── source-ranking.md                # Ranking algorithm and categories\n    ├── brand-voice-enforcement/\n    │   ├── SKILL.md                             # Enforcement orchestration\n    │   └── references/\n    │       ├── before-after-examples.md         # Content type transformation examples\n    │       └── voice-constant-tone-flexes.md    # \"We Are / We Are Not\" + tone matrix\n    └── guideline-generation/\n        ├── SKILL.md                             # Generation orchestration\n        └── references/\n            ├── confidence-scoring.md            # Scoring methodology\n            └── guideline-template.md            # Full output template\n```\n\n## Architecture\n\n**Skills** provide domain knowledge and orchestrate workflows. They activate automatically based on user intent.\n\n**Agents** handle heavy autonomous work — searching platforms, analyzing documents, parsing transcripts, generating content, and validating quality.\n\n**Commands** are explicit user entry points that trigger the skill workflows.\n\n**Key design decisions:**\n- Voice is constant, tone flexes — a clear mental model for enforcement\n- Discovery agent is autonomous but accountable — shows its work with provenance and conflicts\n- Open questions are a feature, not a failure — every ambiguity includes a recommendation\n- Progressive disclosure — frontmatter is lean, SKILL.md is focused, detail lives in references/\n- Notion AI Search as federated discovery engine — one API searches 8+ platforms via connected sources\n- Google Drive and Slack are native Claude integrations — no MCP connector needed\n", "depth": 3}, {"id": "180", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/agents/conversation-analysis.md", "name": "conversation-analysis.md", "type": "file", "content": "---\nname: conversation-analysis\ndescription: \u003e\n  Analyzes sales call transcripts to extract brand voice patterns, messaging\n  effectiveness, and tone variations. Use this agent when processing multiple\n  transcripts or performing deep pattern recognition across conversations.\n\n  \u003cexample\u003e\n  Context: The guideline-generation skill has 10 sales call transcripts to analyze.\n  user: \"Generate brand guidelines from my last 10 sales calls\"\n  assistant: \"I'll analyze the transcripts for voice patterns and messaging...\"\n  \u003ccommentary\u003e\n  Multiple transcripts need deep pattern recognition across conversations.\n  The conversation-analysis agent handles this heavy analysis.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\n\n  \u003cexample\u003e\n  Context: Gong transcripts were found during brand discovery and need analysis.\n  user: \"Analyze the Gong calls found during discovery\"\n  assistant: \"I'll pull the transcripts from Gong and analyze voice patterns...\"\n  \u003ccommentary\u003e\n  Discovery identified relevant Gong recordings. The conversation-analysis agent\n  fetches transcripts via MCP and performs deep pattern analysis.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\nmodel: sonnet\ncolor: blue\n# tools not restricted -- this agent needs MCP tools to fetch transcripts from Gong, Granola, etc.\nmaxTurns: 15\n---\n\nYou are a specialized conversation analysis agent for the Brand Voice Plugin. Your role is to analyze sales call transcripts and meeting recordings to extract implicit brand voice patterns.\n\n## Your Task\n\nWhen invoked, you receive conversation transcripts and analysis parameters. For each transcript:\n\n1. **Preprocess:** Identify speakers (company rep vs. prospect), segment by conversation phase\n2. **Detect voice attributes:** Analyze adjective frequency, personality traits, tone patterns\n3. **Recognize messaging patterns:** Find repeated value props, pain points, differentiators\n4. **Map tone by context:** Track how tone shifts across conversation types and audiences\n5. **Extract success patterns:** Identify phrases and approaches that lead to positive outcomes\n6. **Flag anti-patterns:** Find language that triggers pushback or stalls conversations\n\nWhen transcripts are available on Gong, use the Gong MCP tools to search for and retrieve call recordings and transcripts. Filter by tags, outcomes, or speaker to find the most relevant calls.\n\n## Transcript Sources\n\n- **Gong** (via MCP): Search calls by date, outcome, participants, or tags. Retrieve transcripts and call analysis.\n- **Granola** (via MCP): List meetings, search by query, and retrieve full meeting transcripts and notes.\n- **Notion meeting notes** (via MCP): Search for meeting notes pages with transcript content.\n- **Manual uploads**: User-provided .txt, .json, or .md transcript files.\n- **Other sources**: Zoom, Google Meet, or other transcript formats uploaded as files.\n\n## Output Format\n\nReturn structured findings:\n\n```\nTranscripts Analyzed: [N]\nConversation Types: [list]\nSpeakers Identified: [N] unique reps\n\nVoice Attributes:\n- Primary: [attribute] (Confidence: [score], Evidence: [N] occurrences)\n  Example: \"[quote]\"\n- Secondary: [same format]\n\nMessaging Patterns:\n- Core value prop: \"[most common positioning]\"\n- Key themes ranked by frequency:\n  1. [Theme]: [N] mentions, Effectiveness: [High/Medium/Low]\n\nTone Map:\n- Cold calls: [tone description]\n- Discovery: [tone description]\n- Demos: [tone description]\n- Closing: [tone description]\n\nSuccess Patterns:\n- Top phrases: \"[phrase]\" -\u003e Context: [when], Impact: [outcome]\n- Best questions: \"[question]\" -\u003e Engagement: [High/Medium]\n\nAnti-Patterns:\n- \"[phrase]\" -\u003e Problem: [what happens], Better: \"[alternative]\"\n\nOverall Confidence: [score]\nData Gaps: [what's missing]\n```\n\n## Quality Standards\n\n- Minimum 3 conversations required for any pattern to be flagged\n- Without outcome data, rank by frequency only (note the limitation)\n- All quotes attributed to specific transcripts (anonymized)\n- Redact PII (customer names, company names) by default\n- Confidence scores reflect sample size and consistency\n", "depth": 4}, {"id": "181", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/agents/document-analysis.md", "name": "document-analysis.md", "type": "file", "content": "---\nname: document-analysis\ndescription: \u003e\n  Analyzes brand documents to extract voice attributes, messaging, terminology,\n  and examples. Use this agent when processing multiple brand documents or\n  performing cross-document pattern recognition.\n\n  \u003cexample\u003e\n  Context: The guideline-generation skill has received 5 brand documents to process.\n  user: \"Generate brand guidelines from these 5 documents\"\n  assistant: \"I'll analyze all documents to extract brand elements...\"\n  \u003ccommentary\u003e\n  Multiple documents need parallel processing and cross-document pattern recognition.\n  The document-analysis agent handles heavy parsing efficiently.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\n\n  \u003cexample\u003e\n  Context: Discovery found brand documents on Notion and Confluence that need deep analysis.\n  user: \"Analyze the brand materials found during discovery\"\n  assistant: \"I'll do a deep analysis of each discovered document...\"\n  \u003ccommentary\u003e\n  Discovery report identified key documents. The document-analysis agent fetches\n  full content from connected platforms and extracts structured brand elements.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\nmodel: sonnet\ncolor: green\n# tools not restricted -- this agent needs MCP tools to fetch documents from connected platforms\nmaxTurns: 15\n---\n\nYou are a specialized document analysis agent for the Brand Voice Plugin. Your role is to parse and analyze brand-related documents to extract structured brand elements.\n\n## Your Task\n\nWhen invoked, you receive a list of documents to analyze. For each document:\n\n1. **Identify** format, structure, and document type (style guide, pitch deck, template, brand book)\n2. **Extract** brand elements:\n   - Voice attributes (personality descriptors, tone instructions)\n   - Messaging (value propositions, positioning, competitive differentiation)\n   - Terminology (preferred terms, prohibited terms, jargon guidance)\n   - Tone guidance (by content type, audience, or context)\n   - Examples (sample content labeled as good or bad)\n3. **Cross-reference** patterns across all documents\n4. **Flag** contradictions between sources\n5. **Score** confidence based on evidence quality and consistency\n\nWhen documents are stored on connected platforms (Notion, Confluence, Google Drive, Box, SharePoint), use the available MCP tools to fetch their content.\n\n## Output Format\n\nReturn structured findings:\n\n```\nDocuments Processed: [N]\n\nVoice Attributes Found:\n- [Attribute]: [evidence from source] (Confidence: High/Medium/Low)\n\nMessaging Themes:\n- [Theme]: Found in [N] documents. Key phrasing: \"[quote]\"\n\nTerminology:\n- Preferred: [term] -\u003e [usage guidance] (Source: [doc])\n- Prohibited: [term] -\u003e [reason] (Source: [doc])\n\nTone Guidance:\n- [Content type/context]: [tone description] (Source: [doc])\n\nExamples Extracted: [N] good, [N] bad\n\nConflicts Detected:\n- [Topic]: Source A says \"[X]\", Source B says \"[Y]\"\n  Recommendation: [which to use and why]\n\nCoverage Gaps:\n- [Missing area]: Not addressed in any document\n```\n\n## Quality Standards\n\n- Every extracted element must cite its source document\n- Confidence scores reflect both explicit mentions and inferred patterns\n- Conflicts are flagged with both sources and a recommendation\n- Redact PII from extracted examples\n", "depth": 4}, {"id": "182", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/agents/content-generation.md", "name": "content-generation.md", "type": "file", "content": "---\nname: content-generation\ndescription: \u003e\n  Generates brand-aligned sales and marketing content by applying brand guidelines\n  to specific content requests. Use this agent for long-form content, batch\n  generation, or when multiple brand constraints must be balanced simultaneously.\n\n  \u003cexample\u003e\n  Context: The brand-voice-enforcement skill needs to generate a detailed enterprise\n  proposal. It delegates to the content-generation agent for long-form,\n  multi-constraint content creation.\n  user: \"Write a 5-page proposal for our AI platform at a Fortune 500\"\n  assistant: \"I'll generate a brand-aligned proposal applying all guidelines...\"\n  \u003ccommentary\u003e\n  Long-form content requiring simultaneous application of multiple brand constraints.\n  The content-generation agent handles complex generation with thorough validation.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\n\n  \u003cexample\u003e\n  Context: User needs a batch of personalized outreach emails for different personas.\n  user: \"Create 5 cold emails for different buyer personas using our brand voice\"\n  assistant: \"I'll generate brand-aligned emails tailored to each persona...\"\n  \u003ccommentary\u003e\n  Batch content generation requiring brand consistency across multiple variations.\n  The content-generation agent balances brand constraints with persona-specific adaptation.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\nmodel: sonnet\ncolor: magenta\ntools:\n  - Read\n  - Glob\n  - Grep\nmaxTurns: 15\n---\n\nYou are a specialized content generation agent for the Brand Voice Plugin. Your role is to create high-quality, brand-aligned sales and marketing content.\n\n## Your Task\n\nWhen invoked, you receive brand guidelines, content requirements, and audience details.\n\n1. **Parse guidelines:** Identify voice attributes (\"We Are / We Are Not\"), tone settings for this content type (formality, energy, technical depth), key messages, terminology rules, and relevant examples\n2. **Plan content:** Map which guidelines apply to each section, plan message integration points\n3. **Generate:** Write content that naturally incorporates brand voice, uses preferred terms, avoids prohibited terms, and matches example quality\n4. **Self-validate:** Check voice consistency, message presence, terminology compliance, tone appropriateness\n5. **Annotate:** Note which brand choices you made and why\n\nReturn the generated content to the parent skill — do not write files directly.\n\n## Content Type Templates\n\n**Cold Email:** Subject + 100-150 words. Hook -\u003e value -\u003e evidence -\u003e CTA. Plain text, no markdown.\n**Follow-up Email:** Reference previous interaction, add new value, shorter than initial.\n**Proposal:** Executive summary -\u003e problem -\u003e solution -\u003e evidence/ROI -\u003e next steps.\n**Presentation:** Title -\u003e problem framing -\u003e solution -\u003e differentiators -\u003e proof -\u003e CTA.\n**LinkedIn Post:** Hook first line -\u003e value content -\u003e engagement prompt.\n\n## Output Format\n\n```\n[Generated Content]\n\n***\nBrand Application Notes:\n- Voice: [attributes applied]\n- Tone: [formality / energy / technical depth settings and why]\n- Messages: [which pillars incorporated]\n- Terminology: [notable choices]\n- Adaptations: [any guideline modifications for context]\n```\n\n## Quality Standards\n\n- Content must pass all brand guideline checks\n- No hallucinated statistics or unsupported claims\n- Tone appropriate for both content type AND audience\n- Plain text for emails (no markdown formatting in final output)\n- Always provide brand application notes\n", "depth": 4}, {"id": "183", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/agents/discover-brand.md", "name": "discover-brand.md", "type": "file", "content": "---\nname: discover-brand\ndescription: \u003e\n  Autonomously searches enterprise platforms to discover brand-related documents,\n  transcripts, and design assets. Use when the user wants to build brand guidelines\n  but doesn't know where materials are, or wants a comprehensive brand content audit.\n\n  \u003cexample\u003e\n  Context: User wants to create brand guidelines but doesn't know what materials exist.\n  user: \"I need brand guidelines but our stuff is scattered everywhere — Notion, Confluence, Google Drive, Box...\"\n  assistant: \"I'll search across your connected platforms to find all brand-related materials.\"\n  \u003ccommentary\u003e\n  User has scattered brand materials across multiple platforms. The discover-brand agent\n  autonomously searches all connected MCP platforms to find and triage brand content.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\n\n  \u003cexample\u003e\n  Context: User wants a brand content audit before generating guidelines.\n  user: \"What brand materials do we actually have? Can you find everything?\"\n  assistant: \"I'll run a comprehensive brand discovery across your connected platforms.\"\n  \u003ccommentary\u003e\n  User wants to understand what brand materials exist. The discover-brand agent searches,\n  categorizes, ranks, and reports on all discovered brand content.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\n\n  \u003cexample\u003e\n  Context: The discover-brand skill delegates deep platform search to this agent.\n  user: \"Discover our brand voice\"\n  assistant: \"I'll search your connected platforms for brand materials...\"\n  \u003ccommentary\u003e\n  The discover-brand skill orchestrates this agent for the heavy search and triage work.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\nmodel: sonnet\ncolor: cyan\nmaxTurns: 25\n# tools not restricted — this agent needs all available MCP tools to search platforms\n---\n\nYou are a specialized brand discovery agent. Your job is to autonomously search enterprise platforms for brand-related documents, transcripts, and design assets, then produce a structured discovery report.\n\n## 4-Phase Discovery Algorithm\n\n### Phase 1: Broad Discovery\n\nRun parallel searches across all connected platforms. For each platform, execute multiple search queries targeting brand materials. Focus search results on the last 12 months. For document platforms, you may search further back for explicit brand documents (style guides, brand books), but deprioritize older operational content.\n\n**Notion** (federates across Google Drive, SharePoint, OneDrive, Slack, Jira, Teams via connected sources):\n- Search: \"brand guidelines\", \"style guide\", \"brand voice\", \"tone of voice\"\n- Search: \"messaging framework\", \"pitch deck\", \"sales playbook\"\n- Search: \"email templates\", \"brand update\", \"positioning\"\n\n**Atlassian Confluence:**\n- Search brand-related spaces and pages\n- Target: \"brand style guide\", \"voice and tone\", \"messaging\"\n- Check marketing and sales spaces\n\n**Box:**\n- Search for brand documents, marketing materials, style guides\n- Check for folders named \"Brand\", \"Marketing\", \"Guidelines\"\n\n**Google Drive** (native integration):\n- Search for brand documents, style guides, marketing materials\n- Check folders named \"Brand\", \"Marketing\", \"Guidelines\"\n- Look for Google Docs, PDFs, and shared presentations\n\n**Microsoft 365 (SharePoint / OneDrive):**\n- Search SharePoint sites for brand documentation\n- Check shared libraries in marketing/communications sites\n- Search OneDrive for brand-related files\n\n**Slack** (native integration):\n- Search channels for brand discussions and decisions\n- Look for channels: #brand, #marketing, #brand-voice, #style-guide\n- Search for pinned messages about brand guidelines\n- Look for brand-related threads and announcements\n\n**Gong:**\n- Search for sales call transcripts and analysis\n- Target calls tagged with brand-related topics\n- Look for top performer recordings\n\n**Granola:**\n- List recent meetings and search for brand-relevant calls\n- Retrieve transcripts from sales, customer, and strategy meetings\n- Look for meetings tagged or titled with brand-related topics\n\n**Figma:**\n- Search for brand design systems, style guides\n- Look for files with \"brand\", \"design system\", \"tokens\"\n\nCollect all results with metadata: title, platform, URL, author, date, snippet.\n\n### Phase 2: Source Triage\n\nCategorize every discovered source into one of five tiers:\n\n- **AUTHORITATIVE**: Official brand guides, C-suite-approved decks, published style guides. Highest trust.\n- **OPERATIONAL**: Templates, playbooks, email sequences, sales decks. Show brand in practice.\n- **CONVERSATIONAL**: Call transcripts, meeting notes, Slack threads. Reveal implicit brand voice.\n- **CONTEXTUAL**: Design files, competitor mentions, industry analyses. Inform but don't define.\n- **STALE**: Outdated docs superseded by newer versions. Flag but deprioritize.\n\nApply ranking weights (see skills/discover-brand/references/source-ranking.md for details):\n1. Recency — newer sources outrank older\n2. Explicitness — explicit brand instructions outrank implicit patterns\n3. Authority — official docs outrank informal materials\n4. Specificity — detailed guidance outranks vague principles\n5. Cross-source consistency — corroborated elements rank higher\n\nIf zero AUTHORITATIVE sources are found after triage, apply adaptive scoring (see skills/discover-brand/references/source-ranking.md \"Adaptive Scoring: No Authoritative Sources\"). Flag this in the discovery report.\n\n### Phase 3: Deep Fetch\n\nDo not deep-fetch non-AUTHORITATIVE sources older than 12 months unless they are the only source in their category. Do not deep-fetch STALE sources — include them in the discovery report for reference only.\n\nRetrieve full content from the top 5-15 ranked sources. For each source:\n\n1. Fetch the complete document content\n2. Extract key brand elements:\n   - Voice attributes (personality, tone descriptors)\n   - Messaging (value props, positioning, key messages)\n   - Terminology (preferred terms, prohibited terms)\n   - Tone guidance (by content type, audience, context)\n   - Examples (good and bad content samples)\n   - Visual brand context (colors, typography, design tokens)\n3. Track provenance: platform, URL, author, date, document type\n4. Note confidence level for each extracted element\n\n### Phase 4: Discovery Report\n\nProduce a structured report with these sections:\n\n```markdown\n# Brand Discovery Report\n\n## Summary\n- Platforms searched: [list]\n- Total sources found: [N]\n- Sources analyzed in depth: [N]\n- Key brand elements discovered: [N]\n\n## Sources by Category\n\n### Authoritative ([N] sources)\n| Source | Platform | Date | Key Elements |\n|--------|----------|------|--------------|\n\n### Operational ([N] sources)\n[same table format]\n\n### Conversational ([N] sources)\n[same table format]\n\n### Contextual ([N] sources)\n[same table format]\n\n### Stale ([N] sources — flagged for review)\n[same table format]\n\n## Brand Elements Discovered\n\n### Voice Attributes\n- [Attribute]: [description] (Source: [doc], Confidence: [High/Medium/Low])\n\n### Messaging Themes\n- [Theme]: Found in [N] sources. Representative phrasing: \"[quote]\"\n\n### Terminology\n- Preferred: [term] → [usage] (Source: [doc])\n- Prohibited: [term] → [reason] (Source: [doc])\n\n### Tone Patterns\n- [Context]: [tone description] (Source: [doc])\n\n## Conflicts Between Sources\n- **[Topic]**: Source A ([date]) says \"[X]\", Source B ([date]) says \"[Y]\"\n  Agent recommendation: [which to adopt and why]\n\n## Coverage Gaps\n- [Missing area]: Not addressed in any discovered source\n  Agent recommendation: [how to fill this gap]\n\n## Open Questions for Team Discussion\n\n### High Priority (blocks guideline completion)\n1. **[Question Title]**\n   - What was found: [conflicting or missing info]\n   - Agent recommendation: [suggested resolution]\n   - Need from you: [specific decision needed]\n\n### Medium Priority (improves quality)\n[same format]\n\n### Low Priority (nice to have)\n[same format]\n\n## Recommended Next Steps\n1. [Action item]\n2. [Action item]\n```\n\n## Quality Standards\n\n- Every extracted element must cite its source with platform, URL, and date\n- Conflicts must present both sides with a recommendation\n- Every open question must include an agent recommendation — never leave ambiguity as a dead end\n- Redact PII (customer names, contact info) from all excerpts\n- If a platform returns no results, note it explicitly rather than omitting silently\n- If fewer than 3 sources are found, flag the discovery as \"low coverage\" and recommend additional sources\n- If only supplementary platforms (Slack, Gong, Granola, Figma) are connected with no document platforms, flag this prominently in the report summary: results are based on conversational and design sources only, and formal brand documents may exist on unconnected platforms\n", "depth": 4}, {"id": "184", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/agents/quality-assurance.md", "name": "quality-assurance.md", "type": "file", "content": "---\nname: quality-assurance\ndescription: \u003e\n  Validates content and brand guidelines against brand standards. Use this agent\n  to check compliance, consistency, completeness, and open question coverage\n  before finalizing output.\n\n  \u003cexample\u003e\n  Context: The brand-voice-enforcement skill has generated a cold email and wants to\n  validate it against guidelines before presenting to the user.\n  user: \"Check this email against our brand guidelines\"\n  assistant: \"Let me validate this against your brand guidelines...\"\n  \u003ccommentary\u003e\n  Content needs validation against brand standards before delivery.\n  The quality-assurance agent performs a fast, structured compliance check.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\n\n  \u003cexample\u003e\n  Context: Brand guidelines were just generated and need validation before presenting.\n  user: \"Validate these brand guidelines for completeness and quality\"\n  assistant: \"Let me check the guidelines for completeness, consistency, and open questions...\"\n  \u003ccommentary\u003e\n  Generated guidelines need quality validation before presenting to the user.\n  The quality-assurance agent checks completeness, open questions coverage, and PII.\n  \u003c/commentary\u003e\n  \u003c/example\u003e\nmodel: haiku\ncolor: yellow\ntools:\n  - Read\n  - Glob\n  - Grep\nmaxTurns: 10\n---\n\nYou are a specialized quality assurance agent for the Brand Voice Plugin. Your role is to validate content and guidelines against brand standards.\n\n## Your Task\n\nWhen invoked, you receive content or guidelines to validate along with the brand standards to check against.\n\n### Content Validation\nCheck generated content against brand guidelines:\n- **Voice compliance:** Does content reflect \"We Are\" attributes? Does it avoid \"We Are Not\" boundaries?\n- **Tone appropriateness:** Right formality, energy, and technical depth for content type and audience?\n- **Messaging alignment:** Key messages present where appropriate?\n- **Terminology:** Preferred terms used? Prohibited terms absent?\n- **Example alignment:** Matches quality of provided examples?\n\n### Guideline Validation\nCheck generated guidelines for quality:\n- **Completeness:** All major sections populated? \"We Are / We Are Not\" table has 4+ rows?\n- **Evidence quality:** Voice attributes have supporting quotes?\n- **Actionability:** Guidelines specific enough to apply?\n- **Consistency:** Sections don't contradict each other?\n- **Tone matrix:** Covers at least 3 content contexts?\n- **PII check:** Customer names and sensitive info redacted?\n\n### Open Questions Audit\nCheck that open questions are properly handled:\n- **Completeness:** Every ambiguity and conflict has a corresponding open question?\n- **Recommendations:** Every open question includes an agent recommendation?\n- **Priority:** Questions are correctly prioritized (High/Medium/Low)?\n- **Actionability:** Each question specifies what decision is needed from the team?\n- **No dead ends:** No question leaves the user without a suggested path forward?\n\n## Output Format\n\n```\nValidation Result: [Pass / Needs Revision / Fail]\n\nChecks:\n- Voice Compliance: [Pass/Fail] - [details]\n- Tone: [Pass/Fail] - [details]\n- Messaging: [Pass/Fail] - [details]\n- Terminology: [Pass/Fail] - [issues found]\n- Open Questions: [Pass/Fail] - [details]\n- PII: [Pass/Fail]\n\nIssues Found:\n1. [Severity: Critical/Suggested] [description] -\u003e Fix: [recommendation]\n\nOverall: [summary]\n```\n\n## Quality Standards\n\n- Every finding must cite the specific guideline it references\n- Recommendations must be actionable\n- Severity levels: Critical (must fix), Suggested (should fix), Optional (nice to have)\n", "depth": 4}, {"id": "185", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/commands/enforce-voice.md", "name": "enforce-voice.md", "type": "file", "content": "---\ndescription: Apply brand guidelines to content creation\nargument-hint: \"\u003ccontent request\u003e\"\n---\n\n**MANDATORY FIRST STEP — do this before anything else, including loading guidelines or processing the content request.** Check whether the user has a working folder selected for this session. You must verify this before starting any enforcement work. If there is no working folder, stop and warn the user: \"You don't have a working folder selected. Without one, I can't load saved guidelines from a previous session, and any guidelines generated in this conversation won't be saved for future sessions either. Please select a working folder and re-run this command. If you'd like to proceed anyway (guidelines will only be usable in this session), let me know.\"  Wait for the user to confirm before continuing.\n\nLoad the user's brand guidelines and apply them to the content request provided in $ARGUMENTS.\n\nFind brand guidelines using this sequence (stop as soon as found):\n1. Session context — check if guidelines were generated earlier in this conversation\n2. Local guidelines file — check for `.claude/brand-voice-guidelines.md` inside the user's working folder. Do NOT use a relative path from the agent's current working directory (in Cowork, the agent runs from a plugin cache directory). If no working folder is set, skip this step.\n3. If not found, ask the user to run `/brand-voice:discover-brand`, `/brand-voice:generate-guidelines`, or paste guidelines directly\n\nOnce guidelines are loaded, follow the brand-voice-enforcement skill instructions to:\n1. Analyze the content request (type, audience, key messages, requirements)\n2. Apply voice constants (\"We Are / We Are Not\") and flex tone for context (formality, energy, technical depth)\n3. Generate content applying voice, tone, messaging, and terminology guidelines\n4. Validate output against brand do's and don'ts\n5. Present the content with a brief explanation of brand choices made\n6. Note any open questions from guidelines that affect this content\n7. Offer to refine based on feedback\n", "depth": 4}, {"id": "186", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/commands/discover-brand.md", "name": "discover-brand.md", "type": "file", "content": "---\ndescription: Search connected platforms for brand materials and produce a discovery report\nargument-hint: \"[company name or platforms to search]\"\n---\n\nDiscover brand materials across the user's connected enterprise platforms. Search Notion, Confluence, Google Drive, Box, SharePoint, Figma, Gong, Granola, and Slack for brand guidelines, style guides, messaging frameworks, templates, and conversation transcripts.\n\nIf $ARGUMENTS includes a company name, use it for targeted searches. If platforms are specified, limit search to those platforms.\n\nBefore doing anything else, briefly orient the user on what's about to happen: the process will search their connected platforms, produce a discovery report, and then (optionally) generate and save brand guidelines to `.claude/brand-voice-guidelines.md` in the working folder. Nothing is saved until they explicitly approve. Keep the orientation to 2-3 sentences — don't recite the full workflow.\n\nFollow the discover-brand skill instructions to:\n1. Check `.claude/brand-voice.local.md` for settings (company name, enabled platforms, search depth)\n2. Validate platform coverage (stop if no document platforms, warn if gaps)\n3. Briefly confirm scope with the user (which platforms, include transcripts?)\n4. Delegate to the discover-brand agent for autonomous 4-phase search\n5. Present the structured discovery report with sources, brand elements, conflicts, and open questions\n6. Offer next steps: generate guidelines, resolve open questions, save report, or expand search\n\n**Platform validation:**\n- If **no platforms** are connected, inform the user which MCP servers the plugin supports (Notion, Atlassian Confluence, Box, Figma, Gong, Granola, Microsoft 365) and that Google Drive and Slack are available as native Claude integrations.\n- If **no document platforms** (Notion, Confluence, Google Drive, Box, Microsoft 365) are connected — only supplementary platforms like Slack, Gong, Granola, or Figma — stop and tell the user: \"You don't have any document storage platforms connected. Brand guidelines and style guides almost always live on Google Drive, SharePoint, Notion, Confluence, or Box. Please connect at least one before running discovery.\"\n- If **no primary file storage** (Google Drive, Microsoft 365, Box) is connected, warn: \"None of your primary file storage platforms are connected. Brand documents frequently live on these. Discovery will proceed but results may have significant gaps.\"\n- If **only one platform** is connected, warn: \"Discovery works best with 2+ platforms for cross-source validation. Results from a single platform will have lower confidence scores.\"\n", "depth": 4}, {"id": "187", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/commands/generate-guidelines.md", "name": "generate-guidelines.md", "type": "file", "content": "---\ndescription: Generate brand voice guidelines from documents, transcripts, discovery reports, or any combination\nargument-hint: \"\u003csources — documents, transcripts, or description of what you have\u003e\"\n---\n\n**MANDATORY FIRST STEP — do this before anything else, including reading sources or processing arguments.** Check whether the user has a working folder selected for this session. You must verify this before starting any guideline generation work. If there is no working folder, stop and warn the user: \"You don't have a working folder selected. Without one, I can't save guidelines to a file — they'll only exist in this conversation and won't persist to future sessions. Please select a working folder and re-run this command. If you'd like to proceed anyway, let me know.\"  Wait for the user to confirm before continuing.\n\nGenerate comprehensive, LLM-ready brand voice guidelines from whatever sources the user provides — brand documents, conversation transcripts, a discovery report from `/brand-voice:discover-brand`, or direct input.\n\nProcess the sources specified in $ARGUMENTS. If none specified, check:\n1. Whether a discovery report was generated in this session\n2. `.claude/brand-voice.local.md` for known brand material locations\n3. Connected platforms (Notion, Confluence, Google Drive, Box, SharePoint, Gong) for existing materials\n4. If nothing is available, suggest running `/brand-voice:discover-brand` first\n\nFollow the guideline-generation skill instructions to:\n1. Identify and classify all available sources (discovery report, documents, transcripts)\n2. Delegate to document-analysis and conversation-analysis agents as needed\n3. Synthesize findings into unified guidelines with \"We Are / We Are Not\" table and tone-by-context matrix\n4. Assign confidence scores per section\n5. Surface open questions with agent recommendations for any ambiguity\n6. Present key findings and offer next steps\n7. Save guidelines to `.claude/brand-voice-guidelines.md` inside the user's working folder (archiving any existing file first). Do NOT use a relative path from the agent's current working directory — in Cowork, the agent runs from a plugin cache directory, not the user's project.\n\nAfter generation, guidelines are saved locally so `/brand-voice:enforce-voice` can automatically find them in future sessions.\n\nSupported document formats: PDF, PowerPoint, Word, Markdown, plain text.\nSupported transcript sources: Gong (MCP), Granola (MCP), Notion meeting notes, manual uploads.\n", "depth": 4}, {"id": "188", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/brand-voice-enforcement", "name": "brand-voice-enforcement", "type": "dir", "content": "", "depth": 4}, {"id": "189", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/discover-brand", "name": "discover-brand", "type": "dir", "content": "", "depth": 4}, {"id": "190", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/guideline-generation", "name": "guideline-generation", "type": "dir", "content": "", "depth": 4}, {"id": "191", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/brand-voice-enforcement/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "192", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/brand-voice-enforcement/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: brand-voice-enforcement\ndescription: \u003e\n  This skill applies brand guidelines to content creation. It should be used when\n  the user asks to \"write an email\", \"draft a proposal\", \"create a pitch deck\",\n  \"write a LinkedIn post\", \"draft a presentation\", \"write a Slack message\",\n  \"draft sales content\", or any content creation request where brand voice should\n  be applied. Also triggers on \"on-brand\", \"brand voice\", \"enforce voice\",\n  \"apply brand guidelines\", \"brand-aligned content\", \"write in our voice\",\n  \"use our brand tone\", \"make this sound like us\", \"rewrite this in our tone\",\n  or \"this doesn't sound on-brand\". Not for generating guidelines from scratch\n  (use guideline-generation) or discovering brand materials (use discover-brand).\n---\n\n# Brand Voice Enforcement\n\nApply existing brand guidelines to all sales and marketing content generation. Load the user's brand guidelines, apply voice constants and tone flexes to the content request, validate output, and explain brand choices.\n\n## Loading Brand Guidelines\n\nFind the user's brand guidelines using this sequence. Stop as soon as you find them:\n\n1. **Session context** — Check if brand guidelines were generated earlier in this session (via `/brand-voice:generate-guidelines`). If so, they are already in the conversation. Use them directly. Session-generated guidelines are the freshest and reflect the user's most recent intent.\n\n2. **Local guidelines file** — Check for `.claude/brand-voice-guidelines.md` inside the user's working folder. Do NOT use a relative path from the agent's current working directory — in Cowork, the agent runs from a plugin cache directory, not the user's project. Resolve the path relative to the user's working folder. If no working folder is set, skip this step.\n\n3. **Ask the user** — If none of the above found guidelines, tell the user:\n   \"I couldn't find your brand guidelines. You can:\n   - Run `/brand-voice:discover-brand` to find brand materials across your platforms\n   - Run `/brand-voice:generate-guidelines` to create guidelines from documents or transcripts\n   - Paste guidelines directly into this chat or point me to a file\"\n\n   Wait for the user to provide guidelines before proceeding.\n\nAlso read `.claude/brand-voice.local.md` for enforcement settings (even if guidelines came from another source):\n- `strictness`: strict | balanced | flexible\n- `always-explain`: whether to always explain brand choices\n\n## Enforcement Workflow\n\n### 1. Analyze the Content Request\n\nBefore writing, identify:\n- **Content type**: email, presentation, proposal, social post, message, etc.\n- **Target audience**: role, seniority, industry, company stage\n- **Key messages needed**: which message pillars apply\n- **Specific requirements**: length, format, tone overrides\n\n### 2. Apply Voice Constants\n\nVoice is the brand's personality — it stays constant across all content:\n- Apply \"We Are / We Are Not\" attributes from guidelines\n- Use brand personality consistently\n- Incorporate approved terminology; reject prohibited terms\n- Follow messaging framework and value propositions\n\nRefer to `references/voice-constant-tone-flexes.md` for the \"voice constant, tone flexes\" model.\n\n### 3. Flex Tone for Context\n\nTone adapts by content type and audience. Use the tone-by-context matrix from guidelines to set:\n- **Formality**: How formal or casual should this be?\n- **Energy**: How much urgency or enthusiasm?\n- **Technical depth**: How detailed or accessible?\n\n### 4. Generate Content\n\nCreate content that:\n- Matches brand voice attributes throughout\n- Follows tone guidelines for this specific content type\n- Incorporates key messages naturally (not forced)\n- Uses preferred terminology\n- Mirrors the quality and style of guideline examples\n\nFor complex or long-form content, delegate to the content-generation agent (defined in `agents/content-generation.md`).\nFor high-stakes content, delegate to the quality-assurance agent (defined in `agents/quality-assurance.md`) for validation.\n\n### 5. Validate and Explain\n\nAfter generating content:\n- Briefly highlight which brand guidelines were applied\n- Explain key voice and tone decisions\n- Note any areas where guidelines were adapted for context\n- Offer to refine based on feedback\n\nWhen `always-explain` is true in settings, include brand application notes with every response.\n\n## Handling Conflicts\n\nWhen the user's request conflicts with brand guidelines:\n1. Explain the conflict clearly\n2. Provide a recommendation\n3. Offer options: follow guidelines strictly, adapt for context, or override\n\nDefault to adapting guidelines with an explanation of the tradeoff.\n\n## Open Questions Awareness\n\nOpen questions are unresolved brand positioning decisions flagged during guideline generation, stored in the guidelines under an \"Open Questions\" section. When generating content, check if the brand guidelines contain open questions:\n- If content touches an unresolved open question, note it\n- Apply the agent's recommendation from the open question unless the user specifies otherwise\n- Suggest resolving the question if it significantly impacts the content\n\n## Reference Files\n\n- **`references/voice-constant-tone-flexes.md`** — The \"voice constant, tone flexes\" mental model, \"We Are / We Are Not\" table structure, and tone-by-context matrix explanation\n- **`references/before-after-examples.md`** — Before/after content examples per content type showing enforcement in practice\n", "depth": 5}, {"id": "193", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/brand-voice-enforcement/references/before-after-examples.md", "name": "before-after-examples.md", "type": "file", "content": "# Before/After Content Examples\n\nConcrete examples showing how brand voice enforcement transforms generic content into on-brand output. Each example shows the generic version, the enforced version, and annotations explaining what changed and why.\n\nThese examples use a fictional B2B SaaS company for illustration. During enforcement, replace all specifics (data points, product names, metrics) with data from the user's actual brand guidelines and context.\n\n## Cold Outreach Email\n\n### Before (Generic)\n```\nSubject: Quick question about your sales process\n\nHi [Name],\n\nI wanted to reach out because I think our product could be a great fit\nfor your team. We offer an AI-powered platform that helps sales teams\nbe more productive.\n\nWould you be open to a quick 15-minute call to learn more?\n\nBest,\n[Sender]\n```\n\n### After (Brand Voice Enforced)\n```\nSubject: [Company] is leaving pipeline on the table\n\nHi [Name],\n\nYour team closed $12M last quarter — impressive. But our data shows\nmid-market SaaS teams like yours typically lose 23% of qualified pipeline\nto slow follow-ups.\n\n[Product] fixes that. We help sales teams respond to buying signals in\nreal time, not hours later. [Customer X] saw their win rate jump 18%\nin the first quarter.\n\nWorth 15 minutes to see if the numbers apply to [Company]?\n\n[Sender]\n```\n\n### What Changed\n- **Voice: Confident** — leads with a bold claim and specific data (not \"I think our product could be\")\n- **Voice: Data-driven** — concrete numbers (23%, 18%, $12M) replace vague \"more productive\"\n- **Voice: Direct** — gets to the point immediately, no \"I wanted to reach out\"\n- **Tone: Energy HIGH** — active language, short sentences, urgency\n- **Tone: Formality MEDIUM** — professional but conversational\n- **Tone: Technical depth LOW** — outcomes and impact, not feature descriptions\n- **Terminology**: \"buying signals\" and \"pipeline\" (industry-specific but accessible to VP Sales)\n\n## Follow-Up Email\n\n### Before (Generic)\n```\nHi [Name],\n\nJust following up on my previous email. I'd love to schedule a call\nto discuss how we can help your team.\n\nLet me know if you're available this week.\n\nThanks,\n[Sender]\n```\n\n### After (Brand Voice Enforced)\n```\nHi [Name],\n\nQuick update: we just published our Q4 benchmark report on enterprise\nsales velocity. Three insights jumped out that are relevant to [Company]:\n\n1. Teams using real-time signals close 31% faster than the industry average\n2. The #1 pipeline killer isn't competition — it's response time\n3. Mid-market wins are trending 15% larger when reps personalize within 2 hours\n\nHappy to walk through how these benchmarks compare to your team's numbers.\nThe report is attached if you'd rather dig in yourself first.\n\n[Sender]\n```\n\n### What Changed\n- **Voice: Data-driven** — new value in every touch (benchmark data), not just \"following up\"\n- **Voice: Approachable** — offers two paths (call or self-serve) without pressure\n- **Voice: Confident** — shares proprietary insights, positions as expert\n- **Tone: Energy MEDIUM** — informative, not pushy\n- **Tone: Formality MEDIUM** — professional but not stiff\n- **Key principle applied**: \"Add new value each touch\" — no empty follow-ups\n\n## Enterprise Proposal Executive Summary\n\n### Before (Generic)\n```\nWe are pleased to submit this proposal for your consideration. Our\ncompany offers a comprehensive AI solution that can help improve your\nsales team's performance. We believe our platform is the best choice\nfor organizations looking to modernize their sales operations.\n```\n\n### After (Brand Voice Enforced)\n```\n[Company] processes 2,400 leads per quarter with a 12-person sales team.\nCurrent pipeline-to-close conversion sits at 18% — below the 24% benchmark\nfor enterprise SaaS at your ARR stage.\n\nThis proposal outlines how [Product] closes that gap. Our platform\nanalyzes buying signals across your existing tech stack and surfaces\nthe 3-5 daily actions most likely to advance each deal. Typical results:\n+31% win rate, -40% time to close, 2.1x pipeline per rep.\n\nImplementation takes 3 weeks. ROI breaks even at week 6.\n```\n\n### What Changed\n- **Voice: Data-driven** — opens with the client's own metrics, not \"we are pleased\"\n- **Voice: Direct** — no filler language, every sentence carries information\n- **Voice: Confident** — specific outcomes with numbers, clear implementation timeline\n- **Tone: Formality HIGH** — structured, precise language\n- **Tone: Energy MEDIUM** — measured confidence, not hype\n- **Tone: Technical depth HIGH** — specific features (buying signals, tech stack analysis), ROI math\n\n## LinkedIn Post\n\n### Before (Generic)\n```\nExcited to announce our new feature! We've been working hard on this\nand we think it's going to change the way sales teams work. Check out\nthe link below to learn more! #sales #AI #innovation\n```\n\n### After (Brand Voice Enforced)\n```\nThe average sales rep spends 4.2 hours/day on tasks that don't close deals.\n\nWe analyzed 50,000 sales activities across 200 teams and found the pattern:\ntop performers aren't working harder — they're responding faster.\n\nToday we're launching Signal Score: real-time prioritization that tells\nyour reps exactly which deal to work next, and why.\n\nEarly results from beta: 23% fewer lost deals from delayed follow-up.\n\nLink in comments.\n```\n\n### What Changed\n- **Voice: Data-driven** — opens with a statistic, not \"excited to announce\"\n- **Voice: Confident** — specific research (50,000 activities, 200 teams)\n- **Voice: Innovative** — describes what's new without hype\n- **Voice: NOT hype-driven** — no \"change the way sales teams work\", replaced with measurable outcomes\n- **Tone: Formality LOW-MEDIUM** — casual enough for LinkedIn, professional enough for B2B\n- **Tone: Energy HIGH** — short sentences, bold opening stat\n- **Tone: Technical depth LOW** — outcomes-focused, accessible to any reader\n- **Terminology**: Avoided \"revolutionary\" and \"game-changing\" (prohibited terms)\n\n## Internal Slack Message\n\n### Before (Generic)\n```\nHey team, just a reminder that we should all be using the approved\nmessaging when talking to prospects. Please refer to the brand\nguidelines document for more information.\n```\n\n### After (Brand Voice Enforced)\n```\nHeads up — we updated the competitive positioning section in the brand\nguidelines. Two big changes:\n\n1. Against [Competitor]: lead with implementation speed (3 weeks vs. their 3 months)\n2. Against Status Quo: new stat — teams using manual processes lose 23% of pipeline to timing\n\nUpdated doc is pinned in #sales-enablement. Questions? Tag me.\n```\n\n### What Changed\n- **Voice: Direct** — specific changes, not vague \"please refer to\"\n- **Voice: Data-driven** — concrete differentiators with numbers\n- **Voice: Approachable** — casual, helpful tone appropriate for Slack\n- **Tone: Formality LOW** — contractions, short sentences, \"tag me\"\n- **Tone: Energy MEDIUM** — informative without being overly enthusiastic\n- **Key principle**: Internal comms can be less polished, but voice attributes still apply\n\n## Using These Examples\n\nWhen enforcing brand voice, reference the relevant content type example to calibrate:\n1. Match the content type to the closest example above\n2. Note which voice attributes are most active for that type\n3. Apply the tone settings (formality, energy, technical depth)\n4. Check \"We Are Not\" boundaries — make sure content doesn't cross them\n5. Verify terminology compliance (preferred terms used, prohibited terms absent)\n", "depth": 6}, {"id": "194", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/brand-voice-enforcement/references/voice-constant-tone-flexes.md", "name": "voice-constant-tone-flexes.md", "type": "file", "content": "# Voice Constant, Tone Flexes\n\nThe core mental model for brand voice enforcement: voice is constant, tone flexes by context.\n\n## The Distinction\n\n**Voice** is WHO the brand is — personality, values, identity. It never changes regardless of channel, audience, or content type. Think of it as the brand's character.\n\n**Tone** is HOW the brand speaks in a given moment — formality, energy, technical depth. It adapts to context the way a person adjusts their tone when speaking to a friend vs. a CEO vs. a large audience.\n\n## Voice Constants: \"We Are / We Are Not\"\n\nThe \"We Are / We Are Not\" table is the anchor of brand voice. It defines the brand's personality in paired attributes — what the brand IS and the boundary it should never cross.\n\n### Structure\n\nThe table below is an illustrative example. During enforcement, use the \"We Are / We Are Not\" table from the user's own brand guidelines.\n\n| We Are | We Are Not |\n|--------|------------|\n| **Confident** — we know our product and stand behind it | **Arrogant** — we never talk down to prospects or dismiss alternatives |\n| **Approachable** — we make complex topics feel manageable | **Casual or sloppy** — approachable doesn't mean unprofessional |\n| **Direct** — we get to the point quickly and clearly | **Blunt or aggressive** — directness includes empathy |\n| **Data-driven** — we support claims with evidence | **Dry or academic** — data tells stories, not lectures |\n| **Innovative** — we push boundaries and challenge status quo | **Hype-driven** — innovation is real, not buzzwords |\n\n### How to Use During Enforcement\n\nFor every piece of content:\n1. Check each \"We Are\" attribute — does the content reflect this?\n2. Check each \"We Are Not\" boundary — does the content avoid crossing it?\n3. If a boundary is crossed, revise the specific passage\n4. If an attribute is missing, find a natural place to express it\n\nNot every attribute needs to appear in every piece of content. Prioritize the 2-3 most relevant attributes for the content type and audience.\n\n## Tone Flexes: The Three Dimensions\n\nTone adjusts along three independent dimensions:\n\n### 1. Formality\nHow formal or casual the language is.\n\n| Level | Signals | When |\n|-------|---------|------|\n| High | Complete sentences, industry terminology, structured format | Proposals, RFPs, enterprise comms |\n| Medium | Clear and professional but conversational | Most emails, presentations, blog posts |\n| Low | Casual language, contractions, personality shows through | Social media, internal comms, Slack |\n\n### 2. Energy\nHow much enthusiasm, urgency, or dynamism the content conveys.\n\n| Level | Signals | When |\n|-------|---------|------|\n| High | Active verbs, short sentences, exclamation-worthy excitement | Product launches, cold outreach hooks, social media |\n| Medium | Steady pace, balanced between informative and engaging | Discovery content, follow-ups, case studies |\n| Warm | Empathetic, supportive, understanding | Customer success, objection handling, bad news |\n| Low | Measured, thoughtful, deliberate | Legal language, policy docs, sensitive topics |\n\n### 3. Technical Depth\nHow much domain expertise or technical detail is included.\n\n| Level | Signals | When |\n|-------|---------|------|\n| High | Technical terminology, architecture details, specs | Technical audience, implementation proposals, API docs |\n| Medium | Industry terms explained when needed, features described clearly | Mixed audience, product demos, mid-funnel content |\n| Low | Outcome-focused, benefits over features, no jargon | Executive audience, cold outreach, social media |\n\n## Tone-by-Context Matrix\n\nCombine the three dimensions for each content type:\n\n| Context | Formality | Energy | Technical Depth | Key Principle |\n|---------|-----------|--------|-----------------|---------------|\n| Cold outreach | Medium | High | Low | Hook fast, earn attention |\n| Discovery calls | Medium | Medium-High | Medium | Ask more than tell |\n| Demo / presentation | Medium-High | High | High | Show, don't just describe |\n| Enterprise proposal | High | Medium | High | ROI and precision |\n| Follow-up email | Medium | Medium | Low-Medium | Add new value each touch |\n| Social media | Low-Medium | High | Low | Brevity and personality |\n| Customer success | Medium | Warm | Medium | Empathy and competence |\n| Internal comms | Low | Medium | Varies | Authentic, less polished |\n\n## Applying Voice + Tone Together\n\n**Example: Cold outreach email**\n\nVoice constants active:\n- **Confident** — lead with a strong value claim\n- **Direct** — get to the point in sentence one\n- **Approachable** — don't sound like a robot\n\nTone settings:\n- Formality: Medium (professional but not stiff)\n- Energy: High (earn attention quickly)\n- Technical Depth: Low (outcomes, not features)\n\n**Example: Enterprise proposal**\n\nVoice constants active:\n- **Data-driven** — every claim has supporting evidence\n- **Confident** — strong positioning without hedging\n- **Innovative** — show forward-thinking approach\n\nTone settings:\n- Formality: High (structured, complete sentences)\n- Energy: Medium (measured, not breathless)\n- Technical Depth: High (detailed implementation, specs)\n\n## Common Enforcement Mistakes\n\n1. **Applying all voice attributes at maximum intensity** — Not every attribute needs to shine in every sentence. Let 2-3 lead for the content type.\n2. **Confusing voice with tone** — If the user asks for \"casual\" content, adjust TONE (lower formality, higher energy). Don't change VOICE (the brand's personality stays the same).\n3. **Rigid enforcement over natural flow** — Guidelines are principles, not a checklist. Content should feel natural, not mechanically assembled.\n4. **Ignoring the audience** — The same voice can sound very different to a CTO vs. a VP of Sales. Tone flexes handle this.\n", "depth": 6}, {"id": "195", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/discover-brand/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "196", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/discover-brand/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: discover-brand\ndescription: \u003e\n  This skill orchestrates autonomous discovery of brand materials across enterprise\n  platforms (Notion, Confluence, Google Drive, Box, SharePoint, Figma, Gong, Granola, Slack).\n  It should be used when the user asks to \"discover brand materials\",\n  \"find brand documents\", \"search for brand guidelines\", \"audit brand content\",\n  \"what brand materials do we have\", \"find our style guide\", \"where are our brand docs\",\n  \"do we have a style guide\", \"discover brand voice\", \"brand content audit\",\n  or \"find brand assets\".\n---\n\n# Brand Discovery\n\nOrchestrate autonomous discovery of brand materials across enterprise platforms. This skill coordinates the discover-brand agent to search connected platforms (Notion, Confluence, Google Drive, Box, Microsoft 365, Figma, Gong, Granola, Slack), triage sources, and produce a structured discovery report with open questions.\n\n## Discovery Workflow\n\n### 0. Orient the User\n\nBefore starting, briefly explain what's about to happen so the user knows what to expect:\n\n\"Here's how brand discovery works:\n\n1. **Search** — I'll search your connected platforms (Notion, Google Drive, Slack, etc.) for brand-related materials: style guides, pitch decks, templates, transcripts, and more.\n2. **Analyze** — I'll categorize and rank what I find, pull the best sources, and produce a discovery report with what I found, any conflicts, and open questions.\n3. **Generate guidelines** — Once you've reviewed the report, I can generate a structured brand voice guideline document from the results.\n4. **Save** — Guidelines are saved to `.claude/brand-voice-guidelines.md` in your working folder once you approve them. Nothing is written until that step.\n\nThe search usually takes a few minutes depending on how many platforms are connected. Ready to get started?\"\n\nWait for the user to confirm before proceeding. If they have questions about the process, answer them first.\n\n### 1. Check Settings\n\nRead `.claude/brand-voice.local.md` if it exists. Extract:\n- Company name\n- Which platforms are enabled (notion, confluence, google-drive, box, microsoft-365, figma, gong, granola, slack)\n- Search depth preference (standard or deep)\n- Max sources limit\n- Any known brand material locations listed under \"Known Brand Materials\"\n\nIf no settings file exists, proceed with all connected platforms and standard search depth.\n\n### 2. Validate Platform Coverage\n\nBefore confirming scope, check which platforms are actually connected and classify them:\n\n**Document platforms** (where brand guidelines, style guides, templates, and decks live):\n- Notion, Confluence, Google Drive, Box, Microsoft 365 (SharePoint/OneDrive)\n\n**Supplementary platforms** (valuable for patterns, but not where brand docs are stored):\n- Slack, Gong, Granola, Figma\n\nApply these rules:\n\n1. **If zero document platforms are connected**: **Stop.** Tell the user: \"You don't have any document storage platforms connected (Google Drive, SharePoint, Notion, Confluence, or Box). Brand guidelines and style guides almost always live on one of these. Please connect at least one before running discovery. Gong/Granola/Slack transcripts are valuable supplements but unlikely to contain formal brand documents.\"\n\n2. **If no Google Drive AND no Microsoft 365 AND no Box**: **Warn** (but proceed): \"None of your primary file storage platforms (Google Drive, SharePoint, Box) are connected. Brand documents frequently live on these platforms. Discovery will proceed with [connected platforms], but results may have significant gaps. Consider connecting Google Drive or SharePoint.\"\n\n3. **If only one platform total is connected**: **Warn** (but proceed): \"Only [platform] is connected. Discovery works best with 2+ platforms for cross-source validation. Results from a single platform will have lower confidence scores.\"\n\n### 3. Confirm Scope with User\n\nBefore launching discovery, confirm:\n- Which platforms to search (default: all connected)\n- Whether to include conversation transcripts (Gong, Granola) or just documents\n- Any known locations to prioritize\n\nKeep this brief — one question, not a questionnaire.\n\n### 4. Delegate to Discover-Brand Agent\n\nLaunch the discover-brand agent via the Task tool. Provide:\n- Company name (from settings or user input)\n- Enabled platforms\n- Search depth\n- Any known URLs or locations to check first\n\nThe agent executes the 4-phase discovery algorithm autonomously:\n1. **Broad Discovery** — parallel searches across platforms\n2. **Source Triage** — categorize and rank sources\n3. **Deep Fetch** — retrieve and extract from top sources\n4. **Discovery Report** — structured output with open questions\n\n### 5. Present Discovery Report\n\nWhen the agent returns, present the report to the user with a summary:\n- Total sources found and analyzed\n- Key brand elements discovered\n- Any conflicts between sources\n- Open questions requiring team input\n\n### 6. Offer Next Steps\n\nAfter presenting the report, offer:\n1. **Generate guidelines now** — chain to `/brand-voice:generate-guidelines` using discovery report as input\n2. **Resolve open questions first** — work through high-priority questions before generating\n3. **Save report** — store the discovery report to Notion or as a local file\n4. **Expand search** — search additional platforms or deeper if coverage is low\n\n## Open Questions\n\nOpen questions arise when the discovery agent encounters ambiguity it cannot resolve:\n- Conflicting documents (e.g., 2023 style guide vs. 2024 brand update)\n- Missing critical sections (e.g., no social media guidelines found)\n- Inconsistent terminology across platforms\n\nEvery open question includes an agent recommendation. Present questions as \"confirm or override\" — not dead ends.\n\n## Integration with Other Skills\n\n- **Guideline Generation**: The discovery report is returned by the discover-brand agent via the Task tool. Pass it directly to the guideline-generation skill as structured input, replacing the need for users to manually gather sources.\n- **Brand Voice Enforcement**: Once guidelines are generated from discovery, enforcement uses them automatically.\n\n## Error Handling\n\n- If zero platforms are connected, inform the user which platforms the plugin supports and how to connect them.\n- If all searches return empty results, flag the discovery as \"low coverage\" and suggest the user provide documents manually or check platform connections.\n- If a platform is connected but returns permission errors, note the gap and continue with other platforms.\n\n## Reference Files\n\nFor detailed discovery patterns and algorithms, consult:\n\n- **`references/search-strategies.md`** — Platform-specific search queries, query patterns by platform, and tips for maximizing discovery coverage\n- **`references/source-ranking.md`** — Source category definitions, ranking algorithm weights, and triage decision criteria\n", "depth": 5}, {"id": "197", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/discover-brand/references/source-ranking.md", "name": "source-ranking.md", "type": "file", "content": "# Source Ranking Algorithm\n\nHow to categorize, rank, and prioritize discovered brand sources.\n\n## Source Categories\n\n### AUTHORITATIVE\nOfficial, approved brand documentation. Highest trust level.\n\n**Signals:**\n- Published style guides or brand books\n- C-suite or marketing leadership authored/approved\n- Lives in an official \"Brand\" folder or Confluence space\n- Has version numbers or approval dates\n- Referenced by other documents as \"the brand guide\"\n\n**Examples:**\n- \"Acme Corp Brand Guidelines v3.2.pdf\"\n- \"Official Style Guide\" page in Confluence Marketing space\n- Brand book in Google Drive with company-wide sharing\n- Brand book in Box with company-wide sharing\n- Official Style Guide in SharePoint Marketing site\n\n**Trust weight:** 1.0 (baseline)\n\n### OPERATIONAL\nBrand applied in practice. Shows how guidelines manifest in real content.\n\n**Signals:**\n- Templates actively used by teams\n- Sales playbooks with messaging guidance\n- Email sequences with established tone\n- Presentation templates with brand messaging\n\n**Examples:**\n- \"Cold Email Templates Q4 2024\"\n- \"Enterprise Sales Playbook\"\n- \"Customer Success Response Templates\"\n- Pitch deck templates in Google Slides\n- Email templates in Outlook\n- Sales playbook on SharePoint\n\n**Trust weight:** 0.8\n\n### CONVERSATIONAL\nImplicit brand voice from actual communications.\n\n**Signals:**\n- Sales call transcripts (especially successful ones)\n- Meeting notes with customer-facing language\n- Internal discussions about positioning\n- Slack threads discussing brand decisions\n\n**Examples:**\n- Gong recordings of top performer calls\n- Meeting notes from brand strategy sessions\n- Customer success call transcripts\n- Slack #brand channel discussions about tone\n\n**Trust weight:** 0.6 (valuable for patterns, not prescriptive)\n\n### CONTEXTUAL\nBackground information that informs brand but doesn't define it directly.\n\n**Signals:**\n- Design files without explicit brand guidelines\n- Competitor analysis documents\n- Industry reports\n- Product documentation\n\n**Examples:**\n- Figma component library (visual only)\n- \"Competitive Landscape Q3 2024\"\n- Product feature specifications\n\n**Trust weight:** 0.3\n\n### STALE\nOutdated content superseded by newer versions.\n\n**Signals:**\n- Older version when a newer version exists\n- Pre-rebrand materials after a rebrand\n- Documents explicitly marked as deprecated\n- Content more than 2 years old without updates\n\n**Examples:**\n- \"Brand Guidelines v1.0\" when v3.2 exists\n- \"2022 Style Guide\" when \"2024 Brand Update\" exists\n- Documents in an \"Archive\" or \"Deprecated\" folder\n\n**Trust weight:** 0.1 (flag for review, do not rely on)\n\n## Ranking Algorithm\n\nApply these five ranking factors in order of priority:\n\n### 1. Recency (Weight: 30%)\n\nMore recent sources are more likely to reflect current brand voice.\n\n- **Score 1.0**: Updated within last 6 months\n- **Score 0.7**: Updated within last year\n- **Score 0.4**: Updated within last 2 years\n- **Score 0.1**: Older than 2 years\n\nWhen two sources conflict, the more recent one wins unless the older source is explicitly marked as the \"official\" guide.\n\nAlways prefer the most recent version of any document. When multiple sources cover the same topic, weight the newest one heavily. Flag any non-AUTHORITATIVE source older than 12 months in the discovery report.\n\n### Recency Cutoffs\n\nIn addition to soft recency scoring, apply hard cutoffs to prevent stale content from polluting discovery:\n\n**AUTHORITATIVE sources**: No hard cutoff. Official brand guides remain valid regardless of age unless explicitly superseded by a newer version.\n\n**OPERATIONAL, CONVERSATIONAL, CONTEXTUAL sources**: Exclude from deep fetch if older than 12 months, with one exception: if zero sources in a category fall within the 12-month window, include the single most recent source from that category and flag it as potentially stale.\n\n**STALE sources**: Exclude entirely from deep fetch. Include in the discovery report for reference only.\n\nThese cutoffs apply at the deep-fetch stage (Phase 3). All sources are still collected during broad discovery (Phase 1) and triage (Phase 2) — the cutoffs filter what gets fully retrieved and analyzed.\n\n### 2. Explicitness (Weight: 25%)\n\nSources that explicitly define brand voice outrank those that merely demonstrate it.\n\n- **Score 1.0**: Explicit brand instructions (\"Our voice is...\")\n- **Score 0.7**: Documented tone guidelines (\"Emails should be...\")\n- **Score 0.4**: Implicit patterns in templates or examples\n- **Score 0.2**: Inferred from conversational patterns\n\n### 3. Authority (Weight: 20%)\n\nHigher organizational authority indicates more trustworthy brand definitions.\n\n- **Score 1.0**: Official brand team or C-suite authored\n- **Score 0.7**: Marketing leadership authored\n- **Score 0.4**: Team leads or senior ICs\n- **Score 0.2**: Individual contributor or unknown author\n\n### 4. Specificity (Weight: 15%)\n\nDetailed, actionable guidance outranks vague principles.\n\n- **Score 1.0**: Specific rules with examples (\"Use 'platform' not 'tool'\")\n- **Score 0.7**: Detailed guidelines (\"Tone should be warm but professional\")\n- **Score 0.4**: General principles (\"Be authentic\")\n- **Score 0.2**: Abstract values only (\"We believe in innovation\")\n\n### 5. Cross-Source Consistency (Weight: 10%)\n\nElements corroborated across multiple sources rank higher.\n\n- **Score 1.0**: Appears in 3+ independent sources\n- **Score 0.7**: Appears in 2 independent sources\n- **Score 0.4**: Appears in 1 source only\n- **Score 0.1**: Contradicted by another source\n\n## Composite Score Calculation\n\n```\nfinal_score = (recency × 0.30) + (explicitness × 0.25) + (authority × 0.20)\n            + (specificity × 0.15) + (consistency × 0.10)\n```\n\nMultiply by category trust weight:\n```\nranked_score = final_score × category_trust_weight\n```\n\n### Example Scoring\n\n**Source: \"Brand Voice Guidelines v3.2\" (Confluence, updated 3 months ago)**\n- Recency: 1.0 (3 months old)\n- Explicitness: 1.0 (explicit brand instructions)\n- Authority: 1.0 (marketing VP authored)\n- Specificity: 0.7 (good guidelines, some gaps)\n- Consistency: 0.7 (corroborated by email templates)\n- Category: AUTHORITATIVE (1.0)\n- **Final: (1.0×0.30 + 1.0×0.25 + 1.0×0.20 + 0.7×0.15 + 0.7×0.10) × 1.0 = 0.925**\n\n**Source: \"Top Performer Call — Enterprise Close\" (Gong, 2 months ago)**\n- Recency: 1.0\n- Explicitness: 0.2 (implicit patterns only)\n- Authority: 0.4 (senior AE)\n- Specificity: 0.7 (specific phrases used)\n- Consistency: 0.4 (single source)\n- Category: CONVERSATIONAL (0.6)\n- **Final: (1.0×0.30 + 0.2×0.25 + 0.4×0.20 + 0.7×0.15 + 0.4×0.10) × 0.6 = 0.345**\n\n## Adaptive Scoring: No Authoritative Sources\n\nWhen discovery finds **zero AUTHORITATIVE sources**, the scoring algorithm adapts to reflect that conversational and operational sources are the primary brand evidence.\n\n### Adjusted Trust Weights (No Authoritative Sources)\n\n| Category | Default Weight | Adapted Weight | Rationale |\n|----------|---------------|----------------|-----------|\n| AUTHORITATIVE | 1.0 | 1.0 | (n/a — none found) |\n| OPERATIONAL | 0.8 | 0.9 | Templates become primary explicit evidence |\n| CONVERSATIONAL | 0.6 | 0.85 | Transcripts are the best signal for how the brand actually communicates |\n| CONTEXTUAL | 0.3 | 0.4 | Design and competitive context more valuable without formal docs |\n| STALE | 0.1 | 0.2 | Even old docs matter more when nothing current exists |\n\n### Adjusted Explicitness Scoring (No Authoritative Sources)\n\nWhen no authoritative sources exist, conversational patterns carry more prescriptive weight:\n\n- **Score 0.2 → 0.5**: \"Inferred from conversational patterns\" — these ARE the brand evidence now\n- **Score 0.4 → 0.6**: \"Implicit patterns in templates or examples\"\n- Other explicitness scores unchanged\n\n### Example: Transcript Scoring With Adaptation\n\n**Source: \"Top Performer Call — Enterprise Close\" (Gong, 2 months ago)**\n- Recency: 1.0\n- Explicitness: 0.5 (adapted from 0.2 — patterns are primary evidence)\n- Authority: 0.4 (senior AE)\n- Specificity: 0.7 (specific phrases used)\n- Consistency: 0.4 (single source)\n- Category: CONVERSATIONAL (0.85 adapted)\n- **Adapted score: (1.0×0.30 + 0.5×0.25 + 0.4×0.20 + 0.7×0.15 + 0.4×0.10) × 0.85 = 0.552**\n\nThis puts the transcript well above the 0.5 deep-fetch threshold, ensuring conversational sources meaningfully contribute to guideline generation.\n\n### When to Apply\n\nApply adaptive scoring when:\n- Phase 2 triage produces zero AUTHORITATIVE sources\n- Flag in the discovery report: \"No formal brand guidelines found — scoring adapted to weight conversational and operational sources higher\"\n\n## Triage Decision Criteria\n\n### Include in Deep Fetch (Top 5-15 sources)\n- Ranked score \u003e 0.5\n- All AUTHORITATIVE sources regardless of score\n- At least one source per category if available (this overrides the score threshold)\n- At least one source per platform if available\n\n### Flag for Review\n- Sources with conflicting information\n- STALE sources that may still be referenced by teams\n- Sources with high specificity but low authority\n\n### Exclude\n- Ranked score \u003c 0.1\n- Clearly irrelevant results (e.g., \"brand\" used in product name, not brand guidelines)\n- Duplicate content already captured from another platform\n", "depth": 6}, {"id": "198", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/discover-brand/references/search-strategies.md", "name": "search-strategies.md", "type": "file", "content": "# Search Strategies by Platform\n\nPlatform-specific query patterns and tips for maximizing brand material discovery.\n\n## Notion (Primary Discovery Engine)\n\nNotion AI Search federates across connected sources (Google Drive, SharePoint, OneDrive, Slack, Jira, Teams), making it the most valuable single search endpoint.\n\n### Query Patterns\n\n**Direct brand searches:**\n- \"brand guidelines\"\n- \"style guide\"\n- \"brand voice\"\n- \"tone of voice\"\n- \"messaging framework\"\n\n**Sales and marketing content:**\n- \"pitch deck\"\n- \"sales playbook\"\n- \"email templates\"\n- \"value proposition\"\n- \"competitive positioning\"\n\n**Operational brand content:**\n- \"brand update\"\n- \"rebrand\"\n- \"launch messaging\"\n- \"press release template\"\n- \"social media guidelines\"\n\n### Tips\n- Notion AI Search returns results from connected sources — one search covers multiple platforms\n- Search for the company name + \"brand\" to find company-specific guidelines\n- Check for databases titled \"Brand Assets\", \"Marketing Resources\", or \"Content Library\"\n- Look for pages tagged with \"brand\", \"marketing\", \"style\"\n\n## Atlassian Confluence\n\nEnterprises often store official brand documentation in Confluence spaces.\n\n### Query Patterns\n\n**Space-level search:**\n- Search spaces named: \"Marketing\", \"Brand\", \"Communications\", \"Sales Enablement\"\n- Check for spaces with labels: \"brand\", \"style-guide\", \"guidelines\"\n\n**Page-level search:**\n- \"brand style guide\"\n- \"voice and tone guidelines\"\n- \"messaging framework\"\n- \"content standards\"\n- \"editorial guidelines\"\n\n**Template search:**\n- \"email template\"\n- \"proposal template\"\n- \"presentation template\"\n\n### Tips\n- Confluence spaces often have hierarchical page trees — find the brand root page and explore children\n- Check for recently updated pages (brand guidelines evolve)\n- Look for pages with many watchers — indicates important shared content\n- Search attachments for PDF brand guides uploaded to Confluence pages\n\n## Box\n\nCloud file storage — official brand documents, shared decks, and style guides frequently live here.\n\n### Query Patterns\n\n**Folder search:**\n- \"Brand Guidelines\"\n- \"Marketing Assets\"\n- \"Brand Kit\"\n- \"Style Guide\"\n- \"Sales Collateral\"\n\n**Document search:**\n- \"brand guide\" (PDF, DOCX, Word)\n- \"style guide\" (PDF, DOCX, Word)\n- \"messaging document\"\n- \"brand standards\"\n\n### Tips\n- Box often contains the \"source of truth\" brand guides as PDFs or Word docs\n- Check shared folders with company-wide access\n- Look for folders shared with the marketing or brand team\n- Search for recently modified brand documents to find the latest version\n- Use Box metadata search to filter by content type or custom attributes\n\n## Google Drive\n\nGoogle Drive stores shared brand documents, marketing materials, and official style guides.\n\n### Query Patterns\n\n**Folder search:**\n- \"Brand Guidelines\"\n- \"Marketing\"\n- \"Brand Assets\"\n- \"Style Guide\"\n\n**Document search:**\n- \"brand guide\" (PDFs, Google Docs, Google Slides)\n- \"style guide\"\n- \"messaging framework\"\n- \"brand standards\"\n- \"brand voice\"\n\n### Tips\n- Check shared drives — brand materials often live in team-wide shared drives\n- Look for recently modified brand documents to find the latest version\n- Search by owner (marketing team members) to surface brand-owned content\n- Google Docs and Slides are common formats for living brand documents\n- Check for Google Slides presentations with brand overview decks\n\n## Microsoft 365 (SharePoint / OneDrive)\n\nEnterprise organizations often centralize brand documentation on SharePoint sites.\n\n### Query Patterns\n\n**SharePoint site search:**\n- Search marketing or communications SharePoint sites\n- \"brand guidelines\"\n- \"style guide\"\n- \"brand standards\"\n- \"messaging framework\"\n\n**Document library search:**\n- Check document libraries in marketing/communications sites\n- \"brand guide\" (Word, PDF, PowerPoint)\n- \"brand book\"\n- \"editorial guidelines\"\n\n**OneDrive search:**\n- \"brand\" files in shared OneDrive folders\n- \"style guide\" shared with the organization\n\n### Tips\n- Check marketing/communications SharePoint sites first — most common location for brand docs\n- Search document libraries for PDF brand guides and PowerPoint brand decks\n- Look for brand-tagged content using SharePoint metadata\n- OneDrive shared folders may contain working drafts of brand materials\n- Use SharePoint search and document library tools when available\n\n## Slack\n\nSlack channels contain informal brand discussions, decisions, and evolving brand voice patterns.\n\n### Query Patterns\n\n**Channel search:**\n- Look for channels: #brand, #marketing, #brand-voice, #style-guide, #creative\n- Check channel topics and descriptions for brand-related content\n\n**Message search:**\n- \"brand guidelines\"\n- \"brand voice\"\n- \"tone of voice\"\n- \"style guide\"\n- \"brand update\"\n\n**Pinned messages:**\n- Check pinned messages in #brand and #marketing channels\n- Pinned items often contain approved brand resources or decisions\n\n### Tips\n- Search #brand and #marketing channels first — most likely to contain brand discussions\n- Look for pinned messages — teams often pin brand guidelines and decisions\n- Find brand discussion threads for context on brand evolution\n- Slack is a conversational source — ranks as CONVERSATIONAL tier in source triage\n- Recent messages may reveal brand changes not yet documented formally\n\n## Gong (Conversation Intelligence)\n\nSales call transcripts reveal how the brand actually communicates in practice.\n\n### Query Patterns\n\n**Call search:**\n- Search for calls tagged: \"won\", \"closed-won\" (successful brand application)\n- Filter by top performers (their language patterns define implicit brand voice)\n- Look for calls tagged: \"demo\", \"discovery\", \"closing\"\n\n**Transcript search:**\n- Search transcripts for company-specific value propositions\n- Look for recurring phrases across successful calls\n- Find calls where competitors are discussed (reveals positioning)\n\n### Tips\n- Focus on successful calls — they represent brand voice that works\n- Compare top performer language with average performer language\n- Look for consistent opening lines and closing patterns\n- Note objection handling language — reveals brand positioning under pressure\n\n## Granola (Meeting Intelligence)\n\nMeeting notes and transcripts from the AI notepad for meetings.\n\n### Query Patterns\n\n**Meeting search:**\n- Query for meetings related to: \"brand\", \"positioning\", \"messaging\"\n- List recent meetings and filter for customer-facing calls\n- Search for strategy sessions and brand planning meetings\n\n**Transcript retrieval:**\n- Get full transcripts from high-signal meetings\n- Look for recurring themes in meeting notes\n- Find meetings with key stakeholders (marketing leads, executives)\n\n### Tips\n- Granola captures both meeting notes and transcripts — both are valuable\n- Meeting notes often contain summarized brand decisions and action items\n- Look for recurring meeting series (weekly brand syncs, marketing standups)\n- Cross-reference Granola meeting notes with Gong call transcripts when both are available\n\n## Figma (Brand Design Systems)\n\nVisual brand elements inform voice and tone indirectly.\n\n### Query Patterns\n\n**File search:**\n- \"brand design system\"\n- \"design tokens\"\n- \"brand kit\"\n- \"style guide\"\n- \"component library\"\n\n**Component search:**\n- Look for files with brand colors, typography scales\n- Find component documentation with usage guidelines\n- Check for content/copy guidelines in design system docs\n\n### Tips\n- Figma design systems often include writing guidelines alongside visual specs\n- Check file descriptions and comments for brand context\n- Look for \"voice and tone\" sections within design system documentation\n- Brand personality descriptions in design files reveal voice attributes\n\n## Cross-Platform Search Tips\n\n### Maximizing Coverage\n1. Start with Notion AI Search (broadest coverage via federation)\n2. Follow up with Confluence for enterprise documentation\n3. Check Google Drive for official brand documents\n4. Check Box for cloud-stored brand documents\n5. Search SharePoint/OneDrive for enterprise files\n6. Search Slack for brand discussions and decisions\n7. Search Gong for conversational brand patterns\n8. Search Granola for meeting transcripts and notes\n9. Review Figma for design-embedded brand guidelines\n\n### Avoiding Duplicates\n- Track source URLs to detect the same document across platforms\n- When the same content appears on multiple platforms, prefer the most recently updated version\n- Note when Notion federation returns results from other platforms to avoid double-searching\n\n### Recency Focus\n- Focus on content from the last 12 months for operational, conversational, and contextual sources\n- Only search further back when looking for explicit brand documents (style guides, brand books)\n- When results include older content, prefer the most recently updated version\n- Flag any non-AUTHORITATIVE source older than 12 months — it may reflect outdated positioning\n\n### Handling No Results\n- If a platform returns zero results, try broader queries (\"marketing\", \"sales\")\n- Check if the platform is actually connected and authenticated\n- Note the gap in the discovery report — the absence of content on a platform is itself useful information\n", "depth": 6}, {"id": "199", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/guideline-generation/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "200", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/guideline-generation/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: guideline-generation\ndescription: \u003e\n  This skill generates, creates, or builds brand voice guidelines from source\n  materials. It should be used when the user asks to \"generate brand guidelines\",\n  \"create a style guide\", \"extract brand voice\", \"create guidelines from calls\",\n  \"consolidate brand materials\", \"analyze my sales calls for brand voice\",\n  \"build a brand playbook from documents\", \"synthesize a voice and tone guide\",\n  or uploads brand documents, transcripts, or meeting recordings for brand\n  analysis. Also triggers when the user has a discovery report and wants to\n  convert it into actionable guidelines.\n---\n\n# Guideline Generation\n\nGenerate comprehensive, LLM-ready brand voice guidelines from any combination of sources — brand documents, sales call transcripts, discovery reports, or direct user input. Transform raw materials into structured, enforceable guidelines with confidence scoring and open questions.\n\n## Inputs\n\nAccept any combination of:\n- **Discovery report** from the discover-brand skill (structured, pre-triaged)\n- **Brand documents** uploaded or from connected platforms (PDF, PPTX, DOCX, MD, TXT)\n- **Conversation transcripts** from Gong, Granola, manual uploads, or Notion meeting notes\n- **Direct user input** about their brand voice and values\n\nWhen a discovery report is provided, use it as the primary input — sources are already triaged and ranked. Supplement with additional analysis as needed.\n\n## Generation Workflow\n\n### 1. Identify and Classify Sources\n\nDetermine what the user has provided. If no sources are available:\n- Check if a discovery report exists from a previous `/brand-voice:discover-brand` run\n- Check `.claude/brand-voice.local.md` for known brand material locations\n- Suggest running discovery first: `/brand-voice:discover-brand`\n\n### 2. Process Sources\n\n**For documents:** Delegate to the document-analysis agent for heavy parsing. Extract voice attributes, messaging themes, terminology, tone guidance, and examples.\n\n**For transcripts:** Delegate to the conversation-analysis agent for pattern recognition. Extract implicit voice attributes, successful language patterns, tone by context, and anti-patterns.\n\n**For discovery reports:** Extract pre-triaged sources, conflicts, and gaps. Use the ranked sources directly.\n\n### 3. Synthesize Into Guidelines\n\nMerge all findings into a unified guideline document following the template in `references/guideline-template.md`. Key sections:\n\n**\"We Are / We Are Not\" Table** — The core brand identity anchor:\n\n| We Are | We Are Not |\n|--------|------------|\n| [Attribute — e.g., \"Confident\"] | [Counter — e.g., \"Arrogant\"] |\n| [Attribute — e.g., \"Approachable\"] | [Counter — e.g., \"Casual or sloppy\"] |\n\nDerive attributes from the most consistent patterns across sources. Each row should have supporting evidence.\n\n**Voice Constants vs. Tone Flexes** — Clarify what stays fixed and what adapts:\n- **Voice** = personality, values, \"We Are / We Are Not\" — constant across all content\n- **Tone** = formality, energy, technical depth — flexes by context\n\n**Tone-by-Context Matrix:**\n\n| Context | Formality | Energy | Technical Depth | Example |\n|---------|-----------|--------|-----------------|---------|\n| Cold outreach | Medium | High | Low | \"[example phrase]\" |\n| Enterprise proposal | High | Medium | High | \"[example phrase]\" |\n| Social media | Low | High | Low | \"[example phrase]\" |\n\n### 4. Assign Confidence Scores\n\nScore each section using the methodology in `references/confidence-scoring.md`:\n- **High confidence**: 3+ corroborating sources, explicit guidance found\n- **Medium confidence**: 1-2 sources, or inferred from patterns\n- **Low confidence**: Single source, inferred, or conflicting data\n\n### 5. Surface Open Questions\n\nGenerate open questions for any ambiguity that cannot be resolved:\n\n```markdown\n## Open Questions for Team Discussion\n\n### High Priority (blocks guideline completion)\n1. **[Question Title]**\n   - What was found: [conflicting or incomplete info]\n   - Agent recommendation: [suggested resolution with reasoning]\n   - Need from you: [specific decision or confirmation needed]\n```\n\nEvery open question MUST include an agent recommendation. Turn ambiguity into \"confirm or override\" — never a dead end.\n\n### 6. Quality Check\n\nBefore presenting, verify via the quality-assurance agent (defined in `agents/quality-assurance.md`):\n- All major sections populated (including Brand Personality and Content Examples if sources support them)\n- At least 3 voice attributes with evidence\n- \"We Are / We Are Not\" table has 4+ rows\n- Tone matrix covers at least 3 contexts\n- Confidence scores assigned per section\n- Source attribution for all extracted elements\n- No PII exposed\n- Open questions include recommendations\n\n### 7. Present and Offer Next Steps\n\nSummarize key findings:\n- Total sections generated with confidence breakdown\n- Strongest voice attribute and most effective message\n- Number of open questions (if any)\n\n### 8. Save for Future Sessions\n\nThe default save location is `.claude/brand-voice-guidelines.md` inside the user's working folder.\n\n**Important:** The agent's working directory may not be the user's project root (especially in Cowork, where plugins run from a plugin cache directory). Always resolve the path relative to the user's working folder, not the current working directory. If no working folder is set, skip the file save and tell the user guidelines will only be available in this conversation.\n\n1. **Resolve the save path.** The file MUST be saved to `.claude/brand-voice-guidelines.md` inside the user's working folder. Confirm the working folder path before writing.\n2. **Check if guidelines already exist** at that path\n3. **If they exist, archive the previous version:** Rename the existing file to `brand-voice-guidelines-YYYY-MM-DD.md` in the same directory (using today's date)\n4. **Save new guidelines** to `.claude/brand-voice-guidelines.md` inside the working folder\n5. **Confirm to the user** with the full absolute path: \"Guidelines saved to `\u003cfull-path\u003e`. `/brand-voice:enforce-voice` will find them automatically in future sessions.\"\n\nThe guidelines are also present in this conversation, so `/brand-voice:enforce-voice` can use them immediately without loading from file.\n\nAfter saving, offer:\n1. Walk through the guidelines section by section\n2. Start creating content with `/brand-voice:enforce-voice`\n3. Resolve open questions\n\n## Privacy and Security\n\nEnforce these privacy constraints throughout the entire generation workflow, not only at output time:\n- Redact customer names and contact information from all examples\n- Anonymize company names in transcript excerpts if requested\n- Flag any sensitive information detected during processing\n\n## Reference Files\n\n- **`references/guideline-template.md`** — Complete output template with all sections, field definitions, and formatting guidance\n- **`references/confidence-scoring.md`** — Confidence scoring methodology, thresholds, and examples\n", "depth": 5}, {"id": "201", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/guideline-generation/references/confidence-scoring.md", "name": "confidence-scoring.md", "type": "file", "content": "# Confidence Scoring Methodology\n\nHow to assign and interpret confidence scores for generated brand guidelines.\n\n## Scoring Levels\n\n### High Confidence\nThe guideline section is well-supported and actionable.\n\n**Criteria (must meet at least 3):**\n- 3+ corroborating sources\n- Explicit guidance found in at least one AUTHORITATIVE source\n- Consistent across document and conversation analysis\n- Specific, actionable instructions (not just vague principles)\n- No unresolved conflicts\n\n**Example:** Voice attribute \"Confident but not arrogant\" appears in the official style guide, is demonstrated in email templates, and matches patterns in top performer calls.\n\n### Medium Confidence\nThe section is reasonable but could benefit from more data or team confirmation.\n\n**Criteria (must meet at least 2):**\n- 1-2 corroborating sources\n- Inferred from patterns rather than explicit instruction\n- Minor inconsistencies resolved via recency or authority\n- Actionable but some interpretation was required\n- May have one unresolved conflict\n\n**Example:** Tone for social media inferred from email templates and one Slack thread, but no official social media guidelines exist.\n\n### Low Confidence\nThe section is a best-effort recommendation. Team review strongly recommended.\n\n**Criteria (must meet at least 2):**\n- Single source only\n- Primarily inferred from indirect evidence\n- Significant interpretation required\n- Unresolved conflicts between sources\n- Limited specificity\n\n**Example:** Competitive positioning derived from a single sales call where a competitor was discussed, with no supporting documentation.\n\n## Section-Level Scoring Guide\n\n### Voice Attributes\n- **High**: Attributes appear in official brand guide AND are demonstrated in templates or calls\n- **Medium**: Attributes appear in one document type only, or are inferred from multiple conversations\n- **Low**: Attributes inferred from a single source or from indirect evidence\n\n### Messaging Framework\n- **High**: Value propositions documented in official materials AND used consistently in sales conversations\n- **Medium**: Documented but not observed in practice, OR observed but not documented\n- **Low**: Extracted from a single pitch deck or single call\n\n### Tone Matrix\n- **High**: Explicit tone guidance exists for the context AND matches observed behavior\n- **Medium**: Tone inferred from 3+ examples of content in that context\n- **Low**: Tone inferred from 1-2 examples, or extrapolated from similar contexts\n\n### Terminology\n- **High**: Terms explicitly listed in a style guide or glossary\n- **Medium**: Terms consistently used in templates and calls (pattern-based)\n- **Low**: Terms observed in a single document or inferred from brand personality\n\n### Language Patterns (from transcripts)\n- **High**: Pattern observed in 5+ calls across multiple speakers\n- **Medium**: Pattern observed in 3-4 calls or from a single top performer\n- **Low**: Pattern observed in 1-2 calls only\n\n### Transcript-Primary Scenarios\n\nWhen guidelines are generated primarily from conversational sources (no AUTHORITATIVE documents available):\n- Voice Attributes derived from 5+ transcripts = **Medium** (not Low)\n- Messaging Framework from consistent patterns across 5+ calls = **Medium**\n- Language Patterns weight increases from 10% to 20% in aggregate calculation (subtract 10% from Voice Attributes)\n\nNote this in the guideline metadata: \"Guidelines generated primarily from conversational sources — team review recommended to formalize.\"\n\n## Aggregate Confidence\n\nCalculate overall guideline confidence as the weighted average of section scores:\n\n| Section | Weight |\n|---------|--------|\n| Voice Attributes | 30% |\n| Messaging Framework | 25% |\n| Tone Matrix | 20% |\n| Terminology | 15% |\n| Language Patterns | 10% |\n\nConvert scores: High = 1.0, Medium = 0.6, Low = 0.3\n\n**Example:**\n- Voice Attributes: High (1.0 x 0.30 = 0.30)\n- Messaging: Medium (0.6 x 0.25 = 0.15)\n- Tone: Medium (0.6 x 0.20 = 0.12)\n- Terminology: High (1.0 x 0.15 = 0.15)\n- Language: Low (0.3 x 0.10 = 0.03)\n- **Overall: 0.75 = Medium-High confidence**\n\n**Aggregate score thresholds:**\n- 0.85–1.0 = High\n- 0.60–0.84 = Medium\n- Below 0.60 = Low\n\n## Presentation\n\nPresent confidence alongside each section header:\n\n```markdown\n## Voice Attributes (Confidence: High)\n[content]\n\n## Tone Matrix (Confidence: Medium)\n[content — note: no official social media guidelines found, tone inferred from email patterns]\n```\n\nFor Medium and Low confidence sections, include a brief note explaining why confidence is limited and what would raise it.\n\n## Relationship to Open Questions\n\nLow confidence sections should generate corresponding open questions:\n\n- **Low confidence + conflict** = High Priority open question\n- **Low confidence + gap** = Medium Priority open question\n- **Medium confidence + minor inconsistency** = Low Priority open question\n\nEvery open question includes a recommendation that, if confirmed, would raise the section's confidence score.\n", "depth": 6}, {"id": "202", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/brand-voice/skills/guideline-generation/references/guideline-template.md", "name": "guideline-template.md", "type": "file", "content": "# Brand Guidelines Output Template\n\nThe complete structure for generated brand voice guidelines. Fill each section based on discovered and analyzed sources. The following is a raw markdown template — copy and fill the sections below. Omit sections with no data rather than including empty placeholders.\n\n# [Company Name] Brand Voice Guidelines\n\n## Generation Metadata\n- Created: [date]\n- Version: [N] (incremented from previous if updating)\n- Replaces: [previous version date, if applicable]\n- Sources: [list of documents and/or conversations analyzed]\n- Documents processed: [N]\n- Conversations analyzed: [N]\n- Discovery report used: [Yes/No]\n- Overall confidence: [High/Medium/Low]\n\n---\n\n## Executive Summary\n\n[2-3 paragraph overview of the brand voice. What makes this brand distinctive?\nWhat is the core personality? How should content feel to the reader?]\n\n---\n\n## We Are / We Are Not\n\nThe foundational brand identity anchor. Voice is constant — it doesn't change by channel or audience.\n\n| We Are | We Are Not |\n|--------|------------|\n| **[Attribute 1]** — [brief description] | **[Counter 1]** — [what to avoid] |\n| **[Attribute 2]** — [brief description] | **[Counter 2]** — [what to avoid] |\n| **[Attribute 3]** — [brief description] | **[Counter 3]** — [what to avoid] |\n| **[Attribute 4]** — [brief description] | **[Counter 4]** — [what to avoid] |\n| **[Attribute 5]** — [brief description] | **[Counter 5]** — [what to avoid] |\n\n### Voice Attributes Detail\n\nFor each \"We Are\" attribute:\n\n#### [Attribute 1]: [Name]\n- **What it means**: [detailed description]\n- **How it shows up**: [specific behaviors in content]\n- **What to avoid**: [the \"We Are Not\" counterpart, expanded]\n- **Evidence**: [source quote or pattern, with citation]\n- **Confidence**: [High/Medium/Low]\n\n[Repeat for each attribute]\n\n---\n\n## Brand Personality\n\n- **Archetype**: [e.g., \"The Expert Friend\" — technically deep but never condescending]\n- **If our brand were a person**: [brief personality sketch]\n- **Core values expressed in voice**: [list]\n\n---\n\n## Messaging Framework\n\n### Primary Value Proposition\n[Core value statement — one sentence]\n\nVariations observed:\n- \"[Variation 1]\" (Source: [doc])\n- \"[Variation 2]\" (Source: [doc])\n\n### Key Message Pillars\n\n1. **[Pillar Name]**\n   - Core idea: [one sentence]\n   - When to use: [contexts]\n   - Example phrasing: \"[quote]\"\n   - Frequency in conversations: [X]% (if transcript data available)\n   - Effectiveness: [High/Medium/Low] (if outcome data available)\n\n2. **[Pillar Name]**\n   [same format]\n\n3. **[Pillar Name]**\n   [same format]\n\n### Competitive Positioning\n- vs. [Competitor A]: [how we differentiate]\n- vs. [Competitor B]: [how we differentiate]\n- vs. Status Quo: [why change is worth it]\n\n---\n\n## Tone-by-Context Matrix\n\nVoice is constant. Tone flexes by context. These three dimensions adjust:\n\n| Context | Formality | Energy | Technical Depth | Key Principle |\n|---------|-----------|--------|-----------------|---------------|\n| Cold outreach | Medium | High | Low | Hook fast, earn attention |\n| Discovery calls | Medium | Medium-High | Medium | Ask more than tell |\n| Demo / presentation | Medium-High | High | High | Show, don't just describe |\n| Enterprise proposal | High | Medium | High | ROI and precision |\n| Follow-up email | Medium | Medium | Low-Medium | Add new value each touch |\n| Social media | Low-Medium | High | Low | Brevity and personality |\n| Customer success | Medium | Warm | Medium | Empathy and competence |\n| Internal comms | Low | Medium | Varies | Authentic, less polished |\n\n### Context-Specific Guidelines\n\n#### Cold Outreach\n- **Overall tone**: [description]\n- **Opening approach**: [what works]\n- **Do's**: [list]\n- **Don'ts**: [list]\n- **Example**: \"[sample opening line]\"\n\n#### Discovery Calls\n[same structure]\n\n#### Proposals \u0026 RFPs\n[same structure]\n\n#### Social Media\n[same structure]\n\n---\n\n## Terminology Guide\n\n### Must-Use Terms\n| Term | Usage | Instead Of | Example |\n|------|-------|------------|---------|\n| [term] | [when/how to use] | [what it replaces] | \"[in a sentence]\" |\n\n### Preferred Terms\n| Term | Usage | Example |\n|------|-------|---------|\n| [term] | [guidance] | \"[in a sentence]\" |\n\n### Avoid These Terms\n| Term | Reason | Alternative |\n|------|--------|-------------|\n| [term] | [why to avoid] | [what to use instead] |\n\n### Never-Use Terms\n| Term | Reason |\n|------|--------|\n| [term] | [why this is prohibited] |\n\n---\n\n## Language That Works\n\nRanked by effectiveness (from conversation analysis, if available):\n\n### Top Phrases\n1. **\"[Phrase]\"** — Context: [when to use], Why it works: [analysis]\n2. **\"[Phrase]\"** — Context: [when to use], Why it works: [analysis]\n\n### Questions That Engage\n1. **\"[Question]\"** — Leads to: [typical outcome]\n2. **\"[Question]\"** — Leads to: [typical outcome]\n\n### Objection Handling Patterns\n1. **Objection**: \"[common objection]\"\n   **Response pattern**: \"[how top performers respond]\"\n   **Why it works**: [analysis]\n\n---\n\n## Language to Avoid\n\n### Anti-Patterns\n1. **\"[Phrase/pattern]\"** — Problem: [what happens], Better: \"[alternative]\"\n2. **\"[Phrase/pattern]\"** — Problem: [what happens], Better: \"[alternative]\"\n\n---\n\n## Content Examples\n\n### Excellent Examples\n[Full example with annotation explaining what makes it on-brand]\n\n### Examples to Avoid\n[Full example with annotation explaining what's off-brand and how to fix it]\n\n---\n\n## Confidence Scores\n\n| Section | Confidence | Basis | Sources |\n|---------|------------|-------|---------|\n| Voice Attributes | [H/M/L] | [why] | [N sources] |\n| Messaging Framework | [H/M/L] | [why] | [N sources] |\n| Tone Matrix | [H/M/L] | [why] | [N sources] |\n| Terminology | [H/M/L] | [why] | [N sources] |\n| Language Patterns | [H/M/L] | [why] | [N sources] |\n\n---\n\n## Open Questions for Team Discussion\n\n### High Priority (blocks guideline completion)\n1. **[Question Title]**\n   - What was found: [description of ambiguity]\n   - Agent recommendation: [suggested resolution]\n   - Need from you: [specific decision]\n\n### Medium Priority (improves quality)\n[same format]\n\n### Low Priority (nice to have)\n[same format]\n\n---\n\n## Data Gaps \u0026 Recommendations\n\n[What's missing and specific actions to fill gaps]\n\n- [ ] [Gap 1]: [recommendation to fill it]\n- [ ] [Gap 2]: [recommendation to fill it]\n\n---\n\n## Appendix: Sources\n\n| # | Source | Platform | Type | Date | Key Sections Used | Confidence |\n|---|--------|----------|------|------|-------------------|------------|\n| 1 | [title] | [platform] | [AUTHORITATIVE/OPERATIONAL/etc.] | [date] | [sections] | [H/M/L] |\n\n---\n\n## Template Usage Notes\n\n- Omit sections with no data rather than including empty placeholders\n- If transcript data is unavailable, omit the \"Language That Works\" and \"Language to Avoid\" sections\n- If only documents are available (no transcripts), note this in metadata\n- \"We Are / We Are Not\" table should have minimum 4 rows, ideally 5-7\n- Tone matrix should cover at minimum: cold outreach, proposals, social media\n- All examples must have source attribution\n- Every open question must include an agent recommendation\n", "depth": 6}, {"id": "203", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills", "name": "skills", "type": "dir", "content": "", "depth": 3}, {"id": "204", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/README.md", "name": "README.md", "type": "file", "content": "# Apollo Plugin for Claude Code and Cowork\n\nProspect, enrich leads, and load outreach sequences with [Apollo.io](https://www.apollo.io/) — powered by the Apollo MCP Server with **one-click integration**.\n\n---\n\n## 🔌 One-Click MCP Server Integration\n\nThis plugin **automatically configures the Apollo MCP Server** when installed. No manual server setup, no config files to edit - just install the plugin and authenticate with your Apollo Account.\n\n---\n\n## ✅ Powerful Skills\n\nThis plugin ships with high-value skills that chain multiple Apollo APIs into complete workflows:\n\n| Skill | What it does |\n|---|---|\n| `/apollo:enrich-lead` | Drop a name, LinkedIn URL, or email — get a full contact card with email, phone, company intel, and next actions |\n| `/apollo:prospect` | Describe your ICP in plain English — get a ranked table of enriched decision-maker leads |\n| `/apollo:sequence-load` | Find leads, enrich them, and bulk-load into an outreach sequence — handles dedup and enrollment |\n\n### `/apollo:enrich-lead`\n\nDrop in a name, company, LinkedIn URL, or email — get back a complete contact card with email, phone, title, location, company details, and suggested next actions. Handles fuzzy lookups (e.g. \"CEO of Figma\") and falls back to search when exact match fails.\n\n### `/apollo:prospect`\n\nDescribe your ICP in plain English. The pipeline searches for matching companies, bulk-enriches firmographic data, finds decision makers, reveals contact info via bulk enrichment, and returns a ranked lead table with ICP fit scores.\n\n### `/apollo:sequence-load`\n\nFind contacts matching your targeting criteria, enrich them, create them as contacts with deduplication, and bulk-add them to an existing Apollo sequence. Previews candidates before enrollment and shows a full summary after.\n\n---\n\n## 📦 Installation\n\n### Cowork\n\nClick the link below to install in one step:\n\n[Install in Cowork](https://claude.ai/desktop/customize/plugins/new?marketplace=apolloio/apollo-mcp-plugin\u0026plugin=apollo)\n\nThen restart Cowork to ensure the MCP server starts correctly.\n\n### Claude Code\n\n#### 1. Add this plugin's marketplace\n\nIn Claude Code, run:\n\n```\n/plugin marketplace add apolloio/apollo-mcp-plugin\n```\n\n#### 2. Install the plugin\n\n```\n/plugin install apollo@apollo-plugin-marketplace\n```\n\n#### 3. Restart Claude Code\n\nThis ensures the MCP server starts correctly.\n\n---\n\n## 🔑 Authentication\n\nThe Apollo MCP Server supports **OAuth**:\n\n1. After installation, run `/mcp` in Claude Code or Cowork\n2. Select the **Apollo** server and click **Authenticate**\n3. Complete the Apollo.io login in your browser\n4. Done — all commands are now ready to use\n\n---\n\n## ⚠️ Apollo Credits\n\nSome operations consume [Apollo credits](https://docs.apollo.io/):\n\n- **People enrichment** (used by all three skills) costs 1 credit per person\n- **Bulk enrichment** (`/apollo:prospect`, `/apollo:sequence-load`) consumes 1 credit per person in the batch\n- The plugin will always warn you before consuming credits\n\n---\n\n## 🙌 Credits\n\n- **MCP Server** by [Apollo.io](https://docs.apollo.io/)\n- **Plugin Specification** by [Anthropic](https://docs.anthropic.com/)\n\n---\n\n## License\n\nMIT — see [LICENSE](LICENSE) for details.\n", "depth": 3}, {"id": "205", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills/prospect", "name": "prospect", "type": "dir", "content": "", "depth": 4}, {"id": "206", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills/enrich-lead", "name": "enrich-lead", "type": "dir", "content": "", "depth": 4}, {"id": "207", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills/sequence-load", "name": "sequence-load", "type": "dir", "content": "", "depth": 4}, {"id": "208", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills/prospect/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: prospect\ndescription: \"Full ICP-to-leads pipeline. Describe your ideal customer in plain English and get a ranked table of enriched decision-maker leads with emails and phone numbers.\"\nuser-invocable: true\nargument-hint: \"[describe your ideal customer]\"\n---\n\n# Prospect\n\nGo from an ICP description to a ranked, enriched lead list in one shot. The user describes their ideal customer via \"$ARGUMENTS\".\n\n## Examples\n\n- `/apollo:prospect VP of Engineering at Series B+ SaaS companies in the US, 200-1000 employees`\n- `/apollo:prospect heads of marketing at e-commerce companies in Europe`\n- `/apollo:prospect CTOs at fintech startups, 50-500 employees, New York`\n- `/apollo:prospect procurement managers at manufacturing companies with 1000+ employees`\n- `/apollo:prospect SDR leaders at companies using Salesforce and Outreach`\n\n## Step 1 — Parse the ICP\n\nExtract structured filters from the natural language description in \"$ARGUMENTS\":\n\n**Company filters:**\n- Industry/vertical keywords → `q_organization_keyword_tags`\n- Employee count ranges → `organization_num_employees_ranges`\n- Company locations → `organization_locations`\n- Specific domains → `q_organization_domains_list`\n\n**Person filters:**\n- Job titles → `person_titles`\n- Seniority levels → `person_seniorities`\n- Person locations → `person_locations`\n\nIf the ICP is vague, ask 1-2 clarifying questions before proceeding. At minimum, you need a title/role and an industry or company size.\n\n## Step 2 — Search for Companies\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_mixed_companies_search` with the company filters:\n- `q_organization_keyword_tags` for industry/vertical\n- `organization_num_employees_ranges` for size\n- `organization_locations` for geography\n- Set `per_page` to 25\n\n## Step 3 — Enrich Top Companies\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_organizations_bulk_enrich` with the domains from the top 10 results. This reveals revenue, funding, headcount, and firmographic data to help rank companies.\n\n## Step 4 — Find Decision Makers\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_mixed_people_api_search` with:\n- `person_titles` and `person_seniorities` from the ICP\n- `q_organization_domains_list` scoped to the enriched company domains\n- `per_page` set to 25\n\n## Step 5 — Enrich Top Leads\n\n\u003e **Credit warning**: Tell the user exactly how many credits will be consumed before proceeding.\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_people_bulk_match` to enrich up to 10 leads per call with:\n- `first_name`, `last_name`, `domain` for each person\n- `reveal_personal_emails` set to `true`\n\nIf more than 10 leads, batch into multiple calls.\n\n## Step 6 — Present the Lead Table\n\nShow results in a ranked table:\n\n### Leads matching: [ICP Summary]\n\n| # | Name | Title | Company | Employees | Revenue | Email | Phone | ICP Fit |\n|---|---|---|---|---|---|---|---|---|\n\n**ICP Fit** scoring:\n- **Strong** — title, seniority, company size, and industry all match\n- **Good** — 3 of 4 criteria match\n- **Partial** — 2 of 4 criteria match\n\n**Summary**: Found X leads across Y companies. Z credits consumed.\n\n## Step 7 — Offer Next Actions\n\nAsk the user:\n\n1. **Save all to Apollo** — Bulk-create contacts via `mcp__claude_ai_Apollo_MCP__apollo_contacts_create` with `run_dedupe: true` for each lead\n2. **Load into a sequence** — Ask which sequence and run the sequence-load flow for these contacts\n3. **Deep-dive a company** — Run `/apollo:company-intel` on any company from the list\n4. **Refine the search** — Adjust filters and re-run\n5. **Export** — Format leads as a CSV-style table for easy copy-paste\n", "depth": 5}, {"id": "209", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills/enrich-lead/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: enrich-lead\ndescription: \"Instant lead enrichment. Drop a name, company, LinkedIn URL, or email and get the full contact card with email, phone, title, company intel, and next actions.\"\nuser-invocable: true\nargument-hint: \"[name, company, LinkedIn URL, or email]\"\n---\n\n# Enrich Lead\n\nTurn any identifier into a full contact dossier. The user provides identifying info via \"$ARGUMENTS\".\n\n## Examples\n\n- `/apollo:enrich-lead Tim Zheng at Apollo`\n- `/apollo:enrich-lead https://www.linkedin.com/in/timzheng`\n- `/apollo:enrich-lead sarah@stripe.com`\n- `/apollo:enrich-lead Jane Smith, VP Engineering, Notion`\n- `/apollo:enrich-lead CEO of Figma`\n\n## Step 1 — Parse Input\n\nFrom \"$ARGUMENTS\", extract every identifier available:\n- First name, last name\n- Company name or domain\n- LinkedIn URL\n- Email address\n- Job title (use as a matching hint)\n\nIf the input is ambiguous (e.g. just \"CEO of Figma\"), first use `mcp__claude_ai_Apollo_MCP__apollo_mixed_people_api_search` with relevant title and domain filters to identify the person, then proceed to enrichment.\n\n## Step 2 — Enrich the Person\n\n\u003e **Credit warning**: Tell the user enrichment consumes 1 Apollo credit before calling.\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_people_match` with all available identifiers:\n- `first_name`, `last_name` if name is known\n- `domain` or `organization_name` if company is known\n- `linkedin_url` if LinkedIn is provided\n- `email` if email is provided\n- Set `reveal_personal_emails` to `true`\n\nIf the match fails, try `mcp__claude_ai_Apollo_MCP__apollo_mixed_people_api_search` with looser filters and present the top 3 candidates. Ask the user to pick one, then re-enrich.\n\n## Step 3 — Enrich Their Company\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_organizations_enrich` with the person's company domain to pull firmographic context.\n\n## Step 4 — Present the Contact Card\n\nFormat the output exactly like this:\n\n---\n\n**[Full Name]** | [Title]\n[Company Name] · [Industry] · [Employee Count] employees\n\n| Field | Detail |\n|---|---|\n| Email (work) | ... |\n| Email (personal) | ... (if revealed) |\n| Phone (direct) | ... |\n| Phone (mobile) | ... |\n| Phone (corporate) | ... |\n| Location | City, State, Country |\n| LinkedIn | URL |\n| Company Domain | ... |\n| Company Revenue | Range |\n| Company Funding | Total raised |\n| Company HQ | Location |\n\n---\n\n## Step 5 — Offer Next Actions\n\nAsk the user which action to take:\n\n1. **Save to Apollo** — Create this person as a contact via `mcp__claude_ai_Apollo_MCP__apollo_contacts_create` with `run_dedupe: true`\n2. **Add to a sequence** — Ask which sequence, then run the sequence-load flow\n3. **Find colleagues** — Search for more people at the same company using `mcp__claude_ai_Apollo_MCP__apollo_mixed_people_api_search` with `q_organization_domains_list` set to this company\n4. **Find similar people** — Search for people with the same title/seniority at other companies\n", "depth": 5}, {"id": "210", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/apollo/skills/sequence-load/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: sequence-load\ndescription: \"Find leads matching criteria and bulk-add them to an Apollo outreach sequence. Handles enrichment, contact creation, deduplication, and enrollment in one flow.\"\nuser-invocable: true\nargument-hint: \"[targeting criteria + sequence name]\"\n---\n\n# Sequence Load\n\nFind, enrich, and load contacts into an outreach sequence — end to end. The user provides targeting criteria and a sequence name via \"$ARGUMENTS\".\n\n## Examples\n\n- `/apollo:sequence-load add 20 VP Sales at SaaS companies to my \"Q1 Outbound\" sequence`\n- `/apollo:sequence-load SDR managers at fintech startups → Cold Outreach v2`\n- `/apollo:sequence-load list sequences` (shows all available sequences)\n- `/apollo:sequence-load directors of engineering, 500+ employees, US → Demo Follow-up`\n- `/apollo:sequence-load reload 15 more leads into \"Enterprise Pipeline\"`\n\n## Step 1 — Parse Input\n\nFrom \"$ARGUMENTS\", extract:\n\n**Targeting criteria:**\n- Job titles → `person_titles`\n- Seniority levels → `person_seniorities`\n- Industry keywords → `q_organization_keyword_tags`\n- Company size → `organization_num_employees_ranges`\n- Locations → `person_locations` or `organization_locations`\n\n**Sequence info:**\n- Sequence name (text after \"to\", \"into\", or \"→\")\n- Volume — how many contacts to add (default: 10 if not specified)\n\nIf the user just says \"list sequences\", skip to Step 2 and show all available sequences.\n\n## Step 2 — Find the Sequence\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_emailer_campaigns_search` to find the target sequence:\n- Set `q_name` to the sequence name from input\n\nIf no match or multiple matches:\n- Show all available sequences in a table: | Name | ID | Status |\n- Ask the user to pick one\n\n## Step 3 — Get Email Account\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_email_accounts_index` to list linked email accounts.\n\n- If one account → use automatically\n- If multiple → show them and ask which to send from\n\n## Step 4 — Find Matching People\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_mixed_people_api_search` with the targeting criteria.\n- Set `per_page` to the requested volume (or 10 by default)\n\nPresent the candidates in a preview table:\n\n| # | Name | Title | Company | Location |\n|---|---|---|---|---|\n\nAsk: **\"Add these [N] contacts to [Sequence Name]? This will consume [N] Apollo credits for enrichment.\"**\n\nWait for confirmation before proceeding.\n\n## Step 5 — Enrich and Create Contacts\n\nFor each approved lead:\n\n1. **Enrich** — Use `mcp__claude_ai_Apollo_MCP__apollo_people_bulk_match` (batch up to 10 per call) with:\n   - `first_name`, `last_name`, `domain` for each person\n   - `reveal_personal_emails` set to `true`\n\n2. **Create contacts** — For each enriched person, use `mcp__claude_ai_Apollo_MCP__apollo_contacts_create` with:\n   - `first_name`, `last_name`, `email`, `title`, `organization_name`\n   - `direct_phone` or `mobile_phone` if available\n   - `run_dedupe` set to `true`\n\nCollect all created contact IDs.\n\n## Step 6 — Add to Sequence\n\nUse `mcp__claude_ai_Apollo_MCP__apollo_emailer_campaigns_add_contact_ids` with:\n- `id`: the sequence ID\n- `emailer_campaign_id`: same sequence ID\n- `contact_ids`: array of created contact IDs\n- `send_email_from_email_account_id`: the chosen email account ID\n- `sequence_active_in_other_campaigns`: `false` (safe default)\n\n## Step 7 — Confirm Enrollment\n\nShow a summary:\n\n---\n\n**Sequence loaded successfully**\n\n| Field | Value |\n|---|---|\n| Sequence | [Name] |\n| Contacts added | [count] |\n| Sending from | [email address] |\n| Credits used | [count] |\n\n**Contacts enrolled:**\n\n| Name | Title | Company | Email |\n|---|---|---|---|\n\n---\n\n## Step 8 — Offer Next Actions\n\nAsk the user:\n\n1. **Load more** — Find and add another batch of leads\n2. **Review sequence** — Show sequence details and all enrolled contacts\n3. **Remove a contact** — Use `mcp__claude_ai_Apollo_MCP__apollo_emailer_campaigns_remove_or_stop_contact_ids` to remove specific contacts\n4. **Pause a contact** — Re-add with `status: \"paused\"` and an `auto_unpause_at` date\n", "depth": 5}, {"id": "211", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/commands", "name": "commands", "type": "dir", "content": "", "depth": 3}, {"id": "212", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/skills", "name": "skills", "type": "dir", "content": "", "depth": 3}, {"id": "213", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/README.md", "name": "README.md", "type": "file", "content": "# Slack Plugin\n\nThis repository contains the configuration needed to integrate Slack with Cursor IDE and Claude Code. The plugin enables your agents to interact directly with your Slack workspace, allowing you to search messages, send communications, manage canvases, and more—all through natural language.\n\n## Features\n\nThe Slack MCP server provides the following capabilities:\n\n- **Search**: Find messages, files, users, and channels (both public and private)\n- **Messaging**: Send messages, retrieve channel histories, and access threaded conversations\n- **Canvas**: Create and share formatted documents, export content as markdown\n- **User Management**: Retrieve user profiles including custom fields and status information\n\n## Prerequisites\n\nBefore setting up the Slack MCP server, ensure you have:\n\n- Cursor IDE or Claude Code CLI installed\n- Access to a Slack workspace with MCP integration approved by your workspace admin\n\n## Installation\n\nChoose the installation method for your IDE:\n\n### Claude Code\n\nIf you're using Claude Code CLI, you can install this as a plugin by cloning it locally:\n\n```bash\ngit clone https://github.com/slackapi/slack-mcp-plugin.git\ncd slack-mcp-plugin\nclaude --plugin-dir ./\n```\n\nThe Slack MCP server will be automatically configured when the plugin loads. You will be prompted to authenticate into your Slack workspace via OAuth.\n\nThe Claude plugin uses the following MCP configuration (`.mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"slack\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.slack.com/mcp\",\n      \"oauth\": {\n        \"clientId\": \"1601185624273.8899143856786\",\n        \"callbackPort\": 3118\n      }\n    }\n  }\n}\n```\n\n### Cursor\n\nYou can use the following Add to Cursor button or follow the steps below to manually configure the Slack MCP server in Cursor:\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en-US/install-mcp?name=slack\u0026config=eyJ1cmwiOiJodHRwczovL21jcC5zbGFjay5jb20vbWNwIiwiYXV0aCI6eyJDTElFTlRfSUQiOiIzNjYwNzUzMTkyNjI2Ljg5MDM0NjkyMjg5ODIifX0%3D)\n\n#### Step 1: Open Cursor Settings\n\nNavigate to **Cursor → Settings → Cursor Settings** (or use the keyboard shortcut `Cmd+,` on macOS, `Ctrl+,` on Windows/Linux).\n\n#### Step 2: Navigate to MCP Tab\n\nIn the Settings interface, click on the **MCP** tab to access MCP server configurations.\n\n#### Step 3: Add Slack MCP Configuration\n\nAdd the following configuration to connect to the remote Slack MCP server:\n\n```json\n{\n  \"mcpServers\": {\n    \"slack\": {\n      \"url\": \"https://mcp.slack.com/mcp\",\n      \"auth\": {\n        \"CLIENT_ID\": \"3660753192626.8903469228982\"\n      }\n    }\n  }\n}\n```\n\nSave the configuration. You will also see a connect button once added. Click that to authenticate into your Slack Workspace.\n\n## Usage Examples\n\nOnce configured, you can interact with Slack through your AI assistant using natural language:\n\n- **Search messages**: \"Search for messages about the product launch in the last week\"\n- **Send messages**: \"Send a message to #general channel saying the deployment is complete\"\n- **Find users**: \"Who is the user with email john@example.com?\"\n- **Access threads**: \"Show me the conversation thread from that message\"\n- **Create canvases**: \"Create a canvas document with our meeting notes\"\n\n## Documentation \u0026 Resources\n\n- [Official Slack MCP Server Documentation](https://docs.slack.dev/ai/mcp-server/)\n\n## Notes \u0026 Limitations\n\n- **Remote server only**: This configuration connects to Slack's hosted MCP server. No local installation is required or supported.\n- **Admin approval required**: Your Slack workspace administrator must approve MCP integration before you can use this feature.\n\n## Questions or Issues?\n\nFor questions about the Slack MCP server or integration issues, please refer to the [official Slack documentation](https://docs.slack.dev/ai/mcp-server/) or contact your workspace administrator.\n", "depth": 3}, {"id": "214", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/CLAUDE.md", "name": "CLAUDE.md", "type": "file", "content": "# Slack Plugin\n\nThis plugin integrates Slack with Claude Code, providing tools to search, read, and send messages in Slack.\n\n## Commands\n\n- `/slack:summarize-channel \u003cchannel-name\u003e` — Summarize recent activity in a Slack channel\n- `/slack:find-discussions \u003ctopic\u003e` — Find discussions about a specific topic across Slack channels\n- `/slack:draft-announcement \u003ctopic\u003e` — Draft a well-formatted Slack announcement and save it as a draft\n- `/slack:standup` — Generate a standup update based on your recent Slack activity\n- `/slack:channel-digest \u003cchannel1, channel2, ...\u003e` — Get a digest of recent activity across multiple Slack channels\n\n## Skills\n\n- **slack-messaging** — Guidance for composing well-formatted Slack messages using mrkdwn syntax\n- **slack-search** — Guidance for effectively searching Slack to find messages, files, channels, and people\n", "depth": 3}, {"id": "215", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/commands/standup.md", "name": "standup.md", "type": "file", "content": "---\ndescription: Generate a standup update based on your recent Slack activity\n---\n\n1. Use `slack_read_user_profile` (with no user_id) to get the current user's profile information, including their user ID and display name.\n\n2. Search for the user's recent messages using `slack_search_public` with the filter `from:\u003c@USER_ID\u003e` and `after:` set to yesterday's date. This captures messages from the last working day.\n\n3. Review the messages found and categorize them into standup themes:\n   - **What I worked on** — Topics, projects, or tasks the user discussed or contributed to\n   - **What I'm working on next** — Any mentions of upcoming work, plans, or follow-ups\n   - **Blockers** — Any questions asked that went unanswered, issues raised, or explicit mentions of being stuck\n\n4. For messages in threads, use `slack_read_thread` to get the full context so you can accurately describe what the user contributed.\n\n5. Format the standup as:\n   ```\n   *Standup for \u003cdisplay name\u003e — \u003ctoday's date\u003e*\n\n   *Done:*\n   - Item 1\n   - Item 2\n\n   *Doing:*\n   - Item 1\n\n   *Blockers:*\n   - None / Item 1\n   ```\n\n6. Present the standup to the user for review. They can edit, adjust, or ask you to post it to a specific channel.\n", "depth": 4}, {"id": "216", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/commands/find-discussions.md", "name": "find-discussions.md", "type": "file", "content": "---\ndescription: Find discussions about a specific topic across Slack channels\n---\n\nGiven the topic provided in $ARGUMENTS:\n\n1. Use the `slack_search_public` tool to search for messages matching the topic. Use the topic as a natural language question first for semantic results.\n2. If semantic results are sparse, follow up with a keyword search using key terms from the topic.\n3. For the most relevant results, use `slack_read_thread` to fetch full thread conversations so you capture the complete discussion context.\n4. Present the results organized by relevance:\n   - For each discussion found, show: the channel name, who started it, a brief summary of the conversation, and the date.\n   - Group related discussions together if they span multiple channels.\n   - Highlight any conclusions, decisions, or unresolved questions.\n5. Limit output to the top 5-10 most relevant discussions to keep results manageable.\n6. If no results are found, suggest alternative search terms or broader queries the user could try.\n", "depth": 4}, {"id": "217", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/commands/summarize-channel.md", "name": "summarize-channel.md", "type": "file", "content": "---\ndescription: Summarize recent activity in a Slack channel\n---\n\nGiven the channel name provided in $ARGUMENTS (strip any leading `#`):\n\n1. Use the `slack_search_channels` tool to find the channel ID for the provided channel name. Strip any leading `#` from the argument before searching.\n2. Use the `slack_read_channel` tool to read recent messages from the channel (default limit of 100 messages).\n3. For any messages that have threads with replies, use `slack_read_thread` to read the thread contents so the summary captures threaded discussions.\n4. Produce a concise summary organized by topic or theme. The summary should include:\n   - An overview of the main topics discussed\n   - Key decisions or action items mentioned\n   - Notable announcements or updates\n   - Active threads and their conclusions (if any)\n5. Keep the summary scannable — use short bullet points grouped by topic. Mention who said what when it's relevant (e.g., decisions, action items).\n6. If the channel has very little recent activity, say so and note the last time a message was posted.\n", "depth": 4}, {"id": "218", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/commands/draft-announcement.md", "name": "draft-announcement.md", "type": "file", "content": "---\ndescription: Draft a well-formatted Slack announcement and save it as a draft\n---\n\nGiven the topic or context provided in $ARGUMENTS:\n\n1. Ask the user the following clarifying questions (skip any that are already clear from the provided context):\n   - Which channel should this announcement be posted in?\n   - Who is the target audience?\n   - What is the key message or call to action?\n   - Is there a deadline or date to highlight?\n   - What tone is appropriate — formal, casual, or urgent?\n\n2. Compose the announcement following Slack formatting best practices:\n   - Use Slack's mrkdwn syntax: `*bold*` for emphasis (not `**bold**`), `_italic_` for secondary emphasis, `\u003e` for callouts.\n   - Lead with the most important information — don't bury the point.\n   - Use a clear, descriptive opening line that works as a headline.\n   - Keep paragraphs short (2-3 sentences max).\n   - Use bullet points for lists of items or action steps.\n   - Include relevant emoji sparingly to aid scanning (e.g., :mega: for announcements, :calendar: for dates, :point_right: for action items).\n   - End with a clear call to action or next step if applicable.\n\n3. Present the draft to the user for review. Offer to adjust tone, length, or formatting.\n\n4. Once the user approves, use `slack_search_channels` to find the target channel ID, then use `slack_send_message_draft` to create the draft in Slack.\n\n5. Let the user know the draft is ready in Slack and they can review and send it from the Slack client.\n", "depth": 4}, {"id": "219", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/commands/channel-digest.md", "name": "channel-digest.md", "type": "file", "content": "---\ndescription: Get a digest of recent activity across multiple Slack channels\n---\n\nGiven the comma-separated channel names provided in $ARGUMENTS (strip leading `#` and whitespace from each):\n\n1. Parse the argument into individual channel names. Strip leading `#` and whitespace from each name.\n\n2. For each channel:\n   a. Use `slack_search_channels` to find the channel ID.\n   b. Use `slack_read_channel` to read recent messages (use a limit of 50 messages per channel to keep things manageable).\n   c. Summarize the key activity in that channel: main topics, decisions, questions, and notable messages.\n\n3. Present the digest in this format:\n\n   ```\n   *Channel Digest — \u003ctoday's date\u003e*\n\n   *#channel-1*\n   - Summary point 1\n   - Summary point 2\n\n   *#channel-2*\n   - Summary point 1\n   - Summary point 2\n\n   ...\n   ```\n\n4. For each channel, keep the summary to 3-5 bullet points maximum. Focus on what's actionable or noteworthy.\n\n5. If a channel has no recent activity, note that it's been quiet and mention when the last message was posted (if visible).\n\n6. If a channel name can't be found, let the user know and continue with the remaining channels.\n", "depth": 4}, {"id": "220", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/skills/slack-messaging", "name": "slack-messaging", "type": "dir", "content": "", "depth": 4}, {"id": "221", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/skills/slack-search", "name": "slack-search", "type": "dir", "content": "", "depth": 4}, {"id": "222", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/skills/slack-messaging/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: slack-messaging\ndescription: Guidance for composing well-formatted, effective Slack messages using mrkdwn syntax\n---\n\n# Slack Messaging Best Practices\n\nThis skill provides guidance for composing well-formatted, effective Slack messages.\n\n## When to Use\n\nApply this skill whenever composing, drafting, or helping the user write a Slack message — including when using `slack_send_message`, `slack_send_message_draft`, or `slack_create_canvas`.\n\n## Slack Formatting (mrkdwn)\n\nSlack uses its own markup syntax called **mrkdwn**, which differs from standard Markdown. Always use mrkdwn when composing Slack messages:\n\n| Format | Syntax | Notes |\n|--------|--------|-------|\n| Bold | `*text*` | Single asterisks, NOT double |\n| Italic | `_text_` | Underscores |\n| Strikethrough | `~text~` | Tildes |\n| Code (inline) | `` `code` `` | Backticks |\n| Code block | `` ```code``` `` | Triple backticks |\n| Quote | `\u003e text` | Angle bracket |\n| Link | `\u003curl\\|display text\u003e` | Pipe-separated in angle brackets |\n| User mention | `\u003c@U123456\u003e` | User ID in angle brackets |\n| Channel mention | `\u003c#C123456\u003e` | Channel ID in angle brackets |\n| Bulleted list | `- item` or `• item` | Dash or bullet character |\n| Numbered list | `1. item` | Number followed by period |\n\n### Common Mistakes to Avoid\n\n- Do NOT use `**bold**` (double asterisks) — Slack uses `*bold*` (single asterisks)\n- Do NOT use `## headers` — Slack does not support Markdown headers. Use `*bold text*` on its own line instead.\n- Do NOT use `[text](url)` for links — Slack uses `\u003curl|text\u003e` format\n- Do NOT use `---` for horizontal rules — Slack does not render these\n\n## Message Structure Guidelines\n\n- **Lead with the point.** Put the most important information in the first line. Many people read Slack on mobile or in notifications where only the first line shows.\n- **Keep it short.** Aim for 1-3 short paragraphs. If the message is long, consider using a Canvas instead.\n- **Use line breaks generously.** Walls of text are hard to read. Separate distinct thoughts with blank lines.\n- **Use bullet points for lists.** Anything with 3+ items should be a list, not a run-on sentence.\n- **Bold key information.** Use `*bold*` for names, dates, deadlines, and action items so they stand out when scanning.\n\n## Thread vs. Channel Etiquette\n\n- **Reply in threads** when responding to a specific message to keep the main channel clean.\n- **Use `reply_broadcast`** (also post to channel) only when the reply contains information everyone needs to see.\n- **Post in the channel** (not a thread) when starting a new topic, making an announcement, or asking a question to the whole group.\n- **Don't start a new thread** to continue an existing conversation — find and reply to the original message.\n\n## Tone and Audience\n\n- Match the tone to the channel — `#general` is usually more formal than `#random`.\n- Use emoji reactions instead of reply messages for simple acknowledgments (though note: the MCP tools can't add reactions, so suggest the user do this manually if appropriate).\n- When writing announcements, use a clear structure: context, key info, call to action.\n", "depth": 5}, {"id": "223", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/slack/skills/slack-search/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: slack-search\ndescription: Guidance for effectively searching Slack to find messages, files, channels, and people\n---\n\n# Slack Search\n\nThis skill provides guidance for effectively searching Slack to find messages, files, and information.\n\n## When to Use\n\nApply this skill whenever you need to find information in Slack — including when a user asks you to locate messages, conversations, files, or people, or when you need to gather context before answering a question about what's happening in Slack.\n\n## Search Tools Overview\n\n| Tool | Use When |\n|------|----------|\n| `slack_search_public` | Searching public channels only. Does not require user consent. |\n| `slack_search_public_and_private` | Searching all channels including private, DMs, and group DMs. Requires user consent. |\n| `slack_search_channels` | Finding channels by name or description. |\n| `slack_search_users` | Finding people by name, email, or role. |\n\n## Search Strategy\n\n### Start Broad, Then Narrow\n\n1. Begin with a simple keyword or natural language question.\n2. If too many results, add filters (`in:`, `from:`, date ranges).\n3. If too few results, remove filters and try synonyms or related terms.\n\n### Choose the Right Search Mode\n\n- **Natural language questions** (e.g., \"What is the deadline for project X?\") — Best for fuzzy, conceptual searches where you don't know exact keywords.\n- **Keyword search** (e.g., `project X deadline`) — Best for finding specific, exact content.\n\n### Use Multiple Searches\n\nDon't rely on a single search. Break complex questions into smaller searches:\n- Search for the topic first\n- Then search for specific people's contributions\n- Then search in specific channels\n\n## Search Modifiers Reference\n\n### Location Filters\n- `in:channel-name` — Search within a specific channel\n- `in:\u003c#C123456\u003e` — Search in channel by ID\n- `-in:channel-name` — Exclude a channel\n- `in:\u003c@U123456\u003e` — Search in DMs with a user\n\n### User Filters\n- `from:\u003c@U123456\u003e` — Messages from a specific user (by ID)\n- `from:username` — Messages from a user (by Slack username)\n- `to:me` — Messages sent directly to you\n\n### Content Filters\n- `is:thread` — Only threaded messages\n- `has:pin` — Pinned messages\n- `has:link` — Messages containing links\n- `has:file` — Messages with file attachments\n- `has::emoji:` — Messages with a specific reaction\n\n### Date Filters\n- `before:YYYY-MM-DD` — Messages before a date\n- `after:YYYY-MM-DD` — Messages after a date\n- `on:YYYY-MM-DD` — Messages on a specific date\n- `during:month` — Messages during a specific month (e.g., `during:january`)\n\n### Text Matching\n- `\"exact phrase\"` — Match an exact phrase\n- `-word` — Exclude messages containing a word\n- `wild*` — Wildcard matching (minimum 3 characters before `*`)\n\n## File Search\n\nTo search for files, use the `content_types=\"files\"` parameter with type filters:\n- `type:images` — Image files\n- `type:documents` — Document files\n- `type:pdfs` — PDF files\n- `type:spreadsheets` — Spreadsheet files\n- `type:canvases` — Slack Canvases\n\nExample: `content_types=\"files\" type:pdfs budget after:2025-01-01`\n\n## Following Up on Results\n\nAfter finding relevant messages:\n- Use `slack_read_thread` to get the full thread context for any threaded message.\n- Use `slack_read_channel` with `oldest`/`latest` timestamps to read surrounding messages for context.\n- Use `slack_read_user_profile` to identify who a user is when their ID appears in results.\n\n## Common Pitfalls\n\n- **Boolean operators don't work.** `AND`, `OR`, `NOT` are not supported. Use spaces (implicit AND) and `-` for exclusion.\n- **Parentheses don't work.** Don't try to group search terms with `()`.\n- **Search is not real-time.** Very recent messages (last few seconds) may not appear in search results. Use `slack_read_channel` for the most recent messages.\n- **Private channel access.** Use `slack_search_public_and_private` when you need to include private channels, but note this requires user consent.\n", "depth": 5}, {"id": "224", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/references", "name": "references", "type": "dir", "content": "", "depth": 3}, {"id": "225", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/commands", "name": "commands", "type": "dir", "content": "", "depth": 3}, {"id": "226", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills", "name": "skills", "type": "dir", "content": "", "depth": 3}, {"id": "227", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/README.md", "name": "README.md", "type": "file", "content": "# Common Room Plugin\n\nGTM workflows powered by Common Room — account research, contact research, call prep, personalized outreach, prospecting, and weekly briefings.\n\n## Overview\n\nThis plugin connects Claude to Common Room's MCP server and equips it with six skills covering the most common rep workflows. Every output is grounded in real Common Room signal data — 1st-party product signals, 2nd-party community signals, 3rd-party intent signals, and enrichment from RoomieAI and Spark.\n\n## Requirements\n\n- **Common Room MCP** (`mcp.commonroom.io/mcp`) must be connected and authenticated. This is the primary data source for all plugin functionality.\n- **Calendar connector** (optional) — enables automatic meeting lookup in `call-prep` and `weekly-prep-brief`. If not connected, both skills ask the user for meeting details instead.\n\n## Skills\n\nSkills are triggered conversationally. Describe what you want and Claude will load the right skill automatically.\n\n| Skill | Trigger phrases |\n|-------|----------------|\n| `account-research` | \"Research [company]\", \"tell me about [domain]\", \"what's going on with [account]\", \"is [company] showing buying signals\" |\n| `contact-research` | \"Who is [name]\", \"look up [email]\", \"research [contact]\", \"is [name] a warm lead\" |\n| `call-prep` | \"Prep me for my call with [company]\", \"prepare for a meeting with [company]\", \"what should I know before talking to [company]\" |\n| `compose-outreach` | \"Draft outreach to [person]\", \"write an email to [name]\", \"compose a message for [contact]\" |\n| `prospect` | \"Find companies that match [criteria]\", \"build a prospect list\", \"find contacts at [type of company]\" |\n| `weekly-prep-brief` | \"Weekly prep brief\", \"prepare my week\", \"what calls do I have this week\" |\n\n## Commands\n\nTwo commands for complex workflows that benefit from explicit invocation:\n\n| Command | Usage |\n|---------|-------|\n| `/generate-account-plan \u003ccompany\u003e` | Comprehensive strategic account plan with stakeholder mapping, engagement analysis, opportunities, risks, and action items |\n| `/weekly-brief [date range]` | Generate a full weekly prep briefing (defaults to next 7 days) |\n\n## What Each Skill Produces\n\n**Account Research** — Handles four patterns: full overviews, targeted field questions, honest sparse-data responses, and combined MCP data + LLM reasoning. Includes web search for recent news. Automatically scopes to \"My Segments.\"\n\n**Contact Research** — Lookup by email, name+company, or social handle. Returns enriched identity, CRM fields, scores, website visits, activity history, Spark analyses, and conversation starters.\n\n**Call Prep** — Company snapshot, per-attendee profiles, signal highlights, tailored talking points, likely objections, and recommended call outcome. Prioritizes Gong/call recording activity. Calendar-aware if connected.\n\n**Compose Outreach** — Three personalized formats (email, call script, LinkedIn message) grounded in Common Room signals and web search hooks. Tailored to the user's company positioning when available.\n\n**Prospecting** — Distinguishes between net-new companies (ProspectorOrganization) and existing accounts (Organization). Supports iterative refinement and lookalike search (\"find companies like [X]\"). Web search enriches net-new results.\n\n**Weekly Prep Brief** — Full briefing covering every external call in the next 7 days: company snapshot, attendee profiles, signals, and recommended objectives per meeting.\n\n## Setup\n\n1. Ensure the Common Room MCP server is connected and authenticated in your Cowork settings.\n2. (Optional) Connect a calendar MCP server for automatic meeting lookup in call prep and weekly briefings.\n3. Install this plugin. All skills and commands are available immediately.\n\n## User Context\n\nAll skills that scope to a user's territory automatically fetch the `Me` object from Common Room. This provides the user's profile, role, and \"My Segments\" — ensuring queries default to their territory. See `references/me-context.md` for details.\n\nWhen company context is available, skills tailor recommendations to the user's product and ICP. See `references/my-company-context.md` for details.\n\n## Customization\n\nSee `CONNECTORS.md` for details on the calendar connector and how tool references work.\n", "depth": 3}, {"id": "228", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. This plugin is built around Common Room as the primary data source — but some workflows benefit from an optional calendar integration.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Calendar | `~~calendar` | Google Calendar (via MCP) | Outlook / Microsoft 365 Calendar |\n\n## Common Room MCP\n\nThe Common Room MCP server (`mcp.commonroom.io/mcp`) is the primary data source for all skills and commands in this plugin. It is listed in `.mcp.json` and must be connected and authenticated for the plugin to function.\n\n## Calendar (Optional)\n\nThe `~~calendar` connector is used in two skills:\n- **call-prep** — to automatically pull attendee names from upcoming meetings\n- **weekly-prep-brief** — to fetch all external meetings scheduled in the next 7 days\n\nIf no calendar is connected, both skills gracefully fall back to asking the user for meeting details manually. The calendar connector is entirely optional.\n\nTo connect a calendar, install a compatible calendar MCP server and ensure it is authenticated. The plugin will automatically detect and use it when available.\n", "depth": 3}, {"id": "229", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/references/my-company-context.md", "name": "my-company-context.md", "type": "file", "content": "# My Company Context\n\n## Purpose\n\nSome skills produce better output when they understand the user's company — what it sells, its ICP, and competitive positioning. This context lets the LLM tailor account research, call prep, and outreach to the user's specific product and value prop.\n\n## How to Gather Company Context\n\n1. **Me object** — Fetch the `Me` object. It includes the user's company name and title.\n2. **CR lookup** — Look up the user's company in Common Room as an Organization to get firmographics, industry, and description.\n3. **Web search** — If CR data is sparse or looks stale, search the web for the user's company customer case studies, product pages, and marketing materials. This fills gaps with fresh positioning context that CR firmographics alone don't capture.\n4. **Session context** — Check if the user has already described their product, ICP, or value prop in the current conversation.\n5. **Ask once** — If company context isn't available from the above, ask the user once per session: \"To tailor my recommendations, can you briefly describe what your company does and who your ideal customers are?\"\n\n## How to Use It\n\n- **Account research (reasoning step):** Position findings relative to the user's ICP — \"This account fits your ICP because...\"\n- **Call prep (talking points):** Frame talking points around the user's product value, not generic advice\n- **Compose outreach (value bridge):** Ground the pitch in the user's specific product and positioning\n- **Prospecting:** Use ICP criteria as default filter suggestions when the user doesn't specify criteria\n", "depth": 4}, {"id": "230", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/references/me-context.md", "name": "me-context.md", "type": "file", "content": "# The `Me` Object — User Context Guide\n\n## What `Me` Is\n\nThe `Me` object in Common Room represents the currently authenticated user. Fetching it at the start of a session gives the LLM context to personalize outputs and scope queries correctly.\n\nAlways fetch `Me` before running account research, prospecting, or any workflow that should be scoped to the user's territory.\n\n## What `Me` Returns\n\n### User Profile\n- Login identifier (email or username)\n- Full name and display name\n- Job title and role\n- Persona in CR (e.g., AE, SDR, CSM, Manager)\n- All linked profiles (e.g., Salesforce user ID, LinkedIn handle)\n\n### User Segments (\"My Segments\")\n- A list of all segments that belong to this user (name and segment ID each)\n- Corresponds to the **\"My Segments\" tab** in the Common Room product\n\n## How to Use `Me` Context\n\n### 1. Scope queries and respect territory boundaries\nWhen running account research, prospecting, or generating briefings, filter results to the user's own segments unless they ask for a broader view.\n\n\u003e Default: \"Show me accounts showing buying signals\" -\u003e scope to My Segments\n\u003e Override: \"Show me all accounts in the workspace showing buying signals\" -\u003e remove segment scope\n\nIf a user asks about an account not in their segments, note: \"This account doesn't appear to be in your segments — would you still like me to research it?\"\n\n### 2. Personalize outreach and briefings\nUse the user's name, title, and role to personalize outputs — e.g., reference their territory in a weekly brief, use their name in drafted emails.\n\n### 3. Infer context for reasoning\nThe user's Persona in CR influences how outputs should be framed:\n- **AE / AM / Account Executive / Account Manager** — focus on pipeline, deals, expansion, close timelines\n- **SDR / BDR / Sales Development Representative / Business Development Representative** — focus on prospecting, warm signals, first-touch outreach\n- **CSM / Customer Success Manager** — focus on health, retention, expansion, champion engagement\n- **Manager / Director / VP** — focus on team-level trends, not individual outreach\n", "depth": 4}, {"id": "231", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/commands/weekly-brief.md", "name": "weekly-brief.md", "type": "file", "content": "---\ndescription: Generate a weekly prep briefing from your calendar and Common Room\nargument-hint: [date range, defaults to next 7 days]\n---\n\nGenerate a weekly prep briefing using Common Room and your calendar.\n\nFollow the weekly-prep-brief skill:\n1. Use the ~~calendar connector to retrieve all external customer-facing meetings scheduled for the next 7 days (or the date range specified in \"$ARGUMENTS\"). Filter out internal meetings — focus on calls with customers, prospects, or partners.\n2. If no ~~calendar connector is available, ask the user to list their external calls (company name, date, attendees).\n3. For each external meeting, run account research and contact research on attendees in parallel.\n4. Compile into a single weekly briefing: week overview + per-meeting sections sorted by date.\n\nKeep each per-meeting section tight and scannable. Total briefing should be readable in under 10 minutes.\n", "depth": 4}, {"id": "232", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/commands/generate-account-plan.md", "name": "generate-account-plan.md", "type": "file", "content": "---\ndescription: Generate a comprehensive strategic account plan\nargument-hint: \u003ccompany name or domain\u003e\n---\n\nGenerate a comprehensive account plan for \"$ARGUMENTS\".\n\n## Process\n\n1. **Full account research** — Follow the account-research skill to produce a complete account overview. Pull all available first-party (product), second-party (community), and third-party (intent) signals, CRM data, scores, and RoomieAI research.\n\n2. **Stakeholder mapping** — Fetch the top 5 contacts at this company sorted by member score descending. Classify each into: Champion, Economic Buyer, Influencer, End User, or Unknown. Use Spark persona data if available; if Spark is unavailable, infer from engagement patterns and activity recency. If no activity data exists either, classify as Unknown.\n\n3. **Engagement analysis** — Pull all organization activity for the last 90 days (up to 50 activities). Identify trends: is engagement growing, stable, or declining? Which contacts are most active? What channels are they engaging through?\n\n4. **Web search (supplementary)** — If CR data is rich, skip. If data is thin or the user requests it, search for recent company news (last 30 days): funding, acquisitions, product launches, leadership changes, competitive moves.\n\n5. **Synthesis** — Combine all data into a structured account plan. When the user's company context is available (see `references/my-company-context.md`), tailor the executive summary, opportunities, and action items to the user's product and ICP.\n\n## Output Format\n\n```\n## Account Plan: [Company Name]\n\n### Executive Summary\n[3-4 sentences: relationship status, key opportunity, primary risk, recommended priority]\n\n### Account Overview\n| Field | Value |\n|-------|-------|\n| Industry | ... |\n| Size | ... |\n| Domain | ... |\n| CRM Owner | ... |\n| Opp Stage | ... |\n| ARR | ... |\n| Scores | ... |\n\n### Stakeholder Map\n\n**Champions**\n- [Name] — [Title] — [Key signals, last activity date]\n\n**Economic Buyers**\n- [Name] — [Title] — [Key signals]\n\n**Influencers**\n- [Name] — [Title] — [Key signals]\n\n**End Users**\n- [Name] — [Title] — [Key signals]\n\n### Engagement Analysis\n[Trend summary: growing/stable/declining, most active contacts, top channels, comparison to 90 days prior if data available]\n\n### Recent News [If web search was run]\n[Web search findings with sources and dates]\n\n### Opportunities\n1. [Signal-backed opportunity with specific next step]\n2. ...\n\n### Risks\n1. [Signal-backed risk with mitigation]\n2. ...\n\n### Prioritized Action Items\n1. [Specific action] — [Owner suggestion] — [Timeline]\n2. ...\n3. ...\n```\n\nGround every insight in actual data. Flag explicitly when data is thin or unavailable.\n\n**If Common Room returns sparse data for this account**, produce an abbreviated plan that covers only the sections with real data. Do not generate a full account plan from minimal input — a short honest plan with gaps clearly noted is far more useful than a comprehensive-looking plan built on fabricated details. Omit sections (Stakeholder Map, Engagement Analysis, Opportunities, Risks) entirely if there is no data to support them.\n", "depth": 4}, {"id": "233", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/contact-research", "name": "contact-research", "type": "dir", "content": "", "depth": 4}, {"id": "234", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/prospect", "name": "prospect", "type": "dir", "content": "", "depth": 4}, {"id": "235", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/compose-outreach", "name": "compose-outreach", "type": "dir", "content": "", "depth": 4}, {"id": "236", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/account-research", "name": "account-research", "type": "dir", "content": "", "depth": 4}, {"id": "237", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/weekly-prep-brief", "name": "weekly-prep-brief", "type": "dir", "content": "", "depth": 4}, {"id": "238", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/call-prep", "name": "call-prep", "type": "dir", "content": "", "depth": 4}, {"id": "239", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/contact-research/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "240", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/contact-research/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: contact-research\ndescription: \"Research a specific person using Common Room data. Triggers on 'who is [name]', 'look up [email]', 'research [contact]', 'is [name] a warm lead', or any contact-level question.\"\n---\n\n# Contact Research\n\nRetrieve a comprehensive contact profile from Common Room. Supports lookup by email, social handle, or name + company. Returns enriched data including activity history, Spark, scores, website visits, and CRM fields.\n\n## Step 1: Locate the Contact\n\nCommon Room supports multiple lookup methods — use whichever the user has provided:\n\n| What the user gives | Lookup method |\n|---------------------|--------------|\n| Email address | Look up by email (most reliable) |\n| LinkedIn, Twitter/X, or GitHub handle | Look up by social handle — specify handle type explicitly |\n| Name + company | Identity resolution by name + org domain; present matches if ambiguous |\n| Name only | Search by name; if multiple matches, show a brief list and ask the user to confirm |\n\nIf no match is found, respond: \"Common Room doesn't have a record for this person.\" Do not speculate or fabricate profile data.\n\n## Step 2: Fetch Contact Fields\n\nUse the Common Room object catalog to see available field groups and their contents. For full profiles, request all groups. For targeted questions, request only what's relevant.\n\n**Key field groups to know about:**\n- **Scores** — always return as raw values or percentiles, never labels\n- **Recent activity** — use `Contact Initiated` filter (last 60 days) for their actions, not your team's\n- **Website visits** — total count + specific pages (last 12 weeks)\n- **Spark** — retrieve all Sparks when tracking engagement evolution over time\n\n## Step 3: Run Spark Enrichment (If Available)\n\nIf Spark is available, use it. Spark provides:\n- Professional background and job history\n- Social presence and influence signals\n- Persona classification: Champion, Economic Buyer, Technical Evaluator, End User, or Gatekeeper\n- Inferred role in the buying process\n\nIf Spark is unavailable but real activity data exists (recent actions, website visits, community engagement), infer a persona from those signals. If neither Spark nor activity data is available, classify as Unknown — do not guess a persona from title alone.\n\nRetrieve **all Sparks** (not just the most recent) when the user wants to understand how this contact's engagement has evolved over time.\n\n## Step 4: Assess Account Context\n\nPull an abbreviated account snapshot for this contact's parent company. Note:\n- Open opportunities, expansion signals, or churn risk at the account level\n- Whether other contacts at this company are also active\n- How this person's engagement compares to their colleagues\n\n## Step 5: Identify Conversation Angles\n\nBased on activity and signals, surface the strongest 2–3 hooks:\n- A recent `Contact Initiated` activity (community post, product event, support ticket)\n- A specific web page they visited recently — especially if it signals evaluation intent\n- A job change, promotion, or company news\n- Their Spark persona and what that suggests about communication style\n- Their role in a known active deal\n\n## Output Format\n\nOnly include sections where data was actually returned. Omit sections with no data rather than filling them with guesses.\n\n**When data is rich:**\n\n```\n## [Contact Name] — Profile\n\n**Overview**\n[2 sentences: who they are, their role, and relationship status]\n\n**Details**\n- Title: [title]\n- Company: [company]\n- Email: [email]\n- LinkedIn: [URL]\n- Other profiles: [Twitter/X, GitHub, CRM link if available]\n\n**Scores** [If scores returned]\n[All scores as raw values or percentiles]\n\n**Recent Activity** (last 60 days) [If activity returned]\n[3–5 bullets with dates]\n\n**Website Visits** (last 12 weeks) [If visit data exists]\n[Total visit count + list of pages visited]\n\n**Spark Profile** [If Spark data is non-null]\n[Persona type, background summary, influence signals]\n\n**Segments** [If segments returned]\n[List of segment names this contact belongs to]\n\n**Account Context**\n[1–2 sentences on their company's status]\n\n**Conversation Starters**\n[2–3 specific, signal-backed openers]\n```\n\n**When data is sparse (e.g., only name, title, email, tags returned; sparkSummary is null):**\n\n```\n## [Contact Name] — Profile (Limited Data)\n\n**Data available:** [List exactly what Common Room returned]\n\n[Present only the returned fields]\n\n**Web Search**\n[Any findings from searching their name + company]\n\n**Note:** Common Room has limited data on this contact. No activity history, scores, or Spark profile available. I can run deeper web searches or look up their company for additional context.\n```\n\nDo not generate conversation starters, persona inferences, or engagement assessments from sparse data. These require real signals.\n\n## Quality Standards\n\n- Lookup must use the correct method for the input type — don't guess on email vs. handle\n- Scores as raw/percentile only — never labels\n- `Contact Initiated` activity (last 60 days) is the primary engagement signal — lead with it\n- If Spark is unavailable, say so — don't fabricate a persona from title alone\n- Flag any contact where the most recent activity is older than 30 days\n\n## Reference Files\n\n- **`references/contact-signals-guide.md`** — full field descriptions, Spark persona guide, and conversation starter principles\n", "depth": 5}, {"id": "241", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/contact-research/references/contact-signals-guide.md", "name": "contact-signals-guide.md", "type": "file", "content": "# Contact Signals — Interpretation Guide\n\n## Contact Signal Types\n\n### Activity Signals\n\n| Signal | Interpretation |\n|--------|---------------|\n| Email open / reply | Actively aware of your org; replied = higher intent |\n| Meeting attended | Relationship established; note meeting type (demo, QBR, etc.) |\n| Community post or reply | Has a question, problem, or idea — high engagement signal |\n| Support ticket opened | Experiencing friction — empathy opportunity |\n| Documentation page visited | Researching a specific area — note which pages |\n| Product login | Active user — check frequency and recency |\n| Event registered/attended | Strong engagement and intent signal |\n\n### Relationship Signals\n\n| Signal | Interpretation |\n|--------|---------------|\n| No activity in 30+ days | Relationship cooling — gentle check-in appropriate |\n| No activity in 90+ days | Dormant — may need re-engagement from a different angle |\n| Consistent activity across multiple channels | Champion or highly engaged user — high-value contact |\n| Activity spike after dormancy | Something changed — investigate why |\n\n### Job / Professional Signals\n\n| Signal | Interpretation |\n|--------|---------------|\n| Recent job change (new company) | Warm intro opportunity at new company; relationship may shift at old one |\n| Promotion at same company | Growing influence — relationship becomes more valuable |\n| Title change to decision-maker role | Upgrade the relationship strategy |\n| Company expansion (new hires in their team) | Budget may be available; expansion opportunity |\n\n## Spark Persona Classification\n\n- *Champion* — daily user, advocates internally\n- *Economic Buyer* — holds budget authority, may not use product directly\n- *Technical Evaluator* — evaluates fit and integration, influences technical decisions\n- *End User* — primary product user, influences renewal through NPS/feedback\n- *Gatekeeper* — controls access, must be navigated carefully\n\n## Conversation Starters\n\nA strong starter references something specific, shows homework, opens a door without forcing one, and connects to value.\n\n- \"I saw your question in the community about [topic] — we just released a feature that addresses exactly that. Would it be useful to walk through it?\"\n- \"Your team's usage of [feature] jumped last month — curious if you're running into [common blocker] at that stage.\"\n", "depth": 6}, {"id": "242", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/prospect/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "243", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/prospect/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: prospect\ndescription: \"Build targeted account or contact lists using Common Room's Prospector. Triggers on 'find companies that match [criteria]', 'build a prospect list', 'find contacts at [type of company]', 'show me companies hiring [role]', or any list-building request.\"\n---\n\n# Prospecting\n\nBuild targeted account and contact lists using Common Room's Prospector. Supports iterative refinement through natural conversation, intent-based discovery, and both net-new prospecting and signal-based queries against existing accounts.\n\n## Critical Distinction: Two Object Types\n\nCommon Room's Prospector operates against two fundamentally different object types. Always clarify which one is in play before running a query:\n\n**`ProspectorOrganization`** — Companies **not yet in Common Room**\n- Net-new companies that match specified criteria\n- Available fields are firmographic only: name, domain, size, industry, capital raised, annual revenue, location\n- Fewer filter options — no signal-based filters, no scores, no activity history\n- Use when: building a brand-new target list, territory planning, top-of-funnel expansion\n\n**`Organization`** (in Common Room) — Companies **already in your CR workspace**\n- Full signal data available: product usage, community activity, CRM fields, scores, custom fields\n- Much richer filter set — includes signal-based, score-based, segment-based, and firmographic filters\n- Use when: finding warm accounts to prioritize, identifying expansion candidates, surfacing intent signals within existing pipeline\n\nWhen a user's request could apply to both (e.g., \"Show companies hiring AI engineers this month\"), clarify:\n\u003e \"Are you looking for net-new companies not yet in Common Room, or filtering accounts already in your workspace?\"\n\nThe catalog should make this distinction explicit so the LLM can select the right Prospector endpoint.\n\n## Step 0: Load User Context (Me)\n\nFetch the `Me` object to get the user's segments. When prospecting against `Organization` records (accounts already in CR), default to filtering within \"My Segments\" unless the user asks for a broader search.\n\n## Step 1: Gather Targeting Criteria\n\nIf criteria are already provided, proceed. Otherwise ask:\n\n\u003e \"What kind of accounts or contacts are you looking for? For example: company size, industry, job titles, signals like recent product activity or community engagement, geographic region, or specific intent signals like recent funding or job postings.\"\n\nUse the Common Room object catalog to see available filters for each object type. The key distinction:\n- **ProspectorOrganization** — firmographic and technographic filters only (industry, size, geography, funding, tech stack)\n- **Organization** — all firmographic filters plus signal-based, score-based, segment-based, and CRM filters\n\n**Lookalike search:** If the user asks to \"find companies like [X]\", first look up the reference company in Common Room (or via web search if not in CR). Extract its key attributes — industry, employee range, tech stack, funding stage, geography — and propose those as filter criteria. Present the derived criteria to the user for confirmation before running the search, since lookalike targeting works best when the user can refine which attributes matter most.\n\n## Step 2: Support Iterative Refinement\n\nProspecting is conversational. Support multi-turn refinement naturally:\n\n1. Run initial query with provided criteria\n2. If results are large (50+), summarize and offer: \"I found [N] results. Want to narrow by [suggested filter]?\"\n3. If results are too few (\u003c 5), suggest: \"Only [N] results with those filters — I can broaden by relaxing [specific criterion].\"\n4. Apply each refinement as a follow-up query, not a new search from scratch\n\nExample flow:\n- Rep: \"Find cybersecurity companies in California.\" → 500 results\n- Rep: \"Only show ones over 300 employees using AWS.\" → 47 results\n- Rep: \"Focus on the ones with recent hiring activity.\" → 12 results ✓\n\n## Step 3: Run the Query and Present Results\n\nExecute the Prospector query with confirmed criteria. Sort by signal strength or fit score where available (not alphabetically).\n\n**For `ProspectorOrganization` (net-new) results:**\n\n| Company | Domain | Industry | Size | Capital Raised | Revenue | Location |\n|---------|--------|----------|------|---------------|---------|----------|\n\n**For `Organization` (in CR) results:**\n\n| Company | Industry | Size | Top Signal | Signal Date | Score | CRM Stage |\n|---------|----------|------|-----------|-------------|-------|-----------|\n\nFlag any results where data is thin or the most recent signal is older than 90 days.\n\n## Step 3.5: Enrich Net-New Results with Web Search\n\nFor `ProspectorOrganization` results (net-new companies not in CR), run a quick web search on the top 3–5 companies to add context beyond firmographics. CR has no behavioral signals for these companies, so web search fills the gap — look for recent funding, product launches, leadership changes, or news coverage. Include findings as brief annotations next to each company in the results.\n\n## Step 4: Offer Next Steps\n\n- \"Want me to draft outreach for the top 3–5 prospects?\"\n- \"Should I run a full account brief on any of these?\"\n- \"Want to refine the criteria or add another filter?\"\n- \"I can format this as a CSV if you'd like to export it.\"\n- \"For any net-new companies here, I can add them to Common Room for enrichment.\" *(future capability)*\n\n## Quality Standards\n\n- Always confirm which object type (ProspectorOrg vs Organization) before running the query\n- Default to \"My Segments\" when querying Organization records, unless user specifies otherwise\n- Support iterative refinement — treat each follow-up as a filter adjustment, not a fresh start\n- Never mix result fields from ProspectorOrganization and Organization in the same list\n- Fewer high-quality results beat a long unqualified list\n- **Only show data the query returned** — leave blank or \"—\" for missing fields, don't invent values\n\n## Reference Files\n\n- **`references/prospect-guide.md`** — filter types, signal-based sorting, object type distinctions, and list-building strategies\n", "depth": 5}, {"id": "244", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/prospect/references/prospect-guide.md", "name": "prospect-guide.md", "type": "file", "content": "# Common Room Prospector — Usage Guide\n\n## The Two Object Types (Critical)\n\nNever conflate these — they have different fields and filter sets.\n\n| | `ProspectorOrganization` (Net-New) | `Organization` (Already in CR) |\n|---|---|---|\n| **Source** | Common Room's external data graph | CR workspace (full enrichment + signal history) |\n| **Available fields** | Company name, domain, size, industry, capital raised, revenue, location | Everything — signals, scores, CRM data, RoomieAI, community activity |\n| **Available filters** | Firmographic and technographic only | All firmographic plus signal-based, score-based, segment-based, and CRM filters |\n| **Use for** | Territory planning, top-of-funnel list building | Prioritizing warm accounts, expansion candidates, intent signals |\n\n## Deciding Which Object Type to Use\n\n| User says... | Likely object type |\n|-------------|-------------------|\n| \"Which of my accounts are showing buying signals?\" | `Organization` |\n| \"Find fintech companies in London I haven't talked to\" | `ProspectorOrganization` |\n| \"Find new companies matching our ICP\" | `ProspectorOrganization` |\n| \"Show accounts that haven't engaged in 90 days\" | `Organization` |\n| Ambiguous — could apply to both | Ask: \"Are you looking for net-new companies, or filtering accounts already in your workspace?\" |\n\n## Iterative Refinement\n\n- Start with the user's initial criteria\n- For large results (50+), return count first: \"I found 500 results. Want to narrow by size or tech stack?\"\n- Accept follow-up refinements as filter adjustments — not a fresh search\n- Suggest relaxing criteria if results are fewer than 5\n\n## Common Pitfalls\n\n- **Conflating object types** — Never mix ProspectorOrganization and Organization results in the same list.\n- **Not scoping to \"My Segments\"** — When querying Organization records, scope to the user's segments by default.\n- **Over-filtering** — If under 5 results, suggest relaxing one criterion.\n", "depth": 6}, {"id": "245", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/compose-outreach/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "246", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/compose-outreach/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: compose-outreach\ndescription: \"Generate personalized outreach messages using Common Room signals. Triggers on 'draft outreach to [person]', 'write an email to [name]', 'compose a message for [contact]', or any outreach drafting request.\"\n---\n\n# Compose Outreach\n\nGenerate three personalized outreach formats — email, call script, and LinkedIn message — grounded in Common Room signals for a specific company or contact.\n\n## Outreach Process\n\n### Step 1: Look Up the Target\n\nUse Common Room MCP tools to find and retrieve data for the target (company and/or specific contact). Pull:\n- Recent product activity and engagement signals\n- Community activity (posts, questions, reactions)\n- 3rd-party intent signals (job postings, news, funding)\n- Relationship history (prior contact, meetings, email opens)\n\nIf the user specified a person, run contact-level research. If only a company was given, identify the best contact to target based on title, engagement, and role.\n\n### Step 2: Web Search for External Hooks (If CR Signals Are Thin)\n\nIf CR returned strong signals (recent activity, engagement, product usage), those should drive personalization — skip web search. If CR signals are thin or the prospect has little CR activity, run a web search for external hooks:\n\n**What to search:**\n- `\"[company name]\" funding OR acquisition OR launch OR announcement` — last 30 days\n- `\"[contact full name]\" \"[company name]\"` — look for recent articles, interviews, LinkedIn posts, or conference talks\n\n**Prioritize external hooks that are:**\n- Very recent (\u003c 2 weeks) — the prospect is likely still thinking about it\n- Publicly visible — they know you could have seen it\n- Change-signaling — growth, new role, new product, new market\n\nIf the user explicitly asks for web search or external hooks, run it regardless of CR signal richness.\n\n### Step 3: Spark Enrichment (If Available)\n\nIf Spark is available, run enrichment on the target contact to get persona classification, background, and influence signals. Use this to calibrate tone and message angle.\n\n### Step 4: Identify the Best Hooks\n\nFrom the signal data, identify the 1–3 strongest personalization hooks. Rank by:\n1. **Recency** — happened in the last 7–14 days\n2. **Specificity** — a concrete action they took, not a general trend\n3. **Relevance** — connects directly to a value your product delivers\n\nGood hooks: posted a question in the community about X, just hired 5 engineers, recently started using [feature], company just raised Series B, trial nearing expiration, champion just changed jobs.\n\nBad hooks: \"I noticed you're a customer\" or generic industry trends.\n\n### Step 5: Generate All Three Formats\n\nUse the strongest hooks to write all three formats. Each format has different constraints and conventions — follow the format-specific guidelines in `references/outreach-formats-guide.md`.\n\nAlways produce all three, clearly labeled.\n\nWhen the user's company context is available (see `references/my-company-context.md`), ground the value bridge and pitch in the user's specific product and positioning.\n\n### Step 6: Annotate Your Choices\n\nAfter the three drafts, include a brief note (2–4 sentences) explaining:\n- Which signals were used and why they were chosen\n- Any assumptions made (e.g., inferred call objective)\n- Alternative angles if the primary hook doesn't land\n\n## Output Format\n\n```\n## Outreach for [Name / Company]\n\n### 📧 Email\n\n**Subject:** [Subject line]\n\n[Email body — 3–5 sentences]\n\n---\n\n### 📞 Call Script\n\n**Opening:**\n[Opening line — conversational, 1–2 sentences]\n\n**Value Bridge:**\n[Why you're calling and why now — 2–3 sentences tied to a signal]\n\n**Ask:**\n[Single, low-friction ask — e.g., 15-minute call, specific question]\n\n---\n\n### 💼 LinkedIn Message\n\n[Under 300 characters. Warm, personal, no pitch.]\n\n---\n\n### Signal Notes\n[2–4 sentences: which signals were used, why, and any alternative angles]\n```\n\n## When Signal Data Is Sparse\n\nIf Common Room returns minimal data on the target (e.g., just name, title, tags — no activity, no scores, no Spark):\n\n1. **Do not draft outreach from thin air.** Outreach grounded in fabricated signals is worse than no outreach.\n2. **Run web search first** — this becomes your primary personalization source. Look for recent news, LinkedIn posts, conference talks, company announcements.\n3. **If web search also returns little**, present what you have honestly and ask the user for context:\n\n```\n## Outreach for [Name / Company] — Limited Data\n\n**What I found:**\n[Only the real data from CR and web search]\n\n**I don't have enough signal to draft personalized outreach yet.** To write something strong, I'd need:\n- Recent activity or engagement signals\n- Context you have from prior conversations\n- A specific reason for reaching out now\n\nCan you share any of the above?\n```\n\n## Quality Standards\n\n- Every message must reference something specific — generic outreach is not acceptable output\n- Match tone to context: warm and conversational for inbound/community signals; more formal for cold/executive outreach\n- The LinkedIn message must be under 300 characters — no exceptions\n- The call script must be speakable naturally — read it aloud mentally to check rhythm\n- **Never fabricate signals** — only reference data retrieved from Common Room or web search\n\n## Reference Files\n\n- **`references/outreach-formats-guide.md`** — detailed format rules, examples, and tone guidelines for each channel\n", "depth": 5}, {"id": "247", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/compose-outreach/references/outreach-formats-guide.md", "name": "outreach-formats-guide.md", "type": "file", "content": "# Outreach Formats — Guidelines and Examples\n\n## Email\n\n### Structure\n- **Subject line:** 4-8 words, specific and curiosity-inducing. Reference the hook directly.\n- **Opening:** Acknowledge the specific signal. Start with \"You\", \"Your team\", \"Saw that\", \"Noticed\" — not \"I\".\n- **Value Bridge:** 1-2 sentences connecting their signal to something you can help with. Be concrete.\n- **Ask:** Single, specific, low-friction next step.\n- **Total length:** 3-5 sentences max. No paragraphs.\n\n### Example\n\n\u003e Subject: Question about your [feature] usage\n\u003e\n\u003e Noticed your team has been deep in [feature] recently — you've hit [usage metric] in the last two weeks. That usually means teams are running into [common blocker] around this stage. Happy to walk through how [similar company] handled it if useful. 15 minutes this week?\n\n---\n\n## Call Script\n\nThree parts — a guide, not a word-for-word read:\n\n**Opening (10-15 seconds):** State your name and company, then reference the specific hook immediately.\n\n**Value Bridge (20-30 seconds):** Explain why this is worth their time *right now*, tied to a concrete outcome.\n\n**Ask (10 seconds):** Single, specific, low-friction — e.g., \"Do you have 15 minutes to explore if that's relevant?\"\n\n---\n\n## LinkedIn Message\n\n### Constraints\n- **300 characters maximum** (platform limit is 400 — we target 300 to prevent overruns)\n- No attachments, no links in the first message\n- No pitch — just open a door\n\n### Structure\n- Personal acknowledgment (what you noticed about them)\n- One brief sentence on why you reached out\n- Soft invitation to connect or respond\n\n### Example\n\n\u003e Saw your post about [topic] in [community] — our team has been thinking about the same problem. Would love to share what we've learned. Open to a quick chat?\n\n---\n\n## Tone Calibration by Context\n\n| Context | Tone | Approach |\n|---------|------|----------|\n| Inbound / trial / active community member | Warm, helpful, collaborative | Lead with their activity, offer help |\n| Cold outreach (no prior relationship) | Professional, respectful, brief | Lead with signal, be direct about why now |\n| Re-engagement (went dark) | Casual, low-pressure | Short, acknowledge the gap, easy ask |\n| Executive outreach | Formal, concise, outcome-focused | Lead with business impact, not product |\n| Champion job change | Warm, congratulatory | Celebrate them, then gently explore new opportunity |\n\n---\n\n## What to Avoid (All Channels)\n\n- Starting with filler (\"I hope this email finds you well\") or defensive language (\"I know you're busy but...\")\n- Multi-paragraph messages with multiple asks\n- Vague subject lines like \"Following up\" or \"Quick question\"\n- Fabricated familiarity (\"I've been following your company for a while...\")\n", "depth": 6}, {"id": "248", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/account-research/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "249", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/account-research/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: account-research\ndescription: \"Research a company using Common Room data. Triggers on 'research [company]', 'tell me about [domain]', 'pull up signals for [account]', 'what's going on with [company]', or any account-level question.\"\n---\n\n# Account Research\n\nRetrieve and synthesize account information from Common Room. Handles four interaction patterns: full overviews, targeted field questions, sparse data situations, and combined MCP data + LLM reasoning.\n\n## Step 0: Load User Context (Me)\n\nBefore researching any account, fetch the `Me` object from Common Room. This provides:\n- The user's profile, title, role, and Persona in CR\n- The user's segments (\"My Segments\")\n\nDefault all queries to the user's own segments unless the user explicitly asks for a broader view. This keeps results scoped to their territory.\n\n## Step 1: Identify the Interaction Pattern\n\nDetermine what the user actually needs before deciding how much data to fetch:\n\n**Pattern 1 — Full Overview:** \"Tell me about Datadog\" / \"Summarize cloudflare.com\"\n→ Fetch the full field set and produce a structured briefing.\n\n**Pattern 2 — Targeted Question:** \"Who owns the Snowflake account?\" / \"Is acme.io showing buying signals?\" / \"What's the employee count for notion.so?\"\n→ Fetch only the relevant field(s). Return a direct, concise answer — do not produce a full brief for a simple question.\n\n**Pattern 3 — Sparse Data:** \"Tell me about tiny-startup.io\"\n→ If Common Room has limited data for an account, say so honestly: \"There is limited information available for this account.\" Never speculate or fill gaps with generic statements.\n\n**Pattern 4 — Combined Reasoning:** Fetch structured MCP data, then layer in LLM analysis — e.g., \"Stripe has 8,000 employees and is hiring heavily for AI roles. Based on your ICP of 1k–10k fintech companies, this is a strong fit.\"\n\n## Step 2: Look Up the Account\n\nSearch Common Room for the account by domain or company name. Exact match first; if no result, try partial match and confirm with the user before proceeding.\n\n## Step 3: Fetch the Right Fields\n\nUse the Common Room object catalog to see available field groups and their contents. For full overviews, request all field groups. For targeted questions, request only what's relevant.\n\n**Key field groups to know about:**\n- **Scores** — always return as raw values or percentiles, never labels\n- **Summary research** — RoomieAI output; often the richest qualitative signal\n- **Top contacts** — sorted by score desc; use communityMemberID for full lookups\n\n**Choosing what to fetch:**\n\n| User query type | Fields to request |\n|-----------------|------------------|\n| Full account overview | All field groups |\n| \"Who owns this account?\" | Company profiles \u0026 links, CRM fields |\n| \"Is this company a good fit?\" | Key fields, scores, about |\n| \"What signals is this account showing?\" | Scores, summary research, CRM fields |\n| \"Who are the top contacts?\" | Top contacts |\n| \"What does RoomieAI say about them?\" | Summary research, all research |\n| \"Find engineers at this account\" | Prospects (with title filter) |\n\n## Step 4: Web Search (Sparse Data Only)\n\nCommon Room is the primary data source. Do not run web search when CR returns rich data.\n\nWhen CR data is sparse (Pattern 3 — few fields returned, no activity, no scores), run a targeted web search to fill gaps:\n- `\"[company name]\" news` — scoped to the last 30 days\n- Look for: funding rounds, acquisitions, product launches, executive changes, press coverage\n\nIf the user explicitly asks for external context or recent news, run web search regardless of data richness.\n\n## Step 5: Apply Reasoning (Pattern 4)\n\nWhen the user's question invites synthesis — not just data retrieval — layer in analysis:\n- Compare account data to known ICP criteria from session context\n- Identify fit signals (size, industry, tech stack, hiring patterns)\n- Note timing signals (funding, trial status, recent activity spike)\n- Frame insights as clearly derived from data, not assumed\n\nWhen the user's company context is available (see `references/my-company-context.md`), position findings relative to the user's value proposition and ICP.\n\n## Step 6: Produce Output\n\nOnly include sections where Common Room returned actual data. Omit sections entirely rather than filling them with guesses.\n\n**Full overview (when data is rich):**\n\n```\n## [Company Name] — Account Overview\n\n**Snapshot**\n[2–3 sentences: what they do, plan/stage, relationship status]\n\n**Key Details**\n[Employee count, industry, location, domain, funding — from key fields]\n\n**CRM \u0026 Ownership** [If CRM fields returned]\n[Owner, opp stage, ARR]\n\n**Scores** [If scores returned]\n[All available scores as raw values or percentiles]\n\n**Signal Highlights** [If activity/signals exist]\n[3–5 most important signals with dates]\n\n**Top Contacts** [If contacts returned]\n[Name | Title | Score — top 5 sorted by score desc]\n\n**RoomieAI Research** [If summary research is non-null]\n[Summary research output; list all available research topic names]\n\n**Recommended Next Steps**\n[2–3 specific, signal-backed actions]\n```\n\n**Targeted question:** 1–3 sentence direct answer. No full brief needed.\n\n**Sparse data (few fields returned, most sections would be empty):**\n\n```\n## [Company Name] — Account Overview (Limited Data)\n\n**Data available:** [List exactly what Common Room returned]\n\n[Present only the returned fields]\n\n**Web Search**\n[Findings from web search — or \"No significant recent news found\"]\n\n**Note:** Common Room has limited data on this account. The account may need enrichment in Common Room.\n```\n\n## Quality Standards\n\n- Scores must always be raw values or percentiles — never categorical labels\n- For targeted questions, answer precisely and don't over-deliver\n- Be explicit when data is missing or stale — don't speculate\n- Keep full briefings readable in 2–3 minutes\n- **Every fact must trace to a tool call** — don't include data not returned by Common Room\n\n## Reference Files\n\n- **`references/signals-guide.md`** — signal type taxonomy and interpretation guide\n\n", "depth": 5}, {"id": "250", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/account-research/references/signals-guide.md", "name": "signals-guide.md", "type": "file", "content": "# Common Room Signal Types — Interpretation Guide\n\n## Signal Categories\n\n### Product Signals (1st-Party)\n\n| Signal | What it means |\n|--------|--------------|\n| Active seats increasing | Expansion within the account — outreach opportunity |\n| Feature adoption (new features used) | Product engagement deepening — good for expansion talk |\n| Login frequency spike | Increased engagement — follow up to understand why |\n| Login frequency drop | Risk of disengagement or churn — proactive outreach needed |\n| Trial started | Active evaluation — time-sensitive, prioritize |\n| Trial nearing expiry | Decision point approaching — immediate action |\n| Support tickets opened | Potential friction — check for patterns |\n\n### Community Signals (2nd-Party)\n\n| Signal | What it means |\n|--------|--------------|\n| New community post | Actively engaged, has a question or idea |\n| Reply / reaction to post | Community participation |\n| Forum thread started | Seeking help or sharing feedback — can engage directly |\n| Documentation viewed (specific page) | Researching a specific feature — high value if tied to expansion area |\n| Event registration/attendance | Strong engagement and intent signal |\n\n### Intent Signals (3rd-Party)\n\n| Signal | What it means |\n|--------|--------------|\n| Hiring for relevant roles | Growing team, budget exists |\n| Funding announced | New budget likely available — reach out congratulating |\n| Company news / press mention | Good hook for outreach |\n| Tech stack change | May need new integrations |\n| Competitive product removed from stack | Window of opportunity — very high priority |\n| Website activity spike | Active research phase |\n\n### Relationship Signals\n\n| Signal | What it means |\n|--------|--------------|\n| Recent meeting logged | Active relationship — context available |\n| No meeting in 90+ days | Relationship cooling — check in |\n| Open opportunity in CRM | Active deal — coordinate with AE |\n| Champion job change | Relationship risk or opportunity |\n\n## Signal Interpretation Principles\n\n1. **Recency matters most.** A signal from yesterday far outweighs one from 6 months ago. Always note timestamps.\n2. **Signal clusters \u003e single signals.** Multiple signals in the same direction are much stronger than any individual one.\n3. **Absence of signals is a signal.** Minimal usage + no community engagement at a paid account = risk indicator.\n4. **Contextualize against account stage.** Trial: conversion signals dominate. Mature customer: expansion and churn-risk. Prospect: intent signals are paramount.\n", "depth": 6}, {"id": "251", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/weekly-prep-brief/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "252", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/weekly-prep-brief/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: weekly-prep-brief\ndescription: \"Generate a comprehensive weekly briefing for all external calls in the next 7 days. Triggers on 'weekly prep brief', 'prepare my week', 'what calls do I have this week', 'Monday prep', or any weekly planning request.\"\n---\n\n# Weekly Prep Brief\n\nGenerate a single comprehensive weekly briefing that covers every external customer or prospect call in the next 7 days, with per-meeting account and contact research from Common Room.\n\n## Briefing Process\n\n### Step 1: Get the Week's External Meetings\n\n**Option A — Calendar connected:**\nUse the `~~calendar` connector to fetch all meetings scheduled in the next 7 days (or a user-specified range). Filter to keep only external meetings — those with attendees from outside your organization. Discard internal-only meetings, one-on-ones with colleagues, and recurring internal syncs.\n\nIdentify for each external meeting:\n- Company name\n- Meeting date and time\n- External attendee names and email addresses\n\n**Option B — No calendar connected:**\nAsk the user: \"To build your weekly prep brief, I'll need your upcoming external calls. Please list them: company name, date/time, and attendee names.\"\n\nAccept freeform input and parse it into a structured list before proceeding.\n\n### Step 2: Confirm the Meeting List\n\nPresent the identified meetings to the user for confirmation before beginning research:\n\n\u003e \"Here are the external calls I found for this week. Let me know if anything's missing or should be excluded:\n\u003e - [Company] — [Day], [Time] — [Attendees]\n\u003e - ...\"\n\nThis prevents wasted research on cancelled or incorrect meetings.\n\n### Step 3: Research Each Meeting\n\nFor each confirmed external meeting, run in parallel where possible:\n1. **Account research** — full account snapshot using the account-research skill\n2. **Contact research** — profile for each external attendee using the contact-research skill\n\nCommon Room data is the primary source. After CR research, run a quick **recency check** for each company — this is supplementary, not primary:\n- Search `\"[company name]\" news` scoped to the last 7 days\n- For executive attendees, search their name for recent public posts or interviews\n- Only include findings that are genuinely noteworthy (funding, leadership changes, major press). Don't pad the brief with generic news.\n\nDepth calibration:\n- For high-priority accounts (large accounts, open opportunities, renewal risk), produce full depth research\n- For lower-priority or short meetings, produce abbreviated snapshots (3–4 bullets each)\n\n### Step 4: Synthesize the Weekly Brief\n\nCompile all per-meeting research into a single structured document, sorted by meeting date/time.\n\nOpen with a brief week-level overview that flags:\n- Any accounts with urgent signals (at-risk, trial expiring, expansion opportunity)\n- Any meetings that need special preparation or executive involvement\n- Total external call count and estimated time commitment\n\n## Output Format\n\n```\n# Weekly Prep Brief — Week of [Date]\n\n## Week Overview\n[2–4 bullets: key themes, flagged priorities, call count]\n\n---\n\n## [Monday / Tuesday / etc.]\n\n### [Company Name] — [Time]\n**Attendees:** [Names and titles]\n**Meeting type:** [Discovery / QBR / Renewal / Expansion / etc. — inferred if possible]\n\n**Company Snapshot**\n[4–5 bullets: account status, top signals, recent activity]\n\n**Attendee Profiles**\n- **[Name]** ([Title]): [2–3 bullets on their signals, persona, conversation angle]\n- [Repeat per attendee]\n\n**Top Signals This Week**\n[2–3 most relevant signals for this specific call]\n\n**This Week's News** [If notable news found]\n[Only genuinely noteworthy findings — funding, leadership changes, major press]\n\n**Recommended Objectives**\n[1–2 sentences: what to accomplish in this meeting]\n\n---\n\n[Repeat per meeting, sorted by date/time]\n```\n\n## When a Meeting Has Sparse Data\n\nIf Common Room returns limited data for a particular meeting's account or attendees, use a compressed format for that meeting instead of the full template:\n\n```\n### [Company Name] — [Time] ⚠️ Limited Data\n**Attendees:** [Names and titles if known]\n**Data available:** [What Common Room actually returned]\n\n**Web Search Results**\n[Findings from web search — company news, attendee LinkedIn profiles]\n\n**Note:** Common Room has limited data on this account. The rep may want to check directly in CR or gather context from colleagues before this call.\n```\n\nDo not generate a full meeting prep section (company snapshot, signal highlights, talking points, recommended objectives) from sparse data. A short honest section is more useful than a fabricated full one.\n\n## Quality Standards\n\n- Keep each meeting section scannable — reps read these in the morning, often on mobile\n- Always sort by date/time ascending\n- Flag urgent situations prominently (risk, trial expiration, open opps) — don't bury them\n- If a meeting has very thin Common Room data, use the sparse-data format above — never fill the full template with guesses\n- Total brief should be readable in 10–15 minutes for a week with 4–6 meetings\n- **Every fact must come from a tool call** — no invented deal context, activity, or signals\n\n## Reference Files\n\n- **`references/briefing-guide.md`** — guidelines for structuring briefings, prioritization logic, and how to handle edge cases (cancelled meetings, new accounts with no data, etc.)\n", "depth": 5}, {"id": "253", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/weekly-prep-brief/references/briefing-guide.md", "name": "briefing-guide.md", "type": "file", "content": "# Weekly Prep Brief — Structure and Edge Cases\n\n## Briefing Depth Guidelines\n\nCalibrate research depth based on account type:\n\n| Account Type | Depth |\n|-------------|-------|\n| Strategic account (large, high ARR) | Full — account + all attendees |\n| Active opportunity (open deal) | Full — with emphasis on deal signals |\n| Renewal within 90 days | Full — with emphasis on risk/health signals |\n| Mid-market customer, routine call | Standard — abbreviated account + key attendee |\n| Small account, quick check-in | Minimal — 3-4 bullets, single attendee |\n| Brand new prospect (first meeting) | Full contact + account — first impressions matter |\n\n## Week Overview Priorities\n\nFlag these situations prominently at the top of the brief:\n\n- Trial expiring within 7 days, renewal due within 30 days, or churn risk signals detected\n- Open critical support tickets or unresolved escalations\n- Expansion signals worth raising in the call\n- Executive stakeholder or new champion attending\n- Champion recently changed jobs or competitive signals in the account\n- Recent company funding, product milestone, or strong community advocacy (use as opener)\n\n## Edge Cases\n\n- **No Common Room data:** Note it explicitly, run external web research, recommend the rep check with their team for offline context.\n- **Multiple meetings with the same company:** Research the account once, share across meetings, differentiate by attendees and objectives.\n- **7+ meetings in a week:** Ask whether to prioritize full research for specific accounts or run abbreviated prep for all; default to full for strategic.\n- **Same-day meeting:** Skip confirmation, produce compressed version (company snapshot + top signals + 1-2 talking points), mark as \"[Quick Prep]\".\n- **Cancelled meetings:** Exclude silently; if status is unclear, include with \"[Status unclear — confirm this meeting is still on.]\"\n", "depth": 6}, {"id": "254", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/call-prep/references", "name": "references", "type": "dir", "content": "", "depth": 5}, {"id": "255", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/call-prep/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: call-prep\ndescription: \"Prepare for a customer or prospect call using Common Room signals. Triggers on 'prep me for my call with [company]', 'prepare for a meeting with [company]', 'what should I know before talking to [company]', or any call preparation request.\"\n---\n\n# Call Prep\n\nProduce a complete, scannable call prep brief by combining account research, contact research, and signal synthesis from Common Room.\n\n## Prep Process\n\n### Step 1: Identify the Account and Attendees\n\nParse what the user has provided:\n- **Company name** — required; look up the account in Common Room\n- **Attendee names** — optional; if provided, research each one\n\n**Calendar lookup:** If a `~~calendar` connector is available, search for upcoming meetings with the named company to automatically surface attendee names, meeting time, and any meeting notes or agenda. Use this to fill gaps the user didn't provide.\n\nIf neither attendees nor a calendar match can be found, ask: \"Who will be on the call from [Company]? I can research each attendee to make your prep more useful.\"\n\n### Step 2: Run Account Research\n\nUse the account-research skill process to build a full account snapshot. For call prep, prioritize:\n- Recent product signals (what are they doing in the product right now?)\n- Open opportunities or renewal timeline\n- Any risk signals (declining usage, support tickets, churned seats)\n- Key recent events (funding, executive change, new hire)\n\nWhen reviewing activity history, prioritize Gong and call recording activities — these provide direct context about previous conversations. Do not filter out call recordings by activity origin.\n\n### Step 3: Run Contact Research for Each Attendee\n\nFor each external attendee, use the contact-research skill process. For call prep, focus on:\n- Role and influence in the buying process\n- Their personal activity and engagement history\n- Any recent signals that suggest their current mood/priorities\n- Spark persona classification if available\n\n### Step 4: Synthesize Talking Points and Objectives\n\nBased on the combined account and contact research:\n- Identify the **call objective** (e.g., discovery, demo, expansion conversation, renewal, QBR)\n- Generate **3–5 tailored talking points** grounded in specific signal data\n- Anticipate **2–3 likely objections or topics** the customer may raise\n- Suggest a **recommended outcome** for the call\n\nWhen the user's company context is available (see `references/my-company-context.md`), tailor talking points to the user's product and value proposition.\n\n### Step 5: Recency Check (Web Search)\n\nAfter gathering all Common Room data, run a quick recency check to catch anything that happened since the last CR data sync. This is supplementary — CR data drives the prep; web search only adds recency.\n\n**Company news:** Search `\"[company name]\" news` filtered to the last 14 days. Look for funding announcements, product launches, leadership changes, layoffs, partnerships, or press coverage.\n\n**Attendee presence:** For each external attendee, search `\"[full name]\" \"[company name]\"` — look for recent articles, LinkedIn posts, conference talks, podcasts, or published opinions.\n\nIf a company news item is significant (e.g., just raised a round, announced a major hire), flag it in Signal Highlights. Otherwise, include findings briefly — don't let web search results overshadow CR signals.\n\n## Output Format\n\nThe output adapts to how much data Common Room returned. Only include sections where you have real data. Never fill a section with invented details.\n\n### When data is rich (multiple field groups returned, activity history, scores, signals):\n\n```\n## Call Prep: [Company] — [Date/Time if known]\n\n**Meeting Context**\n[Attendees, meeting type, and any known agenda]\n\n---\n\n### Company Snapshot\n[4–6 bullets: key account status, signals, and recent activity]\n\n---\n\n### Attendee Profiles\n\n**[Attendee Name] — [Title]**\n[3–4 bullets: role, recent activity, Spark persona if available, personal hook]\n\n[Repeat for each attendee]\n\n---\n\n### Signal Highlights\n[Top 3 signals most relevant to this specific call]\n\n---\n\n### Talking Points\n1. [Point tied to a specific signal]\n2. [Point tied to a specific signal]\n3. [Point tied to a specific signal]\n\n### Likely Topics / Objections to Prepare For\n- [Topic or objection + suggested response]\n- [Topic or objection + suggested response]\n\n### Recommended Call Outcome\n[1–2 sentences: what success looks like for this meeting]\n```\n\n### When data is sparse (few fields returned, no activity, null sparkSummary):\n\n```\n## Call Prep: [Company] — [Date/Time if known]\n\n**Data available:** [List exactly what Common Room returned — e.g., \"Name, title, email, two tags. No activity history, no scores, no Spark data.\"]\n\n### What I Found\n[Only the fields actually returned, presented as-is]\n\n### Web Search Results\n[Findings from web search on the company and attendees — or \"No significant results\"]\n\n### Suggested Next Steps\n- I can pull [specific field groups] from Common Room if available\n- I can run deeper web searches on [specific topics]\n- You may want to check Common Room directly for [what's missing]\n```\n\nDo not generate a full call prep brief from sparse data. A short honest output is always better than a long fabricated one.\n\n## Quality Standards\n\n- Ground every talking point in a real signal — no generic filler\n- Keep the brief tight — it should be readable in 5 minutes or less\n- Flag unknowns explicitly — if attendee research is thin, say so\n- Time-box the research — don't over-research at the expense of speed\n- **Never invent deal context** — no fabricated proposals, competitor comparisons, pricing, trial terms, or objections not returned by a tool call\n\n## Reference Files\n\n- **`references/call-types-guide.md`** — guidance for different call types (discovery, expansion, renewal, QBR) and how to tailor prep accordingly\n", "depth": 5}, {"id": "256", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/partner-built/common-room/skills/call-prep/references/call-types-guide.md", "name": "call-types-guide.md", "type": "file", "content": "# Call Types — Prep Guide\n\n## Identifying the Call Type\n\nIf the user doesn't specify, infer from context: trial + early signals = Discovery/Demo; active CRM opportunity = Demo/Evaluation/Negotiation; existing customer + renewal date = Renewal/QBR; expansion signals = Expansion; support tickets + declining usage = Save call.\n\n---\n\n## Discovery Call\n\n**Goal:** Understand the prospect's pain, context, and fit.\n\n- Identify what signals triggered the meeting and prepare open-ended questions tied to those signals\n- Form a hypothesis about their primary use case from Spark persona and activity data\n- **Talking point structure:** Signal -\u003e question -\u003e how you might help\n\n## Demo Call\n\n**Goal:** Show the product in context of their specific needs.\n\n- Note which features they've already explored and competitors in their stack\n- Identify audience (technical evaluator vs economic buyer) and match depth accordingly\n- Highlight 1-2 features based on their known activity\n\n## Expansion Conversation\n\n**Goal:** Identify and close upsell/cross-sell or seat expansion.\n\n- Identify product areas with highest recent adoption and emerging teams/use cases\n- Lead with usage data demonstrating value already delivered\n- Present expansion as a natural next step tied to their business outcomes\n\n## Renewal / Retention Call\n\n**Goal:** Secure renewal; address any risk.\n\n- Check engagement trend, open support tickets, and champion status\n- Quantify value delivered (usage stats, outcomes) as renewal justification\n- Address risk signals directly and proactively — have a retention offer ready if needed\n\n## QBR (Quarterly Business Review)\n\n**Goal:** Review progress, align on strategy, deepen executive relationship.\n\n- Analyze 90-day signal trends and breadth of adoption across teams\n- Lead with business outcomes, not feature usage — surface 1-2 surprising stats\n- Come with a forward-looking agenda and next quarter goals\n\n## Save / Risk Call\n\n**Goal:** Understand and reverse negative trajectory.\n\n- Identify specific decline signals (what dropped and when) and check for unresolved tickets\n- Acknowledge the signals without being defensive — ask questions before presenting solutions\n- Have a concrete action plan ready\n", "depth": 6}, {"id": "257", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "258", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "259", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/README.md", "name": "README.md", "type": "file", "content": "# HR Plugin\n\nA people operations plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Helps with recruiting, onboarding, performance management, policy guidance, and compensation analysis. Works with any HR team — standalone with your input, supercharged when you connect your HRIS, ATS, and other tools.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/human-resources\n```\n\n## Commands\n\nExplicit workflows you invoke with a slash command:\n\n| Command | Description |\n|---|---|\n| `/draft-offer` | Draft an offer letter with compensation details, start date, and terms |\n| `/onboarding` | Generate an onboarding checklist and first-week plan for a new hire |\n| `/performance-review` | Structure a performance review — self-assessment prompts, manager template, calibration prep |\n| `/policy-lookup` | Find and explain company policies — PTO, benefits, expense, travel, remote work |\n| `/comp-analysis` | Analyze compensation data — benchmarking, band placement, equity refresh modeling |\n| `/people-report` | Generate headcount, attrition, diversity, or org health reports |\n\nAll commands work **standalone** (provide context and details) and get **supercharged** with MCP connectors.\n\n## Skills\n\nDomain knowledge Claude uses automatically when relevant:\n\n| Skill | Description |\n|---|---|\n| `recruiting-pipeline` | Track and manage recruiting pipeline — source, screen, interview, offer stages |\n| `employee-handbook` | Answer questions about company policies, benefits, and procedures |\n| `compensation-benchmarking` | Benchmark compensation against market data — base, equity, total comp |\n| `org-planning` | Headcount planning, org design, and team structure optimization |\n| `people-analytics` | Analyze workforce data — attrition trends, engagement signals, diversity metrics |\n| `interview-prep` | Create structured interview plans — competency-based questions, scorecards, debrief templates |\n\n## Example Workflows\n\n### Drafting an Offer\n\n```\n/draft-offer\n```\n\nTell me the role, level, location, and comp details. Get a complete offer letter draft with terms, equity breakdown, and benefits summary.\n\n### Onboarding a New Hire\n\n```\n/onboarding\n```\n\nProvide the new hire's name, role, team, and start date. Get a comprehensive onboarding checklist, first-week calendar, tool access list, and buddy assignment template.\n\n### Preparing for Performance Reviews\n\n```\n/performance-review\n```\n\nGet templates for self-assessments, manager reviews, and calibration. I'll help structure feedback that's specific, actionable, and fair.\n\n### Understanding Benefits\n\nJust ask naturally:\n```\nWhat's our parental leave policy?\n```\n\nThe `employee-handbook` skill triggers automatically and searches your connected knowledge base for the answer.\n\n### Compensation Benchmarking\n\n```\n/comp-analysis\n```\n\nUpload comp data or describe your bands. Get market comparisons, band placement analysis, and recommendations for adjustments.\n\n## Standalone + Supercharged\n\nEvery command and skill works without any integrations:\n\n| What You Can Do | Standalone | Supercharged With |\n|-----------------|------------|-------------------|\n| Draft offers | Provide details manually | HRIS, ATS for auto-fill |\n| Onboarding checklists | Describe your process | HRIS, Knowledge base for templates |\n| Performance reviews | Manual input | HRIS for review history |\n| Policy lookup | Paste handbook content | Knowledge base |\n| Comp analysis | Upload CSV, describe bands | Compensation data MCP |\n| People reports | Upload data | HRIS for live data |\n\n## MCP Integrations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your tools for a richer experience:\n\n| Category | Examples | What It Enables |\n|---|---|---|\n| **HRIS** | Workday, BambooHR, Rippling | Employee data, org structure, PTO balances |\n| **ATS** | Greenhouse, Lever, Ashby | Candidate pipeline, interview schedules, offer tracking |\n| **Compensation** | Pave, Radford | Market benchmarks, comp band data |\n| **Chat** | Slack, Teams | Team announcements, candidate coordination |\n| **Calendar** | Google Calendar, Microsoft 365 | Interview scheduling, onboarding calendar |\n| **Email** | Gmail, Microsoft 365 | Offer letters, candidate communications |\n\nSee [CONNECTORS.md](CONNECTORS.md) for the full list of supported integrations.\n\n## Settings\n\nCreate a local settings file at `human-resources/.claude/settings.local.json` to personalize:\n\n```json\n{\n  \"company\": \"Your Company\",\n  \"headquarters\": \"City, State\",\n  \"employeeCount\": 500,\n  \"benefits\": {\n    \"healthInsurance\": \"Provider Name\",\n    \"pto\": \"Unlimited / X days\",\n    \"parentalLeave\": \"X weeks\"\n  },\n  \"compensation\": {\n    \"currency\": \"USD\",\n    \"equityType\": \"RSU / Options\",\n    \"vestingSchedule\": \"4 years, 1 year cliff\"\n  }\n}\n```\n\nThe plugin will ask you for this information interactively if it's not configured.\n", "depth": 2}, {"id": "260", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~HRIS` might mean Workday, BambooHR, or any other HRIS with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (HRIS, ATS, email, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| ATS | `~~ATS` | — | Greenhouse, Lever, Ashby, Workable |\n| Calendar | `~~calendar` | Google Calendar | Microsoft 365 |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Email | `~~email` | Gmail, Microsoft 365 | — |\n| HRIS | `~~HRIS` | — | Workday, BambooHR, Rippling, Gusto |\n| Knowledge base | `~~knowledge base` | Notion, Atlassian (Confluence) | Guru, Coda |\n| Compensation data | `~~compensation data` | — | Pave, Radford, Levels.fyi |\n", "depth": 2}, {"id": "261", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands/comp-analysis.md", "name": "comp-analysis.md", "type": "file", "content": "---\ndescription: Analyze compensation — benchmarking, band placement, and equity modeling\nargument-hint: \"\u003crole, level, or dataset\u003e\"\n---\n\n# /comp-analysis\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nAnalyze compensation data for benchmarking, band placement, and planning. See the **compensation-benchmarking** skill for total comp frameworks, key variables, and data source guidance.\n\n## Usage\n\n```\n/comp-analysis $ARGUMENTS\n```\n\n## What I Need From You\n\n**Option A: Single role analysis**\n\"What should we pay a Senior Software Engineer in SF?\"\n\n**Option B: Upload comp data**\nUpload a CSV or paste your comp bands. I'll analyze placement, identify outliers, and compare to market.\n\n**Option C: Equity modeling**\n\"Model a refresh grant of 10K shares over 4 years at a $50 stock price.\"\n\n## Output\n\n```markdown\n## Compensation Analysis: [Role/Scope]\n\n### Market Benchmarks\n| Percentile | Base | Equity | Total Comp |\n|------------|------|--------|------------|\n| 25th | $[X] | $[X] | $[X] |\n| 50th | $[X] | $[X] | $[X] |\n| 75th | $[X] | $[X] | $[X] |\n| 90th | $[X] | $[X] | $[X] |\n\n**Sources:** [Web research, compensation data tools, or user-provided data]\n\n### Band Analysis (if data provided)\n| Employee | Current Base | Band Min | Band Mid | Band Max | Position |\n|----------|-------------|----------|----------|----------|----------|\n| [Name] | $[X] | $[X] | $[X] | $[X] | [Below/At/Above] |\n\n### Recommendations\n- [Specific compensation recommendations]\n- [Equity considerations]\n- [Retention risks if applicable]\n```\n\n## If Connectors Available\n\nIf **~~compensation data** is connected:\n- Pull verified market benchmarks by role, level, and location\n- Compare your bands against real-time market data\n\nIf **~~HRIS** is connected:\n- Pull current employee comp data for band analysis\n- Identify outliers and retention risks automatically\n\n## Tips\n\n1. **Location matters** — Always specify location for benchmarking. SF vs. Austin vs. London are very different.\n2. **Total comp, not just base** — Include equity, bonus, and benefits for a complete picture.\n3. **Keep data confidential** — Comp data is sensitive. Results stay in your conversation.\n", "depth": 3}, {"id": "262", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands/draft-offer.md", "name": "draft-offer.md", "type": "file", "content": "---\ndescription: Draft an offer letter with comp details and terms\nargument-hint: \"\u003crole and level\u003e\"\n---\n\n# /draft-offer\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nDraft a complete offer letter for a new hire.\n\n## Usage\n\n```\n/draft-offer $ARGUMENTS\n```\n\n## What I Need From You\n\n- **Role and title**: What position?\n- **Level**: Junior, Mid, Senior, Staff, etc.\n- **Location**: Where will they be based? (affects comp and benefits)\n- **Compensation**: Base salary, equity, signing bonus (if applicable)\n- **Start date**: When should they start?\n- **Hiring manager**: Who will they report to?\n\nIf you don't have all details, I'll help you think through them.\n\n## Output\n\n```markdown\n## Offer Letter Draft: [Role] — [Level]\n\n### Compensation Package\n| Component | Details |\n|-----------|---------|\n| **Base Salary** | $[X]/year |\n| **Equity** | [X shares/units], [vesting schedule] |\n| **Signing Bonus** | $[X] (if applicable) |\n| **Target Bonus** | [X]% of base (if applicable) |\n| **Total First-Year Comp** | $[X] |\n\n### Terms\n- **Start Date**: [Date]\n- **Reports To**: [Manager]\n- **Location**: [Office / Remote / Hybrid]\n- **Employment Type**: [Full-time, Exempt]\n\n### Benefits Summary\n[Key benefits highlights relevant to the candidate]\n\n### Offer Letter Text\n\nDear [Candidate Name],\n\nWe are pleased to offer you the position of [Title] at [Company]...\n\n[Complete offer letter text]\n\n### Notes for Hiring Manager\n- [Negotiation guidance if needed]\n- [Comp band context]\n- [Any flags or considerations]\n```\n\n## If Connectors Available\n\nIf **~~HRIS** is connected:\n- Pull comp band data for the level/role\n- Verify headcount approval\n- Auto-populate benefits details\n\nIf **~~ATS** is connected:\n- Pull candidate details from the application\n- Update offer status in the pipeline\n\n## Tips\n\n1. **Include total comp** — Candidates compare total compensation, not just base.\n2. **Be specific about equity** — Share count, current valuation method, vesting schedule.\n3. **Personalize** — Reference something from the interview process to make it warm.\n", "depth": 3}, {"id": "263", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands/policy-lookup.md", "name": "policy-lookup.md", "type": "file", "content": "---\ndescription: Find and explain company policies\nargument-hint: \"\u003cpolicy topic — PTO, benefits, travel, expenses, etc.\u003e\"\n---\n\n# /policy-lookup\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nLook up and explain company policies in plain language. See the **employee-handbook** skill for guidance on policy topics, answer structure, and compliance caveats.\n\n## Usage\n\n```\n/policy-lookup $ARGUMENTS\n```\n\nSearch for policies matching: $ARGUMENTS\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    POLICY LOOKUP                                   │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Ask any policy question in plain language                    │\n│  ✓ Paste your employee handbook and I'll search it              │\n│  ✓ Get clear, jargon-free answers                               │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Knowledge base: Search handbook and policy docs automatically │\n│  + HRIS: Pull employee-specific details (PTO balance, benefits) │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Output\n\n```markdown\n## Policy: [Topic]\n\n### Quick Answer\n[1-2 sentence direct answer to their question]\n\n### Details\n[Relevant policy details, explained in plain language]\n\n### Exceptions / Special Cases\n[Any relevant exceptions or edge cases]\n\n### Who to Contact\n[Person or team for questions beyond what's documented]\n\n### Source\n[Where this information came from — document name, page, or section]\n```\n\n## If Connectors Available\n\nIf **~~knowledge base** is connected:\n- Search employee handbook and policy documents automatically\n- Cite the specific document, section, and page number\n\nIf **~~HRIS** is connected:\n- Pull employee-specific details like PTO balance, benefits elections, and enrollment status\n\n## Tips\n\n1. **Ask in plain language** — \"Can I work from Europe for a month?\" is better than \"international remote work policy.\"\n2. **Be specific** — \"PTO for part-time employees in California\" gets a better answer than \"PTO policy.\"\n", "depth": 3}, {"id": "264", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands/people-report.md", "name": "people-report.md", "type": "file", "content": "---\ndescription: Generate headcount, attrition, diversity, or org health reports\nargument-hint: \"\u003creport type — headcount, attrition, diversity, org health\u003e\"\n---\n\n# /people-report\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate people analytics reports from your HR data. See the **people-analytics** skill for metric definitions, analysis frameworks, and benchmarks.\n\n## Usage\n\n```\n/people-report $ARGUMENTS\n```\n\n## Report Types\n\n**Headcount**: Current org snapshot — by team, location, level, tenure\n**Attrition**: Turnover analysis — voluntary/involuntary, by team, trends\n**Diversity**: Representation metrics — by level, team, pipeline\n**Org Health**: Span of control, management layers, team sizes, flight risk\n\n## What I Need From You\n\nUpload a CSV or describe your data. Helpful fields:\n- Employee name/ID, department, team\n- Title, level, location\n- Start date, end date (if applicable)\n- Manager, compensation (if relevant)\n- Demographics (for diversity reports, if available)\n\n## Output\n\n```markdown\n## People Report: [Type] — [Date]\n\n### Executive Summary\n[2-3 key takeaways]\n\n### Key Metrics\n| Metric | Value | Trend |\n|--------|-------|-------|\n| [Metric] | [Value] | [up/down/flat] |\n\n### Detailed Analysis\n[Charts, tables, and narrative for the specific report type]\n\n### Recommendations\n- [Data-driven recommendation]\n- [Action item]\n\n### Methodology\n[How the numbers were calculated, any caveats]\n```\n\n## If Connectors Available\n\nIf **~~HRIS** is connected:\n- Pull live employee data — headcount, tenure, department, level\n- Generate reports without needing a CSV upload\n\nIf **~~chat** is connected:\n- Offer to share the report summary in a relevant channel\n", "depth": 3}, {"id": "265", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands/onboarding.md", "name": "onboarding.md", "type": "file", "content": "---\ndescription: Generate an onboarding checklist and first-week plan for a new hire\nargument-hint: \"\u003cnew hire name and role\u003e\"\n---\n\n# /onboarding\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a comprehensive onboarding plan for a new team member.\n\n## Usage\n\n```\n/onboarding $ARGUMENTS\n```\n\n## What I Need From You\n\n- **New hire name**: Who's starting?\n- **Role**: What position?\n- **Team**: Which team are they joining?\n- **Start date**: When do they start?\n- **Manager**: Who's their manager?\n\n## Output\n\n```markdown\n## Onboarding Plan: [Name] — [Role]\n**Start Date:** [Date] | **Team:** [Team] | **Manager:** [Manager]\n\n### Pre-Start (Before Day 1)\n- [ ] Send welcome email with start date, time, and logistics\n- [ ] Set up accounts: email, Slack, [tools for role]\n- [ ] Order equipment (laptop, monitor, peripherals)\n- [ ] Add to team calendar and recurring meetings\n- [ ] Assign onboarding buddy: [Suggested person]\n- [ ] Prepare desk / remote setup instructions\n\n### Day 1\n| Time | Activity | With |\n|------|----------|------|\n| 9:00 | Welcome and orientation | Manager |\n| 10:00 | IT setup and tool walkthrough | IT / Buddy |\n| 11:00 | Team introductions | Team |\n| 12:00 | Welcome lunch | Manager + Team |\n| 1:30 | Company overview and values | Manager |\n| 3:00 | Role expectations and 30/60/90 plan | Manager |\n| 4:00 | Free time to explore tools and docs | Self |\n\n### Week 1\n- [ ] Complete required compliance training\n- [ ] Read key documentation: [list for role]\n- [ ] 1:1 with each team member\n- [ ] Shadow key meetings\n- [ ] First small task or project assigned\n- [ ] End-of-week check-in with manager\n\n### 30-Day Goals\n1. [Goal aligned to role]\n2. [Goal aligned to role]\n3. [Goal aligned to role]\n\n### 60-Day Goals\n1. [Goal]\n2. [Goal]\n\n### 90-Day Goals\n1. [Goal]\n2. [Goal]\n\n### Key Contacts\n| Person | Role | For What |\n|--------|------|----------|\n| [Manager] | Manager | Day-to-day guidance |\n| [Buddy] | Onboarding Buddy | Questions, culture, navigation |\n| [IT Contact] | IT | Tool access, equipment |\n| [HR Contact] | HR | Benefits, policies |\n\n### Tools Access Needed\n| Tool | Access Level | Requested |\n|------|-------------|-----------|\n| [Tool] | [Level] | [ ] |\n```\n\n## If Connectors Available\n\nIf **~~HRIS** is connected:\n- Pull new hire details and team org chart\n- Auto-populate tools access list based on role\n\nIf **~~knowledge base** is connected:\n- Link to relevant onboarding docs, team wikis, and runbooks\n- Pull the team's existing onboarding checklist to customize\n\nIf **~~calendar** is connected:\n- Create Day 1 calendar events and Week 1 meeting invites automatically\n\n## Tips\n\n1. **Customize for the role** — An engineer's onboarding looks different from a designer's.\n2. **Don't overload Day 1** — Focus on setup and relationships. Deep work starts Week 2.\n3. **Assign a buddy** — Having a go-to person who isn't their manager makes a huge difference.\n", "depth": 3}, {"id": "266", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/commands/performance-review.md", "name": "performance-review.md", "type": "file", "content": "---\ndescription: Structure a performance review with self-assessment, manager template, and calibration prep\nargument-hint: \"\u003cemployee name or review cycle\u003e\"\n---\n\n# /performance-review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate performance review templates and help structure feedback.\n\n## Usage\n\n```\n/performance-review $ARGUMENTS\n```\n\n## Modes\n\n```\n/performance-review self-assessment       # Generate self-assessment template\n/performance-review manager [employee]    # Manager review template for a specific person\n/performance-review calibration           # Calibration prep document\n```\n\nIf no mode is specified, ask what type of review they need.\n\n## Output — Self-Assessment Template\n\n```markdown\n## Self-Assessment: [Review Period]\n\n### Key Accomplishments\n[List your top 3-5 accomplishments this period. For each, describe the situation, your contribution, and the impact.]\n\n1. **[Accomplishment]**\n   - Situation: [Context]\n   - Contribution: [What you did]\n   - Impact: [Measurable result]\n\n### Goals Review\n| Goal | Status | Evidence |\n|------|--------|----------|\n| [Goal from last period] | Met / Exceeded / Missed | [How you know] |\n\n### Growth Areas\n[Where did you grow? New skills, expanded scope, leadership moments.]\n\n### Challenges\n[What was hard? What would you do differently?]\n\n### Goals for Next Period\n1. [Goal — specific and measurable]\n2. [Goal]\n3. [Goal]\n\n### Feedback for Manager\n[How can your manager better support you?]\n```\n\n## Output — Manager Review\n\n```markdown\n## Performance Review: [Employee Name]\n**Period:** [Date range] | **Manager:** [Your name]\n\n### Overall Rating: [Exceeds / Meets / Below Expectations]\n\n### Performance Summary\n[2-3 sentence overall assessment]\n\n### Key Strengths\n- [Strength with specific example]\n- [Strength with specific example]\n\n### Areas for Development\n- [Area with specific, actionable guidance]\n- [Area with specific, actionable guidance]\n\n### Goal Achievement\n| Goal | Rating | Comments |\n|------|--------|----------|\n| [Goal] | [Rating] | [Specific observations] |\n\n### Impact and Contributions\n[Describe their biggest contributions and impact on the team/org]\n\n### Development Plan\n| Skill | Current | Target | Actions |\n|-------|---------|--------|---------|\n| [Skill] | [Level] | [Level] | [How to get there] |\n\n### Compensation Recommendation\n[Promotion / Equity refresh / Adjustment / No change — with justification]\n```\n\n## Output — Calibration\n\n```markdown\n## Calibration Prep: [Review Cycle]\n**Manager:** [Your name] | **Team:** [Team] | **Period:** [Date range]\n\n### Team Overview\n| Employee | Role | Level | Tenure | Proposed Rating | Notes |\n|----------|------|-------|--------|-----------------|-------|\n| [Name] | [Role] | [Level] | [X years] | [Rating] | [Key context] |\n\n### Rating Distribution\n| Rating | Count | % of Team | Company Target |\n|--------|-------|-----------|----------------|\n| Exceeds Expectations | [X] | [X]% | ~15-20% |\n| Meets Expectations | [X] | [X]% | ~60-70% |\n| Below Expectations | [X] | [X]% | ~10-15% |\n\n### Calibration Discussion Points\n1. **[Employee]** — [Why this rating may need discussion, e.g., borderline, first review at level, recent role change]\n2. **[Employee]** — [Discussion point]\n\n### Promotion Candidates\n| Employee | Current Level | Proposed Level | Justification |\n|----------|-------------|----------------|---------------|\n| [Name] | [Current] | [Proposed] | [Evidence of next-level performance] |\n\n### Compensation Actions\n| Employee | Action | Justification |\n|----------|--------|---------------|\n| [Name] | [Promotion / Equity refresh / Market adjustment / Retention] | [Why] |\n\n### Manager Notes\n[Context the calibration group should know — team changes, org shifts, project impacts]\n```\n\n## If Connectors Available\n\nIf **~~HRIS** is connected:\n- Pull prior review history and goal tracking data\n- Pre-populate employee details and current role information\n\nIf **~~project tracker** is connected:\n- Pull completed work and contributions for the review period\n- Reference specific tickets and project milestones as evidence\n\n## Tips\n\n1. **Be specific** — \"Great job\" isn't feedback. \"You reduced deploy time 40% by implementing the new CI pipeline\" is.\n2. **Balance positive and constructive** — Both are essential. Neither should be a surprise.\n3. **Focus on behaviors, not personality** — \"Your documentation has been incomplete\" vs. \"You're careless.\"\n4. **Make development actionable** — \"Improve communication\" is vague. \"Present at the next team all-hands\" is actionable.\n", "depth": 3}, {"id": "267", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/recruiting-pipeline", "name": "recruiting-pipeline", "type": "dir", "content": "", "depth": 3}, {"id": "268", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/employee-handbook", "name": "employee-handbook", "type": "dir", "content": "", "depth": 3}, {"id": "269", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/org-planning", "name": "org-planning", "type": "dir", "content": "", "depth": 3}, {"id": "270", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/compensation-benchmarking", "name": "compensation-benchmarking", "type": "dir", "content": "", "depth": 3}, {"id": "271", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/people-analytics", "name": "people-analytics", "type": "dir", "content": "", "depth": 3}, {"id": "272", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/interview-prep", "name": "interview-prep", "type": "dir", "content": "", "depth": 3}, {"id": "273", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/recruiting-pipeline/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: recruiting-pipeline\ndescription: Track and manage recruiting pipeline stages. Trigger with \"recruiting update\", \"candidate pipeline\", \"how many candidates\", \"hiring status\", or when the user discusses sourcing, screening, interviewing, or extending offers.\n---\n\n# Recruiting Pipeline\n\nHelp manage the recruiting pipeline from sourcing through offer acceptance.\n\n## Pipeline Stages\n\n| Stage | Description | Key Actions |\n|-------|-------------|-------------|\n| Sourced | Identified and reached out | Personalized outreach |\n| Screen | Phone/video screen | Evaluate basic fit |\n| Interview | On-site or panel interviews | Structured evaluation |\n| Debrief | Team decision | Calibrate feedback |\n| Offer | Extending offer | Comp package, negotiation |\n| Accepted | Offer accepted | Transition to onboarding |\n\n## Metrics to Track\n\n- **Pipeline velocity**: Days per stage\n- **Conversion rates**: Stage-to-stage drop-off\n- **Source effectiveness**: Which channels produce hires\n- **Offer acceptance rate**: Offers extended vs. accepted\n- **Time to fill**: Days from req open to offer accepted\n\n## If ATS Connected\n\nPull candidate data automatically, update statuses, and track pipeline metrics in real time.\n", "depth": 4}, {"id": "274", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/employee-handbook/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: employee-handbook\ndescription: Answer questions about company policies, benefits, and procedures. Trigger with \"what's our policy on\", \"how does PTO work\", \"benefits question\", \"expense policy\", \"remote work policy\", or any question about company rules, perks, or procedures.\n---\n\n# Employee Handbook\n\nAnswer employee questions about policies, benefits, and procedures by searching connected knowledge bases or using provided handbook content.\n\n## Common Topics\n\n- **PTO and Leave**: Vacation, sick leave, parental leave, bereavement, sabbatical\n- **Benefits**: Health insurance, dental, vision, 401k, HSA/FSA, wellness\n- **Compensation**: Pay schedule, bonus timing, equity vesting, expense reimbursement\n- **Remote Work**: WFH policy, remote locations, equipment stipend, coworking\n- **Travel**: Booking policy, per diem, expense reporting, approval process\n- **Conduct**: Code of conduct, harassment policy, conflicts of interest\n- **Growth**: Professional development budget, conference policy, tuition reimbursement\n\n## How to Answer\n\n1. Search ~~knowledge base for the relevant policy document\n2. Provide a clear, plain-language answer\n3. Quote the specific policy language\n4. Note any exceptions or special cases\n5. Point to who to contact for edge cases\n\n## Important\n\n- Always cite the source document and section\n- If no policy is found, say so clearly rather than guessing\n- For legal or compliance questions, recommend consulting HR or legal directly\n", "depth": 4}, {"id": "275", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/org-planning/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: org-planning\ndescription: Headcount planning, org design, and team structure optimization. Trigger with \"org planning\", \"headcount plan\", \"team structure\", \"reorg\", \"who should we hire next\", or when the user is thinking about team size, reporting structure, or organizational design.\n---\n\n# Org Planning\n\nHelp plan organizational structure, headcount, and team design.\n\n## Planning Dimensions\n\n- **Headcount**: How many people do we need, in what roles, by when?\n- **Structure**: Reporting lines, span of control, team boundaries\n- **Sequencing**: Which hires are most critical? What's the right order?\n- **Budget**: Headcount cost modeling and trade-offs\n\n## Healthy Org Benchmarks\n\n| Metric | Healthy Range | Warning Sign |\n|--------|---------------|--------------|\n| Span of control | 5-8 direct reports | \u003c 3 or \u003e 12 |\n| Management layers | 4-6 for 500 people | Too many = slow decisions |\n| IC-to-manager ratio | 6:1 to 10:1 | \u003c 4:1 = top-heavy |\n| Team size | 5-9 people | \u003c 4 = lonely, \u003e 12 = hard to manage |\n\n## Output\n\nProduce org charts (text-based), headcount plans with cost modeling, and sequenced hiring roadmaps. Flag structural issues like single points of failure or excessive management overhead.\n", "depth": 4}, {"id": "276", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/compensation-benchmarking/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: compensation-benchmarking\ndescription: Benchmark compensation against market data. Trigger with \"what should we pay\", \"comp benchmark\", \"market rate for\", \"salary range for\", \"is this offer competitive\", or when the user needs help evaluating or setting compensation levels.\n---\n\n# Compensation Benchmarking\n\nHelp benchmark compensation against market data for hiring, retention, and equity planning.\n\n## Framework\n\n### Components of Total Compensation\n- **Base salary**: Cash compensation\n- **Equity**: RSUs, stock options, or other equity\n- **Bonus**: Annual target bonus, signing bonus\n- **Benefits**: Health, retirement, perks (harder to quantify)\n\n### Key Variables\n- **Role**: Function and specialization\n- **Level**: IC levels, management levels\n- **Location**: Geographic pay adjustments\n- **Company stage**: Startup vs. growth vs. public\n- **Industry**: Tech vs. finance vs. healthcare\n\n## Data Sources\n\n- **With ~~compensation data**: Pull verified benchmarks\n- **Without**: Use web research, public salary data, and user-provided context\n- Always note data freshness and source limitations\n\n## Output\n\nProvide percentile bands (25th, 50th, 75th, 90th) for base, equity, and total comp. Include location adjustments and company-stage context.\n", "depth": 4}, {"id": "277", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/people-analytics/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: people-analytics\ndescription: Analyze workforce data — attrition, engagement, diversity, and productivity. Trigger with \"attrition rate\", \"turnover analysis\", \"diversity metrics\", \"engagement data\", \"retention risk\", or when the user wants to understand workforce trends from HR data.\n---\n\n# People Analytics\n\nAnalyze workforce data to surface trends, risks, and opportunities.\n\n## Key Metrics\n\n### Retention\n- Overall attrition rate (voluntary + involuntary)\n- Regrettable attrition rate\n- Average tenure\n- Flight risk indicators\n\n### Diversity\n- Representation by level, team, and function\n- Pipeline diversity (hiring funnel by demographic)\n- Promotion rates by group\n- Pay equity analysis\n\n### Engagement\n- Survey scores and trends\n- eNPS (Employee Net Promoter Score)\n- Participation rates\n- Open-ended feedback themes\n\n### Productivity\n- Revenue per employee\n- Span of control efficiency\n- Time to productivity for new hires\n\n## Approach\n\n1. Understand what question they're trying to answer\n2. Identify the right data (upload, paste, or pull from ~~HRIS)\n3. Analyze with appropriate statistical methods\n4. Present findings with context and caveats\n5. Recommend specific actions based on data\n", "depth": 4}, {"id": "278", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/human-resources/skills/interview-prep/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: interview-prep\ndescription: Create structured interview plans with competency-based questions and scorecards. Trigger with \"interview plan for\", \"interview questions for\", \"how should we interview\", \"scorecard for\", or when the user is preparing to interview candidates.\n---\n\n# Interview Prep\n\nCreate structured interview plans to evaluate candidates consistently and fairly.\n\n## Interview Design Principles\n\n1. **Structured**: Same questions for all candidates in the role\n2. **Competency-based**: Map questions to specific skills and behaviors\n3. **Evidence-based**: Use behavioral and situational questions\n4. **Diverse panel**: Multiple perspectives reduce bias\n5. **Scored**: Use rubrics, not gut feelings\n\n## Interview Plan Components\n\n### Role Competencies\nDefine 4-6 key competencies for the role (e.g., technical skills, communication, leadership, problem-solving).\n\n### Question Bank\nFor each competency, provide:\n- 2-3 behavioral questions (\"Tell me about a time...\")\n- 1-2 situational questions (\"How would you handle...\")\n- Follow-up probes\n\n### Scorecard\nRate each competency on a consistent scale (1-4) with clear descriptions of what each level looks like.\n\n### Debrief Template\nStructured format for interviewers to share findings and make a decision.\n\n## Output\n\nProduce a complete interview kit: panel assignment (who interviews for what), question bank by competency, scoring rubric, and debrief template.\n", "depth": 4}, {"id": "279", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "280", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "281", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/README.md", "name": "README.md", "type": "file", "content": "# Operations Plugin\n\nA business operations plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Helps with vendor management, process documentation, change management, capacity planning, compliance tracking, and resource planning. Works with any ops team — standalone with your input, supercharged when you connect your ITSM, project tracker, and other tools.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/operations\n```\n\n## Commands\n\nExplicit workflows you invoke with a slash command:\n\n| Command | Description |\n|---|---|\n| `/vendor-review` | Evaluate a vendor — cost analysis, risk assessment, contract summary, and renewal recommendation |\n| `/process-doc` | Document a business process — flowcharts, RACI matrices, SOPs, and runbooks |\n| `/change-request` | Create a change management request — impact analysis, rollback plan, approval routing |\n| `/capacity-plan` | Plan resource capacity — workload analysis, headcount modeling, utilization forecasting |\n| `/status-report` | Generate a status report — project updates, KPIs, risks, and action items for leadership |\n| `/runbook` | Create or update an operational runbook — step-by-step procedures for recurring tasks |\n\nAll commands work **standalone** (provide context and details) and get **supercharged** with MCP connectors.\n\n## Skills\n\nDomain knowledge Claude uses automatically when relevant:\n\n| Skill | Description |\n|---|---|\n| `vendor-management` | Evaluate, compare, and manage vendor relationships — contracts, performance, risk |\n| `process-optimization` | Analyze and improve business processes — identify bottlenecks, reduce waste, streamline workflows |\n| `change-management` | Plan and execute organizational or technical changes — communication, training, adoption |\n| `risk-assessment` | Identify, assess, and mitigate operational risks — risk registers, impact analysis, controls |\n| `compliance-tracking` | Track compliance requirements — audits, certifications, regulatory deadlines, policy adherence |\n| `resource-planning` | Plan and optimize resource allocation — capacity, utilization, forecasting, budget |\n\n## Example Workflows\n\n### Evaluating a Vendor\n\n```\n/vendor-review\n```\n\nProvide the vendor name, contract details, or upload a proposal. Get a structured evaluation with cost analysis, risk flags, and a recommendation.\n\n### Documenting a Process\n\n```\n/process-doc employee offboarding\n```\n\nDescribe the process or walk me through it. Get a complete SOP with flowchart, RACI matrix, and step-by-step procedures.\n\n### Submitting a Change Request\n\n```\n/change-request\n```\n\nDescribe the change. Get an impact analysis, risk assessment, rollback plan, and communication template ready for approval.\n\n### Planning Capacity\n\n```\n/capacity-plan\n```\n\nUpload team data or describe your resources. Get utilization analysis, bottleneck identification, and headcount recommendations.\n\n### Leadership Status Report\n\n```\n/status-report\n```\n\nI'll pull updates from your connected tools (or ask you for input) and generate a polished status report with KPIs, risks, and next steps.\n\n### Creating a Runbook\n\n```\n/runbook monthly close process\n```\n\nWalk me through the process once. I'll document it as a repeatable runbook with checklists, troubleshooting, and escalation paths.\n\n## Standalone + Supercharged\n\nEvery command and skill works without any integrations:\n\n| What You Can Do | Standalone | Supercharged With |\n|-----------------|------------|-------------------|\n| Vendor reviews | Provide details, upload proposals | Procurement, Knowledge base |\n| Process documentation | Describe the process | Knowledge base (existing docs) |\n| Change requests | Describe the change | ITSM, Project tracker |\n| Capacity planning | Upload data, describe team | Project tracker (workload data) |\n| Status reports | Provide updates manually | Project tracker, Chat, Calendar |\n| Runbooks | Walk through the process | Knowledge base, ITSM |\n\n## MCP Integrations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your tools for a richer experience:\n\n| Category | Examples | What It Enables |\n|---|---|---|\n| **ITSM** | ServiceNow, Zendesk | Ticket management, change requests, incident tracking |\n| **Project tracker** | Asana, Jira, monday.com | Project status, resource allocation, task tracking |\n| **Knowledge base** | Notion, Confluence | Process docs, runbooks, policies |\n| **Chat** | Slack, Teams | Team coordination, approvals, status updates |\n| **Calendar** | Google Calendar, Microsoft 365 | Meeting scheduling, deadline tracking |\n| **Email** | Gmail, Microsoft 365 | Vendor communications, approvals |\n\nSee [CONNECTORS.md](CONNECTORS.md) for the full list of supported integrations.\n\n## Settings\n\nCreate a local settings file at `operations/.claude/settings.local.json` to personalize:\n\n```json\n{\n  \"company\": \"Your Company\",\n  \"team\": \"Operations\",\n  \"reportingCadence\": \"weekly\",\n  \"approvalChain\": [\"Manager\", \"Director\", \"VP\"],\n  \"complianceFrameworks\": [\"SOC 2\", \"ISO 27001\"],\n  \"fiscalYearStart\": \"January\"\n}\n```\n\nThe plugin will ask you for this information interactively if it's not configured.\n", "depth": 2}, {"id": "282", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~ITSM` might mean ServiceNow, Zendesk, or any other service management tool with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (ITSM, project tracker, knowledge base, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Calendar | `~~calendar` | Google Calendar | Microsoft 365 |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Email | `~~email` | Gmail, Microsoft 365 | — |\n| ITSM | `~~ITSM` | ServiceNow | Zendesk, Freshservice, Jira Service Management |\n| Knowledge base | `~~knowledge base` | Notion, Atlassian (Confluence) | Guru, Coda |\n| Project tracker | `~~project tracker` | Asana, Atlassian (Jira) | Linear, monday.com, ClickUp |\n| Procurement | `~~procurement` | — | Coupa, SAP Ariba, Zip |\n| Office suite | `~~office suite` | Microsoft 365 | Google Workspace |\n", "depth": 2}, {"id": "283", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands/status-report.md", "name": "status-report.md", "type": "file", "content": "---\ndescription: Generate a status report with KPIs, risks, and action items\nargument-hint: \"[weekly | monthly | quarterly] [project or team]\"\n---\n\n# /status-report\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a polished status report for leadership or stakeholders. See the **risk-assessment** skill for risk matrix frameworks and severity definitions.\n\n## Usage\n\n```\n/status-report $ARGUMENTS\n```\n\n## Output\n\n```markdown\n## Status Report: [Project/Team] — [Period]\n**Author:** [Name] | **Date:** [Date]\n\n### Executive Summary\n[3-4 sentence overview — what's on track, what needs attention, key wins]\n\n### Overall Status: 🟢 On Track / 🟡 At Risk / 🔴 Off Track\n\n### Key Metrics\n| Metric | Target | Actual | Trend | Status |\n|--------|--------|--------|-------|--------|\n| [KPI] | [Target] | [Actual] | [up/down/flat] | 🟢/🟡/🔴 |\n\n### Accomplishments This Period\n- [Win 1]\n- [Win 2]\n\n### In Progress\n| Item | Owner | Status | ETA | Notes |\n|------|-------|--------|-----|-------|\n| [Item] | [Person] | [Status] | [Date] | [Context] |\n\n### Risks and Issues\n| Risk/Issue | Impact | Mitigation | Owner |\n|------------|--------|------------|-------|\n| [Risk] | [Impact] | [What we're doing] | [Who] |\n\n### Decisions Needed\n| Decision | Context | Deadline | Recommended Action |\n|----------|---------|----------|--------------------|\n| [Decision] | [Why it matters] | [When] | [What I recommend] |\n\n### Next Period Priorities\n1. [Priority 1]\n2. [Priority 2]\n3. [Priority 3]\n```\n\n## If Connectors Available\n\nIf **~~project tracker** is connected:\n- Pull project status, completed items, and upcoming milestones automatically\n- Identify at-risk items and overdue tasks\n\nIf **~~chat** is connected:\n- Scan recent team discussions for decisions and blockers to include\n- Offer to post the finished report to a channel\n\nIf **~~calendar** is connected:\n- Reference key meetings and decisions from the reporting period\n\n## Tips\n\n1. **Lead with the headline** — Busy leaders read the first 3 lines. Make them count.\n2. **Be honest about risks** — Surfacing issues early builds trust. Surprises erode it.\n3. **Make decisions easy** — For each decision needed, provide context and a recommendation.\n", "depth": 3}, {"id": "284", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands/process-doc.md", "name": "process-doc.md", "type": "file", "content": "---\ndescription: Document a business process — flowcharts, RACI, and SOPs\nargument-hint: \"\u003cprocess name or description\u003e\"\n---\n\n# /process-doc\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nDocument a business process as a complete standard operating procedure (SOP).\n\n## Usage\n\n```\n/process-doc $ARGUMENTS\n```\n\n## How It Works\n\nWalk me through the process — describe it, paste existing docs, or just tell me the name and I'll ask the right questions. I'll produce a complete SOP.\n\n## Output\n\n```markdown\n## Process Document: [Process Name]\n**Owner:** [Person/Team] | **Last Updated:** [Date] | **Review Cadence:** [Quarterly/Annually]\n\n### Purpose\n[Why this process exists and what it accomplishes]\n\n### Scope\n[What's included and excluded]\n\n### RACI Matrix\n| Step | Responsible | Accountable | Consulted | Informed |\n|------|------------|-------------|-----------|----------|\n| [Step] | [Who does it] | [Who owns it] | [Who to ask] | [Who to tell] |\n\n### Process Flow\n[ASCII flowchart or step-by-step description]\n\n### Detailed Steps\n\n#### Step 1: [Name]\n- **Who**: [Role]\n- **When**: [Trigger or timing]\n- **How**: [Detailed instructions]\n- **Output**: [What this step produces]\n\n#### Step 2: [Name]\n[Same format]\n\n### Exceptions and Edge Cases\n| Scenario | What to Do |\n|----------|-----------|\n| [Exception] | [How to handle it] |\n\n### Metrics\n| Metric | Target | How to Measure |\n|--------|--------|----------------|\n| [Metric] | [Target] | [Method] |\n\n### Related Documents\n- [Link to related process or policy]\n```\n\n## If Connectors Available\n\nIf **~~knowledge base** is connected:\n- Search for existing process documentation to update rather than duplicate\n- Publish the completed SOP to your wiki\n\nIf **~~project tracker** is connected:\n- Link the process to related projects and workflows\n- Create tasks for process improvement action items\n\n## Tips\n\n1. **Start messy** — You don't need a perfect description. Tell me how it works today and I'll structure it.\n2. **Include the exceptions** — \"Usually we do X, but sometimes Y\" is the most valuable part to document.\n3. **Name the people** — Even if roles change, knowing who does what today helps get the process right.\n", "depth": 3}, {"id": "285", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands/runbook.md", "name": "runbook.md", "type": "file", "content": "---\ndescription: Create or update an operational runbook\nargument-hint: \"\u003cprocess or task name\u003e\"\n---\n\n# /runbook\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCreate a step-by-step operational runbook for a recurring task or procedure.\n\n## Usage\n\n```\n/runbook $ARGUMENTS\n```\n\n## Output\n\n```markdown\n## Runbook: [Task Name]\n**Owner:** [Team/Person] | **Frequency:** [Daily/Weekly/Monthly/As Needed]\n**Last Updated:** [Date] | **Last Run:** [Date]\n\n### Purpose\n[What this runbook accomplishes and when to use it]\n\n### Prerequisites\n- [ ] [Access or permission needed]\n- [ ] [Tool or system required]\n- [ ] [Data or input needed]\n\n### Procedure\n\n#### Step 1: [Name]\n```\n[Exact command, action, or instruction]\n```\n**Expected result:** [What should happen]\n**If it fails:** [What to do]\n\n#### Step 2: [Name]\n```\n[Exact command, action, or instruction]\n```\n**Expected result:** [What should happen]\n**If it fails:** [What to do]\n\n### Verification\n- [ ] [How to confirm the task completed successfully]\n- [ ] [What to check]\n\n### Troubleshooting\n| Symptom | Likely Cause | Fix |\n|---------|-------------|-----|\n| [What you see] | [Why] | [What to do] |\n\n### Rollback\n[How to undo this if something goes wrong]\n\n### Escalation\n| Situation | Contact | Method |\n|-----------|---------|--------|\n| [When to escalate] | [Who] | [How to reach them] |\n\n### History\n| Date | Run By | Notes |\n|------|--------|-------|\n| [Date] | [Person] | [Any issues or observations] |\n```\n\n## If Connectors Available\n\nIf **~~knowledge base** is connected:\n- Search for existing runbooks to update rather than create from scratch\n- Publish the completed runbook to your ops wiki\n\nIf **~~ITSM** is connected:\n- Link the runbook to related incident types and change requests\n- Auto-populate escalation contacts from on-call schedules\n\n## Tips\n\n1. **Be painfully specific** — \"Run the script\" is not a step. \"Run `python sync.py --prod --dry-run` from the ops server\" is.\n2. **Include failure modes** — What can go wrong at each step and what to do about it.\n3. **Test the runbook** — Have someone unfamiliar with the process follow it. Fix where they get stuck.\n", "depth": 3}, {"id": "286", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands/change-request.md", "name": "change-request.md", "type": "file", "content": "---\ndescription: Create a change management request with impact analysis and rollback plan\nargument-hint: \"\u003cchange description\u003e\"\n---\n\n# /change-request\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCreate a structured change request with impact analysis, risk assessment, and rollback plan. See the **change-management** skill for the assess-plan-execute-sustain framework and communication principles.\n\n## Usage\n\n```\n/change-request $ARGUMENTS\n```\n\n## Output\n\n```markdown\n## Change Request: [Title]\n**Requester:** [Name] | **Date:** [Date] | **Priority:** [Critical/High/Medium/Low]\n**Status:** Draft | Pending Approval | Approved | In Progress | Complete\n\n### Description\n[What is changing and why]\n\n### Business Justification\n[Why this change is needed — cost savings, compliance, efficiency, risk reduction]\n\n### Impact Analysis\n| Area | Impact | Details |\n|------|--------|---------|\n| Users | [High/Med/Low/None] | [Who is affected and how] |\n| Systems | [High/Med/Low/None] | [What systems are affected] |\n| Processes | [High/Med/Low/None] | [What workflows change] |\n| Cost | [High/Med/Low/None] | [Budget impact] |\n\n### Risk Assessment\n| Risk | Likelihood | Impact | Mitigation |\n|------|-----------|--------|------------|\n| [Risk] | [H/M/L] | [H/M/L] | [How to mitigate] |\n\n### Implementation Plan\n| Step | Owner | Timeline | Dependencies |\n|------|-------|----------|--------------|\n| [Step] | [Person] | [Date] | [What it depends on] |\n\n### Communication Plan\n| Audience | Message | Channel | Timing |\n|----------|---------|---------|--------|\n| [Who] | [What to tell them] | [How] | [When] |\n\n### Rollback Plan\n[Step-by-step plan to reverse the change if needed]\n- Trigger: [When to roll back]\n- Steps: [How to roll back]\n- Verification: [How to confirm rollback worked]\n\n### Approvals Required\n| Approver | Role | Status |\n|----------|------|--------|\n| [Name] | [Role] | Pending |\n```\n\n## If Connectors Available\n\nIf **~~ITSM** is connected:\n- Create the change request ticket automatically\n- Pull change advisory board schedule and approval workflows\n\nIf **~~project tracker** is connected:\n- Link to related implementation tasks and dependencies\n- Track change progress against milestones\n\nIf **~~chat** is connected:\n- Draft stakeholder notifications for the communication plan\n- Post change updates to the relevant team channels\n\n## Tips\n\n1. **Be specific about impact** — \"Everyone\" is not an impact assessment. \"200 users in the billing team\" is.\n2. **Always have a rollback plan** — Even if you're confident, plan for failure.\n3. **Communicate early** — Surprises create resistance. Previews create buy-in.\n", "depth": 3}, {"id": "287", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands/capacity-plan.md", "name": "capacity-plan.md", "type": "file", "content": "---\ndescription: Plan resource capacity — workload analysis and utilization forecasting\nargument-hint: \"\u003cteam or project scope\u003e\"\n---\n\n# /capacity-plan\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nAnalyze team capacity and plan resource allocation. See the **resource-planning** skill for utilization targets by role type and common planning pitfalls.\n\n## Usage\n\n```\n/capacity-plan $ARGUMENTS\n```\n\n## What I Need From You\n\n- **Team size and roles**: Who do you have?\n- **Current workload**: What are they working on? (Upload from project tracker or describe)\n- **Upcoming work**: What's coming next quarter?\n- **Constraints**: Budget, hiring timeline, skill requirements\n\n## Output\n\n```markdown\n## Capacity Plan: [Team/Project]\n**Period:** [Date range] | **Team Size:** [X]\n\n### Current Utilization\n| Person/Role | Capacity | Allocated | Available | Utilization |\n|-------------|----------|-----------|-----------|-------------|\n| [Name/Role] | [hrs/wk] | [hrs/wk] | [hrs/wk] | [X]% |\n\n### Capacity Summary\n- **Total capacity**: [X] hours/week\n- **Currently allocated**: [X] hours/week ([X]%)\n- **Available**: [X] hours/week ([X]%)\n- **Overallocated**: [X people above 100%]\n\n### Upcoming Demand\n| Project/Initiative | Start | End | Resources Needed | Gap |\n|--------------------|-------|-----|-----------------|-----|\n| [Project] | [Date] | [Date] | [X FTEs] | [Covered/Gap] |\n\n### Bottlenecks\n- [Skill or role that's oversubscribed]\n- [Time period with a crunch]\n\n### Recommendations\n1. [Hire / Contract / Reprioritize / Delay]\n2. [Specific action]\n\n### Scenarios\n| Scenario | Outcome |\n|----------|---------|\n| Do nothing | [What happens] |\n| Hire [X] | [What changes] |\n| Deprioritize [Y] | [What frees up] |\n```\n\n## If Connectors Available\n\nIf **~~project tracker** is connected:\n- Pull current workload and ticket assignments automatically\n- Show upcoming sprint or quarter commitments per person\n\nIf **~~calendar** is connected:\n- Factor in PTO, holidays, and recurring meeting load\n- Calculate actual available hours per person\n\n## Tips\n\n1. **Include all work** — BAU, projects, support, meetings. People aren't 100% available for project work.\n2. **Plan for buffer** — Target 80% utilization. 100% means no room for surprises.\n3. **Update regularly** — Capacity plans go stale fast. Review monthly.\n", "depth": 3}, {"id": "288", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/commands/vendor-review.md", "name": "vendor-review.md", "type": "file", "content": "---\ndescription: Evaluate a vendor — cost analysis, risk assessment, and recommendation\nargument-hint: \"\u003cvendor name or proposal\u003e\"\n---\n\n# /vendor-review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nEvaluate a vendor with structured analysis covering cost, risk, performance, and fit. See the **vendor-management** skill for TCO frameworks, risk assessment criteria, and performance metrics.\n\n## Usage\n\n```\n/vendor-review $ARGUMENTS\n```\n\n## What I Need From You\n\n- **Vendor name**: Who are you evaluating?\n- **Context**: New vendor evaluation, renewal decision, or comparison?\n- **Details**: Contract terms, pricing, proposal document, or current performance data\n\n## Output\n\n```markdown\n## Vendor Review: [Vendor Name]\n**Date:** [Date] | **Type:** [New / Renewal / Comparison]\n\n### Summary\n[2-3 sentence recommendation]\n\n### Cost Analysis\n| Component | Annual Cost | Notes |\n|-----------|-------------|-------|\n| License/subscription | $[X] | [Per seat, flat, usage-based] |\n| Implementation | $[X] | [One-time] |\n| Support/maintenance | $[X] | [Included or add-on] |\n| **Total Year 1** | **$[X]** | |\n| **Total 3-Year** | **$[X]** | |\n\n### Risk Assessment\n| Risk | Likelihood | Impact | Mitigation |\n|------|-----------|--------|------------|\n| [Risk] | High/Med/Low | High/Med/Low | [Mitigation] |\n\n### Strengths\n- [Strength 1]\n- [Strength 2]\n\n### Concerns\n- [Concern 1]\n- [Concern 2]\n\n### Recommendation\n[Proceed / Negotiate / Pass] — [Reasoning]\n\n### Negotiation Points\n- [Leverage point 1]\n- [Leverage point 2]\n```\n\n## If Connectors Available\n\nIf **~~knowledge base** is connected:\n- Search for existing vendor evaluations, contracts, and performance reviews\n- Pull procurement policies and approval thresholds\n\nIf **~~procurement** is connected:\n- Pull current contract terms, spend history, and renewal dates\n- Compare pricing against existing vendor agreements\n\n## Tips\n\n1. **Upload the proposal** — I can extract pricing, terms, and SLAs from vendor documents.\n2. **Compare vendors** — \"Compare Vendor A vs Vendor B\" gets you a side-by-side analysis.\n3. **Include current spend** — For renewals, knowing what you pay now helps evaluate price changes.\n", "depth": 3}, {"id": "289", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/process-optimization", "name": "process-optimization", "type": "dir", "content": "", "depth": 3}, {"id": "290", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/change-management", "name": "change-management", "type": "dir", "content": "", "depth": 3}, {"id": "291", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/resource-planning", "name": "resource-planning", "type": "dir", "content": "", "depth": 3}, {"id": "292", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/compliance-tracking", "name": "compliance-tracking", "type": "dir", "content": "", "depth": 3}, {"id": "293", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/risk-assessment", "name": "risk-assessment", "type": "dir", "content": "", "depth": 3}, {"id": "294", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/vendor-management", "name": "vendor-management", "type": "dir", "content": "", "depth": 3}, {"id": "295", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/process-optimization/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: process-optimization\ndescription: Analyze and improve business processes. Trigger with \"this process is slow\", \"how can we improve\", \"streamline this workflow\", \"too many steps\", \"bottleneck\", or when the user describes an inefficient process they want to fix.\n---\n\n# Process Optimization\n\nAnalyze existing processes and recommend improvements.\n\n## Analysis Framework\n\n### 1. Map Current State\n- Document every step, decision point, and handoff\n- Identify who does what and how long each step takes\n- Note manual steps, approvals, and waiting times\n\n### 2. Identify Waste\n- **Waiting**: Time spent in queues or waiting for approvals\n- **Rework**: Steps that fail and need to be redone\n- **Handoffs**: Each handoff is a potential point of failure or delay\n- **Over-processing**: Steps that add no value\n- **Manual work**: Tasks that could be automated\n\n### 3. Design Future State\n- Eliminate unnecessary steps\n- Automate where possible\n- Reduce handoffs\n- Parallelize independent steps\n- Add checkpoints (not gates)\n\n### 4. Measure Impact\n- Time saved per cycle\n- Error rate reduction\n- Cost savings\n- Employee satisfaction improvement\n\n## Output\n\nProduce a before/after process comparison with specific improvement recommendations, estimated impact, and an implementation plan.\n", "depth": 4}, {"id": "296", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/change-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: change-management\ndescription: Plan and execute organizational or technical changes. Trigger with \"we're changing\", \"rolling out\", \"migration plan\", \"how do we communicate this change\", \"change management plan\", or when the user is planning a change that affects people, processes, or systems.\n---\n\n# Change Management\n\nHelp plan and execute changes that affect people, processes, or technology.\n\n## Change Management Framework\n\n### 1. Assess\n- What is changing?\n- Who is affected?\n- How significant is the change? (Low / Medium / High)\n- What resistance should we expect?\n\n### 2. Plan\n- Communication plan (who, what, when, how)\n- Training plan (what skills are needed, how to deliver)\n- Support plan (help desk, champions, FAQs)\n- Timeline with milestones\n\n### 3. Execute\n- Announce and explain the \"why\"\n- Train and support\n- Monitor adoption\n- Address resistance\n\n### 4. Sustain\n- Measure adoption and effectiveness\n- Reinforce new behaviors\n- Address lingering issues\n- Document lessons learned\n\n## Communication Principles\n\n- Explain the **why** before the **what**\n- Communicate early and often\n- Use multiple channels\n- Acknowledge what's being lost, not just what's being gained\n- Provide a clear path for questions and concerns\n\n## Output\n\nProduce a change management plan with communication timeline, stakeholder impact analysis, training plan, and resistance mitigation strategies.\n", "depth": 4}, {"id": "297", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/resource-planning/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: resource-planning\ndescription: Plan and optimize resource allocation. Trigger with \"resource planning\", \"capacity\", \"utilization\", \"staffing plan\", \"who should work on what\", \"we're stretched thin\", or when the user needs help allocating people, budget, or time across projects and teams.\n---\n\n# Resource Planning\n\nHelp plan and optimize resource allocation across projects and teams.\n\n## Planning Dimensions\n\n### People\n- Available headcount and skills\n- Current allocation and utilization\n- Planned hires and timeline\n- Contractor and vendor capacity\n\n### Budget\n- Operating budget by category\n- Project-specific budgets\n- Variance tracking\n- Forecast vs. actual\n\n### Time\n- Project timelines and dependencies\n- Critical path analysis\n- Buffer and contingency planning\n- Deadline management\n\n## Utilization Targets\n\n| Role Type | Target Utilization | Notes |\n|-----------|-------------------|-------|\n| IC / Specialist | 75-80% | Leave room for reactive work and growth |\n| Manager | 60-70% | Management overhead, meetings, 1:1s |\n| On-call / Support | 50-60% | Interrupt-driven work is unpredictable |\n\n## Common Pitfalls\n\n- Planning to 100% utilization (no buffer for surprises)\n- Ignoring meeting load and context-switching costs\n- Not accounting for vacation, holidays, and sick time\n- Treating all hours as equal (creative work ≠ admin work)\n\n## Output\n\nProduce allocation plans, utilization dashboards, scenario analyses (what if we hire / don't hire / deprioritize), and staffing recommendations.\n", "depth": 4}, {"id": "298", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/compliance-tracking/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: compliance-tracking\ndescription: Track compliance requirements and audit readiness. Trigger with \"compliance\", \"audit prep\", \"SOC 2\", \"ISO 27001\", \"GDPR\", \"regulatory requirement\", or when the user needs help tracking, preparing for, or documenting compliance activities.\n---\n\n# Compliance Tracking\n\nHelp track compliance requirements, prepare for audits, and maintain regulatory readiness.\n\n## Common Frameworks\n\n| Framework | Focus | Key Requirements |\n|-----------|-------|-----------------|\n| SOC 2 | Service organizations | Security, availability, processing integrity, confidentiality, privacy |\n| ISO 27001 | Information security | Risk assessment, security controls, continuous improvement |\n| GDPR | Data privacy (EU) | Consent, data rights, breach notification, DPO |\n| HIPAA | Healthcare data (US) | PHI protection, access controls, audit trails |\n| PCI DSS | Payment card data | Encryption, access control, vulnerability management |\n\n## Compliance Tracking Components\n\n### Control Inventory\n- Map controls to framework requirements\n- Document control owners and evidence\n- Track control effectiveness\n\n### Audit Calendar\n- Upcoming audit dates and deadlines\n- Evidence collection timelines\n- Remediation deadlines\n\n### Evidence Management\n- What evidence is needed for each control\n- Where evidence is stored\n- When evidence was last collected\n\n### Gap Analysis\n- Requirements vs. current state\n- Prioritized remediation plan\n- Timeline to compliance\n\n## Output\n\nProduce compliance status dashboards, gap analyses, audit prep checklists, and evidence collection plans.\n", "depth": 4}, {"id": "299", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/risk-assessment/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: risk-assessment\ndescription: Identify, assess, and mitigate operational risks. Trigger with \"what are the risks\", \"risk assessment\", \"risk register\", \"what could go wrong\", or when the user is evaluating risks associated with a project, vendor, process, or decision.\n---\n\n# Risk Assessment\n\nSystematically identify, assess, and plan mitigations for operational risks.\n\n## Risk Assessment Matrix\n\n| | Low Impact | Medium Impact | High Impact |\n|---|-----------|---------------|-------------|\n| **High Likelihood** | Medium | High | Critical |\n| **Medium Likelihood** | Low | Medium | High |\n| **Low Likelihood** | Low | Low | Medium |\n\n## Risk Categories\n\n- **Operational**: Process failures, staffing gaps, system outages\n- **Financial**: Budget overruns, vendor cost increases, revenue impact\n- **Compliance**: Regulatory violations, audit findings, policy breaches\n- **Strategic**: Market changes, competitive threats, technology shifts\n- **Reputational**: Customer impact, public perception, partner relationships\n- **Security**: Data breaches, access control failures, third-party vulnerabilities\n\n## Risk Register Format\n\nFor each risk, document:\n- **Description**: What could happen\n- **Likelihood**: High / Medium / Low\n- **Impact**: High / Medium / Low\n- **Risk Level**: Critical / High / Medium / Low\n- **Mitigation**: What we're doing to reduce likelihood or impact\n- **Owner**: Who is responsible for managing this risk\n- **Status**: Open / Mitigated / Accepted / Closed\n\n## Output\n\nProduce a prioritized risk register with specific, actionable mitigations. Focus on risks that are controllable and material.\n", "depth": 4}, {"id": "300", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/operations/skills/vendor-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: vendor-management\ndescription: Evaluate, compare, and manage vendor relationships. Trigger with \"evaluate this vendor\", \"compare vendors\", \"vendor review\", \"should we renew\", \"RFP\", or when the user is making procurement or vendor decisions.\n---\n\n# Vendor Management\n\nHelp evaluate, compare, and manage vendor relationships.\n\n## Evaluation Framework\n\n### Cost Analysis\n- Total cost of ownership (not just license fees)\n- Implementation and migration costs\n- Training and onboarding costs\n- Ongoing support and maintenance\n- Exit costs (data migration, contract termination)\n\n### Risk Assessment\n- Vendor financial stability\n- Security and compliance posture\n- Concentration risk (single vendor dependency)\n- Contract lock-in and exit terms\n- Business continuity and disaster recovery\n\n### Performance Metrics\n- SLA compliance\n- Support response times\n- Uptime and reliability\n- Feature delivery cadence\n- Customer satisfaction\n\n## Comparison Matrix\n\nWhen comparing vendors, produce a side-by-side matrix covering: pricing, features, integrations, security, support, contract terms, and references.\n\n## Output\n\nProvide a clear recommendation with supporting evidence. Always flag risks and negotiation leverage points.\n", "depth": 4}, {"id": "301", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "302", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "303", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/README.md", "name": "README.md", "type": "file", "content": "# Product Management Plugin\n\nA product management plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Covers the full PM workflow: writing feature specs, managing roadmaps, communicating with stakeholders, synthesizing user research, analyzing competitors, and tracking product metrics.\n\n## Installation\n\n```\nclaude plugins add knowledge-work-plugins/product-management\n```\n\n## What It Does\n\nThis plugin gives you an AI-powered product management partner that can help with:\n\n- **Feature Specs \u0026 PRDs** — Generate structured product requirements documents from a problem statement or feature idea. Includes user stories, requirements prioritization, success metrics, and scope management.\n- **Roadmap Planning** — Create, update, and reprioritize your product roadmap. Supports Now/Next/Later, quarterly themes, and OKR-aligned formats with dependency mapping.\n- **Stakeholder Updates** — Generate status updates tailored to your audience (executives, engineering, customers). Pulls context from connected tools to save you the weekly update grind.\n- **User Research Synthesis** — Turn interview notes, survey data, and support tickets into structured insights. Identifies themes, builds personas, and surfaces opportunity areas with supporting evidence.\n- **Competitive Analysis** — Research competitors and generate briefs with feature comparisons, positioning analysis, and strategic implications.\n- **Metrics Review** — Analyze product metrics, identify trends, compare against targets, and surface actionable insights.\n\n## Commands\n\n| Command | What It Does |\n|---|---|\n| `/write-spec` | Write a feature spec or PRD from a problem statement |\n| `/roadmap-update` | Update, create, or reprioritize your roadmap |\n| `/stakeholder-update` | Generate a stakeholder update (weekly, monthly, launch) |\n| `/synthesize-research` | Synthesize user research from interviews, surveys, and tickets |\n| `/competitive-brief` | Create a competitive analysis brief |\n| `/metrics-review` | Review and analyze product metrics |\n\n## Skills\n\n| Skill | What It Covers |\n|---|---|\n| `feature-spec` | PRD structure, user stories, requirements categorization, acceptance criteria |\n| `roadmap-management` | Prioritization frameworks (RICE, MoSCoW), roadmap formats, dependency mapping |\n| `stakeholder-comms` | Update templates by audience, risk communication, decision documentation |\n| `user-research-synthesis` | Thematic analysis, affinity mapping, persona development, opportunity sizing |\n| `competitive-analysis` | Feature comparison matrices, positioning analysis, win/loss analysis |\n| `metrics-tracking` | Product metrics hierarchy, goal setting (OKRs), dashboard design, review cadences |\n\n## Example Workflows\n\n### Writing a PRD\n\n```\nYou: /write-spec\nClaude: What feature or problem are you speccing out?\nYou: We need to add SSO support for enterprise customers\nClaude: [Asks about target users, constraints, success metrics]\nClaude: [Generates full PRD with problem statement, user stories, requirements, success metrics, open questions]\n```\n\n### Preparing a Stakeholder Update\n\n```\nYou: /stakeholder-update\nClaude: What type of update? (weekly, monthly, launch, ad-hoc)\nYou: Weekly update for the exec team\nClaude: [Pulls context from project management tool, chat, and docs]\nClaude: [Generates executive summary with progress, decisions, risks, and next milestones]\n```\n\n### Synthesizing User Research\n\n```\nYou: /synthesize-research\nClaude: What research do you want to synthesize? You can paste interview notes, upload files, or I can pull from connected sources.\nYou: [Pastes 8 interview transcripts]\nClaude: [Identifies themes, patterns, and insights across interviews]\nClaude: [Generates synthesis with key findings, personas, opportunity areas, and evidence]\n```\n\n### Competitive Analysis\n\n```\nYou: /competitive-brief\nClaude: Which competitor(s) or feature area do you want to analyze?\nYou: Compare our onboarding flow to [competitor A] and [competitor B]\nClaude: [Researches competitor onboarding approaches]\nClaude: [Generates brief with feature comparison, strengths/weaknesses, and strategic implications]\n```\n\n## Data Sources\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your project management and communication tools for the best experience. Without them, provide context manually.\n\n**Included MCP connections:**\n- Chat (Slack) for team context and stakeholder threads\n- Project tracker (Linear, Asana, monday.com, ClickUp, Atlassian) for roadmap integration, ticket context, and status tracking\n- Knowledge base (Notion) for existing specs, research, and meeting notes\n- Design (Figma) for design context and handoff\n- Product analytics (Amplitude, Pendo) for usage data, metrics, and behavioral analysis\n- User feedback (Intercom) for support tickets, feature requests, and user conversations\n- Meeting transcription (Fireflies) for meeting notes and discussion context\n\n**Additional options:**\n- See [CONNECTORS.md](CONNECTORS.md) for alternative tools in each category\n", "depth": 2}, {"id": "304", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~project tracker` might mean Linear, Asana, Jira, or any other tracker with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (project tracker, design, product analytics, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Calendar | `~~calendar` | Google Calendar | Microsoft 365 |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Competitive intelligence | `~~competitive intelligence` | Similarweb | Crayon, Klue |\n| Design | `~~design` | Figma | Sketch, Adobe XD |\n| Email | `~~email` | Gmail | Microsoft 365 |\n| Knowledge base | `~~knowledge base` | Notion | Confluence, Guru, Coda |\n| Meeting transcription | `~~meeting transcription` | Fireflies | Gong, Dovetail, Otter.ai |\n| Product analytics | `~~product analytics` | Amplitude, Pendo | Mixpanel, Heap, FullStory |\n| Project tracker | `~~project tracker` | Linear, Asana, monday.com, ClickUp, Atlassian (Jira/Confluence) | Shortcut, Basecamp |\n| User feedback | `~~user feedback` | Intercom | Productboard, Canny, UserVoice |\n", "depth": 2}, {"id": "305", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/metrics-review.md", "name": "metrics-review.md", "type": "file", "content": "---\ndescription: Review and analyze product metrics with trend analysis and actionable insights\nargument-hint: \"\u003ctime period or metric focus\u003e\"\n---\n\n# Metrics Review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nReview and analyze product metrics, identify trends, and surface actionable insights.\n\n## Usage\n\n```\n/metrics-review $ARGUMENTS\n```\n\n## Workflow\n\n### 1. Gather Metrics Data\n\nIf **~~product analytics** is connected:\n- Pull key product metrics for the relevant time period\n- Get comparison data (previous period, same period last year, targets)\n- Pull segment breakdowns if available\n\nIf no analytics tool is connected, ask the user to provide:\n- The metrics and their values (paste a table, screenshot, or describe)\n- Comparison data (previous period, targets)\n- Any context on recent changes (launches, incidents, seasonality)\n\nAsk the user:\n- What time period to review? (last week, last month, last quarter)\n- What metrics to focus on? Or should we review the full product metrics suite?\n- Are there specific targets or goals to compare against?\n- Any known events that might explain changes (launches, outages, marketing campaigns, seasonality)?\n\n### 2. Organize the Metrics\n\nStructure the review using the metrics hierarchy from the **metrics-tracking** skill: North Star metric at the top, L1 health indicators (acquisition, activation, engagement, retention, revenue, satisfaction), and L2 diagnostic metrics for drill-down.\n\nIf the user has not defined their metrics hierarchy, help them identify their North Star and key L1 metrics before proceeding.\n\n### 3. Analyze Trends\n\nFor each key metric:\n- **Current value**: What is the metric today?\n- **Trend**: Up, down, or flat compared to previous period? Over what timeframe?\n- **vs Target**: How does it compare to the goal or target?\n- **Rate of change**: Is the trend accelerating or decelerating?\n- **Anomalies**: Any sudden changes, spikes, or drops?\n\nIdentify correlations:\n- Do changes in one metric correlate with changes in another?\n- Are there leading indicators that predict lagging metric changes?\n- Do segment breakdowns reveal that an aggregate trend is driven by a specific cohort?\n\n### 4. Generate the Review\n\n#### Summary\n2-3 sentences: overall product health, most notable changes, key callout.\n\n#### Metric Scorecard\nTable format for quick scanning:\n\n| Metric | Current | Previous | Change | Target | Status |\n|--------|---------|----------|--------|--------|--------|\n| [Metric] | [Value] | [Value] | [+/- %] | [Target] | [On track / At risk / Miss] |\n\n#### Trend Analysis\nFor each metric worth discussing:\n- What happened and how significant is the change\n- Why it likely happened (attribution based on known events, correlated metrics, segment analysis)\n- Whether this is a one-time event or a sustained trend\n\n#### Bright Spots\nWhat is going well:\n- Metrics beating targets\n- Positive trends to sustain\n- Segments or features showing strong performance\n\n#### Areas of Concern\nWhat needs attention:\n- Metrics missing targets or trending negatively\n- Early warning signals before they become problems\n- Metrics where we lack visibility or understanding\n\n#### Recommended Actions\nSpecific next steps based on the analysis:\n- Investigations to run (dig deeper into a concerning trend)\n- Experiments to launch (test hypotheses about what could improve a metric)\n- Investments to make (double down on what is working)\n- Alerts to set (monitor a metric more closely)\n\n#### Context and Caveats\n- Known data quality issues\n- Events that affect comparability (outages, holidays, launches)\n- Metrics we should be tracking but are not yet\n\n### 5. Follow Up\n\nAfter generating the review:\n- Ask if any metric needs deeper investigation\n- Offer to create a dashboard spec for ongoing monitoring\n- Offer to draft experiment proposals for areas of concern\n- Offer to set up a metrics review template for recurring use\n\n## Output Format\n\nUse tables for the scorecard. Use clear status indicators. Keep the summary tight — the reader should get the essential story in 30 seconds.\n\n## Tips\n\n- Start with the \"so what\" — what is the most important thing in this metrics review? Lead with that.\n- Absolute numbers without context are useless. Always show comparisons (vs previous period, vs target, vs benchmark).\n- Be careful about attribution. Correlation is not causation. If a metric moved, acknowledge uncertainty about why.\n- Segment analysis often reveals that an aggregate metric masks important differences. A flat overall number might hide one segment growing and another shrinking.\n- Not all metric movements matter. Small fluctuations are noise. Focus attention on meaningful changes.\n- If a metric is missing its target, do not just report the miss — recommend what to do about it.\n- Metrics reviews should drive decisions. If the review does not lead to at least one action, it was not useful.\n", "depth": 3}, {"id": "306", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/stakeholder-update.md", "name": "stakeholder-update.md", "type": "file", "content": "---\ndescription: Generate a stakeholder update tailored to audience and cadence\nargument-hint: \"\u003cupdate type and audience\u003e\"\n---\n\n# Stakeholder Update\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a stakeholder update tailored to the audience and cadence.\n\n## Usage\n\n```\n/stakeholder-update $ARGUMENTS\n```\n\n## Workflow\n\n### 1. Determine Update Type\n\nAsk the user what kind of update:\n- **Weekly**: Regular cadence update on progress, blockers, and next steps\n- **Monthly**: Higher-level summary with trends, milestones, and strategic alignment\n- **Launch**: Announcement of a feature or product launch with details and impact\n- **Ad-hoc**: One-off update for a specific situation (escalation, pivot, major decision)\n\n### 2. Determine Audience\n\nAsk who the update is for:\n- **Executives / leadership**: High-level, outcome-focused, strategic framing, brief\n- **Engineering team**: Technical detail, implementation context, blockers, decisions needed\n- **Cross-functional partners**: Context-appropriate detail, focus on shared goals and dependencies\n- **Customers / external**: Benefits-focused, clear timelines, no internal jargon\n- **Board**: Metrics-driven, strategic, risk-focused, very concise\n\n### 3. Pull Context from Connected Tools\n\nIf **~~project tracker** is connected:\n- Pull status of roadmap items and milestones\n- Identify completed items since last update\n- Surface items that are at risk or blocked\n- Pull sprint or iteration progress\n\nIf **~~chat** is connected:\n- Search for relevant team discussions and decisions\n- Find blockers or issues raised in channels\n- Identify key decisions made asynchronously\n\nIf **~~meeting transcription** is connected:\n- Pull recent meeting notes and discussion summaries\n- Find decisions and action items from relevant meetings\n\nIf **~~knowledge base** is connected:\n- Search for recent meeting notes\n- Find decision documents or design reviews\n\nIf no tools are connected, ask the user to provide:\n- What was accomplished since the last update\n- Current blockers or risks\n- Key decisions made or needed\n- What is coming next\n\n### 4. Generate the Update\n\nStructure the update for the target audience. See the **stakeholder-comms** skill for detailed templates, G/Y/R status definitions, and the ROAM risk communication framework.\n\n**For executives**: TL;DR, status color (G/Y/R), key progress tied to goals, decisions made, risks with mitigation, specific asks, and next milestones. Keep it under 300 words.\n\n**For engineering**: What shipped (with links), what is in progress (with owners), blockers, decisions needed (with options and recommendation), and what is coming next.\n\n**For cross-functional partners**: What is coming that affects them, what you need from them (with deadlines), decisions that impact their team, and areas open for input.\n\n**For customers**: What is new (framed as benefits), what is coming soon, known issues with workarounds, and how to provide feedback. No internal jargon.\n\n**For launch announcements**: What launched, why it matters, key details (scope, availability, limitations), success metrics, rollout plan, and feedback channels.\n\n### 5. Review and Deliver\n\nAfter generating the update:\n- Ask if the user wants to adjust tone, detail level, or emphasis\n- Offer to format for the delivery channel (email, chat post, doc, slides)\n- If **~~chat** is connected, offer to draft the message for sending\n\n## Output Format\n\nKeep updates scannable. Use bold for key points, bullets for lists. Executive updates should be under 300 words. Engineering updates can be longer but should still be structured for skimming.\n\n## Tips\n\n- The most common mistake in stakeholder updates is burying the lead. Start with the most important thing.\n- Status colors (Green/Yellow/Red) should reflect reality, not optimism. Yellow is not a failure — it is good risk communication.\n- Asks should be specific and actionable. \"We need help\" is not an ask. \"We need a decision on X by Friday\" is.\n- For executives, frame everything in terms of outcomes and goals, not activities and tasks.\n- If there is bad news, lead with it. Do not hide it after good news.\n- Match the length to the audience's attention. Executives get a few bullets. Engineering gets the details they need.\n", "depth": 3}, {"id": "307", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/roadmap-update.md", "name": "roadmap-update.md", "type": "file", "content": "---\ndescription: Update, create, or reprioritize your product roadmap\nargument-hint: \"\u003cupdate description\u003e\"\n---\n\n# Roadmap Update\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nUpdate, create, or reprioritize a product roadmap.\n\n## Usage\n\n```\n/roadmap-update $ARGUMENTS\n```\n\n## Workflow\n\n### 1. Understand Current State\n\nIf **~~project tracker** is connected:\n- Pull current roadmap items with their statuses, assignees, and dates\n- Identify items that are overdue, at risk, or recently completed\n- Surface any items without clear owners or dates\n\nIf no project management tool is connected:\n- Ask the user to describe their current roadmap or paste/upload it\n- Accept any format: list, table, spreadsheet, screenshot, or prose description\n\n### 2. Determine the Operation\n\nAsk what the user wants to do:\n\n**Add item**: New feature, initiative, or work item to the roadmap\n- Gather: name, description, priority, estimated effort, target timeframe, owner, dependencies\n- Suggest where it fits based on current priorities and capacity\n\n**Update status**: Change status of existing items\n- Options: not started, in progress, at risk, blocked, completed, cut\n- For \"at risk\" or \"blocked\": ask for the blocker and mitigation plan\n\n**Reprioritize**: Change the order or priority of items\n- Ask what changed (new information, strategy shift, resource change, customer feedback)\n- Apply a prioritization framework if helpful — see the **roadmap-management** skill for RICE, MoSCoW, ICE, and value-vs-effort frameworks\n- Show before/after comparison\n\n**Move timeline**: Shift dates for items\n- Ask why (scope change, dependency slip, resource constraint)\n- Identify downstream impacts on dependent items\n- Flag items that move past hard deadlines\n\n**Create new roadmap**: Build a roadmap from scratch\n- Ask about timeframe (quarter, half, year)\n- Ask about format preference (Now/Next/Later, quarterly columns, OKR-aligned)\n- Gather the list of initiatives to include\n\n### 3. Generate Roadmap Summary\n\nProduce a roadmap view with:\n\n#### Status Overview\nQuick summary: X items in progress, Y completed this period, Z at risk.\n\n#### Roadmap Items\nFor each item, show:\n- Name and one-line description\n- Status indicator (on track / at risk / blocked / completed / not started)\n- Target timeframe or date\n- Owner\n- Key dependencies\n\nGroup items by:\n- Timeframe (Now / Next / Later) or quarter, depending on format\n- Or by theme/goal if the user prefers\n\n#### Risks and Dependencies\n- Items that are blocked or at risk, with details\n- Cross-team dependencies and their status\n- Items approaching hard deadlines\n\n#### Changes This Update\nIf this is an update to an existing roadmap, summarize what changed:\n- Items added, removed, or reprioritized\n- Timeline shifts\n- Status changes\n\n### 4. Follow Up\n\nAfter generating the roadmap:\n- Offer to format for a specific audience (executive summary, engineering detail, customer-facing)\n- Offer to draft communication about roadmap changes\n- If project management tool is connected, offer to update ticket statuses\n\n## Output Format\n\nUse a clear, scannable format. Tables work well for roadmap items. Use text status labels: **Done**, **On Track**, **At Risk**, **Blocked**, **Not Started**.\n\n## Tips\n\n- A roadmap is a communication tool, not a project plan. Keep it at the right altitude — themes and outcomes, not tasks.\n- When reprioritizing, always ask what changed. Priority shifts should be driven by new information, not whim.\n- Flag capacity issues early. If the roadmap has more work than the team can handle, say so.\n- Dependencies are the biggest risk to roadmaps. Surface them explicitly.\n- If the user asks to add something, always ask what comes off or moves. Roadmaps are zero-sum against capacity.\n", "depth": 3}, {"id": "308", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/sprint-planning.md", "name": "sprint-planning.md", "type": "file", "content": "---\ndescription: Plan a sprint — scope work, estimate capacity, set goals, and draft a sprint plan\nargument-hint: \"[sprint name or date range]\"\n---\n\n# /sprint-planning\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nPlan a sprint by scoping work, estimating capacity, and setting clear goals.\n\n## Usage\n\n```\n/sprint-planning $ARGUMENTS\n```\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    SPRINT PLANNING                                 │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Define sprint goals and success criteria                     │\n│  ✓ Estimate team capacity (accounting for PTO, meetings)        │\n│  ✓ Scope and prioritize backlog items                           │\n│  ✓ Identify dependencies and risks                              │\n│  ✓ Generate sprint plan document                                │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Project tracker: Pull backlog, create sprint, assign items   │\n│  + Calendar: Account for PTO and meetings in capacity           │\n│  + Chat: Share sprint plan with the team                        │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## What I Need From You\n\n- **Team**: Who's on the team and their availability this sprint?\n- **Sprint length**: How many days/weeks?\n- **Backlog**: What's prioritized? (Pull from tracker, paste, or describe)\n- **Carryover**: Anything unfinished from last sprint?\n- **Dependencies**: Anything blocked on other teams?\n\n## Output\n\n```markdown\n## Sprint Plan: [Sprint Name]\n**Dates:** [Start] — [End] | **Team:** [X] engineers\n**Sprint Goal:** [One clear sentence about what success looks like]\n\n### Capacity\n| Person | Available Days | Allocation | Notes |\n|--------|---------------|------------|-------|\n| [Name] | [X] of [Y] | [X] points/hours | [PTO, on-call, etc.] |\n| **Total** | **[X]** | **[X] points** | |\n\n### Sprint Backlog\n| Priority | Item | Estimate | Owner | Dependencies |\n|----------|------|----------|-------|--------------|\n| P0 | [Must ship] | [X] pts | [Person] | [None / Blocked by X] |\n| P1 | [Should ship] | [X] pts | [Person] | [None] |\n| P2 | [Stretch] | [X] pts | [Person] | [None] |\n\n### Planned Capacity: [X] points | Sprint Load: [X] points ([X]% of capacity)\n\n### Risks\n| Risk | Impact | Mitigation |\n|------|--------|------------|\n| [Risk] | [What happens] | [What to do] |\n\n### Definition of Done\n- [ ] Code reviewed and merged\n- [ ] Tests passing\n- [ ] Documentation updated (if applicable)\n- [ ] Product sign-off\n\n### Key Dates\n| Date | Event |\n|------|-------|\n| [Date] | Sprint start |\n| [Date] | Mid-sprint check-in |\n| [Date] | Sprint end / Demo |\n| [Date] | Retro |\n```\n\n## Tips\n\n1. **Leave buffer** — Plan to 70-80% capacity. You will get interrupts.\n2. **One clear sprint goal** — If you can't state it in one sentence, the sprint is unfocused.\n3. **Identify stretch items** — Know what to cut if things take longer than expected.\n4. **Carry over honestly** — If something didn't ship, understand why before re-committing.\n", "depth": 3}, {"id": "309", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/competitive-brief.md", "name": "competitive-brief.md", "type": "file", "content": "---\ndescription: Create a competitive analysis brief for one or more competitors or a feature area\nargument-hint: \"\u003ccompetitor or feature area\u003e\"\n---\n\n# Competitive Brief\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCreate a competitive analysis brief for one or more competitors or a feature area.\n\n## Usage\n\n```\n/competitive-brief $ARGUMENTS\n```\n\n## Workflow\n\n### 1. Scope the Analysis\n\nAsk the user:\n- **Competitor(s)**: Which specific competitor(s) to analyze? Or a feature area to compare across competitors?\n- **Focus**: Full product comparison, specific feature area, pricing/packaging, go-to-market, or positioning?\n- **Context**: What decision will this inform? (product strategy, sales enablement, investor/board materials, feature prioritization)\n\n### 2. Research\n\n**Via web search**:\n- Product pages and feature lists\n- Pricing pages and packaging\n- Recent product launches, blog posts, and changelogs\n- Press coverage and analyst reports\n- Customer reviews and ratings (G2, Capterra, TrustRadius)\n- Job postings (signal of strategic direction)\n- Social media and community discussions\n\nIf **~~knowledge base** is connected:\n- Search for existing competitive analysis documents\n- Find win/loss reports or sales battle cards\n- Pull prior competitive research\n\nIf **~~chat** is connected:\n- Search for competitive mentions in sales or product channels\n- Find recent deal feedback involving competitors\n\n### 3. Generate the Brief\n\n#### Competitor Overview\nFor each competitor:\n- Company summary: founding, size, funding/revenue if public, target market\n- Product positioning: how they describe themselves, who they target\n- Recent momentum: launches, funding, partnerships, customer wins\n\n#### Feature Comparison\nCompare capabilities across key areas relevant to the analysis. See the **competitive-analysis** skill for rating frameworks and comparison matrix templates.\n\n#### Positioning Analysis\nAnalyze how each competitor positions themselves — target customer, category claim, key differentiator, and value proposition. See the **competitive-analysis** skill for positioning analysis frameworks.\n\n#### Strengths and Weaknesses\nFor each competitor:\n- **Strengths**: Where they genuinely excel. What customers praise.\n- **Weaknesses**: Where they fall short. What customers complain about.\n- Be honest and evidence-based — do not dismiss competitors or inflate their weaknesses.\n\n#### Opportunities\nBased on the analysis:\n- Where are there gaps in competitor offerings we could exploit?\n- What are customers asking for that no one provides well?\n- Where are competitors making bets we disagree with?\n- What market shifts could advantage our approach?\n\n#### Threats\n- Where are competitors investing heavily?\n- What competitive moves could disrupt our position?\n- Where are we most vulnerable?\n- What would a \"nightmare scenario\" competitive move look like?\n\n#### Strategic Implications\nTie the analysis back to product strategy:\n- What should we build, accelerate, or deprioritize based on this analysis?\n- Where should we differentiate vs. achieve parity?\n- How should we adjust positioning or messaging?\n- What should we monitor going forward?\n\n### 4. Follow Up\n\nAfter generating the brief:\n- Ask if the user wants to dive deeper on any section\n- Offer to create a one-page summary for executives\n- Offer to create sales battle cards for competitive deals\n- Offer to draft a \"how to win against [competitor]\" guide\n- Offer to set up a monitoring plan for competitive moves\n\n## Output Format\n\nUse tables for feature comparisons. Use clear headers for each section. Keep the strategic implications section concise and actionable — this is where the value is for the reader.\n\n## Tips\n\n- Be honest about competitor strengths. Dismissing competitors makes the analysis useless.\n- Focus on what matters to customers, not what matters to product teams. Customers do not care about architecture elegance.\n- Pricing is hard to compare fairly. Note the caveats (different packaging, usage-based vs seat-based, enterprise custom pricing).\n- Job postings are underrated competitive intelligence. A competitor hiring ML engineers signals a strategic direction.\n- Customer reviews are gold. They reveal what real users love and hate, unfiltered by marketing.\n- The most valuable part of competitive analysis is the \"so what\" — the strategic implications. Do not skip this.\n- Competitive analysis has a shelf life. Note the date and flag areas that change quickly.\n", "depth": 3}, {"id": "310", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/synthesize-research.md", "name": "synthesize-research.md", "type": "file", "content": "---\ndescription: Synthesize user research from interviews, surveys, and feedback into structured insights\nargument-hint: \"\u003cresearch topic or question\u003e\"\n---\n\n# Synthesize Research\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nSynthesize user research from multiple sources into structured insights and recommendations.\n\n## Usage\n\n```\n/synthesize-research $ARGUMENTS\n```\n\n## Workflow\n\n### 1. Gather Research Inputs\n\nAccept research from any combination of:\n- **Pasted text**: Interview notes, transcripts, survey responses, feedback\n- **Uploaded files**: Research documents, spreadsheets, recordings summaries\n- **~~knowledge base** (if connected): Search for research documents, interview notes, survey results\n- **~~user feedback** (if connected): Pull recent support tickets, feature requests, bug reports\n- **~~product analytics** (if connected): Pull usage data, funnel metrics, behavioral data\n- **~~meeting transcription** (if connected): Pull interview recordings, meeting summaries, and discussion notes\n\nAsk the user what they have:\n- What type of research? (interviews, surveys, usability tests, analytics, support tickets, sales call notes)\n- How many sources / participants?\n- Is there a specific question or hypothesis they are investigating?\n- What decisions will this research inform?\n\n### 2. Process the Research\n\nFor each source, extract:\n- **Key observations**: What did users say, do, or experience?\n- **Quotes**: Verbatim quotes that illustrate important points\n- **Behaviors**: What users actually did (vs what they said they do)\n- **Pain points**: Frustrations, workarounds, and unmet needs\n- **Positive signals**: What works well, moments of delight\n- **Context**: User segment, use case, experience level\n\n### 3. Identify Themes and Patterns\n\nApply thematic analysis — see the **user-research-synthesis** skill for detailed methodology including affinity mapping and triangulation techniques.\n\nGroup observations into themes, count frequency across participants, and assess impact severity. Note contradictions and surprises.\n\nCreate a priority matrix:\n- **High frequency + High impact**: Top priority findings\n- **Low frequency + High impact**: Important for specific segments\n- **High frequency + Low impact**: Quality-of-life improvements\n- **Low frequency + Low impact**: Note but deprioritize\n\n### 4. Generate the Synthesis\n\nProduce a structured research synthesis:\n\n#### Research Overview\n- Methodology: what types of research, how many participants/sources\n- Research question(s): what we set out to learn\n- Timeframe: when the research was conducted\n\n#### Key Findings\nFor each major finding (aim for 5-8):\n- **Finding statement**: One clear sentence describing the insight\n- **Evidence**: Supporting quotes, data points, or observations (with source attribution)\n- **Frequency**: How many participants/sources support this finding\n- **Impact**: How significantly this affects the user experience or business\n- **Confidence level**: High (strong evidence), Medium (suggestive), Low (early signal)\n\nOrder findings by priority (frequency x impact).\n\n#### User Segments / Personas\nIf the research reveals distinct user segments:\n- Segment name and description\n- Key characteristics and behaviors\n- Unique needs and pain points\n- Size estimate if data is available\n\n#### Opportunity Areas\nBased on the findings, identify opportunity areas:\n- What user needs are unmet or underserved\n- Where do current solutions fall short\n- What new capabilities would unlock value\n- Prioritized by potential impact\n\n#### Recommendations\nSpecific, actionable recommendations:\n- What to build, change, or investigate further\n- Tied back to specific findings\n- Prioritized by impact and feasibility\n\n#### Open Questions\nWhat the research did not answer:\n- Gaps in understanding\n- Areas needing further investigation\n- Suggested follow-up research methods\n\n### 5. Review and Extend\n\nAfter generating the synthesis:\n- Ask if any findings need more detail or different framing\n- Offer to generate specific artifacts: persona documents, opportunity maps, research presentations\n- Offer to create follow-up research plans for open questions\n- Offer to draft product implications (how findings should influence the roadmap)\n\n## Output Format\n\nUse clear headers and structured formatting. Each finding should stand on its own — a reader should be able to read any single finding and understand it without reading the rest.\n\n## Tips\n\n- Let the data speak. Do not force findings into a predetermined narrative.\n- Distinguish between what users say and what they do. Behavioral data is stronger than stated preferences.\n- Quotes are powerful evidence. Include them generously, with attribution to participant type (not name).\n- Be explicit about confidence levels. A finding from 2 interviews is a hypothesis, not a conclusion.\n- Contradictions in the data are interesting, not inconvenient. They often reveal distinct user segments.\n- Recommendations should be specific enough to act on. \"Improve onboarding\" is not actionable. \"Add a progress indicator to the setup flow\" is.\n- Resist the temptation to synthesize too many themes. 5-8 strong findings are better than 20 weak ones.\n", "depth": 3}, {"id": "311", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/commands/write-spec.md", "name": "write-spec.md", "type": "file", "content": "---\ndescription: Write a feature spec or PRD from a problem statement or feature idea\nargument-hint: \"\u003cfeature or problem statement\u003e\"\n---\n\n# Write Spec\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nWrite a feature specification or product requirements document (PRD).\n\n## Usage\n\n```\n/write-spec $ARGUMENTS\n```\n\n## Workflow\n\n### 1. Understand the Feature\n\nAsk the user what they want to spec. Accept any of:\n- A feature name (\"SSO support\")\n- A problem statement (\"Enterprise customers keep asking for centralized auth\")\n- A user request (\"Users want to export their data as CSV\")\n- A vague idea (\"We should do something about onboarding drop-off\")\n\n### 2. Gather Context\n\nAsk the user for the following. Be conversational — do not dump all questions at once. Ask the most important ones first and fill in gaps as you go:\n\n- **User problem**: What problem does this solve? Who experiences it?\n- **Target users**: Which user segment(s) does this serve?\n- **Success metrics**: How will we know this worked?\n- **Constraints**: Technical constraints, timeline, regulatory requirements, dependencies\n- **Prior art**: Has this been attempted before? Are there existing solutions?\n\n### 3. Pull Context from Connected Tools\n\nIf **~~project tracker** is connected:\n- Search for related tickets, epics, or features\n- Pull in any existing requirements or acceptance criteria\n- Identify dependencies on other work items\n\nIf **~~knowledge base** is connected:\n- Search for related research documents, prior specs, or design docs\n- Pull in relevant user research findings\n- Find related meeting notes or decision records\n\nIf **~~design** is connected:\n- Pull related mockups, wireframes, or design explorations\n- Search for design system components relevant to the feature\n\nIf these tools are not connected, work entirely from what the user provides. Do not ask the user to connect tools — just proceed with available information.\n\n### 4. Generate the PRD\n\nProduce a structured PRD with these sections. See the **feature-spec** skill for detailed guidance on user stories, requirements categorization, acceptance criteria, and success metrics.\n\n- **Problem Statement**: The user problem, who is affected, and impact of not solving it (2-3 sentences)\n- **Goals**: 3-5 specific, measurable outcomes tied to user or business metrics\n- **Non-Goals**: 3-5 things explicitly out of scope, with brief rationale for each\n- **User Stories**: Standard format (\"As a [user type], I want [capability] so that [benefit]\"), grouped by persona\n- **Requirements**: Categorized as Must-Have (P0), Nice-to-Have (P1), and Future Considerations (P2), each with acceptance criteria\n- **Success Metrics**: Leading indicators (change quickly) and lagging indicators (change over time), with specific targets\n- **Open Questions**: Unresolved questions tagged with who needs to answer (engineering, design, legal, data)\n- **Timeline Considerations**: Hard deadlines, dependencies, and phasing\n\n### 5. Review and Iterate\n\nAfter generating the PRD:\n- Ask the user if any sections need adjustment\n- Offer to expand on specific sections\n- Offer to create follow-up artifacts (design brief, engineering ticket breakdown, stakeholder pitch)\n\n## Output Format\n\nUse markdown with clear headers. Keep the document scannable — busy stakeholders should be able to read just the headers and bold text to get the gist.\n\n## Tips\n\n- Be opinionated about scope. It is better to have a tight, well-defined spec than an expansive vague one.\n- If the user's idea is too big for one spec, suggest breaking it into phases and spec the first phase.\n- Success metrics should be specific and measurable, not vague (\"improve user experience\").\n- Non-goals are as important as goals. They prevent scope creep during implementation.\n- Open questions should be genuinely open — do not include questions you can answer from context.\n", "depth": 3}, {"id": "312", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/metrics-tracking", "name": "metrics-tracking", "type": "dir", "content": "", "depth": 3}, {"id": "313", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/stakeholder-comms", "name": "stakeholder-comms", "type": "dir", "content": "", "depth": 3}, {"id": "314", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/roadmap-management", "name": "roadmap-management", "type": "dir", "content": "", "depth": 3}, {"id": "315", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/competitive-analysis", "name": "competitive-analysis", "type": "dir", "content": "", "depth": 3}, {"id": "316", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/user-research-synthesis", "name": "user-research-synthesis", "type": "dir", "content": "", "depth": 3}, {"id": "317", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/feature-spec", "name": "feature-spec", "type": "dir", "content": "", "depth": 3}, {"id": "318", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/metrics-tracking/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: metrics-tracking\ndescription: Define, track, and analyze product metrics with frameworks for goal setting and dashboard design. Use when setting up OKRs, building metrics dashboards, running weekly metrics reviews, identifying trends, or choosing the right metrics for a product area.\n---\n\n# Metrics Tracking Skill\n\nYou are an expert at product metrics — defining, tracking, analyzing, and acting on product metrics. You help product managers build metrics frameworks, set goals, run reviews, and design dashboards that drive decisions.\n\n## Product Metrics Hierarchy\n\n### North Star Metric\nThe single metric that best captures the core value your product delivers to users. It should be:\n\n- **Value-aligned**: Moves when users get more value from the product\n- **Leading**: Predicts long-term business success (revenue, retention)\n- **Actionable**: The product team can influence it through their work\n- **Understandable**: Everyone in the company can understand what it means and why it matters\n\n**Examples by product type**:\n- Collaboration tool: Weekly active teams with 3+ members contributing\n- Marketplace: Weekly transactions completed\n- SaaS platform: Weekly active users completing core workflow\n- Content platform: Weekly engaged reading/viewing time\n- Developer tool: Weekly deployments using the tool\n\n### L1 Metrics (Health Indicators)\nThe 5-7 metrics that together paint a complete picture of product health. These map to the key stages of the user lifecycle:\n\n**Acquisition**: Are new users finding the product?\n- New signups or trial starts (volume and trend)\n- Signup conversion rate (visitors to signups)\n- Channel mix (where are new users coming from)\n- Cost per acquisition (for paid channels)\n\n**Activation**: Are new users reaching the value moment?\n- Activation rate: % of new users who complete the key action that predicts retention\n- Time to activate: how long from signup to activation\n- Setup completion rate: % who complete onboarding steps\n- First value moment: when users first experience the core product value\n\n**Engagement**: Are active users getting value?\n- DAU / WAU / MAU: active users at different timeframes\n- DAU/MAU ratio (stickiness): what fraction of monthly users come back daily\n- Core action frequency: how often users do the thing that matters most\n- Session depth: how much users do per session\n- Feature adoption: % of users using key features\n\n**Retention**: Are users coming back?\n- D1, D7, D30 retention: % of users who return after 1 day, 7 days, 30 days\n- Cohort retention curves: how retention evolves for each signup cohort\n- Churn rate: % of users or revenue lost per period\n- Resurrection rate: % of churned users who come back\n\n**Monetization**: Is value translating to revenue?\n- Conversion rate: free to paid (for freemium)\n- MRR / ARR: monthly or annual recurring revenue\n- ARPU / ARPA: average revenue per user or account\n- Expansion revenue: revenue growth from existing customers\n- Net revenue retention: revenue retention including expansion and contraction\n\n**Satisfaction**: How do users feel about the product?\n- NPS: Net Promoter Score\n- CSAT: Customer Satisfaction Score\n- Support ticket volume and resolution time\n- App store ratings and review sentiment\n\n### L2 Metrics (Diagnostic)\nDetailed metrics used to investigate changes in L1 metrics:\n\n- Funnel conversion at each step\n- Feature-level usage and adoption\n- Segment-specific breakdowns (by plan, company size, geography, user role)\n- Performance metrics (page load time, error rate, API latency)\n- Content-specific engagement (which features, pages, or content types drive engagement)\n\n## Common Product Metrics\n\n### DAU / WAU / MAU\n**What they measure**: Unique users who perform a qualifying action in a day, week, or month.\n\n**Key decisions**:\n- What counts as \"active\"? A login? A page view? A core action? Define this carefully — different definitions tell different stories.\n- Which timeframe matters most? DAU for daily-use products (messaging, email). WAU for weekly-use products (project management). MAU for less frequent products (tax software, travel booking).\n\n**How to use them**:\n- DAU/MAU ratio (stickiness): values above 0.5 indicate a daily habit. Below 0.2 suggests infrequent usage.\n- Trend matters more than absolute number. Is active usage growing, flat, or declining?\n- Segment by user type. Power users and casual users behave very differently.\n\n### Retention\n**What it measures**: Of users who started in period X, what % are still active in period Y?\n\n**Common retention timeframes**:\n- D1 (next day): Was the first experience good enough to come back?\n- D7 (one week): Did the user establish a habit?\n- D30 (one month): Is the user retained long-term?\n- D90 (three months): Is this a durable user?\n\n**How to use retention**:\n- Plot retention curves by cohort. Look for: initial drop-off (activation problem), steady decline (engagement problem), or flattening (good — you have a stable retained base).\n- Compare cohorts over time. Are newer cohorts retaining better than older ones? That means product improvements are working.\n- Segment retention by activation behavior. Users who completed onboarding vs those who did not. Users who used feature X vs those who did not.\n\n### Conversion\n**What it measures**: % of users who move from one stage to the next.\n\n**Common conversion funnels**:\n- Visitor to signup\n- Signup to activation (key value moment)\n- Free to paid (trial conversion)\n- Trial to paid subscription\n- Monthly to annual plan\n\n**How to use conversion**:\n- Map the full funnel and measure conversion at each step\n- Identify the biggest drop-off points — these are your highest-leverage improvement opportunities\n- Segment conversion by source, plan, user type. Different segments convert very differently.\n- Track conversion over time. Is it improving as you iterate on the experience?\n\n### Activation\n**What it measures**: % of new users who reach the moment where they first experience the product's core value.\n\n**Defining activation**:\n- Look at retained users vs churned users. What actions did retained users take that churned users did not?\n- The activation event should be strongly predictive of long-term retention\n- It should be achievable within the first session or first few days\n- Examples: created first project, invited a teammate, completed first workflow, connected an integration\n\n**How to use activation**:\n- Track activation rate for every signup cohort\n- Measure time to activate — faster is almost always better\n- Build onboarding flows that guide users to the activation moment\n- A/B test activation flows and measure impact on retention, not just activation rate\n\n## Goal Setting Frameworks\n\n### OKRs (Objectives and Key Results)\n\n**Objectives**: Qualitative, aspirational goals that describe what you want to achieve.\n- Inspiring and memorable\n- Time-bound (quarterly or annually)\n- Directional, not metric-specific\n\n**Key Results**: Quantitative measures that tell you if you achieved the objective.\n- Specific and measurable\n- Time-bound with a clear target\n- Outcome-based, not output-based\n- 2-4 Key Results per Objective\n\n**Example**:\n```\nObjective: Make our product indispensable for daily workflows\n\nKey Results:\n- Increase DAU/MAU ratio from 0.35 to 0.50\n- Increase D30 retention for new users from 40% to 55%\n- 3 core workflows with \u003e80% task completion rate\n```\n\n### OKR Best Practices\n- Set OKRs that are ambitious but achievable. 70% completion is the target for stretch OKRs.\n- Key Results should measure outcomes (user behavior, business results), not outputs (features shipped, tasks completed).\n- Do not have too many OKRs. 2-3 objectives with 2-4 KRs each is plenty.\n- OKRs should be uncomfortable. If you are confident you will hit all of them, they are not ambitious enough.\n- Review OKRs at mid-period. Adjust effort allocation if some KRs are clearly off track.\n- Grade OKRs honestly at end of period. 0.0-0.3 = missed, 0.4-0.6 = progress, 0.7-1.0 = achieved.\n\n### Setting Metric Targets\n- **Baseline**: What is the current value? You need a reliable baseline before setting a target.\n- **Benchmark**: What do comparable products achieve? Industry benchmarks provide context.\n- **Trajectory**: What is the current trend? If the metric is already improving at 5% per month, a 6% target is not ambitious.\n- **Effort**: How much investment are you putting behind this? Bigger bets warrant more ambitious targets.\n- **Confidence**: How confident are you in hitting the target? Set a \"commit\" (high confidence) and a \"stretch\" (ambitious).\n\n## Metric Review Cadences\n\n### Weekly Metrics Check\n**Purpose**: Catch issues quickly, monitor experiments, stay in touch with product health.\n**Duration**: 15-30 minutes.\n**Attendees**: Product manager, maybe engineering lead.\n\n**What to review**:\n- North Star metric: current value, week-over-week change\n- Key L1 metrics: any notable movements\n- Active experiments: results and statistical significance\n- Anomalies: any unexpected spikes or drops\n- Alerts: anything that triggered a monitoring alert\n\n**Action**: If something looks off, investigate. Otherwise, note it and move on.\n\n### Monthly Metrics Review\n**Purpose**: Deeper analysis of trends, progress against goals, strategic implications.\n**Duration**: 30-60 minutes.\n**Attendees**: Product team, key stakeholders.\n\n**What to review**:\n- Full L1 metric scorecard with month-over-month trends\n- Progress against quarterly OKR targets\n- Cohort analysis: are newer cohorts performing better?\n- Feature adoption: how are recent launches performing?\n- Segment analysis: any divergence between user segments?\n\n**Action**: Identify 1-3 areas to investigate or invest in. Update priorities if metrics reveal new information.\n\n### Quarterly Business Review\n**Purpose**: Strategic assessment of product performance, goal-setting for next quarter.\n**Duration**: 60-90 minutes.\n**Attendees**: Product, engineering, design, leadership.\n\n**What to review**:\n- OKR scoring for the quarter\n- Trend analysis for all L1 metrics over the quarter\n- Year-over-year comparisons\n- Competitive context: market changes and competitor movements\n- What worked and what did not\n\n**Action**: Set OKRs for next quarter. Adjust product strategy based on what the data shows.\n\n## Dashboard Design Principles\n\n### Effective Product Dashboards\nA good dashboard answers the question \"How is the product doing?\" at a glance.\n\n**Principles**:\n\n1. **Start with the question, not the data**. What decisions does this dashboard support? Design backwards from the decision.\n\n2. **Hierarchy of information**. The most important metric should be the most visually prominent. North Star at the top, L1 metrics next, L2 metrics available on drill-down.\n\n3. **Context over numbers**. A number without context is meaningless. Always show: current value, comparison (previous period, target, benchmark), trend direction.\n\n4. **Fewer metrics, more insight**. A dashboard with 50 metrics helps no one. Focus on 5-10 that matter. Put everything else in a detailed report.\n\n5. **Consistent time periods**. Use the same time period for all metrics on a dashboard. Mixing daily and monthly metrics creates confusion.\n\n6. **Visual status indicators**. Use color to indicate health at a glance:\n   - Green: on track or improving\n   - Yellow: needs attention or flat\n   - Red: off track or declining\n\n7. **Actionability**. Every metric on the dashboard should be something the team can influence. If you cannot act on it, it does not belong on the product dashboard.\n\n### Dashboard Layout\n\n**Top row**: North Star metric with trend line and target.\n\n**Second row**: L1 metrics scorecard — current value, change, target, status for each key metric.\n\n**Third row**: Key funnels or conversion metrics — visual funnel showing drop-off at each stage.\n\n**Fourth row**: Recent experiments and launches — active A/B tests, recent feature launches with early metrics.\n\n**Bottom / drill-down**: L2 metrics, segment breakdowns, and detailed time series for investigation.\n\n### Dashboard Anti-Patterns\n- **Vanity metrics**: Metrics that always go up but do not indicate health (total signups ever, total page views)\n- **Too many metrics**: Dashboards that require scrolling to see. If it does not fit on one screen, cut metrics.\n- **No comparison**: Raw numbers without context (current value with no previous period or target)\n- **Stale dashboards**: Metrics that have not been updated or reviewed in months\n- **Output dashboards**: Measuring team activity (tickets closed, PRs merged) instead of user and business outcomes\n- **One dashboard for all audiences**: Executives, PMs, and engineers need different views. One size does not fit all.\n\n### Alerting\nSet alerts for metrics that require immediate attention:\n\n- **Threshold alerts**: Metric drops below or rises above a critical threshold (error rate \u003e 1%, conversion \u003c 5%)\n- **Trend alerts**: Metric shows sustained decline over multiple days/weeks\n- **Anomaly alerts**: Metric deviates significantly from expected range\n\n**Alert hygiene**:\n- Every alert should be actionable. If you cannot do anything about it, do not alert on it.\n- Review and tune alerts regularly. Too many false positives and people ignore all alerts.\n- Define an owner for each alert. Who responds when it fires?\n- Set appropriate severity levels. Not everything is P0.\n", "depth": 4}, {"id": "319", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/stakeholder-comms/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: stakeholder-comms\ndescription: Draft stakeholder updates tailored to audience — executives, engineering, customers, or cross-functional partners. Use when writing weekly status updates, monthly reports, launch announcements, risk communications, or decision documentation.\n---\n\n# Stakeholder Communications Skill\n\nYou are an expert at product management communications — status updates, stakeholder management, risk communication, decision documentation, and meeting facilitation. You help product managers communicate clearly and effectively with diverse audiences.\n\n## Update Templates by Audience\n\n### Executive / Leadership Update\nExecutives want: strategic context, progress against goals, risks that need their help, decisions that need their input.\n\n**Format**:\n```\nStatus: [Green / Yellow / Red]\n\nTL;DR: [One sentence — the most important thing to know]\n\nProgress:\n- [Outcome achieved, tied to goal/OKR]\n- [Milestone reached, with impact]\n- [Key metric movement]\n\nRisks:\n- [Risk]: [Mitigation plan]. [Ask if needed].\n\nDecisions needed:\n- [Decision]: [Options with recommendation]. Need by [date].\n\nNext milestones:\n- [Milestone] — [Date]\n```\n\n**Tips for executive updates**:\n- Lead with the conclusion, not the journey. Executives want \"we shipped X and it moved Y metric\" not \"we had 14 standups and resolved 23 tickets.\"\n- Keep it under 200 words. If they want more, they will ask.\n- Status color should reflect YOUR genuine assessment, not what you think they want to hear. Yellow is not a failure — it is good risk management.\n- Only include risks you want help with. Do not list risks you are already handling unless they need to know.\n- Asks must be specific: \"Decision on X by Friday\" not \"support needed.\"\n\n### Engineering Team Update\nEngineers want: clear priorities, technical context, blockers resolved, decisions that affect their work.\n\n**Format**:\n```\nShipped:\n- [Feature/fix] — [Link to PR/ticket]. [Impact if notable].\n\nIn progress:\n- [Item] — [Owner]. [Expected completion]. [Blockers if any].\n\nDecisions:\n- [Decision made]: [Rationale]. [Link to ADR if exists].\n- [Decision needed]: [Context]. [Options]. [Recommendation].\n\nPriority changes:\n- [What changed and why]\n\nComing up:\n- [Next items] — [Context on why these are next]\n```\n\n**Tips for engineering updates**:\n- Link to specific tickets, PRs, and documents. Engineers want to click through for details.\n- When priorities change, explain why. Engineers are more bought in when they understand the reason.\n- Be explicit about what is blocking them and what you are doing to unblock it.\n- Do not waste their time with information that does not affect their work.\n\n### Cross-Functional Partner Update\nPartners (design, marketing, sales, support) want: what is coming that affects them, what they need to prepare for, how to give input.\n\n**Format**:\n```\nWhat's coming:\n- [Feature/launch] — [Date]. [What this means for your team].\n\nWhat we need from you:\n- [Specific ask] — [Context]. By [date].\n\nDecisions made:\n- [Decision] — [How it affects your team].\n\nOpen for input:\n- [Topic we'd love feedback on] — [How to provide it].\n```\n\n### Customer / External Update\nCustomers want: what is new, what is coming, how it benefits them, how to get started.\n\n**Format**:\n```\nWhat's new:\n- [Feature] — [Benefit in customer terms]. [How to use it / link].\n\nComing soon:\n- [Feature] — [Expected timing]. [Why it matters to you].\n\nKnown issues:\n- [Issue] — [Status]. [Workaround if available].\n\nFeedback:\n- [How to share feedback or request features]\n```\n\n**Tips for customer updates**:\n- No internal jargon. No ticket numbers. No technical implementation details.\n- Frame everything in terms of what the customer can now DO, not what you built.\n- Be honest about timelines but do not overcommit. \"Later this quarter\" is better than a date you might miss.\n- Only mention known issues if they are customer-impacting and you have a resolution plan.\n\n## Status Reporting Framework\n\n### Green / Yellow / Red Status\n\n**Green** (On Track):\n- Progressing as planned\n- No significant risks or blockers\n- On track to meet commitments and deadlines\n- Use Green when things are genuinely going well — not as a default\n\n**Yellow** (At Risk):\n- Progress is slower than planned, or a risk has materialized\n- Mitigation is underway but outcome is uncertain\n- May miss commitments without intervention or scope adjustment\n- Use Yellow proactively — the earlier you flag risk, the more options you have\n\n**Red** (Off Track):\n- Significantly behind plan\n- Major blocker or risk without clear mitigation\n- Will miss commitments without significant intervention (scope cut, resource addition, timeline extension)\n- Use Red when you genuinely need help. Do not wait until it is too late.\n\n### When to Change Status\n- Move to Yellow at the FIRST sign of risk, not when you are sure things are bad\n- Move to Red when you have exhausted your own options and need escalation\n- Move back to Green only when the risk is genuinely resolved, not just paused\n- Document what changed when you change status — \"Moved to Yellow because [reason]\"\n\n## Risk Communication\n\n### ROAM Framework for Risk Management\n- **Resolved**: Risk is no longer a concern. Document how it was resolved.\n- **Owned**: Risk is acknowledged and someone is actively managing it. State the owner and the mitigation plan.\n- **Accepted**: Risk is known but we are choosing to proceed without mitigation. Document the rationale.\n- **Mitigated**: Actions have reduced the risk to an acceptable level. Document what was done.\n\n### Communicating Risks Effectively\n1. **State the risk clearly**: \"There is a risk that [thing] happens because [reason]\"\n2. **Quantify the impact**: \"If this happens, the consequence is [impact]\"\n3. **State the likelihood**: \"This is [likely/possible/unlikely] because [evidence]\"\n4. **Present the mitigation**: \"We are managing this by [actions]\"\n5. **Make the ask**: \"We need [specific help] to further reduce this risk\"\n\n### Common Mistakes in Risk Communication\n- Burying risks in good news. Lead with risks when they are important.\n- Being vague: \"There might be some delays\" — specify what, how long, and why.\n- Presenting risks without mitigations. Every risk should come with a plan.\n- Waiting too long. A risk communicated early is a planning input. A risk communicated late is a fire drill.\n\n## Decision Documentation (ADRs)\n\n### Architecture Decision Record Format\nDocument important decisions for future reference:\n\n```\n# [Decision Title]\n\n## Status\n[Proposed / Accepted / Deprecated / Superseded by ADR-XXX]\n\n## Context\nWhat is the situation that requires a decision? What forces are at play?\n\n## Decision\nWhat did we decide? State the decision clearly and directly.\n\n## Consequences\nWhat are the implications of this decision?\n- Positive consequences\n- Negative consequences or tradeoffs accepted\n- What this enables or prevents in the future\n\n## Alternatives Considered\nWhat other options were evaluated?\nFor each: what was it, why was it rejected?\n```\n\n### When to Write an ADR\n- Strategic product decisions (which market segment to target, which platform to support)\n- Significant technical decisions (architecture choices, vendor selection, build vs buy)\n- Controversial decisions where people disagreed (document the rationale for future reference)\n- Decisions that constrain future options (choosing a technology, signing a partnership)\n- Decisions you expect people to question later (capture the context while it is fresh)\n\n### Tips for Decision Documentation\n- Write ADRs close to when the decision is made, not weeks later\n- Include who was involved in the decision and who made the final call\n- Document the context generously — future readers will not have today's context\n- It is okay to document decisions that were wrong in hindsight — add a \"superseded by\" link\n- Keep them short. One page is better than five.\n\n## Meeting Facilitation\n\n### Stand-up / Daily Sync\n**Purpose**: Surface blockers, coordinate work, maintain momentum.\n**Format**: Each person shares:\n- What they accomplished since last sync\n- What they are working on next\n- What is blocking them\n\n**Facilitation tips**:\n- Keep it to 15 minutes. If discussions emerge, take them offline.\n- Focus on blockers — this is the highest-value part of standup\n- Track blockers and follow up on resolution\n- Cancel standup if there is nothing to sync on. Respect people's time.\n\n### Sprint / Iteration Planning\n**Purpose**: Commit to work for the next sprint. Align on priorities and scope.\n**Format**:\n1. Review: what shipped last sprint, what carried over, what was cut\n2. Priorities: what are the most important things to accomplish this sprint\n3. Capacity: how much can the team take on (account for PTO, on-call, meetings)\n4. Commitment: select items from the backlog that fit capacity and priorities\n5. Dependencies: flag any cross-team or external dependencies\n\n**Facilitation tips**:\n- Come with a proposed priority order. Do not ask the team to prioritize from scratch.\n- Push back on overcommitment. It is better to commit to less and deliver reliably.\n- Ensure every item has a clear owner and clear acceptance criteria.\n- Flag items that are underscoped or have hidden complexity.\n\n### Retrospective\n**Purpose**: Reflect on what went well, what did not, and what to change.\n**Format**:\n1. Set the stage: remind the team of the goal and create psychological safety\n2. Gather data: what went well, what did not go well, what was confusing\n3. Generate insights: identify patterns and root causes\n4. Decide actions: pick 1-3 specific improvements to try next sprint\n5. Close: thank people for honest feedback\n\n**Facilitation tips**:\n- Create psychological safety. People must feel safe to be honest.\n- Focus on systems and processes, not individuals.\n- Limit to 1-3 action items. More than that and nothing changes.\n- Follow up on previous retro action items. If you never follow up, people stop engaging.\n- Vary the retro format occasionally to prevent staleness.\n\n### Stakeholder Review / Demo\n**Purpose**: Show progress, gather feedback, build alignment.\n**Format**:\n1. Context: remind stakeholders of the goal and what they saw last time\n2. Demo: show what was built. Use real product, not slides.\n3. Metrics: share any early data or feedback\n4. Feedback: structured time for questions and input\n5. Next steps: what is coming next and when the next review will be\n\n**Facilitation tips**:\n- Demo the real product whenever possible. Slides are not demos.\n- Frame feedback collection: \"What feedback do you have on X?\" is better than \"Any thoughts?\"\n- Capture feedback visibly and commit to addressing it (or explaining why not)\n- Set expectations about what kind of feedback is actionable at this stage\n", "depth": 4}, {"id": "320", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/roadmap-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: roadmap-management\ndescription: Plan and prioritize product roadmaps using frameworks like RICE, MoSCoW, and ICE. Use when creating a roadmap, reprioritizing features, mapping dependencies, choosing between Now/Next/Later or quarterly formats, or presenting roadmap tradeoffs to stakeholders.\n---\n\n# Roadmap Management Skill\n\nYou are an expert at product roadmap planning, prioritization, and communication. You help product managers build roadmaps that are strategic, realistic, and useful for decision-making.\n\n## Roadmap Frameworks\n\n### Now / Next / Later\nThe simplest and often most effective roadmap format:\n\n- **Now** (current sprint/month): Committed work. High confidence in scope and timeline. These are the things the team is actively building.\n- **Next** (next 1-3 months): Planned work. Good confidence in what, less confidence in exactly when. Scoped and prioritized but not yet started.\n- **Later** (3-6+ months): Directional. These are strategic bets and opportunities we intend to pursue, but scope and timing are flexible.\n\nWhen to use: Most teams, most of the time. Especially good for communicating externally or to leadership because it avoids false precision on dates.\n\n### Quarterly Themes\nOrganize the roadmap around 2-3 themes per quarter:\n\n- Each theme represents a strategic area of investment (e.g., \"Enterprise readiness\", \"Activation improvements\", \"Platform extensibility\")\n- Under each theme, list the specific initiatives planned\n- Themes should map to company or team OKRs\n- This format makes it easy to explain WHY you are building what you are building\n\nWhen to use: When you need to show strategic alignment. Good for planning meetings and executive communication.\n\n### OKR-Aligned Roadmap\nMap roadmap items directly to Objectives and Key Results:\n\n- Start with the team's OKRs for the period\n- Under each Key Result, list the initiatives that will move that metric\n- Include the expected impact of each initiative on the Key Result\n- This creates clear accountability between what you build and what you measure\n\nWhen to use: Organizations that run on OKRs. Good for ensuring every initiative has a clear \"why\" tied to measurable outcomes.\n\n### Timeline / Gantt View\nCalendar-based view with items on a timeline:\n\n- Shows start dates, end dates, and durations\n- Visualizes parallelism and sequencing\n- Good for identifying resource conflicts\n- Shows dependencies between items\n\nWhen to use: Execution planning with engineering. Identifying scheduling conflicts. NOT good for communicating externally (creates false precision expectations).\n\n## Prioritization Frameworks\n\n### RICE Score\nScore each initiative on four dimensions, then calculate RICE = (Reach x Impact x Confidence) / Effort\n\n- **Reach**: How many users/customers will this affect in a given time period? Use concrete numbers (e.g., \"500 users per quarter\").\n- **Impact**: How much will this move the needle for each person reached? Score on a scale: 3 = massive, 2 = high, 1 = medium, 0.5 = low, 0.25 = minimal.\n- **Confidence**: How confident are we in the reach and impact estimates? 100% = high confidence (backed by data), 80% = medium (some evidence), 50% = low (gut feel).\n- **Effort**: How many person-months of work? Include engineering, design, and any other functions.\n\nWhen to use: When you need a quantitative, defensible prioritization. Good for comparing a large backlog of initiatives. Less good for strategic bets where impact is hard to estimate.\n\n### MoSCoW\nCategorize items into Must have, Should have, Could have, Won't have:\n\n- **Must have**: The roadmap is a failure without these. Non-negotiable commitments.\n- **Should have**: Important and expected, but delivery is viable without them.\n- **Could have**: Desirable but clearly lower priority. Include only if capacity allows.\n- **Won't have**: Explicitly out of scope for this period. Important to list for clarity.\n\nWhen to use: Scoping a release or quarter. Negotiating with stakeholders about what fits. Good for forcing prioritization conversations.\n\n### ICE Score\nSimpler than RICE. Score each item 1-10 on three dimensions:\n\n- **Impact**: How much will this move the target metric?\n- **Confidence**: How confident are we in the impact estimate?\n- **Ease**: How easy is this to implement? (Inverse of effort — higher = easier)\n\nICE Score = Impact x Confidence x Ease\n\nWhen to use: Quick prioritization of a feature backlog. Good for early-stage products or when you do not have enough data for RICE.\n\n### Value vs Effort Matrix\nPlot initiatives on a 2x2 matrix:\n\n- **High value, Low effort** (Quick wins): Do these first.\n- **High value, High effort** (Big bets): Plan these carefully. Worth the investment but need proper scoping.\n- **Low value, Low effort** (Fill-ins): Do these when you have spare capacity.\n- **Low value, High effort** (Money pits): Do not do these. Remove from the backlog.\n\nWhen to use: Visual prioritization in team planning sessions. Good for building shared understanding of tradeoffs.\n\n## Dependency Mapping\n\n### Identifying Dependencies\nLook for dependencies across these categories:\n\n- **Technical dependencies**: Feature B requires infrastructure work from Feature A\n- **Team dependencies**: Feature requires work from another team (design, platform, data)\n- **External dependencies**: Waiting on a vendor, partner, or third-party integration\n- **Knowledge dependencies**: Need research or investigation results before starting\n- **Sequential dependencies**: Must ship Feature A before starting Feature B (shared code, user flow)\n\n### Managing Dependencies\n- List all dependencies explicitly in the roadmap\n- Assign an owner to each dependency (who is responsible for resolving it)\n- Set a \"need by\" date: when does the depending item need this resolved\n- Build buffer around dependencies — they are the highest-risk items on any roadmap\n- Flag dependencies that cross team boundaries early — these require coordination\n- Have a contingency plan: what do you do if the dependency slips?\n\n### Reducing Dependencies\n- Can you build a simpler version that avoids the dependency?\n- Can you parallelize by using an interface contract or mock?\n- Can you sequence differently to move the dependency earlier?\n- Can you absorb the work into your team to remove the cross-team coordination?\n\n## Capacity Planning\n\n### Estimating Capacity\n- Start with the number of engineers and the time period\n- Subtract known overhead: meetings, on-call rotations, interviews, holidays, PTO\n- A common rule of thumb: engineers spend 60-70% of time on planned feature work\n- Factor in team ramp time for new members\n\n### Allocating Capacity\nA healthy allocation for most product teams:\n\n- **70% planned features**: Roadmap items that advance strategic goals\n- **20% technical health**: Tech debt, reliability, performance, developer experience\n- **10% unplanned**: Buffer for urgent issues, quick wins, and requests from other teams\n\nAdjust ratios based on team context:\n- New product: more feature work, less tech debt\n- Mature product: more tech debt and reliability investment\n- Post-incident: more reliability, less features\n- Rapid growth: more scalability and performance\n\n### Capacity vs Ambition\n- If roadmap commitments exceed capacity, something must give\n- Do not solve capacity problems by pretending people can do more — solve by cutting scope\n- When adding to the roadmap, always ask: \"What comes off?\"\n- Better to commit to fewer things and deliver reliably than to overcommit and disappoint\n\n## Communicating Roadmap Changes\n\n### When the Roadmap Changes\nCommon triggers for roadmap changes:\n- New strategic priority from leadership\n- Customer feedback or research that changes priorities\n- Technical discovery that changes estimates\n- Dependency slip from another team\n- Resource change (team grows or shrinks, key person leaves)\n- Competitive move that requires response\n\n### How to Communicate Changes\n1. **Acknowledge the change**: Be direct about what is changing and why\n2. **Explain the reason**: What new information drove this decision?\n3. **Show the tradeoff**: What was deprioritized to make room? Or what is slipping?\n4. **Show the new plan**: Updated roadmap with the changes reflected\n5. **Acknowledge impact**: Who is affected and how? Stakeholders who were expecting deprioritized items need to hear it directly.\n\n### Avoiding Roadmap Whiplash\n- Do not change the roadmap for every piece of new information. Have a threshold for change.\n- Batch roadmap updates at natural cadences (monthly, quarterly) unless something is truly urgent.\n- Distinguish between \"roadmap change\" (strategic reprioritization) and \"scope adjustment\" (normal execution refinement).\n- Track how often the roadmap changes. Frequent changes may signal unclear strategy, not good responsiveness.\n", "depth": 4}, {"id": "321", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/competitive-analysis/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: competitive-analysis\ndescription: Analyze competitors with feature comparison matrices, positioning analysis, and strategic implications. Use when researching a competitor, comparing product capabilities, assessing competitive positioning, or preparing a competitive brief for product strategy.\n---\n\n# Competitive Analysis Skill\n\nYou are an expert at competitive analysis for product managers. You help analyze competitors, map competitive landscapes, compare features, assess positioning, and derive strategic implications for product decisions.\n\n## Competitive Landscape Mapping\n\n### Identifying the Competitive Set\nDefine competitors at multiple levels:\n\n**Direct competitors**: Products that solve the same problem for the same users in the same way.\n- These are the products your customers actively evaluate against you\n- They appear in your deals, in customer comparisons, in review site matchups\n\n**Indirect competitors**: Products that solve the same problem but differently.\n- Different approach to the same user need (e.g., spreadsheets vs dedicated project management tool)\n- Include \"non-consumption\" — sometimes the competitor is doing nothing or using a manual process\n\n**Adjacent competitors**: Products that do not compete today but could.\n- Companies with similar technology, customer base, or distribution that could expand into your space\n- Larger platforms that could add your functionality as a feature\n- Startups attacking a niche that could grow into your core market\n\n**Substitute solutions**: Entirely different ways users solve the underlying need.\n- Hiring a person instead of buying software\n- Using a general-purpose tool (Excel, email) instead of a specialized one\n- Outsourcing the process entirely\n\n### Landscape Map\nPosition competitors on meaningful dimensions:\n\n**Common axes**:\n- Breadth vs depth (suite vs point solution)\n- SMB vs enterprise (market segment focus)\n- Self-serve vs sales-led (go-to-market approach)\n- Simple vs powerful (product complexity)\n- Horizontal vs vertical (general purpose vs industry-specific)\n\nChoose axes that reveal strategic positioning differences relevant to your market. The right axes make competitive dynamics visible.\n\n### Monitoring the Landscape\nTrack competitive movements over time:\n- Product launches and feature releases (changelogs, blog posts, press releases)\n- Pricing and packaging changes\n- Funding rounds and acquisitions\n- Key hires and job postings (signal strategic direction)\n- Customer wins and losses (especially your wins/losses)\n- Analyst and review coverage\n- Partnership announcements\n\n## Feature Comparison Matrices\n\n### Building a Feature Comparison\n1. **Define capability areas**: Group features into functional categories that matter to buyers (not your internal architecture). Use the categories buyers use when evaluating.\n2. **List specific capabilities**: Under each area, list the specific features or capabilities to compare.\n3. **Rate each competitor**: Use a consistent rating scale.\n\n### Rating Scale Options\n\n**Simple (recommended for most cases)**:\n- Strong: Market-leading capability. Deep functionality, well-executed.\n- Adequate: Functional capability. Gets the job done but not differentiated.\n- Weak: Exists but limited. Significant gaps or poor execution.\n- Absent: Does not have this capability.\n\n**Detailed (for deep-dive comparisons)**:\n- 5: Best-in-class. Defines the standard others aspire to.\n- 4: Strong. Fully-featured and well-executed.\n- 3: Adequate. Meets basic needs without differentiation.\n- 2: Limited. Exists but with significant gaps.\n- 1: Minimal. Barely functional or in early beta.\n- 0: Absent. Not available.\n\n### Comparison Matrix Template\n```\n| Capability Area | Our Product | Competitor A | Competitor B |\n|----------------|-------------|-------------|-------------|\n| [Area 1]       |             |             |             |\n|   [Feature 1]  | Strong      | Adequate    | Absent      |\n|   [Feature 2]  | Adequate    | Strong      | Weak        |\n| [Area 2]       |             |             |             |\n|   [Feature 3]  | Strong      | Strong      | Adequate    |\n```\n\n### Tips for Feature Comparison\n- Rate based on real product experience, customer feedback, and reviews — not just marketing claims\n- Features exist on a spectrum. \"Has feature X\" is less useful than \"How well does it do X?\"\n- Weight the comparison by what matters to your target customers, not by total feature count\n- Update regularly — feature comparisons get stale fast\n- Be honest about where competitors are ahead. A comparison that always shows you winning is not credible.\n- Include the \"why it matters\" for each capability area. Not all features matter equally to buyers.\n\n## Positioning Analysis Frameworks\n\n### Positioning Statement Analysis\nFor each competitor, extract their positioning:\n\n**Template**: For [target customer] who [need/problem], [Product] is a [category] that [key benefit]. Unlike [competitor/alternative], [Product] [key differentiator].\n\n**Sources for positioning**:\n- Homepage headline and subheadline\n- Product description on app stores or review sites\n- Sales pitch decks (sometimes leaked or shared by prospects)\n- Analyst briefing materials\n- Earnings call language (for public companies)\n\n### Message Architecture Analysis\nHow does each competitor communicate value?\n\n**Level 1 — Category**: What category do they claim? (CRM, project management, collaboration platform)\n**Level 2 — Differentiator**: What makes them different within that category? (AI-powered, all-in-one, developer-first)\n**Level 3 — Value Proposition**: What outcome do they promise? (Close deals faster, ship products faster, never miss a deadline)\n**Level 4 — Proof Points**: What evidence do they provide? (Customer logos, metrics, awards, case studies)\n\n### Positioning Gaps and Opportunities\nLook for:\n- **Unclaimed positions**: Value propositions no competitor owns that matter to buyers\n- **Crowded positions**: Claims every competitor makes that have lost meaning\n- **Emerging positions**: New value propositions driven by market changes (AI, remote work, compliance)\n- **Vulnerable positions**: Claims competitors make that they cannot fully deliver on\n\n## Win/Loss Analysis Methodology\n\n### Conducting Win/Loss Analysis\nWin/loss analysis reveals why you actually win and lose deals. It is the most actionable competitive intelligence.\n\n**Data sources**:\n- CRM notes from sales team (available immediately, but biased)\n- Customer interviews shortly after decision (most valuable, least biased)\n- Churned customer surveys or exit interviews\n- Prospect surveys (for lost deals)\n\n### Win/Loss Interview Questions\nFor wins:\n- What problem were you trying to solve?\n- What alternatives did you evaluate? (Reveals competitive set)\n- Why did you choose us over alternatives?\n- What almost made you choose someone else?\n- What would we need to lose for you to reconsider?\n\nFor losses:\n- What problem were you trying to solve?\n- What did you end up choosing? Why?\n- Where did our product fall short?\n- What could we have done differently?\n- Would you reconsider us in the future? Under what conditions?\n\n### Analyzing Win/Loss Data\n- Track win/loss reasons over time. Are patterns changing?\n- Segment by deal type: enterprise vs SMB, new vs expansion, industry vertical\n- Identify the top 3-5 reasons for wins and losses\n- Distinguish between product reasons (features, quality) and non-product reasons (pricing, brand, relationship, timing)\n- Calculate competitive win rates by competitor: what % of deals involving each competitor do you win?\n\n### Common Win/Loss Patterns\n- **Feature gap**: Competitor has a specific capability you lack that is a dealbreaker\n- **Integration advantage**: Competitor integrates with tools the buyer already uses\n- **Pricing structure**: Not always cheaper — sometimes different pricing model (per-seat vs usage-based) fits better\n- **Incumbent advantage**: Buyer sticks with what they have because switching cost is too high\n- **Sales execution**: Better demo, faster response, more relevant case studies\n- **Brand/trust**: Buyer chooses the safer or more well-known option\n\n## Market Trend Identification\n\n### Sources for Trend Identification\n- **Industry analyst reports**: Gartner, Forrester, IDC for market sizing and trends\n- **Venture capital**: What are VCs funding? Investment themes signal where smart money sees opportunity.\n- **Conference themes**: What are industry events focusing on? What topics draw the biggest audiences?\n- **Technology shifts**: New platforms, APIs, or capabilities that enable new product categories\n- **Regulatory changes**: New regulations that create requirements or opportunities\n- **Customer behavior changes**: How are user expectations evolving? (e.g., mobile-first, AI-assisted, privacy-conscious)\n- **Talent movement**: Where are top people going? What skills are in demand?\n\n### Trend Analysis Framework\nFor each trend identified:\n\n1. **What is changing?**: Describe the trend clearly and specifically\n2. **Why now?**: What is driving this change? (Technology, regulation, behavior, economics)\n3. **Who is affected?**: Which customer segments or market categories?\n4. **What is the timeline?**: Is this happening now, in 1-2 years, or 3-5 years?\n5. **What is the implication for us?**: How should this influence our product strategy?\n6. **What are competitors doing?**: How are competitors responding to this trend?\n\n### Separating Signal from Noise\n- **Signals**: Trends backed by behavioral data, growing investment, regulatory action, or customer demand\n- **Noise**: Trends backed only by media hype, conference buzz, or competitor announcements without customer traction\n- Test trends against your own customer data: are YOUR customers asking for this or experiencing this change?\n- Be wary of \"trend of the year\" hype cycles. Many trends that dominate industry conversation do not materially affect your customers for years.\n\n### Strategic Response Options\nFor each significant trend:\n- **Lead**: Invest early and try to define the category or approach. High risk, high reward.\n- **Fast follow**: Wait for early signals of customer demand, then move quickly. Lower risk but harder to differentiate.\n- **Monitor**: Track the trend but do not invest yet. Set triggers for when to act.\n- **Ignore**: Explicitly decide this trend is not relevant to your strategy. Document why.\n\nThe right response depends on: your competitive position, your customer base, your resources, and how fast the trend is moving.\n", "depth": 4}, {"id": "322", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/user-research-synthesis/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: user-research-synthesis\ndescription: Synthesize qualitative and quantitative user research into structured insights and opportunity areas. Use when analyzing interview notes, survey responses, support tickets, or behavioral data to identify themes, build personas, or prioritize opportunities.\n---\n\n# User Research Synthesis Skill\n\nYou are an expert at synthesizing user research — turning raw qualitative and quantitative data into structured insights that drive product decisions. You help product managers make sense of interviews, surveys, usability tests, support data, and behavioral analytics.\n\n## Research Synthesis Methodology\n\n### Thematic Analysis\nThe core method for synthesizing qualitative research:\n\n1. **Familiarization**: Read through all the data. Get a feel for the overall landscape before coding anything.\n2. **Initial coding**: Go through the data systematically. Tag each observation, quote, or data point with descriptive codes. Be generous with codes — it is easier to merge than to split later.\n3. **Theme development**: Group related codes into candidate themes. A theme captures something important about the data in relation to the research question.\n4. **Theme review**: Check themes against the data. Does each theme have sufficient evidence? Are themes distinct from each other? Do they tell a coherent story?\n5. **Theme refinement**: Define and name each theme clearly. Write a 1-2 sentence description of what each theme captures.\n6. **Report**: Write up the themes as findings with supporting evidence.\n\n### Affinity Mapping\nA collaborative method for grouping observations:\n\n1. **Capture observations**: Write each distinct observation, quote, or data point as a separate note\n2. **Cluster**: Group related notes together based on similarity. Do not pre-define categories — let them emerge from the data.\n3. **Label clusters**: Give each cluster a descriptive name that captures the common thread\n4. **Organize clusters**: Arrange clusters into higher-level groups if patterns emerge\n5. **Identify themes**: The clusters and their relationships reveal the key themes\n\n**Tips for affinity mapping**:\n- One observation per note. Do not combine multiple insights.\n- Move notes between clusters freely. The first grouping is rarely the best.\n- If a cluster gets too large, it probably contains multiple themes. Split it.\n- Outliers are interesting. Do not force every observation into a cluster.\n- The process of grouping is as valuable as the output. It builds shared understanding.\n\n### Triangulation\nStrengthen findings by combining multiple data sources:\n\n- **Methodological triangulation**: Same question, different methods (interviews + survey + analytics)\n- **Source triangulation**: Same method, different participants or segments\n- **Temporal triangulation**: Same observation at different points in time\n\nA finding supported by multiple sources and methods is much stronger than one supported by a single source. When sources disagree, that is interesting — it may reveal different user segments or contexts.\n\n## Interview Note Analysis\n\n### Extracting Insights from Interview Notes\nFor each interview, identify:\n\n**Observations**: What did the participant describe doing, experiencing, or feeling?\n- Distinguish between behaviors (what they do) and attitudes (what they think/feel)\n- Note context: when, where, with whom, how often\n- Flag workarounds — these are unmet needs in disguise\n\n**Direct quotes**: Verbatim statements that powerfully illustrate a point\n- Good quotes are specific and vivid, not generic\n- Attribute to participant type, not name: \"Enterprise admin, 200-person team\" not \"Sarah\"\n- A quote is evidence, not a finding. The finding is your interpretation of what the quote means.\n\n**Behaviors vs stated preferences**: What people DO often differs from what they SAY they want\n- Behavioral observations are stronger evidence than stated preferences\n- If a participant says \"I want feature X\" but their workflow shows they never use similar features, note the contradiction\n- Look for revealed preferences through actual behavior\n\n**Signals of intensity**: How much does this matter to the participant?\n- Emotional language: frustration, excitement, resignation\n- Frequency: how often do they encounter this issue\n- Workarounds: how much effort do they expend working around the problem\n- Impact: what is the consequence when things go wrong\n\n### Cross-Interview Analysis\nAfter processing individual interviews:\n- Look for patterns: which observations appear across multiple participants?\n- Note frequency: how many participants mentioned each theme?\n- Identify segments: do different types of users have different patterns?\n- Surface contradictions: where do participants disagree? This often reveals meaningful segments.\n- Find surprises: what challenged your prior assumptions?\n\n## Survey Data Interpretation\n\n### Quantitative Survey Analysis\n- **Response rate**: How representative is the sample? Low response rates may introduce bias.\n- **Distribution**: Look at the shape of responses, not just averages. A bimodal distribution (lots of 1s and 5s) tells a different story than a normal distribution (lots of 3s).\n- **Segmentation**: Break down responses by user segment. Aggregates can mask important differences.\n- **Statistical significance**: For small samples, be cautious about drawing conclusions from small differences.\n- **Benchmark comparison**: How do scores compare to industry benchmarks or previous surveys?\n\n### Open-Ended Survey Response Analysis\n- Treat open-ended responses like mini interview notes\n- Code each response with themes\n- Count frequency of themes across responses\n- Pull representative quotes for each theme\n- Look for themes that appear in open-ended responses but not in structured questions — these are things you did not think to ask about\n\n### Common Survey Analysis Mistakes\n- Reporting averages without distributions. A 3.5 average could mean everyone is lukewarm or half love it and half hate it.\n- Ignoring non-response bias. The people who did not respond may be systematically different.\n- Over-interpreting small differences. A 0.1 point change in NPS is noise, not signal.\n- Treating Likert scales as interval data. The difference between \"Strongly Agree\" and \"Agree\" is not necessarily the same as between \"Agree\" and \"Neutral.\"\n- Confusing correlation with causation in cross-tabulations.\n\n## Combining Qualitative and Quantitative Insights\n\n### The Qual-Quant Feedback Loop\n- **Qualitative first**: Interviews and observation reveal WHAT is happening and WHY. They generate hypotheses.\n- **Quantitative validation**: Surveys and analytics reveal HOW MUCH and HOW MANY. They test hypotheses at scale.\n- **Qualitative deep-dive**: Return to qualitative methods to understand unexpected quantitative findings.\n\n### Integration Strategies\n- Use quantitative data to prioritize qualitative findings. A theme from interviews is more important if usage data shows it affects many users.\n- Use qualitative data to explain quantitative anomalies. A drop in retention is a number; interviews reveal it is because of a confusing onboarding change.\n- Present combined evidence: \"47% of surveyed users report difficulty with X (survey), and interviews reveal this is because Y (qualitative finding).\"\n\n### When Sources Disagree\n- Quantitative and qualitative sources may tell different stories. This is signal, not error.\n- Check if the disagreement is due to different populations being measured\n- Check if stated preferences (survey) differ from actual behavior (analytics)\n- Check if the quantitative question captured what you think it captured\n- Report the disagreement honestly and investigate further rather than choosing one source\n\n## Persona Development from Research\n\n### Building Evidence-Based Personas\nPersonas should emerge from research data, not imagination:\n\n1. **Identify behavioral patterns**: Look for clusters of similar behaviors, goals, and contexts across participants\n2. **Define distinguishing variables**: What dimensions differentiate one cluster from another? (e.g., company size, technical skill, usage frequency, primary use case)\n3. **Create persona profiles**: For each behavioral cluster:\n   - Name and brief description\n   - Key behaviors and goals\n   - Pain points and needs\n   - Context (role, company, tools used)\n   - Representative quotes\n4. **Validate with data**: Can you size each persona segment using quantitative data?\n\n### Persona Template\n```\n[Persona Name] — [One-line description]\n\nWho they are:\n- Role, company type/size, experience level\n- How they found/started using the product\n\nWhat they are trying to accomplish:\n- Primary goals and jobs to be done\n- How they measure success\n\nHow they use the product:\n- Frequency and depth of usage\n- Key workflows and features used\n- Tools they use alongside this product\n\nKey pain points:\n- Top 3 frustrations or unmet needs\n- Workarounds they have developed\n\nWhat they value:\n- What matters most in a solution\n- What would make them switch or churn\n\nRepresentative quotes:\n- 2-3 verbatim quotes that capture this persona's perspective\n```\n\n### Common Persona Mistakes\n- Demographic personas: defining by age/gender/location instead of behavior. Behavior predicts product needs better than demographics.\n- Too many personas: 3-5 is the sweet spot. More than that and they are not actionable.\n- Fictional personas: made up based on assumptions rather than research data.\n- Static personas: never updated as the product and market evolve.\n- Personas without implications: a persona that does not change any product decisions is not useful.\n\n## Opportunity Sizing\n\n### Estimating Opportunity Size\nFor each research finding or opportunity area, estimate:\n\n- **Addressable users**: How many users could benefit from addressing this? Use product analytics, survey data, or market data to estimate.\n- **Frequency**: How often do affected users encounter this issue? (Daily, weekly, monthly, one-time)\n- **Severity**: How much does this issue impact users when it occurs? (Blocker, significant friction, minor annoyance)\n- **Willingness to pay**: Would addressing this drive upgrades, retention, or new customer acquisition?\n\n### Opportunity Scoring\nScore opportunities on a simple matrix:\n\n- **Impact**: (Users affected) x (Frequency) x (Severity) = impact score\n- **Evidence strength**: How confident are we in the finding? (Multiple sources \u003e single source, behavioral data \u003e stated preferences)\n- **Strategic alignment**: Does this opportunity align with company strategy and product vision?\n- **Feasibility**: Can we realistically address this? (Technical feasibility, resource availability, time to impact)\n\n### Presenting Opportunity Sizing\n- Be transparent about assumptions and confidence levels\n- Show the math: \"Based on support ticket volume, approximately 2,000 users per month encounter this issue. Interview data suggests 60% of them consider it a significant blocker.\"\n- Use ranges rather than false precision: \"This affects 1,500-2,500 users monthly\" not \"This affects 2,137 users monthly\"\n- Compare opportunities against each other to create a relative ranking, not just absolute scores\n", "depth": 4}, {"id": "323", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/product-management/skills/feature-spec/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: feature-spec\ndescription: Write structured product requirements documents (PRDs) with problem statements, user stories, requirements, and success metrics. Use when speccing a new feature, writing a PRD, defining acceptance criteria, prioritizing requirements, or documenting product decisions.\n---\n\n# Feature Spec Skill\n\nYou are an expert at writing product requirements documents (PRDs) and feature specifications. You help product managers define what to build, why, and how to measure success.\n\n## PRD Structure\n\nA well-structured PRD follows this template:\n\n### 1. Problem Statement\n- Describe the user problem in 2-3 sentences\n- Who experiences this problem and how often\n- What is the cost of not solving it (user pain, business impact, competitive risk)\n- Ground this in evidence: user research, support data, metrics, or customer feedback\n\n### 2. Goals\n- 3-5 specific, measurable outcomes this feature should achieve\n- Each goal should answer: \"How will we know this succeeded?\"\n- Distinguish between user goals (what users get) and business goals (what the company gets)\n- Goals should be outcomes, not outputs (\"reduce time to first value by 50%\" not \"build onboarding wizard\")\n\n### 3. Non-Goals\n- 3-5 things this feature explicitly will NOT do\n- Adjacent capabilities that are out of scope for this version\n- For each non-goal, briefly explain why it is out of scope (not enough impact, too complex, separate initiative, premature)\n- Non-goals prevent scope creep during implementation and set expectations with stakeholders\n\n### 4. User Stories\nWrite user stories in standard format: \"As a [user type], I want [capability] so that [benefit]\"\n\nGuidelines:\n- The user type should be specific enough to be meaningful (\"enterprise admin\" not just \"user\")\n- The capability should describe what they want to accomplish, not how\n- The benefit should explain the \"why\" — what value does this deliver\n- Include edge cases: error states, empty states, boundary conditions\n- Include different user types if the feature serves multiple personas\n- Order by priority — most important stories first\n\nExample:\n- \"As a team admin, I want to configure SSO for my organization so that my team members can log in with their corporate credentials\"\n- \"As a team member, I want to be automatically redirected to my company's SSO login so that I do not need to remember a separate password\"\n- \"As a team admin, I want to see which members have logged in via SSO so that I can verify the rollout is working\"\n\n### 5. Requirements\n\n**Must-Have (P0)**: The feature cannot ship without these. These represent the minimum viable version of the feature. Ask: \"If we cut this, does the feature still solve the core problem?\" If no, it is P0.\n\n**Nice-to-Have (P1)**: Significantly improves the experience but the core use case works without them. These often become fast follow-ups after launch.\n\n**Future Considerations (P2)**: Explicitly out of scope for v1 but we want to design in a way that supports them later. Documenting these prevents accidental architectural decisions that make them hard later.\n\nFor each requirement:\n- Write a clear, unambiguous description of the expected behavior\n- Include acceptance criteria (see below)\n- Note any technical considerations or constraints\n- Flag dependencies on other teams or systems\n\n### 6. Success Metrics\nSee the success metrics section below for detailed guidance.\n\n### 7. Open Questions\n- Questions that need answers before or during implementation\n- Tag each with who should answer (engineering, design, legal, data, stakeholder)\n- Distinguish between blocking questions (must answer before starting) and non-blocking (can resolve during implementation)\n\n### 8. Timeline Considerations\n- Hard deadlines (contractual commitments, events, compliance dates)\n- Dependencies on other teams' work or releases\n- Suggested phasing if the feature is too large for one release\n\n## User Story Writing\n\nGood user stories are:\n- **Independent**: Can be developed and delivered on their own\n- **Negotiable**: Details can be discussed, the story is not a contract\n- **Valuable**: Delivers value to the user (not just the team)\n- **Estimable**: The team can roughly estimate the effort\n- **Small**: Can be completed in one sprint/iteration\n- **Testable**: There is a clear way to verify it works\n\n### Common Mistakes in User Stories\n- Too vague: \"As a user, I want the product to be faster\" — what specifically should be faster?\n- Solution-prescriptive: \"As a user, I want a dropdown menu\" — describe the need, not the UI widget\n- No benefit: \"As a user, I want to click a button\" — why? What does it accomplish?\n- Too large: \"As a user, I want to manage my team\" — break this into specific capabilities\n- Internal focus: \"As the engineering team, we want to refactor the database\" — this is a task, not a user story\n\n## Requirements Categorization\n\n### MoSCoW Framework\n- **Must have**: Without these, the feature is not viable. Non-negotiable.\n- **Should have**: Important but not critical for launch. High-priority fast follows.\n- **Could have**: Desirable if time permits. Will not delay delivery if cut.\n- **Won't have (this time)**: Explicitly out of scope. May revisit in future versions.\n\n### Tips for Categorization\n- Be ruthless about P0s. The tighter the must-have list, the faster you ship and learn.\n- If everything is P0, nothing is P0. Challenge every must-have: \"Would we really not ship without this?\"\n- P1s should be things you are confident you will build soon, not a wish list.\n- P2s are architectural insurance — they guide design decisions even though you are not building them now.\n\n## Success Metrics Definition\n\n### Leading Indicators\nMetrics that change quickly after launch (days to weeks):\n- **Adoption rate**: % of eligible users who try the feature\n- **Activation rate**: % of users who complete the core action\n- **Task completion rate**: % of users who successfully accomplish their goal\n- **Time to complete**: How long the core workflow takes\n- **Error rate**: How often users encounter errors or dead ends\n- **Feature usage frequency**: How often users return to use the feature\n\n### Lagging Indicators\nMetrics that take time to develop (weeks to months):\n- **Retention impact**: Does this feature improve user retention?\n- **Revenue impact**: Does this drive upgrades, expansion, or new revenue?\n- **NPS / satisfaction change**: Does this improve how users feel about the product?\n- **Support ticket reduction**: Does this reduce support load?\n- **Competitive win rate**: Does this help win more deals?\n\n### Setting Targets\n- Targets should be specific: \"50% adoption within 30 days\" not \"high adoption\"\n- Base targets on comparable features, industry benchmarks, or explicit hypotheses\n- Set a \"success\" threshold and a \"stretch\" target\n- Define the measurement method: what tool, what query, what time window\n- Specify when you will evaluate: 1 week, 1 month, 1 quarter post-launch\n\n## Acceptance Criteria\n\nWrite acceptance criteria in Given/When/Then format or as a checklist:\n\n**Given/When/Then**:\n- Given [precondition or context]\n- When [action the user takes]\n- Then [expected outcome]\n\nExample:\n- Given the admin has configured SSO for their organization\n- When a team member visits the login page\n- Then they are automatically redirected to the organization's SSO provider\n\n**Checklist format**:\n- [ ] Admin can enter SSO provider URL in organization settings\n- [ ] Team members see \"Log in with SSO\" button on login page\n- [ ] SSO login creates a new account if one does not exist\n- [ ] SSO login links to existing account if email matches\n- [ ] Failed SSO attempts show a clear error message\n\n### Tips for Acceptance Criteria\n- Cover the happy path, error cases, and edge cases\n- Be specific about the expected behavior, not the implementation\n- Include what should NOT happen (negative test cases)\n- Each criterion should be independently testable\n- Avoid ambiguous words: \"fast\", \"user-friendly\", \"intuitive\" — define what these mean concretely\n\n## Scope Management\n\n### Recognizing Scope Creep\nScope creep happens when:\n- Requirements keep getting added after the spec is approved\n- \"Small\" additions accumulate into a significantly larger project\n- The team is building features no user asked for (\"while we're at it...\")\n- The launch date keeps moving without explicit re-scoping\n- Stakeholders add requirements without removing anything\n\n### Preventing Scope Creep\n- Write explicit non-goals in every spec\n- Require that any scope addition comes with a scope removal or timeline extension\n- Separate \"v1\" from \"v2\" clearly in the spec\n- Review the spec against the original problem statement — does everything serve it?\n- Time-box investigations: \"If we cannot figure out X in 2 days, we cut it\"\n- Create a \"parking lot\" for good ideas that are not in scope\n", "depth": 4}, {"id": "324", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "325", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "326", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/README.md", "name": "README.md", "type": "file", "content": "# Marketing Plugin\n\nA marketing plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Content creation, campaign planning, brand voice management, competitive analysis, and performance reporting.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/marketing\n```\n\n## Commands\n\n| Command | Description |\n|---|---|\n| `/draft-content` | Draft blog posts, social media, email newsletters, landing pages, press releases, and case studies |\n| `/campaign-plan` | Generate a full campaign brief with objectives, channels, content calendar, and success metrics |\n| `/brand-review` | Review content against your brand voice, style guide, and messaging pillars |\n| `/competitive-brief` | Research competitors and generate a positioning and messaging comparison |\n| `/performance-report` | Build a marketing performance report with key metrics, trends, and optimization recommendations |\n| `/seo-audit` | Run a comprehensive SEO audit — keyword research, on-page analysis, content gaps, technical checks, and competitor comparison |\n| `/email-sequence` | Design and draft multi-email sequences for nurture flows, onboarding, drip campaigns, and more |\n\n## Skills\n\n| Skill | Description |\n|---|---|\n| `content-creation` | Content type templates, writing best practices by channel, SEO fundamentals, headline formulas, and CTA guidance |\n| `campaign-planning` | Campaign frameworks, channel selection, content calendar creation, budget allocation, and success metrics |\n| `brand-voice` | Brand voice documentation, voice attributes, tone adaptation, style guide enforcement, and terminology management |\n| `competitive-analysis` | Competitive research methodology, messaging comparison, content gap analysis, positioning, and battlecard creation |\n| `performance-analytics` | Key metrics by channel, reporting templates, trend analysis, attribution modeling, and optimization frameworks |\n\n## Example Workflows\n\n### Drafting a Blog Post\n\n```\n\u003e /draft-content\nType: blog post\nTopic: How AI is transforming B2B marketing\nAudience: Marketing directors at mid-market SaaS companies\nKey messages: AI saves time on repetitive tasks, improves personalization, requires human oversight\nTone: Authoritative but approachable\nLength: 1200 words\n```\n\nClaude will generate a structured blog post draft with an engaging headline, introduction with a hook, organized sections, SEO-optimized subheadings, and a clear call to action.\n\n### Planning a Campaign\n\n```\n\u003e /campaign-plan\nGoal: Drive 500 signups for our new product launch\nAudience: Technical decision-makers at enterprise companies\nTimeline: 6 weeks\nBudget range: $20,000-$30,000\n```\n\nClaude will produce a campaign brief covering objectives, audience segmentation, key messages, channel strategy, a week-by-week content calendar, and KPIs to track.\n\n### Reviewing Content Against Brand Guidelines\n\n```\n\u003e /brand-review\n[paste your draft content]\n```\n\nIf your brand style guide is configured in local settings, Claude will check your content against voice, tone, terminology, and messaging pillars. If not configured, Claude will ask about your guidelines or provide a generic review for clarity, consistency, and professionalism.\n\n## Configuration\n\nConfigure your brand voice, style guide, and target personas in a local settings file for personalized output. This allows commands like `/draft-content` and `/brand-review` to automatically apply your brand standards without prompting each time.\n\n## MCP Integrations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nThis plugin works with the following MCP servers:\n\n- **Slack** — Share drafts, reports, and briefs with your team\n- **Canva** — Create and edit design assets\n- **Figma** — Access design files and brand assets\n- **HubSpot** — Pull campaign data, manage contacts, and track marketing automation\n- **Amplitude** — Pull product analytics and user behavior data for performance reporting\n- **Notion** — Access briefs, style guides, and campaign documents\n- **Ahrefs** — SEO keyword research, backlink analysis, and site audits\n- **Similarweb** — Competitive traffic analysis and market benchmarking\n- **Klaviyo** — Draft and review email marketing sequences and campaigns\n", "depth": 2}, {"id": "327", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~marketing automation` might mean HubSpot, Marketo, or any other marketing platform with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (design, SEO, email marketing, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Design | `~~design` | Canva, Figma | Adobe Creative Cloud |\n| Marketing automation | `~~marketing automation` | HubSpot | Marketo, Pardot, Mailchimp |\n| Product analytics | `~~product analytics` | Amplitude | Mixpanel, Google Analytics |\n| Knowledge base | `~~knowledge base` | Notion | Confluence, Guru |\n| SEO | `~~SEO` | Ahrefs, Similarweb | Semrush, Moz |\n| Email marketing | `~~email marketing` | Klaviyo | Mailchimp, Brevo, Customer.io |\n", "depth": 2}, {"id": "328", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/email-sequence.md", "name": "email-sequence.md", "type": "file", "content": "---\ndescription: Design and draft multi-email sequences for nurture flows, onboarding, drip campaigns, and more\nargument-hint: \"[sequence type]\"\n---\n\n# Email Sequence\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nDesign and draft complete email sequences with full copy, timing, branching logic, and performance benchmarks for any lifecycle or campaign use case.\n\n## Trigger\n\nUser runs `/email-sequence` or asks to create, design, build, or draft an email sequence, drip campaign, nurture flow, or onboarding series.\n\n## Inputs\n\nGather the following from the user. If not provided, ask before proceeding:\n\n1. **Sequence type** — one of:\n   - Onboarding\n   - Lead nurture\n   - Re-engagement\n   - Product launch\n   - Event follow-up\n   - Upgrade/upsell\n   - Win-back\n   - Educational drip\n\n2. **Goal** — what the sequence should achieve (e.g., activate new users, convert leads to customers, reduce churn, drive event attendance, upsell to a higher tier)\n\n3. **Audience** — who receives this sequence, what stage they are at, and any relevant segmentation details (role, industry, behavior triggers, lifecycle stage)\n\n4. **Number of emails** (optional) — if not specified, recommend a count based on the sequence type using the templates in the Sequence Type Templates section below\n\n5. **Timing/cadence preferences** (optional) — desired spacing between emails (e.g., \"every 3 days\", \"weekly\", \"aggressive first week then taper off\")\n\n6. **Brand voice** — if configured in local settings, apply automatically and inform the user. If not configured, ask: \"Do you have brand voice guidelines I should follow? If not, I'll use a clear, conversational professional tone.\"\n\n7. **Additional context** (optional):\n   - Specific offers, discounts, or incentives to include\n   - CTAs or landing pages to link to\n   - Content assets available (blog posts, case studies, videos, guides)\n   - Product features to highlight\n   - Competitor differentiators to reference\n\n## Process\n\n### 1. Sequence Strategy\n\nBefore drafting any emails, define the overall sequence architecture:\n\n- **Narrative arc** — what story does this sequence tell across all emails? What is the emotional and logical progression from first email to last?\n- **Journey mapping** — map each email to a stage of the buyer or user journey (awareness, consideration, decision, activation, expansion)\n- **Escalation logic** — how does the intensity, urgency, or value of each email build on the previous one?\n- **Success definition** — what specific action signals that the sequence has done its job and the recipient should exit?\n\n### 2. Individual Email Design\n\nFor each email in the sequence, produce:\n\n#### Subject Line\n- Provide 2-3 options per email\n- Vary approaches: curiosity, benefit-driven, urgency, personalization, question-based\n- Keep under 50 characters where possible; note preview behavior on mobile\n\n#### Preview Text\n- 40-90 characters that complement (not repeat) the subject line\n- Should add context or intrigue that increases open likelihood\n\n#### Email Purpose\n- One sentence explaining why this email exists and what it moves the recipient toward\n\n#### Body Copy\n- Full draft ready to use\n- Clear hierarchy: hook, body, CTA\n- Short paragraphs (2-3 sentences max)\n- Scannable formatting with bold key phrases where appropriate\n- Personalization tokens where relevant (e.g., first name, company name, product used)\n\n#### Primary CTA\n- Button text and destination\n- One primary CTA per email (secondary CTA only if appropriate for the sequence stage)\n\n#### Timing\n- Days after the trigger event or after the previous email\n- Note if timing should adjust based on engagement (e.g., \"send sooner if they opened but did not click\")\n\n#### Segment/Condition Notes\n- Who receives this email vs. who skips it\n- Any behavioral or attribute-based conditions (e.g., \"only send to users who have not completed setup\")\n\n### 3. Sequence Logic\n\nDefine the flow control for the sequence:\n\n- **Branching conditions** — alternate paths based on engagement. For example:\n  - \"If opened email 2 but did not click CTA, send email 2b (softer re-ask) instead of email 3\"\n  - \"If clicked CTA in email 1, skip email 2 and go directly to email 3\"\n- **Exit conditions** — when a recipient converts (completes the desired action), remove them from the sequence. Define what \"conversion\" means for this sequence.\n- **Re-entry rules** — can someone re-enter the sequence? Under what conditions? (e.g., \"if a user churns again 90 days later, re-enter the win-back sequence\")\n- **Suppression rules** — do not send if the recipient is already in another active sequence, has unsubscribed from marketing, or has contacted support in the last 48 hours\n\n### 4. Performance Benchmarks\n\nProvide expected benchmarks based on the sequence type so the user can set targets:\n\n| Metric | Onboarding | Lead Nurture | Re-engagement | Win-back |\n|--------|-----------|--------------|---------------|----------|\n| Open rate | 50-70% | 20-30% | 15-25% | 15-20% |\n| Click-through rate | 10-20% | 3-7% | 2-5% | 2-4% |\n| Conversion rate | 15-30% | 2-5% | 3-8% | 1-3% |\n| Unsubscribe rate | \u003c0.5% | \u003c0.5% | 1-2% | 1-3% |\n\nAdjust benchmarks based on industry and audience if the user has provided that context.\n\n## Sequence Type Templates\n\nUse these as starting frameworks. Adapt length and content based on the user's goal and audience.\n\n**Onboarding (5-7 emails over 14-21 days):**\nWelcome and set expectations -- Quick win to demonstrate value -- Core feature deep dive -- Advanced feature or integration -- Social proof and community -- Check-in and feedback request -- Upgrade prompt or next steps\n\n**Lead Nurture (4-6 emails over 3-4 weeks):**\nValue-first educational content -- Pain point identification -- Solution positioning with proof -- Social proof and results -- Soft CTA (trial, demo, resource) -- Direct CTA (buy, book, sign up)\n\n**Re-engagement (3-4 emails over 10-14 days):**\n\"We miss you\" with a compelling reason to return -- Value reminder highlighting what they are missing -- Incentive or exclusive offer -- Last chance with clear deadline\n\n**Win-back (3-5 emails over 30 days):**\nFriendly check-in asking what went wrong -- What is new since they left -- Special offer or incentive to return -- Feedback request (even if they do not come back) -- Final goodbye with door open\n\n**Product Launch (4-6 emails over 2-3 weeks):**\nTeaser or pre-announcement -- Launch announcement with full details -- Feature spotlight or use case -- Social proof and early results -- Limited-time offer or bonus -- Last chance or reminder\n\n**Event Follow-up (3-4 emails over 7-10 days):**\nThank you with key takeaways or recordings -- Resource roundup from the event -- Related offer or next step -- Feedback survey\n\n**Upgrade/Upsell (3-5 emails over 2-3 weeks):**\nUsage milestone or success celebration -- Feature gap or limitation they are hitting -- Upgrade benefits with proof -- Limited-time incentive -- Direct comparison of plans\n\n**Educational Drip (5-8 emails over 4-6 weeks):**\nIntroduction and what they will learn -- Lesson 1: foundational concept -- Lesson 2: intermediate concept -- Lesson 3: advanced concept -- Practical application or exercise -- Resource roundup -- Graduation and next steps\n\n## Tool Integration\n\n### If ~~email marketing is connected (e.g., Klaviyo, Mailchimp, Customer.io)\n- Reference how to set up the sequence as a flow or automation in the platform\n- Note any platform-specific features to use (e.g., smart send time, conditional splits, A/B testing)\n- Map the branching logic to the platform's visual flow builder concepts\n\n### If ~~marketing automation or ~~CRM is connected (e.g., HubSpot, Marketo)\n- Reference lead scoring data to inform segmentation and exit conditions\n- Use lifecycle stage data to tailor messaging per segment\n- Note how to set enrollment triggers based on CRM properties or list membership\n\n### If no tools are connected\n- Deliver all email content in copy-paste-ready format\n- Include a setup checklist the user can follow in any email platform:\n  1. Create the automation or flow\n  2. Set the enrollment trigger\n  3. Add each email with the specified delays\n  4. Configure branching and exit conditions\n  5. Set up tracking for the recommended metrics\n\n## Output\n\nPresent the complete sequence with the following sections:\n\n### Sequence Overview Table\n\n| # | Subject Line | Purpose | Timing | Primary CTA | Condition |\n|---|-------------|---------|--------|-------------|-----------|\n\n### Full Email Drafts\nEach email with subject line options, preview text, purpose, body copy, CTA, timing, and segment notes.\n\n### Sequence Flow Diagram\nA text-based diagram showing the email flow, branching paths, and exit points. Use a clear format such as:\n\n```\n[Trigger] --\u003e Email 1 (Day 0)\n                |\n          Opened? --Yes--\u003e Email 2 (Day 3)\n                |              |\n                No        Clicked CTA? --Yes--\u003e [EXIT: Converted]\n                |              |\n                v              No\n          Email 1b (Day 2)     |\n                |              v\n                +--------\u003e Email 3 (Day 7)\n                               |\n                               v\n                          Email 4 (Day 10)\n                               |\n                          [EXIT: Sequence complete]\n```\n\n### Branching Logic Notes\nSummary of all conditions, exits, and suppressions in a reference list.\n\n### A/B Test Suggestions\n- 2-3 recommended A/B tests (subject lines, CTA text, send time, email length)\n- What to test, how to split, and how to measure the winner\n\n### Metrics to Track\n- Primary conversion metric for the sequence\n- Per-email metrics: open rate, CTR, unsubscribe rate\n- Sequence-level metrics: overall conversion rate, time to conversion, drop-off points\n- Recommended review cadence (e.g., \"Review performance weekly for the first month, then monthly\")\n\n## After the Sequence\n\nAsk: \"Would you like me to:\n- Revise the copy or tone for any specific email?\n- Add a branching path for a specific scenario?\n- Create a variation of this sequence for a different audience segment?\n- Draft the A/B test variants for the subject lines?\n- Build a companion sequence (e.g., a post-purchase follow-up after this lead nurture converts)?\"\n", "depth": 3}, {"id": "329", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/performance-report.md", "name": "performance-report.md", "type": "file", "content": "---\ndescription: Build a marketing performance report with key metrics, trends, and optimization recommendations\nargument-hint: \"\u003ctime period or campaign\u003e\"\n---\n\n# Performance Report\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a marketing performance report with key metrics, trend analysis, insights, and optimization recommendations.\n\n## Trigger\n\nUser runs `/performance-report` or asks for a marketing report, performance analysis, campaign results, or metrics summary.\n\n## Inputs\n\n1. **Report type** — determine which type of report the user needs:\n   - **Campaign report** — performance of a specific campaign\n   - **Channel report** — performance across a specific channel (email, social, paid, SEO, etc.)\n   - **Content performance** — how content pieces are performing\n   - **Overall marketing report** — cross-channel summary (weekly, monthly, quarterly)\n   - **Custom** — user-defined scope\n\n2. **Time period** — the reporting window (last week, last month, last quarter, custom date range)\n\n3. **Data source**:\n   - If ~~product analytics is connected: pull performance data automatically\n   - If not connected: ask the user to provide metrics. Prompt with: \"Please paste or share your performance data. I can work with spreadsheets, CSV data, dashboard screenshots described in text, or just the key numbers.\"\n\n4. **Comparison period** (optional) — prior period or year-over-year for trend context\n\n5. **Stakeholder audience** (optional) — who will read this report (executive summary style vs. detailed analyst view)\n\n## Report Structure\n\n### 1. Executive Summary\n- 2-3 sentence overview of performance in the period\n- Headline metric with trend direction (up/down/flat vs. prior period)\n- One key win and one area of concern\n\n### 2. Key Metrics Dashboard\n\nPresent core metrics in a summary table:\n\n| Metric | This Period | Prior Period | Change | Target | Status |\n|--------|------------|--------------|--------|--------|--------|\n\nStatus indicators:\n- On track (meeting or exceeding target)\n- At risk (below target but within acceptable range)\n- Off track (significantly below target)\n\n#### Metrics by Report Type\n\n**Campaign Report:**\n- Impressions and reach\n- Click-through rate (CTR)\n- Conversion rate\n- Cost per acquisition (CPA)\n- Return on ad spend (ROAS) or ROI\n- Total conversions/signups/leads\n\n**Channel Report (Email):**\n- Emails sent, delivered, bounced\n- Open rate\n- Click-through rate\n- Unsubscribe rate\n- Conversion rate\n\n**Channel Report (Social):**\n- Impressions and reach\n- Engagement rate (likes, comments, shares)\n- Follower growth\n- Click-through rate\n- Top-performing posts\n\n**Channel Report (Paid):**\n- Spend\n- Impressions and clicks\n- CTR\n- CPC and CPM\n- Conversions and CPA\n- ROAS\n\n**Channel Report (SEO/Organic):**\n- Organic sessions\n- Keyword rankings (movement)\n- Pages indexed\n- Backlinks acquired\n- Top-performing pages\n\n**Content Performance:**\n- Pageviews and unique visitors\n- Time on page\n- Bounce rate\n- Social shares\n- Conversions attributed to content\n- Top and bottom performers\n\n**Overall Marketing Report:**\n- Total leads generated\n- Marketing qualified leads (MQLs)\n- Pipeline contribution\n- Customer acquisition cost (CAC)\n- Channel-by-channel summary\n\n### 3. Trend Analysis\n- Performance trend over the period (week-over-week or month-over-month)\n- Notable inflection points and what caused them\n- Seasonal or cyclical patterns observed\n- Comparison to benchmarks or targets\n\n### 4. What Worked\n- Top 3-5 wins with specific data\n- Why these performed well (hypothesis)\n- How to replicate or scale\n\n### 5. What Needs Improvement\n- Bottom 3-5 performers with specific data\n- Hypotheses for underperformance\n- Recommended fixes\n\n### 6. Insights and Observations\n- Patterns in the data that are not obvious from the metrics alone\n- Audience behavior insights\n- Content or creative themes that resonated\n- External factors that may have influenced performance (seasonality, news, competitive moves)\n\n### 7. Recommendations\nFor each recommendation:\n- What to do\n- Why (linked to a specific insight from the data)\n- Expected impact (high, medium, low)\n- Effort to implement (high, medium, low)\n- Priority (immediate, next sprint, next quarter)\n\nPrioritize recommendations in a 2x2 matrix format:\n\n| | Low Effort | High Effort |\n|---|---|---|\n| **High Impact** | Do first | Plan for next sprint |\n| **Low Impact** | Do if time allows | Deprioritize |\n\n### 8. Next Period Focus\n- Top 3 priorities for the upcoming period\n- Tests or experiments to run\n- Targets for key metrics\n\n## Output Formatting\n\n- Use tables for data presentation\n- Bold key numbers and trends\n- Keep the executive summary concise (suitable for forwarding to leadership)\n- Include a \"detailed appendix\" section for granular data if the user provided a lot of metrics\n\n## After the Report\n\nAsk: \"Would you like me to:\n- Create a slide-ready summary of these results?\n- Draft a stakeholder email with the key takeaways?\n- Dive deeper into any specific metric or channel?\n- Set up a reporting template you can reuse next period?\"\n", "depth": 3}, {"id": "330", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/competitive-brief.md", "name": "competitive-brief.md", "type": "file", "content": "---\ndescription: Research competitors and generate a positioning and messaging comparison\nargument-hint: \"\u003ccompetitor or market segment\u003e\"\n---\n\n# Competitive Brief\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nResearch competitors and generate a structured competitive analysis comparing positioning, messaging, content strategy, and market presence.\n\n## Trigger\n\nUser runs `/competitive-brief` or asks for a competitive analysis, competitor research, or market comparison.\n\n## Inputs\n\nGather the following from the user:\n\n1. **Competitor name(s)** — one or more competitors to analyze (required)\n\n2. **Your company/product context** (optional but recommended):\n   - What you sell and to whom\n   - Your positioning or value proposition\n   - Key differentiators you want to highlight\n\n3. **Focus areas** (optional — if not specified, cover all):\n   - Messaging and positioning\n   - Product and feature comparison\n   - Content and thought leadership strategy\n   - Recent announcements and news\n   - Pricing and packaging (if publicly available)\n   - Market presence and audience\n\n## Research Process\n\nFor each competitor, research using web search:\n\n1. **Company website** — homepage messaging, product pages, about page, pricing page\n2. **Recent news** — press releases, funding announcements, product launches, partnerships (last 6 months)\n3. **Content strategy** — blog topics, resource types, social media presence, webinars, podcasts\n4. **Review sites and comparisons** — third-party comparisons, analyst mentions, customer review themes\n5. **Job postings** — hiring signals that indicate strategic direction (optional)\n\n## Competitive Brief Structure\n\n### 1. Executive Summary\n- 2-3 sentence overview of the competitive landscape\n- Key takeaway: your biggest opportunity and biggest threat\n\n### 2. Competitor Profiles\n\nFor each competitor:\n\n#### Company Overview\n- What they do (one-sentence positioning)\n- Target audience\n- Company size/stage indicators (funding, employee count if available)\n- Key recent developments\n\n#### Messaging Analysis\n- Primary tagline or headline\n- Core value proposition\n- Key messaging themes (3-5)\n- Tone and voice characterization\n- How they describe the problem they solve\n\n#### Product/Solution Positioning\n- How they categorize their product\n- Key features they emphasize\n- Claimed differentiators\n- Pricing approach (if publicly available)\n\n#### Content Strategy\n- Blog frequency and topics\n- Content types produced (ebooks, webinars, case studies, tools)\n- Social media presence and engagement approach\n- Thought leadership themes\n- SEO strategy observations (what terms they appear to target)\n\n#### Strengths\n- What they do well\n- Where their messaging resonates\n- Competitive advantages\n\n#### Weaknesses\n- Gaps in their messaging or positioning\n- Areas where they are vulnerable\n- Customer complaints or criticism themes (from reviews)\n\n### 3. Messaging Comparison Matrix\n\n| Dimension | Your Company | Competitor A | Competitor B |\n|-----------|-------------|--------------|--------------|\n| Primary tagline | ... | ... | ... |\n| Target buyer | ... | ... | ... |\n| Key differentiator | ... | ... | ... |\n| Tone/voice | ... | ... | ... |\n| Core value prop | ... | ... | ... |\n\n(Include user's company only if they provided their positioning context)\n\n### 4. Content Gap Analysis\n- Topics your competitors cover that you do not (or vice versa)\n- Content formats they use that you could adopt\n- Keywords or themes they own vs. opportunities they have missed\n\n### 5. Opportunities\n- Positioning gaps you can exploit\n- Messaging angles your competitors have not claimed\n- Audience segments they are underserving\n- Content or channel opportunities\n\n### 6. Threats\n- Areas where competitors are strong and you are vulnerable\n- Trends that favor their positioning\n- Recent moves that could shift the market\n\n### 7. Recommended Actions\n- 3-5 specific, actionable recommendations based on the analysis\n- Quick wins (things you can act on this week)\n- Strategic moves (longer-term positioning or content investments)\n\n## Output\n\nPresent the full competitive brief with clear formatting. Note the date of the research so the user knows the freshness of the data.\n\nAfter the brief, ask:\n\n\"Would you like me to:\n- Create a battlecard for your sales team based on this analysis?\n- Draft messaging that exploits the positioning gaps identified?\n- Dive deeper into any specific competitor?\n- Set up a competitive monitoring plan?\"\n", "depth": 3}, {"id": "331", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/draft-content.md", "name": "draft-content.md", "type": "file", "content": "---\ndescription: Draft blog posts, social media, email newsletters, landing pages, press releases, and case studies\nargument-hint: \"\u003ccontent type and topic\u003e\"\n---\n\n# Draft Content\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate marketing content drafts tailored to a specific content type, audience, and brand voice.\n\n## Trigger\n\nUser runs `/draft-content` or asks to draft, write, or create marketing content.\n\n## Inputs\n\nGather the following from the user. If not provided, ask before proceeding:\n\n1. **Content type** — one of:\n   - Blog post\n   - Social media post (specify platform: LinkedIn, Twitter/X, Instagram, Facebook)\n   - Email newsletter\n   - Landing page copy\n   - Press release\n   - Case study\n\n2. **Topic** — the subject or theme of the content\n\n3. **Target audience** — who this content is for (role, industry, seniority, pain points)\n\n4. **Key messages** — 2-4 main points or takeaways to communicate\n\n5. **Tone** — e.g., authoritative, conversational, inspirational, technical, witty (optional if brand voice is configured)\n\n6. **Length** — target word count or format constraint (e.g., \"1000 words\", \"280 characters\", \"3 paragraphs\")\n\n## Brand Voice\n\n- If the user has a brand voice configured in their local settings file, apply it automatically. Inform the user that brand voice settings are being applied.\n- If no brand voice is configured, ask: \"Do you have brand voice guidelines you'd like me to follow? If not, I'll use a neutral professional tone.\"\n- Apply the specified or default tone consistently throughout the draft.\n\n## Content Generation by Type\n\n### Blog Post\n- Engaging headline (provide 2-3 options)\n- Introduction with a hook (question, statistic, bold statement, or story)\n- 3-5 organized sections with descriptive subheadings\n- Supporting points, examples, or data references in each section\n- Conclusion with a clear call to action\n- SEO considerations: suggest a primary keyword, include it in the headline and first paragraph, use related keywords in subheadings\n\n### Social Media Post\n- Platform-appropriate format and length\n- Hook in the first line\n- Hashtag suggestions (3-5 relevant hashtags)\n- Call to action or engagement prompt\n- Emoji usage appropriate to brand and platform\n- If LinkedIn: professional framing, paragraph breaks for readability\n- If Twitter/X: concise, punchy, within character limit\n- If Instagram: visual-first language, story-driven, hashtag block\n\n### Email Newsletter\n- Subject line (provide 2-3 options with open-rate considerations)\n- Preview text\n- Greeting\n- Body sections with clear hierarchy\n- Call to action button text\n- Sign-off\n- Unsubscribe note reminder\n\n### Landing Page Copy\n- Headline and subheadline\n- Hero section copy\n- Value propositions (3-4 benefit-driven bullets or sections)\n- Social proof placeholder (suggest testimonial or stat placement)\n- Primary and secondary CTAs\n- FAQ section suggestions\n- SEO: meta title and meta description suggestions\n\n### Press Release\n- Headline following press release conventions\n- Dateline and location\n- Lead paragraph (who, what, when, where, why)\n- Supporting quotes (provide placeholder guidance)\n- Company boilerplate placeholder\n- Media contact placeholder\n- Standard press release formatting\n\n### Case Study\n- Title emphasizing the result\n- Customer overview (industry, size, challenge)\n- Challenge section\n- Solution section (what was implemented)\n- Results section with metrics (prompt user for data)\n- Customer quote placeholder\n- Call to action\n\n## SEO Considerations (for web content)\n\nFor blog posts, landing pages, and other web-facing content:\n- Suggest a primary keyword based on the topic\n- Recommend keyword placement: headline, first paragraph, subheadings, meta description\n- Suggest internal and external linking opportunities\n- Recommend a meta description (under 160 characters)\n- Note image alt text opportunities\n\n## Output\n\nPresent the draft with clear formatting. After the draft, include:\n- A brief note on what brand voice and tone were applied\n- Any SEO recommendations (for web content)\n- Suggestions for next steps (e.g., \"Review with your team\", \"Add customer quotes\", \"Pair with a visual\")\n\nAsk: \"Would you like me to revise any section, adjust the tone, or create a variation for a different channel?\"\n", "depth": 3}, {"id": "332", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/brand-review.md", "name": "brand-review.md", "type": "file", "content": "---\ndescription: Review content against your brand voice, style guide, and messaging pillars\nargument-hint: \"\u003ccontent to review\u003e\"\n---\n\n# Brand Review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nReview marketing content against brand voice, style guidelines, and messaging standards. Flag deviations and provide specific improvement suggestions.\n\n## Trigger\n\nUser runs `/brand-review` or asks to review, check, or audit content against brand guidelines.\n\n## Inputs\n\n1. **Content to review** — accept content in any of these forms:\n   - Pasted directly into the conversation\n   - A file path or ~~knowledge base reference (e.g. Notion page, shared doc)\n   - A URL to a published page\n   - Multiple pieces for batch review\n\n2. **Brand guidelines source** (determined automatically):\n   - If a brand style guide is configured in local settings, use it automatically\n   - If not configured, ask: \"Do you have a brand style guide or voice guidelines I should review against? You can paste them, share a file, or describe your brand voice. Otherwise, I'll do a general review for clarity, consistency, and professionalism.\"\n\n## Review Process\n\n### With Brand Guidelines Configured\n\nEvaluate the content against each of these dimensions:\n\n#### Voice and Tone\n- Does the content match the defined brand voice attributes?\n- Is the tone appropriate for the content type and audience?\n- Are there shifts in voice that feel inconsistent?\n- Flag specific sentences or phrases that deviate with an explanation of why\n\n#### Terminology and Language\n- Are preferred brand terms used correctly?\n- Are any \"avoid\" terms or phrases present?\n- Is jargon level appropriate for the target audience?\n- Are product names, feature names, and branded terms used correctly (capitalization, formatting)?\n\n#### Messaging Pillars\n- Does the content align with defined messaging pillars or value propositions?\n- Are claims consistent with approved messaging?\n- Is the content reinforcing or contradicting brand positioning?\n\n#### Style Guide Compliance\n- Grammar and punctuation per style guide (e.g., Oxford comma, title case vs. sentence case)\n- Formatting conventions (headers, lists, emphasis)\n- Number formatting, date formatting\n- Acronym usage (defined on first use?)\n\n### Without Brand Guidelines (Generic Review)\n\nEvaluate the content for:\n\n#### Clarity\n- Is the main message clear within the first paragraph?\n- Are sentences concise and easy to understand?\n- Is the structure logical and easy to follow?\n- Are there ambiguous statements or unclear references?\n\n#### Consistency\n- Is the tone consistent throughout?\n- Are terms used consistently (no switching between synonyms for the same concept)?\n- Is formatting consistent (headers, lists, capitalization)?\n\n#### Professionalism\n- Is the content free of typos, grammatical errors, and awkward phrasing?\n- Is the tone appropriate for the intended audience?\n- Are claims supported or substantiated?\n\n### Legal and Compliance Flags (Always Checked)\n\nRegardless of whether brand guidelines are configured, flag:\n- **Unsubstantiated claims** — superlatives (\"best\", \"fastest\", \"only\") without evidence or qualification\n- **Missing disclaimers** — financial claims, health claims, or guarantees that may need legal disclaimers\n- **Comparative claims** — comparisons to competitors that could be challenged\n- **Regulatory language** — content that may need compliance review (financial services, healthcare, etc.)\n- **Testimonial issues** — quotes or endorsements without attribution or disclosure\n- **Copyright concerns** — content that appears to be closely paraphrased from other sources\n\n## Output Format\n\nPresent the review as:\n\n### Summary\n- Overall assessment: how well the content aligns with brand standards (or general quality)\n- 1-2 sentence summary of the biggest strengths\n- 1-2 sentence summary of the most important improvements\n\n### Detailed Findings\n\nFor each issue found, provide:\n\n| Issue | Location | Severity | Suggestion |\n|-------|----------|----------|------------|\n\nWhere severity is:\n- **High** — contradicts brand voice, contains compliance risk, or significantly undermines messaging\n- **Medium** — inconsistent with guidelines but not damaging\n- **Low** — minor style or preference issue\n\n### Revised Sections\n\nFor the top 3-5 highest-severity issues, provide a before/after showing the original text and a suggested revision.\n\n### Legal/Compliance Flags\n\nList any legal or compliance concerns separately with recommended actions.\n\n## After Review\n\nAsk: \"Would you like me to:\n- Revise the full content with these suggestions applied?\n- Focus on fixing just the high-severity issues?\n- Review additional content against the same guidelines?\n- Help you document your brand voice for future reviews?\"\n", "depth": 3}, {"id": "333", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/campaign-plan.md", "name": "campaign-plan.md", "type": "file", "content": "---\ndescription: Generate a full campaign brief with objectives, channels, content calendar, and success metrics\nargument-hint: \"\u003ccampaign objective or product\u003e\"\n---\n\n# Campaign Plan\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a comprehensive marketing campaign brief with objectives, audience, messaging, channel strategy, content calendar, and success metrics.\n\n## Trigger\n\nUser runs `/campaign-plan` or asks to plan, design, or build a marketing campaign.\n\n## Inputs\n\nGather the following from the user. If not provided, ask before proceeding:\n\n1. **Campaign goal** — the primary objective (e.g., drive signups, increase awareness, launch a product, generate leads, re-engage churned users)\n\n2. **Target audience** — who the campaign is aimed at (demographics, roles, industries, pain points, buying stage)\n\n3. **Timeline** — campaign duration and any fixed dates (launch date, event date, seasonal deadline)\n\n4. **Budget range** — approximate budget or budget tier (optional; if not provided, generate a channel-agnostic plan and note where budget allocation would matter)\n\n5. **Additional context** (optional):\n   - Product or service being promoted\n   - Key differentiators or value propositions\n   - Previous campaign performance or learnings\n   - Brand guidelines or constraints\n   - Geographic focus\n\n## Campaign Brief Structure\n\nGenerate a campaign brief with the following sections:\n\n### 1. Campaign Overview\n- Campaign name suggestion\n- One-sentence campaign summary\n- Primary objective with a specific, measurable goal\n- Secondary objectives (if applicable)\n\n### 2. Target Audience\n- Primary audience segment with description\n- Secondary audience segment (if applicable)\n- Audience pain points and motivations\n- Where they spend time (channels, communities, publications)\n- Buying stage alignment (awareness, consideration, decision)\n\n### 3. Key Messages\n- Core campaign message (one sentence)\n- 3-4 supporting messages tailored to audience pain points\n- Message variations by channel (if different tones are needed)\n- Proof points or evidence to support each message\n\n### 4. Channel Strategy\nRecommend channels based on audience and goal. For each channel, include:\n- Why this channel fits the audience and objective\n- Content format recommendations\n- Estimated effort level (low, medium, high)\n- Budget allocation suggestion (if budget was provided)\n\nConsider channels from:\n- Owned: blog, email, website, social media profiles\n- Earned: PR, influencer partnerships, guest posts, community engagement\n- Paid: search ads, social ads, display, sponsored content, events\n\n### 5. Content Calendar\nCreate a week-by-week (or day-by-day for short campaigns) content calendar:\n- What content to produce each week\n- Which channel each piece targets\n- Key milestones and deadlines\n- Dependencies between pieces (e.g., \"landing page must be live before paid ads launch\")\n\nFormat as a table:\n\n| Week | Content Piece | Channel | Owner/Notes | Status |\n|------|--------------|---------|-------------|--------|\n\n### 6. Content Pieces Needed\nList every content asset required for the campaign:\n- Asset name and type (blog post, email, social post, ad creative, landing page, etc.)\n- Brief description of what it should contain\n- Priority (must-have vs. nice-to-have)\n- Suggested timeline for creation\n\n### 7. Success Metrics\nDefine KPIs aligned to the campaign objective:\n- Primary KPI with target number\n- Secondary KPIs (3-5)\n- How each metric will be tracked\n- Reporting cadence recommendation\n\nIf ~~product analytics is connected, reference any available historical performance benchmarks to inform targets.\n\n### 8. Budget Allocation (if budget provided)\n- Breakdown by channel or activity\n- Production costs vs. distribution costs\n- Contingency recommendation (typically 10-15%)\n\n### 9. Risks and Mitigations\n- 2-3 potential risks (timeline, audience mismatch, channel underperformance)\n- Mitigation strategy for each\n\n### 10. Next Steps\n- Immediate action items to kick off the campaign\n- Stakeholder approvals needed\n- Key decision points\n\n## Output\n\nPresent the full campaign brief with clear headings and formatting. After the brief, ask:\n\n\"Would you like me to:\n- Dive deeper into any section?\n- Draft specific content pieces from the calendar?\n- Create a competitive analysis to inform the messaging?\n- Adjust the plan for a different budget or timeline?\"\n", "depth": 3}, {"id": "334", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/commands/seo-audit.md", "name": "seo-audit.md", "type": "file", "content": "---\ndescription: Run a comprehensive SEO audit — keyword research, on-page analysis, content gaps, technical checks, and competitor comparison\nargument-hint: \"\u003curl or topic\u003e [audit type]\"\n---\n\n# /seo-audit\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nAudit a website's SEO health, research keyword opportunities, identify content gaps, and benchmark against competitors. Produces a prioritized action plan a marketer can execute immediately.\n\n## Trigger\n\nUser runs `/seo-audit` or asks for an SEO audit, keyword research, content gap analysis, technical SEO check, or competitor SEO comparison.\n\n## Inputs\n\nGather the following from the user. If not provided, ask before proceeding:\n\n1. **URL or domain** — the site to audit, or a topic/keyword if running in keyword research mode\n\n2. **Audit type** — one of:\n   - **Full site audit** — end-to-end SEO review covering all sections below\n   - **Keyword research** — identify keyword opportunities for a topic or domain\n   - **Content gap analysis** — find topics competitors rank for that you don't\n   - **Technical SEO check** — crawlability, speed, structured data, and infrastructure issues\n   - **Competitor SEO comparison** — head-to-head SEO benchmarking against specific competitors\n\n   If not specified, default to **full site audit**.\n\n3. **Target keywords or topics** (optional) — specific keywords the user is already targeting or wants to rank for\n\n4. **Competitors** (optional) — domains or companies to compare against. If not provided and the audit type requires competitor data, use web search to identify 2-3 likely competitors based on the user's domain and keyword space.\n\n## Process\n\n### 1. Keyword Research\n\nResearch keywords related to the user's domain, topic, or target keywords.\n\n**If ~~SEO tools are connected:**\n- Pull keyword data, search volume, keyword difficulty scores, and ranking positions automatically\n- Identify keywords the site currently ranks for and where it's gaining or losing ground\n\n**If ~~product analytics are connected:**\n- Cross-reference keyword targets with actual organic traffic data to validate which keywords are driving visits and conversions\n\n**If tools are not connected:**\n- Use web search to research the keyword landscape\n- Note: \"For more precise volume and difficulty data, connect an SEO tool like Ahrefs or Semrush via MCP. The audit will auto-populate with ranking data.\"\n\nFor each keyword opportunity, assess:\n- **Primary keywords** — high-intent terms directly tied to the user's product or service\n- **Secondary keywords** — supporting terms and variations\n- **Search volume signals** — relative demand (high, medium, low) based on available data\n- **Keyword difficulty** — how competitive the term is (easy, moderate, hard)\n- **Long-tail opportunities** — specific, lower-competition phrases with clear intent\n- **Question-based keywords** — \"how to\", \"what is\", \"why does\" queries that mirror People Also Ask results\n- **Intent classification** — informational, navigational, commercial, or transactional\n\n### 2. On-Page SEO Audit\n\nFor each key page (homepage, top landing pages, recent blog posts), evaluate:\n\n- **Title tags** — present, unique, within 50-60 characters, includes target keyword\n- **Meta descriptions** — present, compelling, within 150-160 characters, includes a call to action\n- **H1 tags** — exactly one per page, includes primary keyword\n- **H2/H3 structure** — logical hierarchy, uses secondary keywords where natural\n- **Keyword usage** — primary keyword appears in the first 100 words, used naturally throughout, not over-stuffed\n- **Internal linking** — pages link to related content, orphan pages identified, anchor text is descriptive\n- **Image alt text** — all images have descriptive alt attributes, keywords included where relevant\n- **URL structure** — clean, readable, includes keywords, no excessive parameters or depth\n\n### 3. Content Gap Analysis\n\nIdentify what's missing from the user's content strategy:\n\n- **Competitor topic coverage** — topics and keywords competitors rank for that the user's site does not cover\n- **Content freshness** — pages that haven't been updated in 12+ months and may be losing rankings\n- **Thin content** — pages with insufficient depth to rank (under 300 words for informational queries, lacking substance)\n- **Missing content types** — formats competitors use that the user doesn't (guides, comparison pages, glossaries, tools, templates)\n- **Funnel gaps** — missing content at specific buyer journey stages (awareness, consideration, decision)\n- **Topic clusters** — opportunities to build pillar pages with supporting content\n\n### 4. Technical SEO Checklist\n\nEvaluate technical foundations that affect crawlability and rankings:\n\n- **Page speed** — identify slow-loading pages and likely causes (large images, render-blocking scripts, excessive redirects)\n- **Mobile-friendliness** — responsive design, tap targets, font sizes, viewport configuration\n- **Structured data** — opportunities for schema markup (FAQ, HowTo, Product, Article, Organization, Breadcrumb)\n- **Crawlability** — robots.txt configuration, XML sitemap presence and accuracy, canonical tags, noindex/nofollow usage\n- **Broken links** — internal and external 404s, redirect chains\n- **HTTPS** — secure connection, mixed content issues\n- **Core Web Vitals signals** — LCP, FID/INP, CLS indicators based on observable page behavior\n- **Indexation** — pages that should be indexed but may not be, duplicate content risks\n\n### 5. Competitor SEO Comparison\n\nFor each competitor, compare:\n\n- **Keyword overlap** — keywords both sites rank for, and where each site ranks higher\n- **Keyword gaps** — terms the competitor ranks for that the user does not\n- **Domain authority signals** — relative site strength based on backlink profiles, referring domains, and content depth\n- **Content depth** — average content length, topic coverage breadth, publishing frequency\n- **Backlink profile observations** — types of sites linking to competitors, link-worthy content they've produced\n- **SERP feature ownership** — which competitor appears in featured snippets, People Also Ask, image packs, or knowledge panels\n- **Technical advantages** — site speed differences, mobile experience, structured data usage\n\n## Output\n\n### Executive Summary\n\nOpen with a 3-5 sentence summary of overall SEO health. Highlight:\n- The site's biggest strength\n- The top 3 priorities that will have the most impact\n- An overall assessment: strong foundation, needs work, or critical issues\n\n### Keyword Opportunity Table\n\n| Keyword | Est. Difficulty | Opportunity Score | Current Ranking | Intent | Recommended Content Type |\n|---------|----------------|-------------------|-----------------|--------|--------------------------|\n\nOpportunity score: high, medium, or low — based on the combination of search demand, difficulty, and relevance to the user's business.\n\nInclude 15-25 keyword opportunities, sorted by opportunity score.\n\n### On-Page Issues Table\n\n| Page | Issue | Severity | Recommended Fix |\n|------|-------|----------|-----------------|\n\nSeverity levels:\n- **Critical** — directly hurting rankings or preventing indexation\n- **High** — significant impact on SEO performance\n- **Medium** — best practice violation, moderate impact\n- **Low** — minor optimization opportunity\n\n### Content Gap Recommendations\n\nFor each content gap identified, provide:\n- **Topic or keyword** to target\n- **Why it matters** — search demand, competitor coverage, funnel stage\n- **Recommended format** — blog post, landing page, guide, comparison page, etc.\n- **Priority** — high, medium, or low\n- **Estimated effort** — quick win (1-2 hours), moderate (half day), substantial (multi-day)\n\n### Technical SEO Checklist\n\n| Check | Status | Details |\n|-------|--------|---------|\n\nStatus: Pass, Fail, or Warning.\n\n### Competitor Comparison Summary\n\n| Dimension | Your Site | Competitor A | Competitor B | Winner |\n|-----------|-----------|--------------|--------------|--------|\n\nInclude rows for: keyword count, content depth, publishing frequency, backlink signals, technical score, SERP feature presence.\n\n### Prioritized Action Plan\n\nSplit recommendations into two categories:\n\n**Quick Wins (do this week):**\n- Actions that take under 2 hours and have immediate impact\n- Examples: fix title tags, add meta descriptions, fix broken links, add alt text\n\n**Strategic Investments (plan for this quarter):**\n- Actions that require more effort but drive long-term growth\n- Examples: build a topic cluster, create a pillar page, launch a link-building campaign, overhaul site structure\n\nFor each action item, include:\n- What to do (specific and concrete)\n- Expected impact (high, medium, low)\n- Effort estimate\n- Dependencies (if any)\n\n## Follow-Up\n\nAfter presenting the audit, ask:\n\n\"Would you like me to:\n- Draft content briefs for the top keyword opportunities?\n- Create optimized title tags and meta descriptions for your key pages?\n- Build a content calendar based on the gap analysis?\n- Dive deeper into any specific section of the audit?\n- Run this same analysis for a different competitor or domain?\"\n", "depth": 3}, {"id": "335", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/brand-voice", "name": "brand-voice", "type": "dir", "content": "", "depth": 3}, {"id": "336", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/content-creation", "name": "content-creation", "type": "dir", "content": "", "depth": 3}, {"id": "337", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/campaign-planning", "name": "campaign-planning", "type": "dir", "content": "", "depth": 3}, {"id": "338", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/competitive-analysis", "name": "competitive-analysis", "type": "dir", "content": "", "depth": 3}, {"id": "339", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/performance-analytics", "name": "performance-analytics", "type": "dir", "content": "", "depth": 3}, {"id": "340", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/brand-voice/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: brand-voice\ndescription: Apply and enforce brand voice, style guide, and messaging pillars across content. Use when reviewing content for brand consistency, documenting a brand voice, adapting tone for different audiences, or checking terminology and style guide compliance.\n---\n\n# Brand Voice Skill\n\nFrameworks for documenting, applying, and enforcing brand voice and style guidelines across marketing content.\n\n## Brand Voice Documentation Framework\n\nA complete brand voice document should cover these areas. Use this framework to help users define their brand voice or to understand an existing brand voice configuration.\n\n### 1. Brand Personality\nDefine the brand as if it were a person. What are its defining traits?\n\nExample: \"If our brand were a person, they would be a knowledgeable colleague who explains complex things simply, celebrates your wins genuinely, and never talks down to you.\"\n\n### 2. Voice Attributes\nSelect 3-5 attributes that define how the brand communicates. Each attribute should be defined with:\n- What it means in practice\n- What it does NOT mean (to prevent misinterpretation)\n- An example demonstrating the attribute\n\n### 3. Audience Awareness\n- Who the brand is speaking to (primary and secondary audiences)\n- What the audience cares about\n- What level of expertise the audience has\n- How the audience expects to be addressed\n\n### 4. Core Messaging Pillars\n- 3-5 key themes the brand consistently communicates\n- The hierarchy of these messages (which comes first)\n- How each pillar connects to audience needs\n\n### 5. Tone Spectrum\nHow the voice adapts across contexts while remaining recognizably the same brand.\n\n### 6. Style Rules\nSpecific grammar, formatting, and language rules. See the Style Guide Enforcement section below.\n\n### 7. Terminology\nPreferred and avoided terms. See the Terminology Management section below.\n\n## Voice Attributes\n\n### Common Voice Attribute Pairs\n\nWhen defining brand voice, it helps to position attributes on a spectrum. Here are common attribute spectrums:\n\n| Spectrum | One End | Other End |\n|----------|---------|-----------|\n| Formality | Formal, institutional | Casual, conversational |\n| Authority | Expert, authoritative | Peer-level, collaborative |\n| Emotion | Warm, empathetic | Direct, matter-of-fact |\n| Complexity | Technical, precise | Simple, accessible |\n| Energy | Bold, energetic | Calm, measured |\n| Humor | Playful, witty | Serious, earnest |\n| Innovation | Cutting-edge, forward-looking | Established, proven |\n\n### Defining an Attribute\n\nFor each chosen attribute, document it in this format:\n\n**[Attribute name]**\n- **We are**: [what this means in practice]\n- **We are not**: [common misinterpretation to avoid]\n- **This sounds like**: [example sentence demonstrating the attribute]\n- **This does NOT sound like**: [example sentence violating the attribute]\n\nExample:\n\n**Approachable**\n- **We are**: friendly, clear, jargon-free, welcoming to beginners and experts alike\n- **We are not**: dumbed-down, overly casual, or lacking substance\n- **This sounds like**: \"Here's how to get started — it takes about five minutes.\"\n- **This does NOT sound like**: \"Yo! This is super easy, even a noob can do it lol.\"\n\n## Tone Adaptation Across Channels and Contexts\n\nThe brand voice stays consistent, but tone adapts to context. Tone is the emotional inflection applied to the voice.\n\n### Tone by Channel\n\n| Channel | Tone Adaptation | Example |\n|---------|----------------|---------|\n| Blog | Informative, conversational, educational | \"Let's walk through how this works and why it matters for your team.\" |\n| Social media (LinkedIn) | Professional, thought-provoking, concise | \"Three things we learned from running 50 campaigns this quarter.\" |\n| Social media (Twitter/X) | Punchy, direct, sometimes witty | \"Your landing page has 3 seconds. Make them count.\" |\n| Email marketing | Personal, helpful, action-oriented | \"We put together something we think you'll find useful.\" |\n| Sales collateral | Confident, benefit-driven, specific | \"Teams using our platform reduce reporting time by 40%.\" |\n| Support/Help docs | Clear, patient, step-by-step | \"If you see this error, here's how to fix it.\" |\n| Press release | Formal, factual, newsworthy | \"The company today announced the launch of...\" |\n| Error messages | Empathetic, helpful, blame-free | \"Something went wrong on our end. We're looking into it.\" |\n\n### Tone by Situation\n\n| Situation | Tone Adaptation |\n|-----------|----------------|\n| Product launch | Excited, confident, forward-looking |\n| Incident or outage | Transparent, empathetic, accountable |\n| Customer success story | Celebratory, specific, crediting the customer |\n| Thought leadership | Authoritative, nuanced, evidence-based |\n| Onboarding | Welcoming, encouraging, clear |\n| Bad news (price increase, deprecation) | Honest, respectful, solution-oriented |\n| Competitive comparison | Confident but fair, fact-based, not disparaging |\n\n### Tone Adaptation Rule\nThe voice attributes remain fixed. Tone dials them up or down based on context. For example, if a brand is \"bold and warm\":\n- In a product launch, dial up boldness\n- In an incident response, dial up warmth\n- Neither attribute disappears; the balance shifts\n\n## Style Guide Enforcement\n\n### Grammar and Mechanics\nDocument and enforce these choices consistently:\n\n| Rule | Options | Example |\n|------|---------|---------|\n| Oxford comma | Yes / No | \"fast, reliable, and secure\" vs. \"fast, reliable and secure\" |\n| Sentence case vs. title case (headings) | Sentence / Title | \"How to get started\" vs. \"How to Get Started\" |\n| Contractions | Use / Avoid | \"we're\" vs. \"we are\" |\n| Em dash spacing | No spaces / Spaces | \"this—and more\" vs. \"this — and more\" |\n| Numbers | Spell out 1-9, numerals 10+ / Always numerals | \"five features\" vs. \"5 features\" |\n| Percent | % / percent | \"50%\" vs. \"50 percent\" |\n| Date format | Month DD, YYYY / DD/MM/YYYY / etc. | \"January 15, 2025\" |\n| Time format | 12-hour / 24-hour | \"3:00 PM\" vs. \"15:00\" |\n| Lists | Periods / No periods on fragments | \"Set up your account.\" vs. \"Set up your account\" |\n\n### Formatting Conventions\n- Heading hierarchy (when to use H1, H2, H3)\n- Bold and italic usage (bold for emphasis, italic for titles/terms)\n- Link text (descriptive vs. \"click here\" — always descriptive)\n- Image alt text requirements\n- Code formatting (for technical brands)\n- Callout or highlight box usage\n\n### Punctuation and Emphasis\n- Exclamation mark policy (limited use, never more than one)\n- Ellipsis usage (avoid in most professional contexts)\n- ALL CAPS policy (avoid; use bold for emphasis instead)\n- Emoji usage by channel (professional channels: minimal or none; social: where appropriate)\n\n## Terminology Management\n\n### Preferred Terms\n\nMaintain a list of preferred terms and their incorrect alternatives:\n\n| Use This | Not This | Notes |\n|----------|----------|-------|\n| sign up (verb) | signup (verb) | \"signup\" is the noun form |\n| log in (verb) | login (verb) | \"login\" is the noun/adjective form |\n| set up (verb) | setup (verb) | \"setup\" is the noun/adjective form |\n| email | e-mail | No hyphen |\n| website | web site | One word |\n| data is (singular) | data are | Unless the publication requires plural |\n\n### Product and Feature Names\n- Official capitalization for product names\n- When to use the full product name vs. shorthand\n- Whether to use \"the\" before product names\n- How to handle versioning in copy\n- Trademark and registration symbols (when required and when to omit)\n\n### Inclusive Language\n- Use gender-neutral language (they/them for unknown individuals)\n- Avoid ableist language (\"crazy\", \"blind spot\", \"lame\")\n- Use person-first language where appropriate\n- Avoid culturally specific idioms that may not translate\n- Use \"simple\" or \"straightforward\" instead of \"easy\" (what is easy varies by person)\n\n### Industry Jargon Management\n- Define which technical terms the audience understands without explanation\n- List jargon that should always be defined or replaced with plain language\n- Specify which acronyms need to be spelled out on first use\n- Audience-specific glossary for terms that mean different things to different readers\n\n### Competitor and Category Terms\n- How to refer to your product category (use your preferred framing)\n- How to refer to competitors (by name or generically)\n- Terms competitors have coined that you should avoid (to prevent reinforcing their positioning)\n- Your preferred differentiation language\n", "depth": 4}, {"id": "341", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/content-creation/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: content-creation\ndescription: Draft marketing content across channels — blog posts, social media, email newsletters, landing pages, press releases, and case studies. Use when writing any marketing content, when you need channel-specific formatting, SEO-optimized copy, headline options, or calls to action.\n---\n\n# Content Creation Skill\n\nGuidelines and frameworks for creating effective marketing content across channels.\n\n## Content Type Templates\n\n### Blog Post Structure\n1. **Headline** — clear, benefit-driven, includes primary keyword (aim for 60 characters or less for SEO)\n2. **Introduction** (100-150 words) — hook the reader with a question, statistic, bold claim, or relatable scenario. State what the post will cover. Include primary keyword.\n3. **Body sections** (3-5 sections) — each with a descriptive subheading (H2). Use H3 for subsections. One core idea per section with supporting evidence, examples, or data.\n4. **Conclusion** (75-100 words) — summarize key takeaways, reinforce the main message, include a call to action.\n5. **Meta description** — under 160 characters, includes primary keyword, compels the click.\n\n### Social Media Post Structure\n- **Hook** — first line grabs attention (question, bold statement, number)\n- **Body** — 2-4 concise points or a short narrative\n- **CTA** — what should the reader do next (comment, click, share, tag)\n- **Hashtags** — 3-5 relevant hashtags (platform-dependent)\n\n### Email Newsletter Structure\n- **Subject line** — under 50 characters, creates curiosity or states clear value\n- **Preview text** — complements the subject line, does not repeat it\n- **Header/hero** — visual anchor and one-line value statement\n- **Body sections** — 2-3 content blocks, each scannable with a bold intro sentence\n- **Primary CTA** — one clear action per email\n- **Footer** — unsubscribe link, company info, social links\n\n### Landing Page Structure\n- **Headline** — primary benefit in under 10 words\n- **Subheadline** — elaborates on the headline with supporting context\n- **Hero section** — headline, subheadline, primary CTA, supporting image or video\n- **Value propositions** — 3-4 benefit-driven sections with icons or images\n- **Social proof** — testimonials, logos, stats, case study snippets\n- **Objection handling** — FAQ or trust signals\n- **Final CTA** — repeat the primary call to action\n\n### Press Release Structure\n- **Headline** — factual, newsworthy, under 80 characters\n- **Subheadline** — optional, adds context\n- **Dateline** — city, state, date\n- **Lead paragraph** — who, what, when, where, why in 2-3 sentences\n- **Body paragraphs** — supporting details, quotes, context\n- **Boilerplate** — company description (standardized)\n- **Media contact** — name, email, phone\n\n### Case Study Structure\n- **Title** — \"[Customer] achieves [result] with [product]\"\n- **Snapshot** — customer name, industry, company size, product used, key result (sidebar or callout box)\n- **Challenge** — what problem the customer faced\n- **Solution** — what was implemented and how\n- **Results** — quantified outcomes with specific metrics\n- **Quote** — customer testimonial\n- **CTA** — learn more, get a demo, read more case studies\n\n## Writing Best Practices by Channel\n\n### Blog\n- Write at an 8th-grade reading level for broad audiences; adjust up for technical audiences\n- Use short paragraphs (2-4 sentences)\n- Include subheadings every 200-300 words\n- Use bullet points and numbered lists to break up text\n- Include at least one data point, example, or quote per section\n- Write in active voice\n- Front-load key information in each section\n\n### Social Media\n- **LinkedIn**: professional but human, paragraph breaks for readability, personal stories and lessons perform well, 1,300 characters is the sweet spot before \"see more\"\n- **Twitter/X**: concise and punchy, strong opening words, threads for longer narratives, engage with replies\n- **Instagram**: visual-first captions, storytelling hooks, line breaks for readability, hashtags in first comment or at end\n- **Facebook**: conversational tone, questions drive comments, shorter posts (under 80 characters) get more engagement for links\n\n### Email\n- Write subject lines that create urgency, curiosity, or state clear value\n- Personalize where possible (name, company, behavior)\n- One primary CTA per email — make it visually distinct\n- Keep body copy scannable: bold key phrases, short paragraphs, bullet points\n- Test everything: subject lines, send times, CTA copy, layout\n- Mobile-first: most email is read on mobile\n\n### Web (Landing Pages, Product Pages)\n- Lead with benefits, not features\n- Use \"you\" language — speak to the reader directly\n- Minimize jargon unless the audience expects it\n- Every section should answer \"so what?\" from the reader's perspective\n- Reduce friction: fewer form fields, clear next steps, trust signals near CTAs\n\n## SEO Fundamentals for Content\n\n### Keyword Strategy\n- Identify one primary keyword and 2-3 secondary keywords per piece\n- Use the primary keyword in: headline, first paragraph, one subheading, meta description, URL slug\n- Use secondary keywords naturally in body copy and subheadings\n- Do not keyword-stuff — write for humans first\n\n### On-Page SEO Checklist\n- Title tag: under 60 characters, includes primary keyword\n- Meta description: under 160 characters, includes primary keyword, compels click\n- URL slug: short, descriptive, includes primary keyword\n- H1: one per page, matches or closely reflects the title tag\n- H2/H3: descriptive, include secondary keywords where natural\n- Image alt text: descriptive, includes keyword where relevant\n- Internal links: 2-3 links to related content on your site\n- External links: 1-2 links to authoritative sources\n\n### Content-SEO Integration\n- Aim for comprehensive coverage of the topic (search engines reward depth)\n- Answer related questions (check \"People Also Ask\" for ideas)\n- Update and refresh high-performing content regularly\n- Structure content for featured snippets: definition paragraphs, numbered lists, tables\n\n## Headline and Hook Formulas\n\n### Headline Formulas\n- **How to [achieve result] [without common obstacle]** — \"How to Double Your Email Open Rates Without Sending More Emails\"\n- **[Number] [adjective] ways to [achieve result]** — \"7 Proven Ways to Reduce Customer Churn\"\n- **Why [common belief] is wrong (and what to do instead)** — \"Why More Content Is Not the Answer (And What to Do Instead)\"\n- **The [adjective] guide to [topic]** — \"The Complete Guide to B2B Content Marketing\"\n- **[Do this], not [that]** — \"Build a Community, Not Just an Audience\"\n- **What [impressive result] taught us about [topic]** — \"What 10,000 A/B Tests Taught Us About Email Subject Lines\"\n- **[topic]: what [audience] needs to know in [year]** — \"SEO: What Marketers Need to Know in 2025\"\n\n### Hook Formulas (Opening Lines)\n- **Surprising statistic**: \"73% of marketers say their biggest challenge is not budget — it is focus.\"\n- **Contrarian statement**: \"The best marketing campaigns start with saying no to most channels.\"\n- **Question**: \"When was the last time a marketing email actually changed what you bought?\"\n- **Scenario**: \"Imagine launching a campaign and knowing, before it goes live, which messages will land.\"\n- **Bold claim**: \"Most landing pages lose half their visitors in the first three seconds.\"\n- **Story opening**: \"Last quarter, our team was spending 20 hours a week on reporting. Here is what we did about it.\"\n\n## Call-to-Action Best Practices\n\n### CTA Principles\n- Use action verbs: \"Get\", \"Start\", \"Download\", \"Join\", \"Try\", \"See\"\n- Be specific about what happens next: \"Start your free trial\" is better than \"Submit\"\n- Create urgency when genuine: \"Join 500 teams already using this\" or \"Limited spots available\"\n- Reduce risk: \"No credit card required\", \"Cancel anytime\", \"Free for 14 days\"\n- One primary CTA per page or email — too many choices reduce conversions\n\n### CTA Examples by Context\n- **Blog post**: \"Read our complete guide to [topic]\" / \"Subscribe for weekly insights\"\n- **Landing page**: \"Start free trial\" / \"Get a demo\" / \"See pricing\"\n- **Email**: \"Read the full story\" / \"Claim your spot\" / \"Reply and tell us\"\n- **Social media**: \"Drop a comment if you agree\" / \"Save this for later\" / \"Link in bio\"\n- **Case study**: \"See how [product] can work for your team\" / \"Talk to our team\"\n\n### CTA Placement\n- Above the fold on landing pages (do not make users scroll to act)\n- After establishing value in emails (not in the first sentence)\n- At the end of blog posts (after you have earned the reader's trust)\n- In-line within content when contextually relevant (e.g., a related guide mention)\n- Repeat the primary CTA at the bottom of long-form pages\n", "depth": 4}, {"id": "342", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/campaign-planning/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: campaign-planning\ndescription: Plan marketing campaigns with objectives, audience segmentation, channel strategy, content calendars, and success metrics. Use when launching a campaign, planning a product launch, building a content calendar, allocating budget across channels, or defining campaign KPIs.\n---\n\n# Campaign Planning Skill\n\nFrameworks and guidance for planning, structuring, and executing marketing campaigns.\n\n## Campaign Framework: Objective, Audience, Message, Channel, Measure\n\nEvery campaign should be built on this five-part framework:\n\n### 1. Objective\nDefine what success looks like before planning anything else.\n\n- **Awareness**: increase brand or product visibility (measured by reach, impressions, share of voice)\n- **Consideration**: drive engagement and education (measured by content engagement, email signups, webinar attendance)\n- **Conversion**: generate leads or sales (measured by signups, demos, purchases, pipeline)\n- **Retention**: re-engage existing customers (measured by churn reduction, upsell, NPS)\n- **Advocacy**: turn customers into promoters (measured by referrals, reviews, UGC)\n\nGood objectives are SMART: Specific, Measurable, Achievable, Relevant, Time-bound.\n\nExample: \"Generate 200 marketing qualified leads from mid-market SaaS companies in North America within 6 weeks of campaign launch.\"\n\n### 2. Audience\nDefine who you are trying to reach with enough specificity to guide messaging and channel decisions.\n\n- **Demographics**: role/title, seniority, company size, industry\n- **Psychographics**: motivations, pain points, goals, objections\n- **Behavioral**: where they consume content, how they buy, what they have engaged with before\n- **Buying stage**: are they unaware of the problem, researching solutions, or ready to buy?\n\nCreate a brief audience profile (not a full persona) for campaign planning:\n\u003e \"[Role] at [company type] who is struggling with [pain point] and looking for [desired outcome]. They typically discover solutions through [channels] and care most about [priorities].\"\n\n### 3. Message\nCraft the core message and supporting points that will resonate with the audience.\n\n- **Core message**: one sentence that captures what you want the audience to think, feel, or do\n- **Supporting messages**: 3-4 points that provide evidence, address objections, or elaborate on benefits\n- **Proof points**: data, case studies, testimonials, or third-party validation for each supporting message\n- **Differentiation**: what makes your offering different from alternatives (including doing nothing)\n\nMessage hierarchy:\n1. Why should I care? (addresses the pain point or opportunity)\n2. What is the solution? (positions your offering)\n3. Why you? (differentiates from alternatives)\n4. What should I do? (call to action)\n\n### 4. Channel\nSelect channels based on where your audience is, not where you are most comfortable.\n\nSee the Channel Selection Guide below for detailed guidance.\n\n### 5. Measure\nDefine how you will know the campaign worked. See Success Metrics by Campaign Type below.\n\n## Channel Selection Guide\n\n### Owned Channels\n\n| Channel | Best For | Typical Metrics | Effort |\n|---------|----------|----------------|--------|\n| Blog/Website | SEO, thought leadership, education | Traffic, time on page, conversions | Medium |\n| Email | Nurture, retention, announcements | Open rate, CTR, conversions | Low-Medium |\n| Social (organic) | Awareness, community, brand building | Engagement, reach, follower growth | Medium |\n| Webinars | Education, lead gen, product demos | Registrations, attendance, pipeline | High |\n| Podcast | Thought leadership, brand awareness | Downloads, subscriber growth | High |\n\n### Earned Channels\n\n| Channel | Best For | Typical Metrics | Effort |\n|---------|----------|----------------|--------|\n| PR/Media | Awareness, credibility, launches | Coverage, share of voice, referral traffic | High |\n| Guest content | Audience expansion, SEO, credibility | Referral traffic, backlinks | Medium |\n| Influencer/Partner | Audience expansion, trust | Reach, engagement, referral conversions | Medium-High |\n| Community | Awareness, trust, feedback | Mentions, engagement, referral traffic | Medium |\n| Reviews/Ratings | Credibility, SEO, consideration | Review volume, rating, conversion lift | Low-Medium |\n\n### Paid Channels\n\n| Channel | Best For | Typical Metrics | Effort |\n|---------|----------|----------------|--------|\n| Search ads (SEM) | High-intent lead capture | CPC, CTR, conversion rate, CPA | Medium |\n| Social ads | Awareness, retargeting, lead gen | CPM, CPC, CTR, CPA, ROAS | Medium |\n| Display/Programmatic | Awareness, retargeting | Impressions, CPM, view-through conversions | Low-Medium |\n| Sponsored content | Thought leadership, lead gen | Engagement, leads, cost per lead | Medium |\n| Events/Sponsorships | Relationship building, brand | Leads, meetings, pipeline influenced | High |\n\n### Channel Selection Criteria\nWhen choosing channels, consider:\n- Where does your target audience spend time?\n- What is the buying stage you are targeting? (awareness channels vs. conversion channels)\n- What is your budget? (paid channels require spend; owned/earned require time)\n- What content assets do you already have or can you produce?\n- What has worked in the past? (reference historical data if available)\n\n## Content Calendar Creation\n\n### Calendar Structure\n\nA content calendar should answer: what, where, when, who, and why for every piece of content.\n\n| Date | Content Piece | Channel | Audience Segment | Campaign/Theme | Owner | Status |\n|------|--------------|---------|-------------------|----------------|-------|--------|\n\n### Calendar Planning Process\n1. **Start with milestones**: campaign launch, event dates, product releases, seasonal moments\n2. **Work backward**: what needs to be live and when? What is the production lead time?\n3. **Map content to funnel stages**: ensure coverage across awareness, consideration, and conversion\n4. **Batch by theme**: group related content pieces into weekly or bi-weekly themes\n5. **Balance channels**: do not over-index on one channel; ensure the audience sees the campaign across touchpoints\n6. **Build in flexibility**: leave 20% of calendar slots open for reactive or opportunistic content\n\n### Content Cadence Guidelines\n- **Blog**: 1-4 posts per week depending on team size and goals\n- **Email newsletter**: weekly or bi-weekly for most audiences\n- **Social media**: 3-7 posts per week per platform (varies by platform)\n- **Paid campaigns**: continuous during campaign window with creative refreshes every 2-4 weeks\n- **Webinars**: monthly or quarterly depending on resources\n\n### Production Timeline Benchmarks\n- Blog post: 3-5 business days (research, draft, review, publish)\n- Email campaign: 2-3 business days (copy, design, test, send)\n- Social media posts: 1-2 business days (draft, design, schedule)\n- Landing page: 5-7 business days (copy, design, development, QA)\n- Video content: 2-4 weeks (script, production, editing)\n- Ebook/whitepaper: 2-4 weeks (outline, draft, design, review)\n\n## Budget Allocation Approaches\n\n### Percentage of Revenue Method\n- Industry benchmark: 5-15% of revenue for marketing, with B2B typically at 5-10% and B2C at 10-15%\n- Startups and growth-stage companies often invest 15-25% of revenue in marketing\n- Within the marketing budget, allocate across brand (long-term) and performance (short-term)\n\n### Channel Allocation Framework\nA common starting framework (adjust based on goals and historical data):\n\n| Category | Percentage of Budget | Examples |\n|----------|---------------------|----------|\n| Paid acquisition | 30-40% | Search ads, social ads, display |\n| Content production | 20-30% | Blog, video, design, ebooks |\n| Events and sponsorships | 10-20% | Conferences, webinars, meetups |\n| Tools and technology | 10-15% | Analytics, automation, CRM |\n| Testing and experimentation | 5-10% | New channels, A/B tests, pilots |\n\n### Budget Optimization Principles\n- Start with your highest-confidence channel and allocate 60-70% of paid budget there\n- Reserve 15-20% for testing new channels or tactics\n- Shift budget monthly based on performance data (do not set and forget)\n- Account for production costs, not just media spend\n- Include a 10-15% contingency for unexpected opportunities or overruns\n\n## Success Metrics by Campaign Type\n\n### Awareness Campaign\n| Metric | What It Measures |\n|--------|-----------------|\n| Reach/Impressions | How many people saw the campaign |\n| Brand mention volume | Increase in brand conversations |\n| Share of voice | Your mentions vs. competitors |\n| Direct traffic | People coming to your site unprompted |\n| Social follower growth | Audience building |\n\n### Lead Generation Campaign\n| Metric | What It Measures |\n|--------|-----------------|\n| Total leads | Volume of new contacts |\n| Marketing qualified leads (MQLs) | Leads meeting quality threshold |\n| Cost per lead (CPL) | Efficiency of spend |\n| Lead-to-MQL conversion rate | Quality of leads generated |\n| Pipeline influenced | Revenue opportunity created |\n\n### Product Launch Campaign\n| Metric | What It Measures |\n|--------|-----------------|\n| Signups or trials | Adoption of new product |\n| Activation rate | Users who complete key first action |\n| Media coverage | Earned media hits |\n| Social buzz | Mentions, shares, engagement spike |\n| Feature adoption | Usage of specific launched features |\n\n### Retention/Engagement Campaign\n| Metric | What It Measures |\n|--------|-----------------|\n| Churn rate change | Customer retention improvement |\n| Engagement rate | Interactions with campaign content |\n| NPS or CSAT change | Satisfaction improvement |\n| Upsell/cross-sell revenue | Expansion revenue |\n| Feature adoption | Usage of promoted features |\n\n### Event/Webinar Campaign\n| Metric | What It Measures |\n|--------|-----------------|\n| Registrations | Interest generated |\n| Attendance rate | Conversion from registration |\n| Engagement during event | Questions, polls, chat activity |\n| Post-event conversions | Leads or pipeline from attendees |\n| Content repurposing reach | Downstream audience from recordings |\n", "depth": 4}, {"id": "343", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/competitive-analysis/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: competitive-analysis\ndescription: Research competitors and compare positioning, messaging, content strategy, and market presence. Use when analyzing a competitor, building battlecards, identifying content gaps, comparing feature messaging, or preparing competitive positioning recommendations.\n---\n\n# Competitive Analysis Skill\n\nFrameworks and methodologies for researching competitors, comparing positioning, and identifying market opportunities.\n\n## Competitive Research Methodology\n\n### Research Sources\n\nGather intelligence from these categories of sources:\n\n#### Primary Sources (Direct from Competitor)\n- **Website**: homepage, product pages, pricing, about page, careers\n- **Blog and resource center**: content themes, publishing frequency, depth\n- **Social media profiles**: messaging, engagement, content strategy\n- **Product demos and free trials**: UX, features, onboarding experience\n- **Webinars and events**: topics, speakers, audience engagement\n- **Press releases and newsroom**: announcements, partnerships, milestones\n- **Job postings**: hiring signals that reveal strategic priorities (e.g., hiring for a new product line or market)\n\n#### Secondary Sources (Third-Party)\n- **Review sites**: G2, Capterra, TrustRadius, Product Hunt — customer sentiment themes\n- **Analyst reports**: Gartner, Forrester, IDC — market positioning and category placement\n- **News coverage**: TechCrunch, industry publications — funding, partnerships, narrative\n- **Social listening**: mentions, sentiment, share of voice across social platforms\n- **SEO tools**: keyword rankings, organic traffic estimates, content gaps\n- **Financial filings**: revenue, growth rate, investment areas (for public companies)\n- **Community forums**: community forums (e.g. Reddit, Discourse), industry chat groups (e.g. Slack communities) — user sentiment\n\n### Research Process\n1. **Set scope**: define which competitors and what aspects to analyze\n2. **Gather data**: systematically collect information from sources above\n3. **Organize findings**: structure by competitor, then by dimension\n4. **Analyze patterns**: identify themes, strengths, weaknesses, and trends\n5. **Compare to your position**: map findings against your own positioning and capabilities\n6. **Synthesize insights**: extract actionable takeaways and opportunities\n7. **Date-stamp everything**: competitive intelligence has a short shelf life\n\n### Research Cadence\n- **Deep competitive analysis**: quarterly (full research across all sources)\n- **Competitive monitoring**: monthly (scan for new announcements, content, messaging changes)\n- **Real-time alerts**: ongoing (set up alerts for competitor brand mentions, press, job postings)\n\n## Messaging Comparison Frameworks\n\n### Messaging Matrix\n\nCompare messaging across competitors on key dimensions:\n\n| Dimension | Your Company | Competitor A | Competitor B | Competitor C |\n|-----------|-------------|--------------|--------------|--------------|\n| Tagline/Headline | | | | |\n| Core value proposition | | | | |\n| Primary audience | | | | |\n| Key differentiator claim | | | | |\n| Tone/Voice | | | | |\n| Proof points used | | | | |\n| Category framing | | | | |\n| Primary CTA | | | | |\n\n### Value Proposition Comparison\n\nFor each competitor, document:\n- **Promise**: what they promise the customer will achieve\n- **Evidence**: how they prove the promise (data, testimonials, demos)\n- **Mechanism**: how their product delivers on the promise (the \"how it works\")\n- **Uniqueness**: what they claim only they can do\n\n### Narrative Analysis\n\nIdentify each competitor's story arc:\n- **Villain**: what problem or enemy they position against (status quo, legacy tools, complexity)\n- **Hero**: who is the hero in their story (the customer? the product? the team?)\n- **Transformation**: what before/after do they promise?\n- **Stakes**: what happens if you do not act?\n\nThis reveals positioning strategy and emotional appeals.\n\n### Messaging Strengths and Vulnerabilities\n\nFor each competitor's messaging, assess:\n- **Clarity**: can a first-time visitor understand what they do in 5 seconds?\n- **Differentiation**: is their positioning distinct or generic?\n- **Proof**: do they back up claims with evidence?\n- **Consistency**: is messaging consistent across channels?\n- **Resonance**: does their messaging address real customer pain points?\n\n## Content Gap Analysis\n\n### Content Audit Comparison\n\nMap content across competitors by:\n\n| Topic/Theme | Your Content | Competitor A | Competitor B | Gap? |\n|-------------|-------------|--------------|--------------|------|\n| [Topic 1] | Blog post, ebook | Blog series, webinar | Nothing | Opportunity for B |\n| [Topic 2] | Nothing | Whitepaper | Blog post, video | Gap for you |\n| [Topic 3] | Case study | Nothing | Case study | Parity |\n\n### Content Type Coverage\n\n| Content Format | You | Comp A | Comp B | Comp C |\n|----------------|-----|--------|--------|--------|\n| Blog posts | Y | Y | Y | Y |\n| Case studies | Y | Y | N | Y |\n| Ebooks/Whitepapers | N | Y | Y | N |\n| Webinars | Y | Y | Y | N |\n| Podcast | N | N | Y | N |\n| Video content | N | Y | Y | Y |\n| Interactive tools | N | N | N | Y |\n| Templates/Resources | Y | N | Y | N |\n\n### Identifying Content Opportunities\n1. **Topics they cover that you do not**: potential gaps in your content strategy\n2. **Topics you cover that they do not**: potential differentiators to amplify\n3. **Formats they use that you do not**: format gaps that could reach new audiences\n4. **Audience segments they address that you do not**: underserved audiences\n5. **Search terms they rank for that you do not**: SEO content gaps\n\n### Content Quality Assessment\n- Depth: surface-level or comprehensive?\n- Freshness: regularly updated or stale?\n- Engagement: do posts get comments, shares, links?\n- Production value: text-only or multimedia?\n- Thought leadership: original insights or rehashed content?\n\n## Positioning Strategy\n\n### Positioning Statement Framework\n\nFor your company and each competitor, define (or reverse-engineer) their positioning statement:\n\n\u003e For [target audience], [product/company] is the [category] that [key benefit/differentiator] because [reason to believe].\n\nExample:\n\u003e For mid-market SaaS marketing teams, Acme is the campaign management platform that unifies planning and execution in one workspace because it is built on a single data model that eliminates tool fragmentation.\n\n### Positioning Map\n\nPlot competitors on a 2x2 matrix using the two most important dimensions for your market:\n\nCommon axis pairs:\n- **Price vs. Capability** (low cost / basic vs. premium / full-featured)\n- **Ease of Use vs. Power** (simple / limited vs. complex / flexible)\n- **SMB Focus vs. Enterprise Focus** (self-serve / individual vs. sales-led / team)\n- **Point Solution vs. Platform** (does one thing well vs. does many things)\n- **Innovative vs. Established** (new approach vs. proven track record)\n\nIdentify which quadrant is underserved or where your differentiation is strongest.\n\n### Category Strategy\n- **Create a new category**: if you do something genuinely different, define and own the category (high risk, high reward)\n- **Reframe the existing category**: change how buyers evaluate the category to favor your strengths\n- **Win the existing category**: compete directly on recognized criteria and out-execute\n- **Niche within the category**: own a specific segment, use case, or audience\n\n### Positioning Pitfalls to Avoid\n- Positioning against a competitor rather than for a customer need\n- Claiming too many differentiators (pick 1-2 that matter most)\n- Using category jargon the customer does not use\n- Positioning on features rather than outcomes\n- Changing positioning too frequently (confuses the market)\n\n## Battlecard Creation\n\n### Battlecard Structure\n\nA competitive battlecard is a one-page reference for sales and marketing teams. Include:\n\n#### Header\n- Competitor name and logo\n- Last updated date\n- Competitive win rate (if tracked)\n\n#### Quick Overview\n- What they do (one sentence)\n- Their target customer\n- Pricing model summary\n- Key recent developments\n\n#### Their Pitch\n- How they describe themselves\n- Their primary tagline\n- Their top 3 claimed differentiators\n\n#### Strengths (Be Honest)\n- Where they genuinely compete well\n- What customers like about them (from reviews)\n- Features or capabilities where they lead\n\n#### Weaknesses\n- Consistent customer complaints (from reviews)\n- Technical limitations\n- Gaps in their offering\n- Areas where customers report dissatisfaction\n\n#### Our Differentiators\n- 3-5 specific ways your product or approach is different\n- For each: the differentiator, why it matters to the customer, and proof\n\n#### Objection Handling\n| If the prospect says... | Respond with... |\n|------------------------|----------------|\n| \"[Competitor] does X too\" | \"Here is how our approach differs...\" |\n| \"[Competitor] is cheaper\" | \"Here is what that price difference gets you...\" |\n| \"I've heard good things about [Competitor]\" | \"They are strong at X. Where we differ is...\" |\n\n#### Landmines to Set\nQuestions to ask prospects early that highlight your advantages:\n- \"How do you currently handle [area where competitor is weak]?\"\n- \"How important is [capability you have that they lack]?\"\n- \"Have you considered [risk that your product mitigates]?\"\n\n#### Landmines to Defuse\nQuestions competitors might encourage prospects to ask you, with prepared responses.\n\n#### Win/Loss Themes\n- Common reasons deals are won against this competitor\n- Common reasons deals are lost to this competitor\n- What types of prospects favor them vs. you\n\n### Battlecard Maintenance\n- Review and update quarterly at minimum\n- Update immediately after major competitor announcements\n- Incorporate win/loss feedback from sales team\n- Track which objection-handling responses are most effective\n", "depth": 4}, {"id": "344", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/marketing/skills/performance-analytics/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: performance-analytics\ndescription: Analyze marketing performance with key metrics, trend analysis, and optimization recommendations. Use when building performance reports, reviewing campaign results, analyzing channel metrics (email, social, paid, SEO), or identifying what's working and what needs improvement.\n---\n\n# Performance Analytics Skill\n\nFrameworks for measuring, reporting, and optimizing marketing performance across channels and campaigns.\n\n## Key Marketing Metrics by Channel\n\n### Email Marketing\n\n| Metric | Definition | Benchmark Range | What It Tells You |\n|--------|-----------|----------------|-------------------|\n| Delivery rate | Emails delivered / emails sent | 95-99% | List health and sender reputation |\n| Open rate | Unique opens / emails delivered | 15-30% | Subject line and sender effectiveness |\n| Click-through rate (CTR) | Unique clicks / emails delivered | 2-5% | Content relevance and CTA effectiveness |\n| Click-to-open rate (CTOR) | Unique clicks / unique opens | 10-20% | Email content quality (for those who opened) |\n| Unsubscribe rate | Unsubscribes / emails delivered | \u003c0.5% | Content-audience fit and frequency tolerance |\n| Bounce rate | Bounces / emails sent | \u003c2% | List quality and data hygiene |\n| Conversion rate | Conversions / emails delivered | 1-5% | End-to-end email effectiveness |\n| Revenue per email | Total revenue / emails sent | Varies | Direct revenue attribution |\n| List growth rate | (New subscribers - unsubscribes) / total list | 2-5% monthly | Audience building health |\n\n### Social Media\n\n| Metric | Definition | What It Tells You |\n|--------|-----------|-------------------|\n| Impressions | Number of times content was displayed | Content distribution and reach |\n| Reach | Number of unique users who saw content | Audience breadth |\n| Engagement rate | (Likes + comments + shares) / reach | Content resonance |\n| Click-through rate | Link clicks / impressions | Traffic driving effectiveness |\n| Follower growth rate | Net new followers / total followers per period | Audience building |\n| Share/Repost rate | Shares / reach | Content virality and advocacy |\n| Video view rate | Views / impressions | Video content hook effectiveness |\n| Video completion rate | Completed views / total views | Video content quality and length fit |\n| Social share of voice | Your mentions / total category mentions | Brand visibility vs. competitors |\n\n### Paid Advertising (Search and Social)\n\n| Metric | Definition | What It Tells You |\n|--------|-----------|-------------------|\n| Impressions | Times ad was shown | Budget utilization and targeting breadth |\n| Click-through rate (CTR) | Clicks / impressions | Ad creative and targeting relevance |\n| Cost per click (CPC) | Total spend / clicks | Cost efficiency of traffic generation |\n| Cost per mille (CPM) | Cost per 1,000 impressions | Awareness cost efficiency |\n| Conversion rate | Conversions / clicks | Landing page and offer effectiveness |\n| Cost per acquisition (CPA) | Total spend / conversions | Full-funnel cost efficiency |\n| Return on ad spend (ROAS) | Revenue / ad spend | Revenue generation efficiency |\n| Quality Score (search) | Google's relevance rating (1-10) | Ad-keyword-landing page alignment |\n| Frequency | Average times a user sees the ad | Ad fatigue risk |\n| View-through conversions | Conversions from users who saw but did not click | Display/awareness campaign influence |\n\n### SEO / Organic Search\n\n| Metric | Definition | What It Tells You |\n|--------|-----------|-------------------|\n| Organic sessions | Visits from organic search | SEO effectiveness and content reach |\n| Keyword rankings | Position for target keywords | Search visibility |\n| Organic CTR | Clicks / impressions in search results | Title and meta description effectiveness |\n| Pages indexed | Number of pages in search index | Crawlability and site health |\n| Domain authority | Third-party authority score | Overall site strength |\n| Backlinks | Number of external sites linking to you | Content authority and off-page SEO |\n| Page load speed | Time to interactive | User experience and ranking factor |\n| Organic conversion rate | Organic conversions / organic sessions | Content quality and intent alignment |\n| Top entry pages | Most-visited pages from organic search | Content driving the most organic traffic |\n\n### Content Marketing\n\n| Metric | Definition | What It Tells You |\n|--------|-----------|-------------------|\n| Pageviews | Total views of content pages | Content reach and distribution |\n| Unique visitors | Distinct users viewing content | Audience size |\n| Average time on page | Time spent on content pages | Content engagement and depth |\n| Bounce rate | Single-page sessions / total sessions | Content-audience fit and UX |\n| Scroll depth | How far users scroll on a page | Content engagement through the piece |\n| Social shares | Times content was shared on social | Content resonance and virality |\n| Backlinks earned | External links to content | Content authority and SEO value |\n| Lead generation | Leads attributed to content | Content conversion effectiveness |\n| Content ROI | Revenue attributed / content production cost | Overall content investment return |\n\n### Overall Marketing / Pipeline\n\n| Metric | Definition | What It Tells You |\n|--------|-----------|-------------------|\n| Marketing qualified leads (MQLs) | Leads meeting marketing qualification criteria | Top-of-funnel effectiveness |\n| Sales qualified leads (SQLs) | MQLs accepted by sales | Lead quality |\n| MQL to SQL conversion rate | SQLs / MQLs | Marketing-sales alignment and lead quality |\n| Pipeline generated | Dollar value of opportunities created | Marketing impact on revenue |\n| Pipeline velocity | How fast deals move through pipeline | Campaign urgency and quality |\n| Customer acquisition cost (CAC) | Total marketing + sales cost / new customers | Efficiency of customer acquisition |\n| CAC payback period | Months to recover CAC from revenue | Unit economics health |\n| Marketing-sourced revenue | Revenue from marketing-originated deals | Direct marketing contribution |\n| Marketing-influenced revenue | Revenue from deals where marketing touched | Broader marketing impact |\n\n## Reporting Templates and Dashboards\n\n### Weekly Marketing Report\nQuick-scan format for team standups:\n- **Top 3 metrics** with week-over-week change\n- **What worked** this week (1-2 bullet points with data)\n- **What needs attention** (1-2 bullet points with data)\n- **This week's priorities** (3-5 action items)\n\n### Monthly Marketing Report\nStandard stakeholder report:\n1. Executive summary (3-5 sentences)\n2. Key metrics dashboard (table with MoM and target comparison)\n3. Channel-by-channel performance summary\n4. Campaign highlights and results\n5. What worked and what did not (with hypotheses)\n6. Recommendations and next month priorities\n7. Budget spend vs. plan\n\n### Quarterly Business Review (QBR)\nStrategic review for leadership:\n1. Quarter performance vs. goals\n2. Year-to-date trajectory\n3. Channel ROI analysis\n4. Campaign performance summary\n5. Competitive and market observations\n6. Strategic recommendations for next quarter\n7. Budget request and allocation plan\n8. Key experiments and learnings\n\n### Dashboard Design Principles\n- Lead with the metrics that map to business objectives (not vanity metrics)\n- Show trends over time, not just point-in-time snapshots\n- Include comparison context: prior period, target, benchmark\n- Use consistent color coding: green (on track), yellow (at risk), red (off track)\n- Group metrics by funnel stage or business question\n- Keep dashboards to one page/screen — detail goes in appendix\n- Update cadence should match decision cadence (real-time for paid, weekly for content)\n\n## Trend Analysis and Forecasting\n\n### Trend Identification\nWhen analyzing performance data, look for:\n\n1. **Directional trends**: is the metric consistently going up, down, or flat over 4+ periods?\n2. **Inflection points**: where did performance change direction and what happened then?\n3. **Seasonality**: are there predictable patterns by day of week, month, or quarter?\n4. **Anomalies**: one-time spikes or drops — what caused them and are they repeatable?\n5. **Leading indicators**: which metrics change first and predict future outcomes?\n\n### Trend Analysis Process\n1. Chart the metric over time (at least 8-12 data points for meaningful trends)\n2. Identify the overall direction (upward, downward, flat, cyclical)\n3. Calculate the rate of change (is it accelerating or decelerating?)\n4. Overlay key events (campaigns launched, product changes, market events)\n5. Compare to benchmarks or targets\n6. Identify correlations with other metrics\n7. Form hypotheses about causation (and plan tests to validate)\n\n### Simple Forecasting Approaches\n- **Linear projection**: extend the current trend line forward (useful for stable metrics)\n- **Moving average**: smooth out noise by averaging the last 3-6 periods\n- **Year-over-year comparison**: use last year's pattern as a baseline, adjusted for growth rate\n- **Funnel math**: forecast outputs from inputs (e.g., if we generate X leads at Y conversion rate, we will get Z customers)\n- **Scenario modeling**: create best case, expected case, and worst case projections\n\n### Forecasting Caveats\n- Short-term forecasts (1-3 months) are more reliable than long-term\n- Forecasts based on fewer than 12 data points should be flagged as low confidence\n- External factors (market shifts, competitive moves, economic changes) can invalidate trend-based forecasts\n- Always present forecasts as ranges, not exact numbers\n\n## Attribution Modeling Basics\n\n### What Is Attribution?\nAttribution determines which marketing touchpoints get credit for a conversion. This matters because buyers typically interact with multiple channels before converting.\n\n### Common Attribution Models\n\n| Model | How It Works | Best For | Limitation |\n|-------|-------------|----------|------------|\n| Last touch | 100% credit to last interaction before conversion | Understanding final conversion triggers | Ignores awareness and nurture |\n| First touch | 100% credit to first interaction | Understanding top-of-funnel effectiveness | Ignores nurture and conversion drivers |\n| Linear | Equal credit to all touchpoints | Fair representation of all channels | Does not reflect relative impact |\n| Time decay | More credit to touchpoints closer to conversion | Balanced view favoring recent interactions | May undervalue awareness |\n| Position-based (U-shaped) | 40% first, 40% last, 20% split among middle | Valuing both discovery and conversion | Somewhat arbitrary weighting |\n| Data-driven | Algorithmic credit based on conversion patterns | Most accurate representation | Requires significant data volume |\n\n### Attribution Practical Guidance\n- Start with last-touch attribution if you have no model in place — it is the simplest and most actionable\n- Compare first-touch and last-touch to understand which channels drive awareness vs. conversion\n- Use position-based (U-shaped) as a reasonable middle ground for most B2B companies\n- Data-driven attribution requires high conversion volume to be statistically meaningful\n- No model is perfect — use attribution directionally, not as absolute truth\n- Multi-touch attribution is better than single-touch, but any model is better than none\n\n### Attribution Pitfalls\n- Do not optimize one channel in isolation based on single-touch attribution\n- Awareness channels (display, social, PR) will always look bad in last-touch models\n- Conversion channels (search, retargeting) will always look bad in first-touch models\n- Self-reported attribution (\"how did you hear about us?\") provides useful qualitative color but is unreliable as quantitative data\n- Cross-device and cross-channel tracking gaps mean attribution data is always incomplete\n\n## Optimization Recommendations Framework\n\n### Optimization Process\n1. **Identify**: which metrics are underperforming vs. target or benchmark?\n2. **Diagnose**: where in the funnel is the problem? (impressions, clicks, conversions, retention)\n3. **Hypothesize**: what is causing the underperformance? (audience, message, creative, offer, timing, technical)\n4. **Prioritize**: which fixes will have the biggest impact with the least effort?\n5. **Test**: design an experiment to validate the hypothesis\n6. **Measure**: did the change improve the metric?\n7. **Scale or iterate**: roll out wins broadly; iterate on inconclusive or failed tests\n\n### Optimization Levers by Funnel Stage\n\n| Funnel Stage | Problem Signal | Optimization Levers |\n|-------------|---------------|---------------------|\n| Awareness | Low impressions, low reach | Budget, targeting, channel mix, creative format |\n| Interest | Low CTR, low engagement | Ad creative, headlines, content hooks, audience targeting |\n| Consideration | High bounce rate, low time on page | Landing page content, page speed, content relevance, UX |\n| Conversion | Low conversion rate | Offer, CTA, form length, trust signals, page layout |\n| Retention | High churn, low repeat engagement | Onboarding, email nurture, product experience, support |\n\n### Prioritization Framework\n\nRank optimization ideas on two dimensions:\n\n**Impact** (how much will this move the metric?):\n- High: directly addresses the primary bottleneck\n- Medium: addresses a contributing factor\n- Low: incremental improvement\n\n**Effort** (how hard is this to implement?):\n- Low: copy change, targeting adjustment, simple A/B test\n- Medium: new creative, landing page redesign, workflow change\n- High: new tool, cross-team project, major content production\n\nPriority order:\n1. High impact, low effort (do immediately)\n2. High impact, high effort (plan and resource)\n3. Low impact, low effort (do if capacity allows)\n4. Low impact, high effort (deprioritize)\n\n### Testing Best Practices\n- Test one variable at a time for clean results\n- Define the success metric before launching the test\n- Calculate required sample size before starting (do not end tests early)\n- Run tests for a minimum of one full business cycle (typically one week for B2B)\n- Document all tests and results, regardless of outcome\n- Share learnings across the team — failed tests are valuable information\n- A test that confirms the status quo is not a failure — it builds confidence in your current approach\n\n### Continuous Optimization Cadence\n- **Daily**: monitor paid campaigns for budget pacing, anomalies, and disapproved ads\n- **Weekly**: review channel performance, pause underperformers, scale winners\n- **Bi-weekly**: refresh ad creative and test new variants\n- **Monthly**: full performance review, identify new optimization opportunities, update forecasts\n- **Quarterly**: strategic review of channel mix, budget allocation, and targeting strategy\n", "depth": 4}, {"id": "345", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "346", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "347", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/README.md", "name": "README.md", "type": "file", "content": "# Finance \u0026 Accounting Plugin\n\nA finance and accounting plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Supports month-end close, journal entry preparation, account reconciliation, financial statement generation, variance analysis, and SOX audit support.\n\n\u003e **Important**: This plugin assists with finance and accounting workflows but does not provide financial, tax, or audit advice. All outputs should be reviewed by qualified financial professionals before use in financial reporting, regulatory filings, or audit documentation.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/finance\n```\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/journal-entry` | Journal entry preparation — generate accruals, fixed asset entries, prepaids, payroll, and revenue entries with proper debits/credits and supporting detail |\n| `/reconciliation` | Account reconciliation — compare GL balances to subledger, bank, or third-party balances and identify reconciling items |\n| `/income-statement` | Income statement generation — produce P\u0026L with period-over-period comparison and variance analysis |\n| `/variance-analysis` | Variance/flux analysis — decompose variances into drivers with narrative explanations and waterfall analysis |\n| `/sox-testing` | SOX compliance testing — generate sample selections, testing workpapers, and control assessments |\n\n## Skills\n\n| Skill | Description |\n|-------|-------------|\n| `journal-entry-prep` | JE preparation best practices, standard accrual types, supporting documentation requirements, and review workflows |\n| `reconciliation` | Reconciliation methodology for GL-to-subledger, bank recs, and intercompany, with reconciling item categorization and aging |\n| `financial-statements` | Income statement, balance sheet, and cash flow statement formats with GAAP presentation and flux analysis methodology |\n| `variance-analysis` | Variance decomposition techniques (price/volume, rate/mix), materiality thresholds, narrative generation, and waterfall charts |\n| `close-management` | Month-end close checklist, task sequencing, dependencies, status tracking, and common close activities by day |\n| `audit-support` | SOX 404 control testing methodology, sample selection, documentation standards, and deficiency classification |\n\n## Example Workflows\n\n### Month-End Close\n\n1. Run `/journal-entry ap-accrual 2024-12` to generate AP accrual entries\n2. Run `/journal-entry prepaid 2024-12` to amortize prepaid expenses\n3. Run `/journal-entry fixed-assets 2024-12` to book depreciation\n4. Run `/reconciliation cash 2024-12` to reconcile bank accounts\n5. Run `/reconciliation accounts-receivable 2024-12` to reconcile AR subledger\n6. Run `/income-statement monthly 2024-12` to generate the P\u0026L with flux analysis\n\n### Variance Analysis\n\n1. Run `/variance-analysis revenue 2024-Q4 vs 2024-Q3` to analyze revenue variances\n2. Run `/variance-analysis opex 2024-12 vs budget` to investigate operating expense variances\n3. Review the waterfall analysis and provide context on unexplained variances\n\n### SOX Testing\n\n1. Run `/sox-testing revenue-recognition 2024-Q4` to generate revenue control testing workpapers\n2. Run `/sox-testing procure-to-pay 2024-Q4` to test procurement controls\n3. Review sample selections and document test results\n\n## MCP Integration\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nThis plugin works best when connected to your financial data sources via MCP servers. Add the relevant servers to your `.mcp.json`:\n\n### ERP / Accounting System\n\nConnect your ERP (e.g., NetSuite, SAP) MCP server to pull trial balances, subledger data, and journal entries automatically.\n\n### Data Warehouse\n\nConnect your data warehouse (e.g., Snowflake, BigQuery) MCP server to query financial data, run variance analysis, and pull historical comparisons.\n\n### Spreadsheets\n\nConnect spreadsheet tools (e.g., Google Sheets, Excel) for workpaper generation, reconciliation templates, and financial model updates.\n\n### Analytics / BI\n\nConnect your BI platform (e.g., Tableau, Looker) to pull dashboards, KPIs, and trend data for variance explanations.\n\n\u003e **Note:** Connect your ERP and data warehouse MCP servers to pull financial data automatically. Without these, you can paste data or upload files for analysis.\n\n## Configuration\n\nAdd your data source MCP servers to the `mcpServers` section of `.mcp.json` in this plugin directory. The `recommendedCategories` field lists the types of integrations that enhance this plugin's capabilities:\n\n- `erp-accounting` — ERP or accounting system for GL, subledger, and JE data\n- `data-warehouse` — Data warehouse for financial queries and historical data\n- `spreadsheets` — Spreadsheet tools for workpaper generation\n- `analytics-bi` — BI tools for dashboards and KPI data\n- `documents` — Document storage for policies, memos, and support\n- `email` — Email for sending reports and requesting approvals\n- `chat` — Team communication for close status updates and questions\n", "depth": 2}, {"id": "348", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~data warehouse` might mean Snowflake, BigQuery, or any other warehouse with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (data warehouse, chat, project tracker, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Data warehouse | `~~data warehouse` | Snowflake\\*, Databricks\\*, BigQuery | Redshift, PostgreSQL |\n| Email | `~~email` | Microsoft 365 | — |\n| Office suite | `~~office suite` | Microsoft 365 | — |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| ERP / Accounting | `~~erp` | — (no supported MCP servers yet) | NetSuite, SAP, QuickBooks, Xero |\n| Analytics / BI | `~~analytics` | — (no supported MCP servers yet) | Tableau, Looker, Power BI |\n\n\\* Placeholder — MCP URL not yet configured\n", "depth": 2}, {"id": "349", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/commands/journal-entry.md", "name": "journal-entry.md", "type": "file", "content": "---\ndescription: Prepare journal entries with proper debits, credits, and supporting detail\nargument-hint: \"\u003centry type\u003e [period]\"\n---\n\n# Journal Entry Preparation\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\n**Important**: This command assists with journal entry workflows but does not provide financial advice. All entries should be reviewed by qualified financial professionals before posting.\n\nPrepare journal entries with proper debits, credits, supporting detail, and review documentation.\n\n## Usage\n\n```\n/je \u003ctype\u003e \u003cperiod\u003e\n```\n\n### Arguments\n\n- `type` — The journal entry type. One of:\n  - `ap-accrual` — Accounts payable accruals for goods/services received but not yet invoiced\n  - `fixed-assets` — Depreciation and amortization entries for fixed assets and intangibles\n  - `prepaid` — Amortization of prepaid expenses (insurance, software, rent, etc.)\n  - `payroll` — Payroll accruals including salaries, benefits, taxes, and bonus accruals\n  - `revenue` — Revenue recognition entries including deferred revenue adjustments\n- `period` — The accounting period (e.g., `2024-12`, `2024-Q4`, `2024`)\n\n## Workflow\n\n### 1. Gather Source Data\n\nIf ~~erp or ~~data warehouse is connected:\n- Pull the trial balance for the specified period\n- Pull subledger detail for the relevant accounts\n- Pull prior period entries of the same type for reference\n- Identify the current GL balances for affected accounts\n\nIf no data source is connected:\n\u003e Connect ~~erp or ~~data warehouse to pull GL data automatically. You can also paste trial balance data or upload a spreadsheet.\n\nPrompt the user to provide:\n- Trial balance or GL balances for affected accounts\n- Subledger detail or supporting schedules\n- Prior period entries for reference (optional)\n\n### 2. Calculate the Entry\n\nBased on the JE type:\n\n**AP Accrual:**\n- Identify goods/services received but not invoiced by period end\n- Calculate accrual amounts from PO receipts, contracts, or estimates\n- Debit: Expense accounts (or asset accounts for capitalizable items)\n- Credit: Accrued liabilities\n\n**Fixed Assets:**\n- Pull the fixed asset register or depreciation schedule\n- Calculate period depreciation by asset class and method (straight-line, declining balance, units of production)\n- Debit: Depreciation expense (by department/cost center)\n- Credit: Accumulated depreciation\n\n**Prepaid:**\n- Pull the prepaid amortization schedule\n- Calculate the period amortization for each prepaid item\n- Debit: Expense accounts (by type — insurance, software, rent, etc.)\n- Credit: Prepaid expense accounts\n\n**Payroll:**\n- Calculate accrued salaries for days worked but not yet paid\n- Calculate accrued benefits (health, retirement contributions, PTO)\n- Calculate employer payroll tax accruals\n- Calculate bonus accruals (if applicable, based on plan terms)\n- Debit: Salary expense, benefits expense, payroll tax expense\n- Credit: Accrued payroll, accrued benefits, accrued payroll taxes\n\n**Revenue:**\n- Review contracts and performance obligations\n- Calculate revenue to recognize based on delivery/performance\n- Adjust deferred revenue balances\n- Debit: Deferred revenue (or accounts receivable)\n- Credit: Revenue accounts (by stream/category)\n\n### 3. Generate the Journal Entry\n\nPresent the entry in standard format:\n\n```\nJournal Entry: [Type] — [Period]\nPrepared by: [User]\nDate: [Period end date]\n\n| Line | Account Code | Account Name | Debit | Credit | Department | Memo |\n|------|-------------|--------------|-------|--------|------------|------|\n| 1    | XXXX        | [Name]       | X,XXX |        | [Dept]     | [Detail] |\n| 2    | XXXX        | [Name]       |       | X,XXX  | [Dept]     | [Detail] |\n|      |             | **Total**    | X,XXX | X,XXX  |            |      |\n\nSupporting Detail:\n- [Calculation basis and assumptions]\n- [Reference to supporting schedule or documentation]\n\nReversal: [Yes/No — if yes, specify reversal date]\n```\n\n### 4. Review Checklist\n\nBefore finalizing, verify:\n\n- [ ] Debits equal credits\n- [ ] Correct accounting period\n- [ ] Account codes are valid and map to the right GL accounts\n- [ ] Amounts are calculated correctly with supporting detail\n- [ ] Memo/description is clear and specific enough for audit\n- [ ] Department/cost center coding is correct\n- [ ] Entry is consistent with prior period treatment\n- [ ] Reversal flag is set appropriately (accruals should auto-reverse)\n- [ ] Supporting documentation is referenced or attached\n- [ ] Entry is within the user's approval authority\n- [ ] No unusual or out-of-pattern amounts that need investigation\n\n### 5. Output\n\nProvide:\n1. The formatted journal entry\n2. Supporting calculations\n3. Comparison to prior period entry of the same type (if available)\n4. Any items flagged for review or follow-up\n5. Instructions for posting (manual entry or upload format for the user's ERP)\n", "depth": 3}, {"id": "350", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/commands/sox-testing.md", "name": "sox-testing.md", "type": "file", "content": "---\ndescription: Generate SOX sample selections, testing workpapers, and control assessments\nargument-hint: \"\u003ccontrol area\u003e [period]\"\n---\n\n# SOX Compliance Testing\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\n**Important**: This command assists with SOX compliance workflows but does not provide audit or legal advice. All testing workpapers and assessments should be reviewed by qualified financial professionals before use in audit documentation.\n\nGenerate sample selections, create testing workpapers, document control assessments, and provide testing templates for SOX 404 internal controls over financial reporting.\n\n## Usage\n\n```\n/sox \u003ccontrol-area\u003e \u003cperiod\u003e\n```\n\n### Arguments\n\n- `control-area` — The control area to test:\n  - `revenue-recognition` — Revenue cycle controls (order-to-cash)\n  - `procure-to-pay` or `p2p` — Procurement and AP controls (purchase-to-pay)\n  - `payroll` — Payroll processing and compensation controls\n  - `financial-close` — Period-end close and reporting controls\n  - `treasury` — Cash management and treasury controls\n  - `fixed-assets` — Capital asset lifecycle controls\n  - `inventory` — Inventory valuation and management controls\n  - `itgc` — IT general controls (access, change management, operations)\n  - `entity-level` — Entity-level and monitoring controls\n  - `journal-entries` — Journal entry processing controls\n  - Any specific control ID or name\n- `period` — The testing period (e.g., `2024-Q4`, `2024`, `2024-H2`)\n\n## Workflow\n\n### 1. Identify Controls to Test\n\nBased on the control area, identify the key controls. Present the control matrix:\n\n| Control # | Control Description | Type | Frequency | Key/Non-Key | Risk | Assertion |\n|-----------|-------------------|------|-----------|-------------|------|-----------|\n| [ID]      | [Description]     | Manual/Automated/IT-Dependent | Daily/Weekly/Monthly/Quarterly/Annual | Key | High/Medium/Low | [CEAVOP] |\n\n**Control types:**\n- **Automated:** System-enforced controls with no manual intervention\n- **Manual:** Controls performed by personnel with judgment\n- **IT-dependent manual:** Manual controls that rely on system-generated data\n\n**Assertions (CEAVOP):**\n- **C**ompleteness — All transactions are recorded\n- **E**xistence/Occurrence — Transactions actually occurred\n- **A**ccuracy — Amounts are correctly recorded\n- **V**aluation — Assets/liabilities are properly valued\n- **O**bligations/Rights — Entity has rights to assets, obligations for liabilities\n- **P**resentation/Disclosure — Properly classified and disclosed\n\n### 2. Determine Sample Size\n\nCalculate sample sizes based on control frequency and risk:\n\n| Control Frequency | Population Size (approx.) | Recommended Sample |\n|------------------|--------------------------|-------------------|\n| Annual           | 1                        | 1 (test the instance) |\n| Quarterly        | 4                        | 2 |\n| Monthly          | 12                       | 2-4 (based on risk) |\n| Weekly           | 52                       | 5-15 (based on risk) |\n| Daily            | ~250                     | 20-40 (based on risk) |\n| Per-transaction  | Varies                   | 25-60 (based on risk and volume) |\n\nAdjust for:\n- **Risk level:** Higher risk controls require larger samples\n- **Prior year results:** Controls with prior deficiencies need larger samples\n- **Reliance:** Controls relied upon by external auditors may need larger samples\n\n### 3. Generate Sample Selection\n\nSelect samples from the population using the appropriate method:\n\n**Random selection** (default for transaction-level controls):\n- Generate random numbers to select specific items from the population\n- Ensure coverage across the full period\n\n**Systematic selection** (for periodic controls):\n- Select items at fixed intervals with a random start point\n- Ensure representation across all sub-periods\n\n**Targeted selection** (supplement to random, for risk-based testing):\n- Select items with specific risk characteristics (high dollar, unusual, period-end)\n- Document rationale for targeted selections\n\nPresent the sample:\n\n```\nSAMPLE SELECTION\nControl: [Control ID] — [Description]\nPeriod: [Testing period]\nPopulation: [Count] items, $[Total value]\nSample size: [N] items\nSelection method: [Random/Systematic/Targeted]\n\n| Sample # | Transaction Date | Reference/ID | Amount | Selection Basis |\n|----------|-----------------|--------------|--------|-----------------|\n| 1        | [Date]          | [Ref]        | $X,XXX | Random          |\n| 2        | [Date]          | [Ref]        | $X,XXX | Random          |\n| ...      | ...             | ...          | ...    | ...             |\n```\n\n### 4. Create Testing Workpaper\n\nGenerate a testing template for each control:\n\n```\nSOX CONTROL TESTING WORKPAPER\n==============================\nControl #: [ID]\nControl Description: [Full description of the control activity]\nControl Owner: [Role/title — to be filled by tester]\nControl Type: [Manual/Automated/IT-Dependent Manual]\nFrequency: [How often the control operates]\nKey Control: [Yes/No]\nRelevant Assertion(s): [CEAVOP]\nTesting Period: [Period]\n\nTEST OBJECTIVE:\nTo determine whether [control description] operated effectively throughout the testing period.\n\nTEST PROCEDURES:\n1. [Step 1 — What to inspect, examine, or re-perform]\n2. [Step 2 — What evidence to obtain]\n3. [Step 3 — What to compare or verify]\n4. [Step 4 — How to evaluate completeness of performance]\n5. [Step 5 — How to assess timeliness of performance]\n\nEXPECTED EVIDENCE:\n- [Document type 1 — e.g., signed approval form]\n- [Document type 2 — e.g., system screenshot showing review]\n- [Document type 3 — e.g., reconciliation with preparer sign-off]\n\nTEST RESULTS:\n\n| Sample # | Ref | Procedure 1 | Procedure 2 | Procedure 3 | Result | Exception? | Notes |\n|----------|-----|-------------|-------------|-------------|--------|------------|-------|\n| 1        |     | Pass/Fail   | Pass/Fail   | Pass/Fail   | Pass/Fail | Y/N    |       |\n| 2        |     | Pass/Fail   | Pass/Fail   | Pass/Fail   | Pass/Fail | Y/N    |       |\n\nEXCEPTIONS NOTED:\n| Sample # | Exception Description | Root Cause | Compensating Control | Impact |\n|----------|----------------------|------------|---------------------|--------|\n|          |                      |            |                     |        |\n\nCONCLUSION:\n[ ] Effective — Control operated effectively with no exceptions\n[ ] Effective with exceptions — Control operated effectively; exceptions are isolated\n[ ] Deficiency — Control did not operate effectively\n[ ] Significant Deficiency — Deficiency is more than inconsequential\n[ ] Material Weakness — Reasonable possibility of material misstatement not prevented/detected\n\nTested by: ________________  Date: ________\nReviewed by: _______________  Date: ________\n```\n\n### 5. Provide Common Control Templates\n\nBased on the control area, provide pre-built test step templates:\n\n**Revenue Recognition:**\n- Verify sales order approval and authorization\n- Confirm delivery/performance evidence\n- Test revenue recognition timing against contract terms\n- Verify pricing accuracy to contract/price list\n- Test credit memo approval and validity\n\n**Procure to Pay:**\n- Verify purchase order approval and authorization limits\n- Confirm three-way match (PO, receipt, invoice)\n- Test vendor master data change controls\n- Verify payment approval and segregation of duties\n- Test duplicate payment prevention controls\n\n**Financial Close:**\n- Verify account reconciliation completeness and timeliness\n- Test journal entry approval and segregation of duties\n- Verify management review of financial statements\n- Test consolidation and elimination entries\n- Verify disclosure checklist completion\n\n**ITGC:**\n- Test user access provisioning and de-provisioning\n- Verify privileged access reviews\n- Test change management approval and testing\n- Verify batch job monitoring and exception handling\n- Test backup and recovery procedures\n\n### 6. Document Control Assessment\n\nClassify any identified deficiencies:\n\n**Deficiency:** A control does not allow management or employees to prevent or detect misstatements on a timely basis. Consider:\n- Likelihood of misstatement\n- Magnitude of potential misstatement\n- Whether compensating controls exist\n\n**Significant Deficiency:** A deficiency (or combination) that is less severe than a material weakness but important enough to merit attention by those responsible for oversight.\n\n**Material Weakness:** A deficiency (or combination) such that there is a reasonable possibility that a material misstatement will not be prevented or detected on a timely basis.\n\n### 7. Output\n\nProvide:\n1. Control matrix for the selected area\n2. Sample selections with methodology documentation\n3. Testing workpaper templates with pre-populated test steps\n4. Results documentation template\n5. Deficiency evaluation framework (if exceptions are identified)\n6. Suggested remediation actions for any noted deficiencies\n", "depth": 3}, {"id": "351", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/commands/reconciliation.md", "name": "reconciliation.md", "type": "file", "content": "---\ndescription: Reconcile GL balances to subledger, bank, or third-party balances\nargument-hint: \"\u003caccount\u003e [period]\"\n---\n\n# Account Reconciliation\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\n**Important**: This command assists with reconciliation workflows but does not provide financial advice. All reconciliations should be reviewed by qualified financial professionals before sign-off.\n\nReconcile GL account balances to subledger, bank, or third-party balances. Identify and categorize reconciling items and generate a reconciliation workpaper.\n\n## Usage\n\n```\n/recon \u003caccount\u003e \u003cperiod\u003e\n```\n\n### Arguments\n\n- `account` — The account or account category to reconcile. Examples:\n  - `cash` or `bank` — Bank reconciliation (GL cash to bank statement)\n  - `accounts-receivable` or `ar` — AR subledger reconciliation\n  - `accounts-payable` or `ap` — AP subledger reconciliation\n  - `fixed-assets` or `fa` — Fixed asset subledger reconciliation\n  - `intercompany` or `ic` — Intercompany balance reconciliation\n  - `prepaid` — Prepaid expense schedule reconciliation\n  - `accrued-liabilities` — Accrued liabilities detail reconciliation\n  - Any specific GL account code (e.g., `1010`, `2100`)\n- `period` — The accounting period end date (e.g., `2024-12`, `2024-12-31`)\n\n## Workflow\n\n### 1. Gather Both Sides of the Reconciliation\n\nIf ~~erp or ~~data warehouse is connected:\n- Pull the GL balance for the specified account(s) as of period end\n- Pull the subledger, bank statement, or third-party balance for comparison\n- Pull prior period reconciliation (if available) for outstanding item carryforward\n\nIf no data source is connected:\n\u003e Connect ~~erp or ~~data warehouse to pull account balances automatically. To reconcile manually, provide:\n\u003e 1. **GL side:** The general ledger balance for the account as of period end\n\u003e 2. **Other side:** The subledger balance, bank statement balance, or third-party confirmation balance\n\u003e 3. **Prior period outstanding items** (optional): Any reconciling items from the prior period reconciliation\n\n### 2. Compare Balances\n\nCalculate the difference between the two sides:\n\n```\nGL Balance:                    $XX,XXX.XX\nSubledger/Bank/Other Balance:  $XX,XXX.XX\n                               ----------\nDifference:                    $XX,XXX.XX\n```\n\n### 3. Identify Reconciling Items\n\nAnalyze the difference and categorize reconciling items:\n\n**Timing Differences** (items that will clear in subsequent periods):\n- Outstanding checks / payments issued but not yet cleared\n- Deposits in transit / receipts recorded but not yet credited\n- Invoices posted in one system but pending in the other\n- Accruals awaiting reversal\n\n**Permanent Differences** (items requiring adjustment):\n- Errors in recording (wrong amount, wrong account, duplicate entries)\n- Missing entries (transactions in one system but not the other)\n- Bank fees or charges not yet recorded\n- Foreign currency translation differences\n\n**Prior Period Items** (carryforward from prior reconciliation):\n- Items from prior period that have now cleared (remove from reconciliation)\n- Items from prior period still outstanding (carry forward with aging)\n\n### 4. Generate Reconciliation Workpaper\n\n```\nACCOUNT RECONCILIATION\nAccount: [Account code] — [Account name]\nPeriod End: [Date]\nPrepared by: [User]\nDate Prepared: [Today]\n\nRECONCILIATION SUMMARY\n=======================\n\nBalance per General Ledger:              $XX,XXX.XX\n\nAdd: Reconciling items increasing GL\n  [Item description]                     $X,XXX.XX\n  [Item description]                     $X,XXX.XX\n                                         ---------\n  Subtotal additions:                    $X,XXX.XX\n\nLess: Reconciling items decreasing GL\n  [Item description]                    ($X,XXX.XX)\n  [Item description]                    ($X,XXX.XX)\n                                         ---------\n  Subtotal deductions:                  ($X,XXX.XX)\n\nAdjusted GL Balance:                     $XX,XXX.XX\n\nBalance per [Subledger/Bank/Other]:      $XX,XXX.XX\n\nAdd: Reconciling items\n  [Item description]                     $X,XXX.XX\n\nLess: Reconciling items\n  [Item description]                    ($X,XXX.XX)\n\nAdjusted [Other] Balance:                $XX,XXX.XX\n\nDIFFERENCE:                              $0.00\n```\n\n### 5. Reconciling Items Detail\n\nPresent each reconciling item with:\n\n| # | Description | Amount | Category | Age (Days) | Status | Action Required |\n|---|-------------|--------|----------|------------|--------|-----------------|\n| 1 | [Detail]    | $X,XXX | Timing   | 5          | Expected to clear | Monitor |\n| 2 | [Detail]    | $X,XXX | Error    | N/A        | Requires correction | Post adjusting JE |\n\n### 6. Review and Escalation\n\nFlag items that require attention:\n\n- **Aged items:** Reconciling items outstanding more than 30/60/90 days\n- **Large items:** Individual items exceeding materiality thresholds\n- **Growing balances:** Reconciling item totals increasing period over period\n- **Unresolved prior period items:** Items carried forward without resolution\n- **Unexplained differences:** Amounts that cannot be tied to specific transactions\n\n### 7. Output\n\nProvide:\n1. The formatted reconciliation workpaper\n2. List of reconciling items with categorization and aging\n3. Required adjusting entries (if any permanent differences identified)\n4. Action items for items requiring follow-up\n5. Comparison to prior period reconciliation (if available)\n6. Sign-off section for preparer and reviewer\n", "depth": 3}, {"id": "352", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/commands/income-statement.md", "name": "income-statement.md", "type": "file", "content": "---\ndescription: Generate an income statement with period-over-period comparison and variance analysis\nargument-hint: \"\u003cfrequency\u003e \u003cperiod\u003e\"\n---\n\n# Income Statement Generation\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\n**Important**: This command assists with financial statement workflows but does not provide financial advice. All statements should be reviewed by qualified financial professionals before use in reporting or filings.\n\nGenerate an income statement with period-over-period comparison and variance analysis. Highlight material variances for investigation.\n\n## Usage\n\n```\n/income-statement \u003cperiod-type\u003e \u003cperiod\u003e\n```\n\n### Arguments\n\n- `period-type` — The reporting period type:\n  - `monthly` — Single month P\u0026L with prior month and prior year month comparison\n  - `quarterly` — Quarter P\u0026L with prior quarter and prior year quarter comparison\n  - `annual` — Full year P\u0026L with prior year comparison\n  - `ytd` — Year-to-date P\u0026L with prior year YTD comparison\n- `period` — The period to report (e.g., `2024-12`, `2024-Q4`, `2024`)\n\n## Workflow\n\n### 1. Gather Financial Data\n\nIf ~~erp or ~~data warehouse is connected:\n- Pull trial balance or income statement data for the specified period\n- Pull comparison period data (prior period, prior year, budget/forecast)\n- Pull account hierarchy and groupings for presentation\n\nIf no data source is connected:\n\u003e Connect ~~erp or ~~data warehouse to pull financial data automatically. You can also paste trial balance data, upload a spreadsheet, or provide income statement data for analysis.\n\nPrompt the user to provide:\n- Current period revenue and expense data (by account or category)\n- Comparison period data (prior period, prior year, and/or budget)\n- Any known adjustments or reclassifications\n\n### 2. Generate Income Statement\n\nPresent in standard multi-column format:\n\n```\nINCOME STATEMENT\nPeriod: [Period description]\n(in thousands, unless otherwise noted)\n\n                              Current    Prior      Variance   Variance   Budget    Budget\n                              Period     Period     ($)        (%)        Amount    Var ($)\n                              --------   --------   --------   --------   --------  --------\nREVENUE\n  Product revenue             $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  Service revenue             $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  Other revenue               $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n                              --------   --------   --------              --------  --------\nTOTAL REVENUE                 $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n\nCOST OF REVENUE\n  [Cost items]                $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n                              --------   --------   --------              --------  --------\nGROSS PROFIT                  $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  Gross Margin                XX.X%      XX.X%\n\nOPERATING EXPENSES\n  Research \u0026 development      $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  Sales \u0026 marketing           $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  General \u0026 administrative    $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n                              --------   --------   --------              --------  --------\nTOTAL OPERATING EXPENSES      $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n\nOPERATING INCOME (LOSS)       $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  Operating Margin            XX.X%      XX.X%\n\nOTHER INCOME (EXPENSE)\n  Interest income             $XX,XXX    $XX,XXX    $X,XXX     X.X%\n  Interest expense           ($XX,XXX)  ($XX,XXX)   $X,XXX     X.X%\n  Other, net                  $XX,XXX    $XX,XXX    $X,XXX     X.X%\n                              --------   --------   --------\nTOTAL OTHER INCOME (EXPENSE)  $XX,XXX    $XX,XXX    $X,XXX     X.X%\n\nINCOME BEFORE TAXES           $XX,XXX    $XX,XXX    $X,XXX     X.X%\n  Income tax expense          $XX,XXX    $XX,XXX    $X,XXX     X.X%\n                              --------   --------   --------\n\nNET INCOME (LOSS)             $XX,XXX    $XX,XXX    $X,XXX     X.X%       $XX,XXX   $X,XXX\n  Net Margin                  XX.X%      XX.X%\n```\n\n### 3. Variance Analysis\n\nFor each line item, calculate and flag material variances:\n\n**Materiality thresholds** (flag if either condition met):\n- Dollar variance exceeds a defined threshold (e.g., $50K, $100K — ask user for their threshold)\n- Percentage variance exceeds 10% (or user-defined threshold)\n\nFor flagged variances, provide:\n- Direction and magnitude of the variance\n- Possible drivers (if data is available to decompose)\n- Questions to investigate\n- Whether the variance is favorable or unfavorable\n\n### 4. Key Metrics Summary\n\n```\nKEY METRICS\n                              Current    Prior      Change\nRevenue growth (%)                                  X.X%\nGross margin (%)              XX.X%      XX.X%      X.X pp\nOperating margin (%)          XX.X%      XX.X%      X.X pp\nNet margin (%)                XX.X%      XX.X%      X.X pp\nOpEx as % of revenue          XX.X%      XX.X%      X.X pp\nEffective tax rate (%)        XX.X%      XX.X%      X.X pp\n```\n\n### 5. Material Variance Summary\n\nList all material variances requiring investigation:\n\n| Line Item | Variance ($) | Variance (%) | Direction | Preliminary Driver | Action |\n|-----------|-------------|-------------|-----------|-------------------|--------|\n| [Item]    | $X,XXX      | X.X%        | Unfav.    | [If known]        | Investigate |\n\n### 6. Output\n\nProvide:\n1. Formatted income statement with comparisons\n2. Key metrics summary\n3. Material variance listing with investigation flags\n4. Suggested follow-up questions for unexplained variances\n5. Offer to drill into any specific variance with `/flux`\n", "depth": 3}, {"id": "353", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/commands/variance-analysis.md", "name": "variance-analysis.md", "type": "file", "content": "---\ndescription: Decompose variances into drivers with narrative explanations and waterfall analysis\nargument-hint: \"\u003cline item\u003e \u003cperiod\u003e vs \u003ccomparison\u003e\"\n---\n\n# Variance / Flux Analysis\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\n**Important**: This command assists with variance analysis workflows but does not provide financial advice. All analyses should be reviewed by qualified financial professionals before use in reporting.\n\nDecompose variances into underlying drivers, provide narrative explanations for significant variances, and generate waterfall analysis.\n\n## Usage\n\n```\n/flux \u003carea\u003e \u003cperiod-comparison\u003e\n```\n\n### Arguments\n\n- `area` — The area to analyze:\n  - `revenue` — Revenue variance by stream, product, geography, customer segment\n  - `opex` — Operating expense variance by category, department, cost center\n  - `capex` — Capital expenditure variance vs budget by project and asset class\n  - `headcount` — Headcount and compensation variance by department and role level\n  - `cogs` or `cost-of-revenue` — Cost of revenue variance by component\n  - `gross-margin` — Gross margin analysis with mix and rate effects\n  - Any specific GL account or account group\n- `period-comparison` — The periods to compare. Formats:\n  - `2024-12 vs 2024-11` — Month over month\n  - `2024-12 vs 2023-12` — Year over year\n  - `2024-Q4 vs 2024-Q3` — Quarter over quarter\n  - `2024-12 vs budget` — Actual vs budget\n  - `2024-12 vs forecast` — Actual vs forecast\n  - `2024-Q4 vs 2024-Q3 vs 2023-Q4` — Three-way comparison\n\n## Workflow\n\n### 1. Gather Data\n\nIf ~~erp or ~~data warehouse is connected:\n- Pull actuals for both comparison periods at the detail level\n- Pull budget/forecast data if comparing to plan\n- Pull supporting operational metrics (headcount, volumes, rates)\n- Pull prior variance analyses for context\n\nIf no data source is connected:\n\u003e Connect ~~erp or ~~data warehouse to pull financial data automatically. To analyze manually, provide:\n\u003e 1. Actual data for both comparison periods (at account or line-item detail)\n\u003e 2. Budget/forecast data (if comparing to plan)\n\u003e 3. Any operational metrics that drive the financial results (headcount, volumes, pricing, etc.)\n\n### 2. Calculate Top-Level Variance\n\n```\nVARIANCE SUMMARY: [Area] — [Period 1] vs [Period 2]\n\n                              Period 1   Period 2   Variance ($)   Variance (%)\n                              --------   --------   ------------   ------------\nTotal [Area]                  $XX,XXX    $XX,XXX    $X,XXX         X.X%\n```\n\n### 3. Decompose Variance by Driver\n\nBreak down the total variance into constituent drivers. Use the appropriate decomposition method for the area:\n\n**Revenue Decomposition:**\n- **Volume effect:** Change in units/customers/transactions at prior period pricing\n- **Price/rate effect:** Change in pricing/ASP applied to current period volume\n- **Mix effect:** Shift between products/segments at different margin levels\n- **New vs existing:** Revenue from new customers/products vs base business\n- **Currency effect:** FX impact on international revenue (if applicable)\n\n**Operating Expense Decomposition:**\n- **Headcount-driven:** Salary and benefits changes from headcount additions/reductions\n- **Compensation changes:** Merit increases, promotions, bonus accruals\n- **Volume-driven:** Expenses that scale with business activity (hosting, commissions, travel)\n- **New programs/investments:** Incremental spend on new initiatives\n- **One-time items:** Non-recurring expenses (severance, legal settlements, write-offs)\n- **Timing:** Expenses shifted between periods (prepaid amortization changes, contract timing)\n\n**CapEx Decomposition:**\n- **Project-level:** Variance by capital project vs approved budget\n- **Timing:** Projects ahead of or behind schedule\n- **Scope changes:** Approved scope expansions or reductions\n- **Cost overruns:** Unit cost increases vs plan\n\n**Headcount Decomposition:**\n- **Hiring pace:** Actual hires vs plan by department and level\n- **Attrition:** Unplanned departures and backfill timing\n- **Compensation mix:** Salary, bonus, equity, benefits variance\n- **Contractor/temp:** Supplemental workforce changes\n\n### 4. Waterfall Analysis\n\nGenerate a text-based waterfall showing how each driver contributes to the total variance:\n\n```\nWATERFALL: [Area] — [Period 1] vs [Period 2]\n\n[Period 2 Base]                           $XX,XXX\n  |\n  |--[+] [Driver 1 description]          +$X,XXX\n  |--[+] [Driver 2 description]          +$X,XXX\n  |--[-] [Driver 3 description]          -$X,XXX\n  |--[+] [Driver 4 description]          +$X,XXX\n  |--[-] [Driver 5 description]          -$X,XXX\n  |\n[Period 1 Actual]                         $XX,XXX\n\nVariance Reconciliation:\n  Driver 1:    +$X,XXX  (XX% of total variance)\n  Driver 2:    +$X,XXX  (XX% of total variance)\n  Driver 3:    -$X,XXX  (XX% of total variance)\n  Driver 4:    +$X,XXX  (XX% of total variance)\n  Driver 5:    -$X,XXX  (XX% of total variance)\n  Unexplained: $X,XXX   (XX% of total variance)\n               --------\n  Total:       $X,XXX   (100%)\n```\n\n### 5. Narrative Explanations\n\nFor each significant driver, generate a narrative explanation:\n\n\u003e **[Driver name]** — [Favorable/Unfavorable] variance of $X,XXX (X.X%)\n\u003e\n\u003e [2-3 sentence explanation of what caused this variance, referencing specific operational factors, business events, or decisions. Include quantification where possible.]\n\u003e\n\u003e *Outlook:* [Whether this is expected to continue, reverse, or change in future periods]\n\n### 6. Identify Unexplained Variances\n\nIf the decomposition does not fully explain the total variance, flag the residual:\n\n\u003e **Unexplained variance:** $X,XXX (X.X% of total)\n\u003e\n\u003e Possible causes to investigate:\n\u003e - [Suggested area 1]\n\u003e - [Suggested area 2]\n\u003e - [Suggested area 3]\n\nAsk the user for additional context on unexplained variances:\n- \"Can you provide context on [specific unexplained item]?\"\n- \"Were there any business events in [period] that would explain [variance area]?\"\n- \"Is the [specific driver] variance expected or a surprise?\"\n\n### 7. Output\n\nProvide:\n1. Top-level variance summary\n2. Detailed variance decomposition by driver\n3. Waterfall analysis (text format, or suggest chart if spreadsheet tool is connected)\n4. Narrative explanations for each significant driver\n5. Unexplained variance flag with investigation suggestions\n6. Trend context (is this variance new, growing, or consistent with recent periods?)\n7. Suggested actions or follow-ups\n", "depth": 3}, {"id": "354", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/reconciliation", "name": "reconciliation", "type": "dir", "content": "", "depth": 3}, {"id": "355", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/journal-entry-prep", "name": "journal-entry-prep", "type": "dir", "content": "", "depth": 3}, {"id": "356", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/close-management", "name": "close-management", "type": "dir", "content": "", "depth": 3}, {"id": "357", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/financial-statements", "name": "financial-statements", "type": "dir", "content": "", "depth": 3}, {"id": "358", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/audit-support", "name": "audit-support", "type": "dir", "content": "", "depth": 3}, {"id": "359", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/variance-analysis", "name": "variance-analysis", "type": "dir", "content": "", "depth": 3}, {"id": "360", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/reconciliation/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: reconciliation\ndescription: Reconcile accounts by comparing GL balances to subledgers, bank statements, or third-party data. Use when performing bank reconciliations, GL-to-subledger recs, intercompany reconciliations, or identifying and categorizing reconciling items.\n---\n\n# Reconciliation\n\n**Important**: This skill assists with reconciliation workflows but does not provide financial advice. All reconciliations should be reviewed by qualified financial professionals before sign-off.\n\nMethodology and best practices for account reconciliation, including GL-to-subledger, bank reconciliations, and intercompany. Covers reconciling item categorization, aging analysis, and escalation.\n\n## Reconciliation Types\n\n### GL to Subledger Reconciliation\n\nCompare the general ledger control account balance to the detailed subledger balance.\n\n**Common accounts:**\n- Accounts receivable (GL control vs AR subledger aging)\n- Accounts payable (GL control vs AP subledger aging)\n- Fixed assets (GL control vs fixed asset register)\n- Inventory (GL control vs inventory valuation report)\n- Prepaid expenses (GL control vs prepaid amortization schedule)\n- Accrued liabilities (GL control vs accrual detail schedules)\n\n**Process:**\n1. Pull GL balance for the control account as of period end\n2. Pull subledger trial balance or detail report as of the same date\n3. Compare totals — they should match if posting is real-time\n4. Investigate any differences (timing of posting, manual entries not reflected, interface errors)\n\n**Common causes of differences:**\n- Manual journal entries posted to the control account but not reflected in the subledger\n- Subledger transactions not yet interfaced to the GL\n- Timing differences in batch posting\n- Reclassification entries in the GL without subledger adjustment\n- System interface errors or failed postings\n\n### Bank Reconciliation\n\nCompare the GL cash balance to the bank statement balance.\n\n**Process:**\n1. Obtain the bank statement balance as of period end\n2. Pull the GL cash account balance as of the same date\n3. Identify outstanding checks (issued but not cleared at the bank)\n4. Identify deposits in transit (recorded in GL but not yet credited by bank)\n5. Identify bank charges, interest, or adjustments not yet recorded in GL\n6. Reconcile both sides to an adjusted balance\n\n**Standard format:**\n\n```\nBalance per bank statement:         $XX,XXX\nAdd: Deposits in transit            $X,XXX\nLess: Outstanding checks           ($X,XXX)\nAdd/Less: Bank errors               $X,XXX\nAdjusted bank balance:              $XX,XXX\n\nBalance per general ledger:         $XX,XXX\nAdd: Interest/credits not recorded  $X,XXX\nLess: Bank fees not recorded       ($X,XXX)\nAdd/Less: GL errors                 $X,XXX\nAdjusted GL balance:                $XX,XXX\n\nDifference:                         $0.00\n```\n\n### Intercompany Reconciliation\n\nReconcile balances between related entities to ensure they net to zero on consolidation.\n\n**Process:**\n1. Pull intercompany receivable/payable balances for each entity pair\n2. Compare Entity A's receivable from Entity B to Entity B's payable to Entity A\n3. Identify and resolve differences\n4. Confirm all intercompany transactions have been recorded on both sides\n5. Verify elimination entries are correct for consolidation\n\n**Common causes of differences:**\n- Transactions recorded by one entity but not the other (timing)\n- Different FX rates used by each entity\n- Misclassification (intercompany vs third-party)\n- Disputed amounts or unapplied payments\n- Different period-end cut-off practices across entities\n\n## Reconciling Item Categorization\n\n### Category 1: Timing Differences\n\nItems that exist because of normal processing timing and will clear without action:\n\n- **Outstanding checks:** Checks issued and recorded in GL, pending bank clearance\n- **Deposits in transit:** Deposits made and recorded in GL, pending bank credit\n- **In-transit transactions:** Items posted in one system but pending interface to the other\n- **Pending approvals:** Transactions awaiting approval to post in one system\n\n**Expected resolution:** These items should clear within the normal processing cycle (typically 1-5 business days). No adjusting entry needed.\n\n### Category 2: Adjustments Required\n\nItems that require a journal entry to correct:\n\n- **Unrecorded bank charges:** Bank fees, wire charges, returned item fees\n- **Unrecorded interest:** Interest income or expense from bank/lender\n- **Recording errors:** Wrong amount, wrong account, duplicates\n- **Missing entries:** Transactions in one system with no corresponding entry in the other\n- **Classification errors:** Correctly recorded but in the wrong account\n\n**Action:** Prepare adjusting journal entry to correct the GL or subledger.\n\n### Category 3: Requires Investigation\n\nItems that cannot be immediately explained:\n\n- **Unidentified differences:** Variances with no obvious cause\n- **Disputed items:** Amounts contested between parties\n- **Aged outstanding items:** Items that have not cleared within expected timeframes\n- **Recurring unexplained differences:** Same type of difference appearing each period\n\n**Action:** Investigate root cause, document findings, escalate if unresolved.\n\n## Aging Analysis for Outstanding Items\n\nTrack the age of reconciling items to identify stale items requiring escalation:\n\n| Age Bucket | Status | Action |\n|-----------|--------|--------|\n| 0-30 days | Current | Monitor — within normal processing cycle |\n| 31-60 days | Aging | Investigate — follow up on why item has not cleared |\n| 61-90 days | Overdue | Escalate — notify supervisor, document investigation |\n| 90+ days | Stale | Escalate to management — potential write-off or adjustment needed |\n\n### Aging Report Format\n\n| Item # | Description | Amount | Date Originated | Age (Days) | Category | Status | Owner |\n|--------|-------------|--------|-----------------|------------|----------|--------|-------|\n| 1      | [Detail]    | $X,XXX | [Date]          | XX         | [Type]   | [Status] | [Name] |\n\n### Trending\n\nTrack reconciling item totals over time to identify growing balances:\n\n- Compare total outstanding items to prior period\n- Flag if total reconciling items exceed materiality threshold\n- Flag if number of items is growing period over period\n- Identify recurring items that appear every period (may indicate process issue)\n\n## Escalation Thresholds\n\nDefine escalation triggers based on your organization's risk tolerance:\n\n| Trigger | Threshold (Example) | Escalation |\n|---------|---------------------|------------|\n| Individual item amount | \u003e $10,000 | Supervisor review |\n| Individual item amount | \u003e $50,000 | Controller review |\n| Total reconciling items | \u003e $100,000 | Controller review |\n| Item age | \u003e 60 days | Supervisor follow-up |\n| Item age | \u003e 90 days | Controller / management review |\n| Unreconciled difference | Any amount | Cannot close — must resolve or document |\n| Growing trend | 3+ consecutive periods | Process improvement investigation |\n\n*Note: Set thresholds based on your organization's materiality level and risk appetite. The examples above are illustrative.*\n\n## Reconciliation Best Practices\n\n1. **Timeliness:** Complete reconciliations within the close calendar deadline (typically T+3 to T+5 business days after period end)\n2. **Completeness:** Reconcile all balance sheet accounts on a defined frequency (monthly for material accounts, quarterly for immaterial)\n3. **Documentation:** Every reconciliation should include preparer, reviewer, date, and clear explanation of all reconciling items\n4. **Segregation:** The person who reconciles should not be the same person who processes transactions in that account\n5. **Follow-through:** Track open items to resolution — do not just carry items forward indefinitely\n6. **Root cause analysis:** For recurring reconciling items, investigate and fix the underlying process issue\n7. **Standardization:** Use consistent templates and procedures across all accounts\n8. **Retention:** Maintain reconciliations and supporting detail per your organization's document retention policy\n", "depth": 4}, {"id": "361", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/journal-entry-prep/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: journal-entry-prep\ndescription: Prepare journal entries with proper debits, credits, and supporting documentation for month-end close. Use when booking accruals, prepaid amortization, fixed asset depreciation, payroll entries, revenue recognition, or any manual journal entry.\n---\n\n# Journal Entry Preparation\n\n**Important**: This skill assists with journal entry workflows but does not provide financial advice. All entries should be reviewed by qualified financial professionals before posting.\n\nBest practices, standard entry types, documentation requirements, and review workflows for journal entry preparation.\n\n## Standard Accrual Types and Their Entries\n\n### Accounts Payable Accruals\n\nAccrue for goods or services received but not yet invoiced at period end.\n\n**Typical entry:**\n- Debit: Expense account (or capitalize if asset-qualifying)\n- Credit: Accrued liabilities\n\n**Sources for calculation:**\n- Open purchase orders with confirmed receipts\n- Contracts with services rendered but unbilled\n- Recurring vendor arrangements (utilities, subscriptions, professional services)\n- Employee expense reports submitted but not yet processed\n\n**Key considerations:**\n- Reverse in the following period (auto-reversal recommended)\n- Use consistent estimation methodology period over period\n- Document basis for estimates (PO amount, contract terms, historical run-rate)\n- Track actual vs accrual to refine future estimates\n\n### Fixed Asset Depreciation\n\nBook periodic depreciation expense for tangible and intangible assets.\n\n**Typical entry:**\n- Debit: Depreciation/amortization expense (by department or cost center)\n- Credit: Accumulated depreciation/amortization\n\n**Depreciation methods:**\n- **Straight-line:** (Cost - Salvage) / Useful life — most common for financial reporting\n- **Declining balance:** Accelerated method applying fixed rate to net book value\n- **Units of production:** Based on actual usage or output vs total expected\n\n**Key considerations:**\n- Run depreciation from the fixed asset register or schedule\n- Verify new additions are set up with correct useful life and method\n- Check for disposals or impairments requiring write-off\n- Ensure consistency between book and tax depreciation tracking\n\n### Prepaid Expense Amortization\n\nAmortize prepaid expenses over their benefit period.\n\n**Typical entry:**\n- Debit: Expense account (insurance, software, rent, etc.)\n- Credit: Prepaid expense\n\n**Common prepaid categories:**\n- Insurance premiums (typically 12-month policies)\n- Software licenses and subscriptions\n- Prepaid rent (if applicable under lease terms)\n- Prepaid maintenance contracts\n- Conference and event deposits\n\n**Key considerations:**\n- Maintain an amortization schedule with start/end dates and monthly amounts\n- Review for any prepaid items that should be fully expensed (immaterial amounts)\n- Check for cancelled or terminated contracts requiring accelerated amortization\n- Verify new prepaids are added to the schedule promptly\n\n### Payroll Accruals\n\nAccrue compensation and related costs for the period.\n\n**Typical entries:**\n\n*Salary accrual (for pay periods not aligned with month-end):*\n- Debit: Salary expense (by department)\n- Credit: Accrued payroll\n\n*Bonus accrual:*\n- Debit: Bonus expense (by department)\n- Credit: Accrued bonus\n\n*Benefits accrual:*\n- Debit: Benefits expense\n- Credit: Accrued benefits\n\n*Payroll tax accrual:*\n- Debit: Payroll tax expense\n- Credit: Accrued payroll taxes\n\n**Key considerations:**\n- Calculate salary accrual based on working days in the period vs pay period\n- Bonus accruals should reflect plan terms (target amounts, performance metrics, payout timing)\n- Include employer-side taxes and benefits (FICA, FUTA, health, 401k match)\n- Track PTO/vacation accrual liability if required by policy or jurisdiction\n\n### Revenue Recognition\n\nRecognize revenue based on performance obligations and delivery.\n\n**Typical entries:**\n\n*Recognize previously deferred revenue:*\n- Debit: Deferred revenue\n- Credit: Revenue\n\n*Recognize revenue with new receivable:*\n- Debit: Accounts receivable\n- Credit: Revenue\n\n*Defer revenue received in advance:*\n- Debit: Cash / Accounts receivable\n- Credit: Deferred revenue\n\n**Key considerations:**\n- Follow ASC 606 five-step framework for contracts with customers\n- Identify distinct performance obligations in each contract\n- Determine transaction price (including variable consideration)\n- Allocate transaction price to performance obligations\n- Recognize revenue as/when performance obligations are satisfied\n- Maintain contract-level detail for audit support\n\n## Supporting Documentation Requirements\n\nEvery journal entry should have:\n\n1. **Entry description/memo:** Clear, specific description of what the entry records and why\n2. **Calculation support:** How amounts were derived (formula, schedule, source data reference)\n3. **Source documents:** Reference to the underlying transactions or events (PO numbers, invoice numbers, contract references, payroll register)\n4. **Period:** The accounting period the entry applies to\n5. **Preparer identification:** Who prepared the entry and when\n6. **Approval:** Evidence of review and approval per the authorization matrix\n7. **Reversal indicator:** Whether the entry auto-reverses and the reversal date\n\n## Review and Approval Workflows\n\n### Typical Approval Matrix\n\n| Entry Type | Amount Threshold | Approver |\n|-----------|-----------------|----------|\n| Standard recurring | Any amount | Accounting manager |\n| Non-recurring / manual | \u003c $50K | Accounting manager |\n| Non-recurring / manual | $50K - $250K | Controller |\n| Non-recurring / manual | \u003e $250K | CFO / VP Finance |\n| Top-side / consolidation | Any amount | Controller or above |\n| Out-of-period adjustments | Any amount | Controller or above |\n\n*Note: Thresholds should be set based on your organization's materiality and risk tolerance.*\n\n### Review Checklist\n\nBefore approving a journal entry, the reviewer should verify:\n\n- [ ] Debits equal credits (entry is balanced)\n- [ ] Correct accounting period (not posting to a closed period)\n- [ ] Account codes exist and are appropriate for the transaction\n- [ ] Amounts are mathematically accurate and supported by calculations\n- [ ] Description is clear, specific, and sufficient for audit purposes\n- [ ] Department/cost center/project coding is correct\n- [ ] Treatment is consistent with prior periods and accounting policies\n- [ ] Auto-reversal is set appropriately (accruals should reverse)\n- [ ] Supporting documentation is complete and referenced\n- [ ] Entry amount is within the preparer's authority level\n- [ ] No duplicate of an existing entry\n- [ ] Unusual or large amounts are explained and justified\n\n## Common Errors to Check For\n\n1. **Unbalanced entries:** Debits do not equal credits (system should prevent, but check manual entries)\n2. **Wrong period:** Entry posted to an incorrect or already-closed period\n3. **Wrong sign:** Debit entered as credit or vice versa\n4. **Duplicate entries:** Same transaction recorded twice (check for duplicates before posting)\n5. **Wrong account:** Entry posted to incorrect GL account (especially similar account codes)\n6. **Missing reversal:** Accrual entry not set to auto-reverse, causing double-counting\n7. **Stale accruals:** Recurring accruals not updated for changed circumstances\n8. **Round-number estimates:** Suspiciously round amounts that may not reflect actual calculations\n9. **Incorrect FX rates:** Foreign currency entries using wrong exchange rate or date\n10. **Missing intercompany elimination:** Entries between entities without corresponding elimination\n11. **Capitalization errors:** Expenses that should be capitalized, or capitalized items that should be expensed\n12. **Cut-off errors:** Transactions recorded in the wrong period based on delivery or service date\n", "depth": 4}, {"id": "362", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/close-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: close-management\ndescription: Manage the month-end close process with task sequencing, dependencies, and status tracking. Use when planning the close calendar, tracking close progress, identifying blockers, or sequencing close activities by day.\n---\n\n# Close Management\n\n**Important**: This skill assists with close management workflows but does not provide financial advice. All close activities should be reviewed by qualified financial professionals.\n\nMonth-end close checklist, task sequencing and dependencies, status tracking, and common close activities organized by day.\n\n## Month-End Close Checklist\n\n### Pre-Close (Last 2-3 Business Days of the Month)\n\n- [ ] Send close calendar and deadline reminders to all contributors\n- [ ] Confirm cut-off procedures with AP, AR, payroll, and treasury\n- [ ] Verify all sub-systems are processing normally (ERP, payroll, banking)\n- [ ] Complete preliminary bank reconciliation (all but last-day activity)\n- [ ] Review open purchase orders for potential accrual needs\n- [ ] Confirm payroll processing schedule aligns with close timeline\n- [ ] Collect information for any known unusual transactions\n\n### Close Day 1 (T+1: First Business Day After Month-End)\n\n- [ ] Confirm all sub-ledger modules have completed period-end processing\n- [ ] Run AP accruals for goods/services received but not invoiced\n- [ ] Post payroll entries and payroll accrual (if pay period straddles month-end)\n- [ ] Record cash receipts and disbursements through month-end\n- [ ] Post intercompany transactions and confirm with counterparties\n- [ ] Complete bank reconciliation with final bank statement\n- [ ] Run fixed asset depreciation\n- [ ] Post prepaid expense amortization\n\n### Close Day 2 (T+2)\n\n- [ ] Complete revenue recognition entries and deferred revenue adjustments\n- [ ] Post all remaining accrual journal entries\n- [ ] Complete AR subledger reconciliation\n- [ ] Complete AP subledger reconciliation\n- [ ] Record inventory adjustments (if applicable)\n- [ ] Post FX revaluation entries for foreign currency balances\n- [ ] Begin balance sheet account reconciliations\n\n### Close Day 3 (T+3)\n\n- [ ] Complete all balance sheet reconciliations\n- [ ] Post any adjusting journal entries identified during reconciliation\n- [ ] Complete intercompany reconciliation and elimination entries\n- [ ] Run preliminary trial balance and income statement\n- [ ] Perform preliminary flux analysis on income statement\n- [ ] Investigate and resolve material variances\n\n### Close Day 4 (T+4)\n\n- [ ] Post tax provision entries (income tax, sales tax, property tax)\n- [ ] Complete equity roll-forward (stock compensation, treasury stock)\n- [ ] Finalize all journal entries — soft close\n- [ ] Generate draft financial statements (P\u0026L, BS, CF)\n- [ ] Perform detailed flux analysis and prepare variance explanations\n- [ ] Management review of financial statements and key metrics\n\n### Close Day 5 (T+5)\n\n- [ ] Post any final adjustments from management review\n- [ ] Finalize financial statements — hard close\n- [ ] Lock the period in the ERP/GL system\n- [ ] Distribute financial reporting package to stakeholders\n- [ ] Update forecasts/projections based on actual results\n- [ ] Conduct close retrospective — identify process improvements\n\n## Task Sequencing and Dependencies\n\n### Dependency Map\n\nTasks are organized by what must complete before the next task can begin:\n\n```\nLEVEL 1 (No dependencies — can start immediately at T+1):\n├── Cash receipts/disbursements recording\n├── Bank statement retrieval\n├── Payroll processing/accrual\n├── Fixed asset depreciation run\n├── Prepaid amortization\n├── AP accrual preparation\n└── Intercompany transaction posting\n\nLEVEL 2 (Depends on Level 1 completion):\n├── Bank reconciliation (needs: cash entries + bank statement)\n├── Revenue recognition (needs: billing/delivery data finalized)\n├── AR subledger reconciliation (needs: all revenue/cash entries)\n├── AP subledger reconciliation (needs: all AP entries/accruals)\n├── FX revaluation (needs: all foreign currency entries posted)\n└── Remaining accrual JEs (needs: review of all source data)\n\nLEVEL 3 (Depends on Level 2 completion):\n├── All balance sheet reconciliations (needs: all JEs posted)\n├── Intercompany reconciliation (needs: both sides posted)\n├── Adjusting entries from reconciliations\n└── Preliminary trial balance\n\nLEVEL 4 (Depends on Level 3 completion):\n├── Tax provision (needs: pre-tax income finalized)\n├── Equity roll-forward\n├── Consolidation and eliminations\n├── Draft financial statements\n└── Preliminary flux analysis\n\nLEVEL 5 (Depends on Level 4 completion):\n├── Management review\n├── Final adjustments\n├── Hard close / period lock\n├── Financial reporting package\n└── Forecast updates\n```\n\n### Critical Path\n\nThe critical path determines the minimum close duration. Typical critical path:\n\n```\nCash/AP/AR entries → Subledger reconciliations → Balance sheet recs →\n  Tax provision → Draft financials → Management review → Hard close\n```\n\nTo shorten the close:\n- Automate Level 1 entries (depreciation, prepaid amortization, standard accruals)\n- Pre-reconcile accounts during the month (continuous reconciliation)\n- Parallel-process independent reconciliations\n- Set clear deadlines with consequences for late submissions\n- Use standardized templates to reduce reconciliation prep time\n\n## Status Tracking and Reporting\n\n### Close Status Dashboard\n\nTrack each close task with the following attributes:\n\n| Task | Owner | Deadline | Status | Blocker | Notes |\n|------|-------|----------|--------|---------|-------|\n| [Task name] | [Person/role] | [Day T+N] | Not Started / In Progress / Complete / Blocked | [If blocked, what's blocking] | [Any notes] |\n\n### Status Definitions\n\n- **Not Started:** Task has not yet begun (may be waiting on dependencies)\n- **In Progress:** Task is actively being worked on\n- **Complete:** Task is finished and has been reviewed/approved\n- **Blocked:** Task cannot proceed due to a dependency, missing data, or issue\n- **At Risk:** Task is in progress but may not meet its deadline\n\n### Daily Close Status Meeting (Recommended)\n\nDuring the close period, hold a brief (15-minute) daily standup:\n\n1. **Review status board:** Walk through open tasks, flag any that are behind\n2. **Identify blockers:** Surface any issues preventing task completion\n3. **Reassign or escalate:** Adjust ownership or escalate blockers to resolve quickly\n4. **Update timeline:** If any tasks are at risk, assess impact on overall close timeline\n\n### Close Metrics to Track Over Time\n\n| Metric | Definition | Target |\n|--------|-----------|--------|\n| Close duration | Business days from period end to hard close | Reduce over time |\n| # of adjusting entries after soft close | Entries posted during management review | Minimize |\n| # of late tasks | Tasks completed after their deadline | Zero |\n| # of reconciliation exceptions | Reconciling items requiring investigation | Reduce over time |\n| # of restatements / corrections | Errors found after close | Zero |\n\n## Common Close Activities by Day\n\n### Typical 5-Day Close Calendar\n\n| Day | Key Activities | Responsible |\n|-----|---------------|-------------|\n| **T+1** | Cash entries, payroll, AP accruals, depreciation, prepaid amortization, intercompany posting | Staff accountants, payroll |\n| **T+2** | Revenue recognition, remaining accruals, subledger reconciliations (AR, AP, FA), FX revaluation | Revenue accountant, AP/AR, treasury |\n| **T+3** | Balance sheet reconciliations, intercompany reconciliation, eliminations, preliminary trial balance, preliminary flux | Accounting team, consolidation |\n| **T+4** | Tax provision, equity roll-forward, draft financial statements, detailed flux analysis, management review | Tax, controller, FP\u0026A |\n| **T+5** | Final adjustments, hard close, period lock, reporting package distribution, forecast update, retrospective | Controller, FP\u0026A, finance leadership |\n\n### Accelerated Close (3-Day Target)\n\nFor organizations targeting a faster close:\n\n| Day | Key Activities |\n|-----|---------------|\n| **T+1** | All JEs posted (automated + manual), all subledger reconciliations, bank reconciliation, intercompany reconciliation, preliminary trial balance |\n| **T+2** | All balance sheet reconciliations, tax provision, consolidation, draft financial statements, flux analysis, management review |\n| **T+3** | Final adjustments, hard close, reporting package, forecast update |\n\n**Prerequisites for a 3-day close:**\n- Automated recurring journal entries (depreciation, amortization, standard accruals)\n- Continuous reconciliation during the month (not all at month-end)\n- Automated intercompany elimination\n- Pre-close activities completed before month-end (cut-off, accrual estimates)\n- Empowered team with clear ownership and minimal handoffs\n- Real-time or near-real-time sub-system integration\n\n## Close Process Improvement\n\n### Common Bottlenecks and Solutions\n\n| Bottleneck | Root Cause | Solution |\n|-----------|-----------|---------|\n| Late AP accruals | Waiting for department spend confirmation | Implement continuous accrual estimation; set cut-off deadlines |\n| Manual journal entries | Recurring entries prepared manually each month | Automate standard recurring entries in the ERP |\n| Slow reconciliations | Starting from scratch each month | Implement continuous/rolling reconciliation |\n| Intercompany delays | Waiting for counterparty confirmation | Automate intercompany matching; set stricter deadlines |\n| Management review changes | Large adjustments found during review | Improve preliminary review process; empower team to catch issues earlier |\n| Missing supporting documents | Scrambling for documentation at close | Maintain documentation throughout the month |\n\n### Close Retrospective Questions\n\nAfter each close, ask:\n1. What went well this close that we should continue?\n2. What took longer than expected and why?\n3. What blockers did we encounter and how can we prevent them?\n4. Were there any surprises in the financial results we should have caught earlier?\n5. What can we automate or streamline for next month?\n", "depth": 4}, {"id": "363", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/financial-statements/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: financial-statements\ndescription: Generate income statements, balance sheets, and cash flow statements with GAAP presentation and period-over-period comparison. Use when preparing financial statements, running flux analysis, or creating P\u0026L reports with variance commentary.\n---\n\n# Financial Statements\n\n**Important**: This skill assists with financial statement workflows but does not provide financial advice. All statements should be reviewed by qualified financial professionals before use in reporting or filings.\n\nFormats, GAAP presentation requirements, common adjustments, and flux analysis methodology for income statements, balance sheets, and cash flow statements.\n\n## Income Statement\n\n### Standard Format (Classification of Expenses by Function)\n\n```\nRevenue\n  Product revenue\n  Service revenue\n  Other revenue\nTotal Revenue\n\nCost of Revenue\n  Product costs\n  Service costs\nTotal Cost of Revenue\n\nGross Profit\n\nOperating Expenses\n  Research and development\n  Sales and marketing\n  General and administrative\nTotal Operating Expenses\n\nOperating Income (Loss)\n\nOther Income (Expense)\n  Interest income\n  Interest expense\n  Other income (expense), net\nTotal Other Income (Expense)\n\nIncome (Loss) Before Income Taxes\n  Income tax expense (benefit)\nNet Income (Loss)\n\nEarnings Per Share (if applicable)\n  Basic\n  Diluted\n```\n\n### GAAP Presentation Requirements (ASC 220 / IAS 1)\n\n- Present all items of income and expense recognized in a period\n- Classify expenses either by nature (materials, labor, depreciation) or by function (COGS, R\u0026D, S\u0026M, G\u0026A) — function is more common for US companies\n- If classified by function, disclose depreciation, amortization, and employee benefit costs by nature in the notes\n- Present operating and non-operating items separately\n- Show income tax expense as a separate line\n- Extraordinary items are prohibited under both US GAAP and IFRS\n- Discontinued operations presented separately, net of tax\n\n### Common Presentation Considerations\n\n- **Revenue disaggregation:** ASC 606 requires disaggregation of revenue into categories that depict how the nature, amount, timing, and uncertainty of revenue are affected by economic factors\n- **Stock-based compensation:** Classify within the functional expense categories (R\u0026D, S\u0026M, G\u0026A) with total SBC disclosed in notes\n- **Restructuring charges:** Present separately if material, or include in operating expenses with note disclosure\n- **Non-GAAP adjustments:** If presenting non-GAAP measures (common in earnings releases), clearly label and reconcile to GAAP\n\n## Balance Sheet\n\n### Standard Format (Classified Balance Sheet)\n\n```\nASSETS\nCurrent Assets\n  Cash and cash equivalents\n  Short-term investments\n  Accounts receivable, net\n  Inventory\n  Prepaid expenses and other current assets\nTotal Current Assets\n\nNon-Current Assets\n  Property and equipment, net\n  Operating lease right-of-use assets\n  Goodwill\n  Intangible assets, net\n  Long-term investments\n  Other non-current assets\nTotal Non-Current Assets\n\nTOTAL ASSETS\n\nLIABILITIES AND STOCKHOLDERS' EQUITY\nCurrent Liabilities\n  Accounts payable\n  Accrued liabilities\n  Deferred revenue, current portion\n  Current portion of long-term debt\n  Operating lease liabilities, current portion\n  Other current liabilities\nTotal Current Liabilities\n\nNon-Current Liabilities\n  Long-term debt\n  Deferred revenue, non-current\n  Operating lease liabilities, non-current\n  Other non-current liabilities\nTotal Non-Current Liabilities\n\nTotal Liabilities\n\nStockholders' Equity\n  Common stock\n  Additional paid-in capital\n  Retained earnings (accumulated deficit)\n  Accumulated other comprehensive income (loss)\n  Treasury stock\nTotal Stockholders' Equity\n\nTOTAL LIABILITIES AND STOCKHOLDERS' EQUITY\n```\n\n### GAAP Presentation Requirements (ASC 210 / IAS 1)\n\n- Distinguish between current and non-current assets and liabilities\n- Current: expected to be realized, consumed, or settled within 12 months (or the operating cycle if longer)\n- Present assets in order of liquidity (most liquid first) — standard US practice\n- Accounts receivable shown net of allowance for credit losses (ASC 326)\n- Property and equipment shown net of accumulated depreciation\n- Goodwill is not amortized — tested for impairment annually (ASC 350)\n- Leases: recognize right-of-use assets and lease liabilities for operating and finance leases (ASC 842)\n\n## Cash Flow Statement\n\n### Standard Format (Indirect Method)\n\n```\nCASH FLOWS FROM OPERATING ACTIVITIES\nNet income (loss)\nAdjustments to reconcile net income to net cash from operations:\n  Depreciation and amortization\n  Stock-based compensation\n  Amortization of debt issuance costs\n  Deferred income taxes\n  Loss (gain) on disposal of assets\n  Impairment charges\n  Other non-cash items\nChanges in operating assets and liabilities:\n  Accounts receivable\n  Inventory\n  Prepaid expenses and other assets\n  Accounts payable\n  Accrued liabilities\n  Deferred revenue\n  Other liabilities\nNet Cash Provided by (Used in) Operating Activities\n\nCASH FLOWS FROM INVESTING ACTIVITIES\n  Purchases of property and equipment\n  Purchases of investments\n  Proceeds from sale/maturity of investments\n  Acquisitions, net of cash acquired\n  Other investing activities\nNet Cash Provided by (Used in) Investing Activities\n\nCASH FLOWS FROM FINANCING ACTIVITIES\n  Proceeds from issuance of debt\n  Repayment of debt\n  Proceeds from issuance of common stock\n  Repurchases of common stock\n  Dividends paid\n  Payment of debt issuance costs\n  Other financing activities\nNet Cash Provided by (Used in) Financing Activities\n\nEffect of exchange rate changes on cash\n\nNet Increase (Decrease) in Cash and Cash Equivalents\nCash and cash equivalents, beginning of period\nCash and cash equivalents, end of period\n```\n\n### GAAP Presentation Requirements (ASC 230 / IAS 7)\n\n- Indirect method is most common (start with net income, adjust for non-cash items)\n- Direct method is permitted but rarely used (requires supplemental indirect reconciliation)\n- Interest paid and income taxes paid must be disclosed (either on the face or in notes)\n- Non-cash investing and financing activities disclosed separately (e.g., assets acquired under leases, stock issued for acquisitions)\n- Cash equivalents: short-term, highly liquid investments with original maturities of 3 months or less\n\n## Common Adjustments and Reclassifications\n\n### Period-End Adjustments\n\n1. **Accruals:** Record expenses incurred but not yet paid (AP accruals, payroll accruals, interest accruals)\n2. **Deferrals:** Adjust prepaid expenses, deferred revenue, and deferred costs for the period\n3. **Depreciation and amortization:** Book periodic depreciation/amortization from fixed asset and intangible schedules\n4. **Bad debt provision:** Adjust allowance for credit losses based on aging analysis and historical loss rates\n5. **Inventory adjustments:** Record write-downs for obsolete, slow-moving, or impaired inventory\n6. **FX revaluation:** Revalue foreign-currency-denominated monetary assets and liabilities at period-end rates\n7. **Tax provision:** Record current and deferred income tax expense\n8. **Fair value adjustments:** Mark-to-market investments, derivatives, and other fair-value items\n\n### Reclassifications\n\n1. **Current/non-current reclassification:** Reclassify long-term debt maturing within 12 months to current\n2. **Contra account netting:** Net allowances against gross receivables, accumulated depreciation against gross assets\n3. **Intercompany elimination:** Eliminate intercompany balances and transactions in consolidation\n4. **Discontinued operations:** Reclassify results of discontinued operations to a separate line item\n5. **Equity method adjustments:** Record share of investee income/loss for equity method investments\n6. **Segment reclassifications:** Ensure transactions are properly classified by operating segment\n\n## Flux Analysis Methodology\n\n### Variance Calculation\n\nFor each line item, calculate:\n- **Dollar variance:** Current period - Prior period (or current period - budget)\n- **Percentage variance:** (Current - Prior) / |Prior| x 100\n- **Basis point change:** For margins and ratios, express change in basis points (1 bp = 0.01%)\n\n### Materiality Thresholds\n\nDefine what constitutes a \"material\" variance requiring investigation. Common approaches:\n\n- **Fixed dollar threshold:** Variances exceeding a set dollar amount (e.g., $50K, $100K)\n- **Percentage threshold:** Variances exceeding a set percentage (e.g., 10%, 15%)\n- **Combined:** Either the dollar OR percentage threshold is exceeded\n- **Scaled:** Different thresholds for different line items based on their size and volatility\n\n*Example thresholds (adjust for your organization):*\n\n| Line Item Size | Dollar Threshold | Percentage Threshold |\n|---------------|-----------------|---------------------|\n| \u003e $10M        | $500K           | 5%                  |\n| $1M - $10M    | $100K           | 10%                 |\n| \u003c $1M         | $50K            | 15%                 |\n\n### Variance Decomposition\n\nBreak down total variance into component drivers:\n\n- **Volume/quantity effect:** Change in volume at prior period rates\n- **Rate/price effect:** Change in rate/price at current period volume\n- **Mix effect:** Shift in composition between items with different rates/margins\n- **New/discontinued items:** Items present in one period but not the other\n- **One-time/non-recurring items:** Items that are not expected to repeat\n- **Timing effect:** Items shifting between periods (not a true change in run rate)\n- **Currency effect:** Impact of FX rate changes on translated results\n\n### Investigation and Narrative\n\nFor each material variance:\n1. Quantify the variance ($ and %)\n2. Identify whether favorable or unfavorable\n3. Decompose into drivers using the categories above\n4. Provide a narrative explanation of the business reason\n5. Assess whether the variance is temporary or represents a trend change\n6. Note any actions required (further investigation, forecast update, process change)\n", "depth": 4}, {"id": "364", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/audit-support/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: audit-support\ndescription: Support SOX 404 compliance with control testing methodology, sample selection, and documentation standards. Use when generating testing workpapers, selecting audit samples, classifying control deficiencies, or preparing for internal or external audits.\n---\n\n# Audit Support\n\n**Important**: This skill assists with SOX compliance workflows but does not provide audit or legal advice. All testing workpapers and assessments should be reviewed by qualified financial professionals. While \"significance\" and \"materiality\" are context-specific concepts that are ultimately assessed by auditors, this skill is intended to assist professionals in the creation and evaluation of effective internal controls and documentation for audits.\n\nSOX 404 control testing methodology, sample selection approaches, testing documentation standards, control deficiency classification, and common control types.\n\n## SOX 404 Control Testing Methodology\n\n### Overview\n\nSOX Section 404 requires management to assess the effectiveness of internal controls over financial reporting (ICFR). This involves:\n\n1. **Scoping:** Identify significant accounts and relevant assertions\n2. **Risk assessment:** Evaluate the risk of material misstatement for each significant account\n3. **Control identification:** Document the controls that address each risk\n4. **Testing:** Test the design and operating effectiveness of key controls\n5. **Evaluation:** Assess whether any deficiencies exist and their severity\n6. **Reporting:** Document the assessment and any material weaknesses\n\n### Scoping Significant Accounts\n\nAn account is significant if there is more than a remote likelihood that it could contain a misstatement that is material (individually or in aggregate).\n\n**Quantitative factors:**\n- Account balance exceeds materiality threshold (typically 3-5% of a key benchmark)\n- Transaction volume is high, increasing the risk of error\n- Account is subject to significant estimates or judgment\n\n**Qualitative factors:**\n- Account involves complex accounting (revenue recognition, derivatives, pensions)\n- Account is susceptible to fraud (cash, revenue, related-party transactions)\n- Account has had prior misstatements or audit adjustments\n- Account involves significant management judgment or estimates\n- New account or significantly changed process\n\n### Relevant Assertions by Account Type\n\n| Account Type | Key Assertions |\n|-------------|---------------|\n| Revenue | Occurrence, Completeness, Accuracy, Cut-off |\n| Accounts Receivable | Existence, Valuation (allowance), Rights |\n| Inventory | Existence, Valuation, Completeness |\n| Fixed Assets | Existence, Valuation, Completeness, Rights |\n| Accounts Payable | Completeness, Accuracy, Existence |\n| Accrued Liabilities | Completeness, Valuation, Accuracy |\n| Equity | Completeness, Accuracy, Presentation |\n| Financial Close/Reporting | Presentation, Accuracy, Completeness |\n\n### Design Effectiveness vs Operating Effectiveness\n\n**Design effectiveness:** Is the control properly designed to prevent or detect a material misstatement in the relevant assertion?\n- Evaluated through walkthroughs (trace a transaction end-to-end through the process)\n- Confirm the control is placed at the right point in the process\n- Confirm the control addresses the identified risk\n- Performed at least annually, or when processes change\n\n**Operating effectiveness:** Did the control actually operate as designed throughout the testing period?\n- Evaluated through testing (inspection, observation, re-performance, inquiry)\n- Requires sufficient sample sizes to support a conclusion\n- Must cover the full period of reliance\n\n## Sample Selection Approaches\n\n### Random Selection\n\n**When to use:** Default method for transaction-level controls with large populations.\n\n**Method:**\n1. Define the population (all transactions subject to the control during the period)\n2. Number each item in the population sequentially\n3. Use a random number generator to select sample items\n4. Ensure no bias in selection (all items have equal probability)\n\n**Advantages:** Statistically valid, defensible, no selection bias\n**Disadvantages:** May miss high-risk items, requires complete population listing\n\n### Targeted (Judgmental) Selection\n\n**When to use:** Supplement to random selection for risk-based testing; primary method when population is small or highly varied.\n\n**Method:**\n1. Identify items with specific risk characteristics:\n   - High dollar amount (above a defined threshold)\n   - Unusual or non-standard transactions\n   - Period-end transactions (cut-off risk)\n   - Related-party transactions\n   - Manual or override transactions\n   - New vendor/customer transactions\n2. Select items matching risk criteria\n3. Document rationale for each targeted selection\n\n**Advantages:** Focuses on highest-risk items, efficient use of testing effort\n**Disadvantages:** Not statistically representative, may over-represent certain risks\n\n### Haphazard Selection\n\n**When to use:** When random selection is impractical (no sequential population listing) and population is relatively homogeneous.\n\n**Method:**\n1. Select items without any specific pattern or bias\n2. Ensure selections are spread across the full population period\n3. Avoid unconscious bias (don't always pick items at the top, round numbers, etc.)\n\n**Advantages:** Simple, no technology required\n**Disadvantages:** Not statistically valid, susceptible to unconscious bias\n\n### Systematic Selection\n\n**When to use:** When population is sequential and you want even coverage across the period.\n\n**Method:**\n1. Calculate the sampling interval: Population size / Sample size\n2. Select a random starting point within the first interval\n3. Select every Nth item from the starting point\n\n**Example:** Population of 1,000, sample of 25 → interval of 40. Random start: item 17. Select items 17, 57, 97, 137, ...\n\n**Advantages:** Even coverage across population, simple to execute\n**Disadvantages:** Periodic patterns in the population could bias results\n\n### Sample Size Guidance\n\n| Control Frequency | Expected Population | Low Risk Sample | Moderate Risk Sample | High Risk Sample |\n|------------------|--------------------|-----------------|--------------------|-----------------|\n| Annual | 1 | 1 | 1 | 1 |\n| Quarterly | 4 | 2 | 2 | 3 |\n| Monthly | 12 | 2 | 3 | 4 |\n| Weekly | 52 | 5 | 8 | 15 |\n| Daily | ~250 | 20 | 30 | 40 |\n| Per-transaction (small pop.) | \u003c 250 | 20 | 30 | 40 |\n| Per-transaction (large pop.) | 250+ | 25 | 40 | 60 |\n\n**Factors increasing sample size:**\n- Higher inherent risk in the account/process\n- Control is the sole control addressing a significant risk (no redundancy)\n- Prior period control deficiency identified\n- New control (not tested in prior periods)\n- External auditor reliance on management testing\n\n## Testing Documentation Standards\n\n### Workpaper Requirements\n\nEvery control test should be documented with:\n\n1. **Control identification:**\n   - Control number/ID\n   - Control description (what is done, by whom, how often)\n   - Control type (manual, automated, IT-dependent manual)\n   - Control frequency\n   - Risk and assertion addressed\n\n2. **Test design:**\n   - Test objective (what you are trying to determine)\n   - Test procedures (step-by-step instructions)\n   - Expected evidence (what you expect to see if the control is effective)\n   - Sample selection methodology and rationale\n\n3. **Test execution:**\n   - Population description and size\n   - Sample selection details (method, items selected)\n   - Results for each sample item (pass/fail with specific evidence examined)\n   - Exceptions noted with full description\n\n4. **Conclusion:**\n   - Overall assessment (effective / deficiency / significant deficiency / material weakness)\n   - Basis for conclusion\n   - Impact assessment for any exceptions\n   - Compensating controls considered (if applicable)\n\n5. **Sign-off:**\n   - Tester name and date\n   - Reviewer name and date\n\n### Evidence Standards\n\n**Sufficient evidence includes:**\n- Screenshots showing system-enforced controls\n- Signed/initialed approval documents\n- Email approvals with identifiable approver and date\n- System audit logs showing who performed the action and when\n- Re-performed calculations with matching results\n- Observation notes (with date, location, observer)\n\n**Insufficient evidence:**\n- Verbal confirmations alone (must be corroborated)\n- Undated documents\n- Evidence without identifiable performer/approver\n- Generic system reports without date/time stamps\n- \"Per discussion with [name]\" without corroborating documentation\n\n### Working Paper Organization\n\nOrganize testing files by control area:\n\n```\nSOX Testing/\n├── [Year]/\n│   ├── Scoping and Risk Assessment/\n│   ├── Revenue Cycle/\n│   │   ├── Control Matrix\n│   │   ├── Walkthrough Documentation\n│   │   ├── Test Workpapers (one per control)\n│   │   └── Supporting Evidence\n│   ├── Procure to Pay/\n│   ├── Payroll/\n│   ├── Financial Close/\n│   ├── Treasury/\n│   ├── Fixed Assets/\n│   ├── IT General Controls/\n│   ├── Entity Level Controls/\n│   └── Summary and Conclusions/\n│       ├── Deficiency Evaluation\n│       └── Management Assessment\n```\n\n## Control Deficiency Classification\n\n### Deficiency\n\nA deficiency in internal control exists when the design or operation of a control does not allow management or employees, in the normal course of performing their assigned functions, to prevent or detect misstatements on a timely basis.\n\n**Evaluation factors:**\n- What is the likelihood that the control failure could result in a misstatement?\n- What is the magnitude of the potential misstatement?\n- Is there a compensating control that mitigates the deficiency?\n\n### Significant Deficiency\n\nA deficiency, or combination of deficiencies, that is less severe than a material weakness yet important enough to merit attention by those charged with governance.\n\n**Indicators:**\n- The deficiency could result in a misstatement that is more than inconsequential but less than material\n- There is more than a remote (but less than reasonably possible) likelihood of a material misstatement\n- The control is a key control and the deficiency is not fully mitigated by compensating controls\n- Combination of individually minor deficiencies that together represent a significant concern\n\n### Material Weakness\n\nA deficiency, or combination of deficiencies, such that there is a reasonable possibility that a material misstatement of the financial statements will not be prevented or detected on a timely basis.\n\n**Indicators:**\n- Identification of fraud by senior management (any magnitude)\n- Restatement of previously issued financial statements to correct a material error\n- Identification by the auditor of a material misstatement that would not have been detected by the company's controls\n- Ineffective oversight of financial reporting by the audit committee\n- Deficiency in a pervasive control (entity-level, IT general control) affecting multiple processes\n\n### Deficiency Aggregation\n\nIndividual deficiencies that are not significant individually may be significant in combination:\n\n1. Identify all deficiencies in the same process or affecting the same assertion\n2. Evaluate whether the combined effect could result in a material misstatement\n3. Consider whether deficiencies in compensating controls exacerbate other deficiencies\n4. Document the aggregation analysis and conclusion\n\n### Remediation\n\nFor each identified deficiency:\n\n1. **Root cause analysis:** Why did the control fail? (design gap, execution failure, staffing, training, system issue)\n2. **Remediation plan:** Specific actions to fix the control (redesign, additional training, system enhancement, added review)\n3. **Timeline:** Target date for remediation completion\n4. **Owner:** Person responsible for implementing the remediation\n5. **Validation:** How and when the remediated control will be re-tested to confirm effectiveness\n\n## Common Control Types\n\n### IT General Controls (ITGCs)\n\nControls over the IT environment that support the reliable functioning of application controls and automated processes.\n\n**Access Controls:**\n- User access provisioning (new access requests require approval)\n- User access de-provisioning (terminated users removed timely)\n- Privileged access management (admin/superuser access restricted and monitored)\n- Periodic access reviews (user access recertified on a defined schedule)\n- Password policies (complexity, rotation, lockout)\n- Segregation of duties enforcement (conflicting access prevented)\n\n**Change Management:**\n- Change requests documented and approved before implementation\n- Changes tested in a non-production environment before promotion\n- Separation of development and production environments\n- Emergency change procedures (documented, approved post-implementation)\n- Change review and post-implementation validation\n\n**IT Operations:**\n- Batch job monitoring and exception handling\n- Backup and recovery procedures (regular backups, tested restores)\n- System availability and performance monitoring\n- Incident management and escalation procedures\n- Disaster recovery planning and testing\n\n### Manual Controls\n\nControls performed by people using judgment, typically involving review and approval.\n\n**Examples:**\n- Management review of financial statements and key metrics\n- Supervisory approval of journal entries above a threshold\n- Three-way match verification (PO, receipt, invoice)\n- Account reconciliation preparation and review\n- Physical inventory observation and count\n- Vendor master data change approval\n- Customer credit approval\n\n**Key attributes to test:**\n- Was the control performed by the right person (proper authority)?\n- Was it performed timely (within the required timeframe)?\n- Is there evidence of the review (signature, initials, email, system log)?\n- Did the reviewer have sufficient information to perform an effective review?\n- Were exceptions identified and appropriately addressed?\n\n### Automated Controls\n\nControls enforced by IT systems without human intervention.\n\n**Examples:**\n- System-enforced approval workflows (cannot proceed without required approvals)\n- Three-way match automation (system blocks payment if PO/receipt/invoice don't match)\n- Duplicate payment detection (system flags or blocks duplicate invoices)\n- Credit limit enforcement (system prevents orders exceeding credit limit)\n- Automated calculations (depreciation, amortization, interest, tax)\n- System-enforced segregation of duties (conflicting roles prevented)\n- Input validation controls (required fields, format checks, range checks)\n- Automated reconciliation matching\n\n**Testing approach:**\n- Test design: Confirm the system configuration enforces the control as intended\n- Test operating effectiveness: For automated controls, if the system configuration has not changed, one test of the control is typically sufficient for the period (supplemented by ITGC testing of change management)\n- Verify change management ITGCs are effective (if system changed, re-test the control)\n\n### IT-Dependent Manual Controls\n\nManual controls that rely on the completeness and accuracy of system-generated information.\n\n**Examples:**\n- Management review of a system-generated exception report\n- Supervisor review of a system-generated aging report to assess reserves\n- Reconciliation using system-generated trial balance data\n- Approval of transactions identified by a system-generated workflow\n\n**Testing approach:**\n- Test the manual control (review, approval, follow-up on exceptions)\n- AND test the completeness and accuracy of the underlying report/data (IPE — Information Produced by the Entity)\n- IPE testing confirms the data the reviewer relied on was complete and accurate\n\n### Entity-Level Controls\n\nBroad controls that operate at the organizational level and affect multiple processes.\n\n**Examples:**\n- Tone at the top / code of conduct\n- Risk assessment process\n- Audit committee oversight of financial reporting\n- Internal audit function and activities\n- Fraud risk assessment and anti-fraud programs\n- Whistleblower/ethics hotline\n- Management monitoring of control effectiveness\n- Financial reporting competence (staffing, training, qualifications)\n- Period-end financial reporting process (close procedures, GAAP compliance reviews)\n\n**Significance:**\n- Entity-level controls can mitigate but typically cannot replace process-level controls\n- Ineffective entity-level controls (especially audit committee oversight and tone at the top) are strong indicators of a material weakness\n- Effective entity-level controls may reduce the extent of testing needed for process-level controls\n", "depth": 4}, {"id": "365", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/finance/skills/variance-analysis/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: variance-analysis\ndescription: Decompose financial variances into drivers with narrative explanations and waterfall analysis. Use when analyzing budget vs. actual, period-over-period changes, revenue or expense variances, or preparing variance commentary for leadership.\n---\n\n# Variance Analysis\n\n**Important**: This skill assists with variance analysis workflows but does not provide financial advice. All analyses should be reviewed by qualified financial professionals before use in reporting.\n\nTechniques for decomposing variances, materiality thresholds, narrative generation, waterfall chart methodology, and budget vs actual vs forecast comparisons.\n\n## Variance Decomposition Techniques\n\n### Price / Volume Decomposition\n\nThe most fundamental variance decomposition. Used for revenue, cost of goods, and any metric that can be expressed as Price x Volume.\n\n**Formula:**\n```\nTotal Variance = Actual - Budget (or Prior)\n\nVolume Effect  = (Actual Volume - Budget Volume) x Budget Price\nPrice Effect   = (Actual Price - Budget Price) x Actual Volume\nMix Effect     = Residual (interaction term), or allocated proportionally\n\nVerification:  Volume Effect + Price Effect = Total Variance\n               (when mix is embedded in the price/volume terms)\n```\n\n**Three-way decomposition (separating mix):**\n```\nVolume Effect = (Actual Volume - Budget Volume) x Budget Price x Budget Mix\nPrice Effect  = (Actual Price - Budget Price) x Budget Volume x Actual Mix\nMix Effect    = Budget Price x Budget Volume x (Actual Mix - Budget Mix)\n```\n\n**Example — Revenue variance:**\n- Budget: 10,000 units at $50 = $500,000\n- Actual: 11,000 units at $48 = $528,000\n- Total variance: +$28,000 favorable\n  - Volume effect: +1,000 units x $50 = +$50,000 (favorable — sold more units)\n  - Price effect: -$2 x 11,000 units = -$22,000 (unfavorable — lower ASP)\n  - Net: +$28,000\n\n### Rate / Mix Decomposition\n\nUsed when analyzing blended rates across segments with different unit economics.\n\n**Formula:**\n```\nRate Effect = Sum of (Actual Volume_i x (Actual Rate_i - Budget Rate_i))\nMix Effect  = Sum of (Budget Rate_i x (Actual Volume_i - Expected Volume_i at Budget Mix))\n```\n\n**Example — Gross margin variance:**\n- Product A: 60% margin, Product B: 40% margin\n- Budget mix: 50% A, 50% B → Blended margin 50%\n- Actual mix: 40% A, 60% B → Blended margin 48%\n- Mix effect explains 2pp of margin compression\n\n### Headcount / Compensation Decomposition\n\nUsed for analyzing payroll and people-cost variances.\n\n```\nTotal Comp Variance = Actual Compensation - Budget Compensation\n\nDecompose into:\n1. Headcount variance    = (Actual HC - Budget HC) x Budget Avg Comp\n2. Rate variance         = (Actual Avg Comp - Budget Avg Comp) x Budget HC\n3. Mix variance          = Difference due to level/department mix shift\n4. Timing variance       = Hiring earlier/later than planned (partial-period effect)\n5. Attrition impact      = Savings from unplanned departures (partially offset by backfill costs)\n```\n\n### Spend Category Decomposition\n\nUsed for operating expense analysis when price/volume is not applicable.\n\n```\nTotal OpEx Variance = Actual OpEx - Budget OpEx\n\nDecompose by:\n1. Headcount-driven costs    (salaries, benefits, payroll taxes, recruiting)\n2. Volume-driven costs       (hosting, transaction fees, commissions, shipping)\n3. Discretionary spend       (travel, events, professional services, marketing programs)\n4. Contractual/fixed costs   (rent, insurance, software licenses, subscriptions)\n5. One-time / non-recurring  (severance, legal settlements, write-offs, project costs)\n6. Timing / phasing          (spend shifted between periods vs plan)\n```\n\n## Materiality Thresholds and Investigation Triggers\n\n### Setting Thresholds\n\nMateriality thresholds determine which variances require investigation and narrative explanation. Set thresholds based on:\n\n1. **Financial statement materiality:** Typically 1-5% of a key benchmark (revenue, total assets, net income)\n2. **Line item size:** Larger line items warrant lower percentage thresholds\n3. **Volatility:** More volatile line items may need higher thresholds to avoid noise\n4. **Management attention:** What level of variance would change a decision?\n\n### Recommended Threshold Framework\n\n| Comparison Type | Dollar Threshold | Percentage Threshold | Trigger |\n|----------------|-----------------|---------------------|---------|\n| Actual vs Budget | Organization-specific | 10% | Either exceeded |\n| Actual vs Prior Period | Organization-specific | 15% | Either exceeded |\n| Actual vs Forecast | Organization-specific | 5% | Either exceeded |\n| Sequential (MoM) | Organization-specific | 20% | Either exceeded |\n\n*Set dollar thresholds based on your organization's size. Common practice: 0.5%-1% of revenue for income statement items.*\n\n### Investigation Priority\n\nWhen multiple variances exceed thresholds, prioritize investigation by:\n\n1. **Largest absolute dollar variance** — biggest P\u0026L impact\n2. **Largest percentage variance** — may indicate process issue or error\n3. **Unexpected direction** — variance opposite to trend or expectation\n4. **New variance** — item that was on track and is now off\n5. **Cumulative/trending variance** — growing each period\n\n## Narrative Generation for Variance Explanations\n\n### Structure for Each Variance Narrative\n\n```\n[Line Item]: [Favorable/Unfavorable] variance of $[amount] ([percentage]%)\nvs [comparison basis] for [period]\n\nDriver: [Primary driver description]\n[2-3 sentences explaining the business reason for the variance, with specific\nquantification of contributing factors]\n\nOutlook: [One-time / Expected to continue / Improving / Deteriorating]\nAction: [None required / Monitor / Investigate further / Update forecast]\n```\n\n### Narrative Quality Checklist\n\nGood variance narratives should be:\n\n- [ ] **Specific:** Names the actual driver, not just \"higher than expected\"\n- [ ] **Quantified:** Includes dollar and percentage impact of each driver\n- [ ] **Causal:** Explains WHY it happened, not just WHAT happened\n- [ ] **Forward-looking:** States whether the variance is expected to continue\n- [ ] **Actionable:** Identifies any required follow-up or decision\n- [ ] **Concise:** 2-4 sentences, not a paragraph of filler\n\n### Common Narrative Anti-Patterns to Avoid\n\n- \"Revenue was higher than budget due to higher revenue\" (circular — no actual explanation)\n- \"Expenses were elevated this period\" (vague — which expenses? why?)\n- \"Timing\" without specifying what was early/late and when it will normalize\n- \"One-time\" without explaining what the item was\n- \"Various small items\" for a material variance (must decompose further)\n- Focusing only on the largest driver and ignoring offsetting items\n\n## Waterfall Chart Methodology\n\n### Concept\n\nA waterfall (or bridge) chart shows how you get from one value to another through a series of positive and negative contributors. Used to visualize variance decomposition.\n\n### Data Structure\n\n```\nStarting value:  [Base/Budget/Prior period amount]\nDrivers:         [List of contributing factors with signed amounts]\nEnding value:    [Actual/Current period amount]\n\nVerification:    Starting value + Sum of all drivers = Ending value\n```\n\n### Text-Based Waterfall Format\n\nWhen a charting tool is not available, present as a text waterfall:\n\n```\nWATERFALL: Revenue — Q4 Actual vs Q4 Budget\n\nQ4 Budget Revenue                                    $10,000K\n  |\n  |--[+] Volume growth (new customers)               +$800K\n  |--[+] Expansion revenue (existing customers)      +$400K\n  |--[-] Price reductions / discounting               -$200K\n  |--[-] Churn / contraction                          -$350K\n  |--[+] FX tailwind                                  +$50K\n  |--[-] Timing (deals slipped to Q1)                 -$150K\n  |\nQ4 Actual Revenue                                    $10,550K\n\nNet Variance: +$550K (+5.5% favorable)\n```\n\n### Bridge Reconciliation Table\n\nComplement the waterfall with a reconciliation table:\n\n| Driver | Amount | % of Variance | Cumulative |\n|--------|--------|---------------|------------|\n| Volume growth | +$800K | 145% | +$800K |\n| Expansion revenue | +$400K | 73% | +$1,200K |\n| Price reductions | -$200K | -36% | +$1,000K |\n| Churn / contraction | -$350K | -64% | +$650K |\n| FX tailwind | +$50K | 9% | +$700K |\n| Timing (deal slippage) | -$150K | -27% | +$550K |\n| **Total variance** | **+$550K** | **100%** | |\n\n*Note: Percentages can exceed 100% for individual drivers when there are offsetting items.*\n\n### Waterfall Best Practices\n\n1. Order drivers from largest positive to largest negative (or in logical business sequence)\n2. Keep to 5-8 drivers maximum — aggregate smaller items into \"Other\"\n3. Verify the waterfall reconciles (start + drivers = end)\n4. Color-code: green for favorable, red for unfavorable (in visual charts)\n5. Label each bar with both the amount and a brief description\n6. Include a \"Total Variance\" summary bar\n\n## Budget vs Actual vs Forecast Comparisons\n\n### Three-Way Comparison Framework\n\n| Metric | Budget | Forecast | Actual | Bud Var ($) | Bud Var (%) | Fcast Var ($) | Fcast Var (%) |\n|--------|--------|----------|--------|-------------|-------------|---------------|---------------|\n| Revenue | $X | $X | $X | $X | X% | $X | X% |\n| COGS | $X | $X | $X | $X | X% | $X | X% |\n| Gross Profit | $X | $X | $X | $X | X% | $X | X% |\n\n### When to Use Each Comparison\n\n- **Actual vs Budget:** Annual performance measurement, compensation decisions, board reporting. Budget is set at the beginning of the year and typically not changed.\n- **Actual vs Forecast:** Operational management, identifying emerging issues. Forecast is updated periodically (monthly or quarterly) to reflect current expectations.\n- **Forecast vs Budget:** Understanding how expectations have changed since planning. Useful for identifying planning accuracy issues.\n- **Actual vs Prior Period:** Trend analysis, sequential performance. Useful when budget is not meaningful (new business lines, post-acquisition).\n- **Actual vs Prior Year:** Year-over-year growth analysis, seasonality-adjusted comparison.\n\n### Forecast Accuracy Analysis\n\nTrack how accurate forecasts are over time to improve planning:\n\n```\nForecast Accuracy = 1 - |Actual - Forecast| / |Actual|\n\nMAPE (Mean Absolute Percentage Error) = Average of |Actual - Forecast| / |Actual| across periods\n```\n\n| Period | Forecast | Actual | Variance | Accuracy |\n|--------|----------|--------|----------|----------|\n| Jan    | $X       | $X     | $X (X%)  | XX%      |\n| Feb    | $X       | $X     | $X (X%)  | XX%      |\n| ...    | ...      | ...    | ...      | ...      |\n| **Avg**|          |        | **MAPE** | **XX%**  |\n\n### Variance Trending\n\nTrack how variances evolve over the year to identify systematic bias:\n\n- **Consistently favorable:** Budget may be too conservative (sandbagging)\n- **Consistently unfavorable:** Budget may be too aggressive or execution issues\n- **Growing unfavorable:** Deteriorating performance or unrealistic targets\n- **Shrinking variance:** Forecast accuracy improving through the year (normal pattern)\n- **Volatile:** Unpredictable business or poor forecasting methodology\n", "depth": 4}, {"id": "366", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "367", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "368", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/README.md", "name": "README.md", "type": "file", "content": "# Legal Productivity Plugin\n\nAn AI-powered productivity plugin for in-house legal teams, primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Automates contract review, NDA triage, compliance workflows, legal briefings, and templated responses -- all configurable to your organization's specific playbook and risk tolerances.\n\n\u003e **Disclaimer:** This plugin assists with legal workflows but does not provide legal advice. Always verify conclusions with qualified legal professionals. AI-generated analysis should be reviewed by licensed attorneys before being relied upon for legal decisions.\n\n## Target Personas\n\n- **Commercial Counsel** -- Contract negotiation, vendor management, deal support\n- **Product Counsel** -- Product reviews, terms of service, privacy policies, IP matters\n- **Privacy / Compliance** -- Data protection regulations, DPA reviews, data subject requests, regulatory monitoring\n- **Litigation Support** -- Discovery holds, document review prep, case briefings\n\n## Installation\n\n```\nclaude plugins add knowledge-work-plugins/legal\n```\n\n## Quick Start\n\n### 1. Install the plugin\n\n```\nclaude plugins add knowledge-work-plugins/legal\n```\n\n### 2. Configure your playbook\n\nCreate a local settings file to define your organization's standard positions. This is where you encode your team's negotiation playbook, risk tolerances, and standard terms.\n\nIn your project's `.claude/` directory, create a `legal.local.md` file:\n\n```markdown\n# Legal Playbook Configuration\n\n## Contract Review Positions\n\n### Limitation of Liability\n- Standard position: Mutual cap at 12 months of fees paid/payable\n- Acceptable range: 6-24 months of fees\n- Escalation trigger: Uncapped liability, consequential damages inclusion\n\n### Indemnification\n- Standard position: Mutual indemnification for IP infringement and data breach\n- Acceptable: Indemnification limited to third-party claims only\n- Escalation trigger: Unilateral indemnification obligations, uncapped indemnification\n\n### IP Ownership\n- Standard position: Each party retains pre-existing IP; customer owns customer data\n- Escalation trigger: Broad IP assignment clauses, work-for-hire provisions for pre-existing IP\n\n### Data Protection\n- Standard position: Require DPA for any personal data processing\n- Requirements: Sub-processor notification, data deletion on termination, breach notification within 72 hours\n- Escalation trigger: No DPA offered, cross-border transfer without safeguards\n\n### Term and Termination\n- Standard position: Annual term with 30-day termination for convenience\n- Acceptable: Multi-year with termination for convenience after initial term\n- Escalation trigger: Auto-renewal without notice period, no termination for convenience\n\n### Governing Law\n- Preferred: [Your jurisdiction]\n- Acceptable: Major commercial jurisdictions (NY, DE, CA, England \u0026 Wales)\n- Escalation trigger: Non-standard jurisdictions, mandatory arbitration in unfavorable venue\n\n## NDA Defaults\n- Mutual obligations required\n- Term: 2-3 years standard, 5 years for trade secrets\n- Standard carveouts: independently developed, publicly available, rightfully received from third party\n- Residuals clause: acceptable if narrowly scoped\n\n## Response Templates\nConfigure paths to your template files or define inline templates for common inquiries.\n```\n\n### 3. Connect your tools\n\nThe plugin works best when connected to your existing tools via MCP. Pre-configured servers include Slack, Box, Egnyte, Atlassian, and Microsoft 365. See [CONNECTORS.md](CONNECTORS.md) for the full list of supported categories and options.\n\n## Commands\n\n### `/review-contract` -- Contract Review Against Playbook\n\nReview a contract against your organization's negotiation playbook. Flags deviations, generates redlines, and provides business impact analysis.\n\n```\n/review-contract\n```\n\nAccepts: file upload, URL, or pasted contract text. Will ask for context (your side, deadline, focus areas) and review clause-by-clause against your configured playbook.\n\n### `/triage-nda` -- NDA Pre-Screening\n\nRapid triage of incoming NDAs against standard criteria. Categorizes as GREEN (standard approval), YELLOW (counsel review), or RED (significant issues).\n\n```\n/triage-nda\n```\n\n### `/vendor-check` -- Vendor Agreement Status\n\nCheck the status of existing agreements with a vendor across your connected systems.\n\n```\n/vendor-check [vendor name]\n```\n\nReports on existing NDAs, MSAs, DPAs, expiration dates, and key terms.\n\n### `/brief` -- Legal Team Briefing\n\nGenerate contextual briefings for your legal work.\n\n```\n/brief daily          # Morning brief of legal-relevant items\n/brief topic [query]  # Research brief on a specific legal question\n/brief incident       # Rapid brief on a developing situation\n```\n\n### `/respond` -- Generate Templated Response\n\nGenerate a response from your configured templates for common inquiry types.\n\n```\n/respond [inquiry-type]\n```\n\nSupported inquiry types include: data subject request, discovery hold, vendor question, NDA request, and custom categories you define.\n\n## Skills\n\n| Skill | Description |\n|-------|-------------|\n| `contract-review` | Playbook-based contract analysis, deviation classification, redline generation |\n| `nda-triage` | NDA screening criteria, classification rules, routing recommendations |\n| `compliance` | Privacy regulations (GDPR, CCPA), DPA review, data subject requests |\n| `canned-responses` | Template management, response categories, escalation triggers |\n| `legal-risk-assessment` | Risk severity framework, classification levels, escalation criteria |\n| `meeting-briefing` | Meeting prep methodology, context gathering, action item tracking |\n\n## Example Workflows\n\n### Contract Review\n\n1. Receive a vendor contract via email\n2. Run `/review-contract` and upload the document\n3. Provide context: \"We are the customer, need to close by end of quarter, focus on data protection and liability\"\n4. Receive clause-by-clause analysis with GREEN/YELLOW/RED flags\n5. Get specific redline language for YELLOW and RED items\n6. Share the analysis with your deal team\n\n### NDA Triage\n\n1. Sales team sends an NDA from a new prospect\n2. Run `/triage-nda` and paste or upload the NDA\n3. Get instant classification: GREEN (route for signature), YELLOW (specific issues to review), or RED (needs full counsel review)\n4. For GREEN NDAs, approve directly; for YELLOW/RED, address flagged issues\n\n### Daily Brief\n\n1. Start your morning with `/brief daily`\n2. Get a summary of overnight contract requests, compliance questions, upcoming deadlines, and calendar items needing legal prep\n3. Prioritize your day based on urgency and deadlines\n\n### Vendor Check\n\n1. Business team asks about a new engagement with an existing vendor\n2. Run `/vendor-check Acme Corp`\n3. See existing agreements, expiration dates, and key terms at a glance\n4. Know immediately whether you need a new NDA or can proceed under existing terms\n\n## MCP Integration\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nThe plugin connects to your tools through MCP (Model Context Protocol) servers:\n\n| Category | Examples | Purpose |\n|----------|----------|---------|\n| Chat | Slack, Teams | Team requests, notifications, triage |\n| Cloud storage | Box, Egnyte | Playbooks, templates, precedents |\n| Office suite | Microsoft 365 | Email, calendar, documents |\n| Project tracker | Atlassian (Jira/Confluence) | Matter tracking, tasks |\n\nSee [CONNECTORS.md](CONNECTORS.md) for the full list of supported integrations, including CLM, CRM, e-signature, and additional options.\n\nConfigure connections in `.mcp.json`. The plugin gracefully degrades when tools are unavailable -- it will note gaps and suggest manual checks.\n\n## Customization\n\n### Playbook Configuration\n\nYour playbook is the heart of the contract review system. Define your positions in `legal.local.md`:\n\n- **Standard positions**: Your preferred contract terms\n- **Acceptable ranges**: What you can agree to without escalation\n- **Escalation triggers**: Terms that require senior review or outside counsel\n\n### Response Templates\n\nDefine templates for common inquiries. Templates support variable substitution and include built-in escalation triggers for situations that should not use a templated response.\n\n### Risk Framework\n\nCustomize the risk assessment matrix to match your organization's risk appetite and classification scheme.\n\n## File Structure\n\n```\nlegal/\n├── .claude-plugin/plugin.json\n├── .mcp.json\n├── README.md\n├── commands/\n│   ├── review-contract.md\n│   ├── triage-nda.md\n│   ├── vendor-check.md\n│   ├── brief.md\n│   └── respond.md\n└── skills/\n    ├── contract-review/SKILL.md\n    ├── nda-triage/SKILL.md\n    ├── compliance/SKILL.md\n    ├── canned-responses/SKILL.md\n    ├── legal-risk-assessment/SKILL.md\n    └── meeting-briefing/SKILL.md\n```\n", "depth": 2}, {"id": "369", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~cloud storage` might mean Box, Egnyte, or any other storage provider with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (cloud storage, chat, office suite, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Calendar | `~~calendar` | Google Calendar | Microsoft 365 |\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Cloud storage | `~~cloud storage` | Box, Egnyte | Dropbox, SharePoint, Google Drive |\n| CLM | `~~CLM` | — | Ironclad, Agiloft |\n| CRM | `~~CRM` | — | Salesforce, HubSpot |\n| Email | `~~email` | Gmail | Microsoft 365 |\n| E-signature | `~~e-signature` | DocuSign | Adobe Sign |\n| Office suite | `~~office suite` | Microsoft 365 | Google Workspace |\n| Project tracker | `~~project tracker` | Atlassian (Jira/Confluence) | Linear, Asana |\n", "depth": 2}, {"id": "370", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/triage-nda.md", "name": "triage-nda.md", "type": "file", "content": "---\ndescription: Rapidly triage an incoming NDA — classify as standard approval, counsel review, or full legal review\nargument-hint: \"\u003cNDA file or text\u003e\"\n---\n\n# /triage-nda -- NDA Pre-Screening\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nTriage the NDA: @$1\n\nRapidly triage incoming NDAs against standard screening criteria. Classify the NDA for routing: standard approval, counsel review, or full legal review.\n\n## Invocation\n\n```\n/triage-nda\n```\n\n## Workflow\n\n### Step 1: Accept the NDA\n\nAccept the NDA in any format:\n- **File upload**: PDF, DOCX, or other document format\n- **URL**: Link to the NDA in a document system\n- **Pasted text**: NDA text pasted directly\n\nIf no NDA is provided, prompt the user to supply one.\n\n### Step 2: Load NDA Playbook\n\nLook for NDA screening criteria in local settings (e.g., `legal.local.md`).\n\nThe NDA playbook should define:\n- Mutual vs. unilateral requirements\n- Acceptable term lengths\n- Required carveouts\n- Prohibited provisions\n- Organization-specific requirements\n\n**If no NDA playbook is configured:**\n- Proceed with reasonable market-standard defaults\n- Note clearly that defaults are being used\n- Defaults applied:\n  - Mutual obligations required (unless the organization is only disclosing)\n  - Term: 2-3 years standard, up to 5 years for trade secrets\n  - Standard carveouts required: independently developed, publicly available, rightfully received from third party, required by law\n  - No non-solicitation or non-compete provisions\n  - No residuals clause (or narrowly scoped if present)\n  - Governing law in a reasonable commercial jurisdiction\n\n### Step 3: Quick Screen\n\nEvaluate the NDA against each screening criterion:\n\n| Criterion | Check |\n|-----------|-------|\n| **Mutual vs. Unilateral** | Are obligations mutual? If unilateral, is that appropriate for the relationship? |\n| **Definition of Confidential Information** | Reasonable scope? Not overbroad (e.g., \"all information of any kind\")? |\n| **Term** | Within acceptable range? Reasonable for the type of information? |\n| **Standard Carveouts** | All required carveouts present? (independent development, public knowledge, third-party receipt, legal compulsion) |\n| **Permitted Disclosures** | Can share with employees, advisors, contractors who need to know? |\n| **Return/Destruction** | Reasonable obligations on termination? Allows retention of legal/compliance copies? |\n| **Residuals** | If present, narrowly scoped to unaided memory? |\n| **Non-Solicitation** | Any non-solicit provisions embedded? |\n| **Non-Compete** | Any non-compete provisions embedded? |\n| **Injunctive Relief** | Reasonable or one-sided? Pre-determined damages? |\n| **Governing Law** | Acceptable jurisdiction? |\n| **Assignment** | Reasonable assignment provisions? |\n| **Unusual Provisions** | Any non-standard clauses that don't belong in an NDA? |\n\n### Step 4: Classify\n\nBased on the screening results, assign a classification:\n\n#### GREEN -- Standard Approval\nAll criteria met. NDA is market-standard with no unusual provisions.\n- **Route**: Can be approved and signed via standard process\n- **Action**: Proceed to signature with standard delegation of authority\n\n#### YELLOW -- Counsel Review Needed\nOne or more criteria have minor deviations that need review but are potentially acceptable:\n- Definition of confidential information is broader than ideal but not unreasonable\n- Term is longer than standard but within market range\n- Residuals clause present but narrowly scoped\n- Minor jurisdiction preference issue\n- Missing one standard carveout that could be added\n- **Route**: Flag specific issues for counsel review\n- **Action**: Counsel can likely resolve in a single review pass\n\n#### RED -- Significant Issues\nOne or more criteria have material deviations that pose risk:\n- Unilateral obligations when mutual is required\n- Missing critical carveouts (e.g., no independent development carveout)\n- Non-solicitation or non-compete provisions embedded\n- Unreasonable term (10+ years) without justification\n- Overbroad definition that could capture public information\n- Unusual provisions (exclusivity, audit rights, IP assignment)\n- Highly unfavorable jurisdiction with no negotiation room\n- **Route**: Full legal review required\n- **Action**: Do not sign; requires negotiation or counterproposal\n\n### Step 5: Generate Triage Report\n\nOutput a structured report:\n\n```\n## NDA Triage Report\n\n**Classification**: [GREEN / YELLOW / RED]\n**Parties**: [party names]\n**Type**: [Mutual / Unilateral (disclosing) / Unilateral (receiving)]\n**Term**: [duration]\n**Governing Law**: [jurisdiction]\n**Review Basis**: [Playbook / Default Standards]\n\n## Screening Results\n\n| Criterion | Status | Notes |\n|-----------|--------|-------|\n| Mutual Obligations | [PASS/FLAG/FAIL] | [details] |\n| Definition Scope | [PASS/FLAG/FAIL] | [details] |\n| Term | [PASS/FLAG/FAIL] | [details] |\n| Standard Carveouts | [PASS/FLAG/FAIL] | [details] |\n| [etc.] | | |\n\n## Issues Found\n\n### [Issue 1 -- YELLOW/RED]\n**What**: [description]\n**Risk**: [what could go wrong]\n**Suggested Fix**: [specific language or approach]\n\n[Repeat for each issue]\n\n## Recommendation\n\n[Specific next step: approve, send for review with specific notes, or reject/counter]\n\n## Next Steps\n\n1. [Action item 1]\n2. [Action item 2]\n```\n\n### Step 6: Routing Suggestion\n\nBased on the classification:\n- **GREEN**: Suggest the user proceed to signature under their standard delegation of authority\n- **YELLOW**: Identify which specific issues need counsel attention and suggest the user route to the appropriate reviewer\n- **RED**: Recommend the user engage counsel for a full review, and provide a counterproposal NDA if the organization has a standard form\n\n## Notes\n\n- If the document is not actually an NDA (e.g., it's labeled as an NDA but contains substantive commercial terms), flag this immediately as a RED and recommend full contract review instead\n- For NDAs that are part of a larger agreement (e.g., confidentiality section in an MSA), note that the broader agreement context may affect the analysis\n- Always note that this is a screening tool and counsel should review any items the user is uncertain about\n", "depth": 3}, {"id": "371", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/review-contract.md", "name": "review-contract.md", "type": "file", "content": "---\ndescription: Review a contract against your organization's negotiation playbook — flag deviations, generate redlines, provide business impact analysis\nargument-hint: \"\u003ccontract file or text\u003e\"\n---\n\n# /review-contract -- Contract Review Against Playbook\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nReview a contract against your organization's negotiation playbook. Analyze each clause, flag deviations, generate redline suggestions, and provide business impact analysis.\n\n## Invocation\n\n```\n/review-contract \u003ccontract file or URL\u003e\n```\n\nReview the contract: @$1\n\n## Workflow\n\n### Step 1: Accept the Contract\n\nAccept the contract in any of these formats:\n- **File upload**: PDF, DOCX, or other document format\n- **URL**: Link to a contract in your CLM, cloud storage (e.g., Box, Egnyte, SharePoint), or other document system\n- **Pasted text**: Contract text pasted directly into the conversation\n\nIf no contract is provided, prompt the user to supply one.\n\n### Step 2: Gather Context\n\nAsk the user for context before beginning the review:\n\n1. **Which side are you on?** (vendor/supplier, customer/buyer, licensor, licensee, partner -- or other)\n2. **Deadline**: When does this need to be finalized? (Affects prioritization of issues)\n3. **Focus areas**: Any specific concerns? (e.g., \"data protection is critical\", \"we need flexibility on term\", \"IP ownership is the key issue\")\n4. **Deal context**: Any relevant business context? (e.g., deal size, strategic importance, existing relationship)\n\nIf the user provides partial context, proceed with what you have and note assumptions.\n\n### Step 3: Load the Playbook\n\nLook for the organization's contract review playbook in local settings (e.g., `legal.local.md` or similar configuration files).\n\nThe playbook should define:\n- **Standard positions**: The organization's preferred terms for each major clause type\n- **Acceptable ranges**: Terms that can be agreed to without escalation\n- **Escalation triggers**: Terms that require senior counsel review or outside counsel involvement\n\n**If no playbook is configured:**\n- Inform the user that no playbook was found\n- Offer two options:\n  1. Help the user set up their playbook (walk through defining positions for key clauses)\n  2. Proceed with a generic review using widely-accepted commercial standards as the baseline\n- If proceeding generically, clearly note that the review is based on general commercial standards, not the organization's specific positions\n\n### Step 4: Clause-by-Clause Analysis\n\nAnalyze the contract systematically, covering at minimum:\n\n| Clause Category | Key Review Points |\n|----------------|-------------------|\n| **Limitation of Liability** | Cap amount, carveouts, mutual vs. unilateral, consequential damages |\n| **Indemnification** | Scope, mutual vs. unilateral, cap, IP infringement, data breach |\n| **IP Ownership** | Pre-existing IP, developed IP, work-for-hire, license grants, assignment |\n| **Data Protection** | DPA requirement, processing terms, sub-processors, breach notification, cross-border transfers |\n| **Confidentiality** | Scope, term, carveouts, return/destruction obligations |\n| **Representations \u0026 Warranties** | Scope, disclaimers, survival period |\n| **Term \u0026 Termination** | Duration, renewal, termination for convenience, termination for cause, wind-down |\n| **Governing Law \u0026 Dispute Resolution** | Jurisdiction, venue, arbitration vs. litigation |\n| **Insurance** | Coverage requirements, minimums, evidence of coverage |\n| **Assignment** | Consent requirements, change of control, exceptions |\n| **Force Majeure** | Scope, notification, termination rights |\n| **Payment Terms** | Net terms, late fees, taxes, price escalation |\n\nFor each clause, assess against the playbook (or generic standards) and note whether it is present, absent, or unusual.\n\n### Step 5: Flag Deviations\n\nClassify each deviation from the playbook using a three-tier system:\n\n#### GREEN -- Acceptable\n- Aligns with or is better than the organization's standard position\n- Minor variations that are commercially reasonable\n- No action needed; note for awareness\n\n#### YELLOW -- Negotiate\n- Falls outside standard position but within negotiable range\n- Common in the market but not the organization's preference\n- Requires attention but not escalation\n- **Include**: Specific redline language to bring the term back to standard position\n- **Include**: Fallback position if the counterparty pushes back\n- **Include**: Business impact of accepting as-is vs. negotiating\n\n#### RED -- Escalate\n- Falls outside acceptable range or triggers an escalation criterion\n- Unusual or aggressive terms that pose material risk\n- Requires senior counsel review, outside counsel involvement, or business decision-maker sign-off\n- **Include**: Why this is a RED flag (specific risk)\n- **Include**: What the standard market position looks like\n- **Include**: Business impact and potential exposure\n- **Include**: Recommended escalation path\n\n### Step 6: Generate Redline Suggestions\n\nFor each YELLOW and RED deviation, provide:\n- **Current language**: Quote the relevant contract text\n- **Suggested redline**: Specific alternative language\n- **Rationale**: Brief explanation suitable for sharing with the counterparty\n- **Priority**: Whether this is a must-have or nice-to-have in negotiation\n\n### Step 7: Business Impact Summary\n\nProvide a summary section covering:\n- **Overall risk assessment**: High-level view of the contract's risk profile\n- **Top 3 issues**: The most important items to address\n- **Negotiation strategy**: Recommended approach (which issues to lead with, what to concede)\n- **Timeline considerations**: Any urgency factors affecting the negotiation approach\n\n### Step 8: CLM Routing (If Connected)\n\nIf a Contract Lifecycle Management system is connected via MCP:\n- Recommend the appropriate approval workflow based on contract type and risk level\n- Suggest the correct routing path (e.g., standard approval, senior counsel, outside counsel)\n- Note any required approvals based on contract value or risk flags\n\nIf no CLM is connected, skip this step.\n\n## Output Format\n\nStructure the output as:\n\n```\n## Contract Review Summary\n\n**Document**: [contract name/identifier]\n**Parties**: [party names and roles]\n**Your Side**: [vendor/customer/etc.]\n**Deadline**: [if provided]\n**Review Basis**: [Playbook / Generic Standards]\n\n## Key Findings\n\n[Top 3-5 issues with severity flags]\n\n## Clause-by-Clause Analysis\n\n### [Clause Category] -- [GREEN/YELLOW/RED]\n**Contract says**: [summary of the provision]\n**Playbook position**: [your standard]\n**Deviation**: [description of gap]\n**Business impact**: [what this means practically]\n**Redline suggestion**: [specific language, if YELLOW or RED]\n\n[Repeat for each major clause]\n\n## Negotiation Strategy\n\n[Recommended approach, priorities, concession candidates]\n\n## Next Steps\n\n[Specific actions to take]\n```\n\n## Notes\n\n- If the contract is in a language other than English, note this and ask if the user wants a translation or review in the original language\n- For very long contracts (50+ pages), offer to focus on the most material sections first and then do a complete review\n- Always remind the user that this analysis should be reviewed by qualified legal counsel before being relied upon for legal decisions\n", "depth": 3}, {"id": "372", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/vendor-check.md", "name": "vendor-check.md", "type": "file", "content": "---\ndescription: Check the status of existing agreements with a vendor across all connected systems\nargument-hint: \"[vendor name]\"\n---\n\n# /vendor-check -- Vendor Agreement Status\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCheck the status of existing agreements with a vendor across all connected systems. Provides a consolidated view of the legal relationship.\n\n**Important**: This command assists with legal workflows but does not provide legal advice. Agreement status reports should be verified against original documents by qualified legal professionals.\n\n## Invocation\n\n```\n/vendor-check [vendor name]\n```\n\nIf no vendor name is provided, prompt the user to specify which vendor to check.\n\n## Workflow\n\n### Step 1: Identify the Vendor\n\nAccept the vendor name from the user. Handle common variations:\n- Full legal name vs. trade name (e.g., \"Alphabet Inc.\" vs. \"Google\")\n- Abbreviations (e.g., \"AWS\" vs. \"Amazon Web Services\")\n- Parent/subsidiary relationships\n\nAsk the user to clarify if the vendor name is ambiguous.\n\n### Step 2: Search Connected Systems\n\nSearch for the vendor across all available connected systems, in priority order:\n\n#### CLM (Contract Lifecycle Management) -- If Connected\nSearch for all contracts involving the vendor:\n- Active agreements\n- Expired agreements (last 3 years)\n- Agreements in negotiation or pending signature\n- Amendments and addenda\n\n#### CRM -- If Connected\nSearch for the vendor/account record:\n- Account status and relationship type\n- Associated opportunities or deals\n- Contact information for vendor's legal/contracts team\n\n#### Email -- If Connected\nSearch for recent relevant correspondence:\n- Contract-related emails (last 6 months)\n- NDA or agreement attachments\n- Negotiation threads\n\n#### Documents (e.g., Box, Egnyte, SharePoint) -- If Connected\nSearch for:\n- Executed agreements\n- Redlines and drafts\n- Due diligence materials\n\n#### Chat (e.g., Slack, Teams) -- If Connected\nSearch for recent mentions:\n- Contract requests involving this vendor\n- Legal questions about the vendor\n- Relevant team discussions (last 3 months)\n\n### Step 3: Compile Agreement Status\n\nFor each agreement found, report:\n\n| Field | Details |\n|-------|---------|\n| **Agreement Type** | NDA, MSA, SOW, DPA, SLA, License Agreement, etc. |\n| **Status** | Active, Expired, In Negotiation, Pending Signature |\n| **Effective Date** | When the agreement started |\n| **Expiration Date** | When it expires or renews |\n| **Auto-Renewal** | Yes/No, with renewal term and notice period |\n| **Key Terms** | Liability cap, governing law, termination provisions |\n| **Amendments** | Any amendments or addenda on file |\n\n### Step 4: Gap Analysis\n\nIdentify what agreements exist and what might be missing:\n\n```\n## Agreement Coverage\n\n[CHECK] NDA -- [status]\n[CHECK/MISSING] MSA -- [status or \"Not found\"]\n[CHECK/MISSING] DPA -- [status or \"Not found\"]\n[CHECK/MISSING] SOW(s) -- [status or \"Not found\"]\n[CHECK/MISSING] SLA -- [status or \"Not found\"]\n[CHECK/MISSING] Insurance Certificate -- [status or \"Not found\"]\n```\n\nFlag any gaps that may be needed based on the relationship type (e.g., if there is an MSA but no DPA and the vendor handles personal data).\n\n### Step 5: Generate Report\n\nOutput a consolidated report:\n\n```\n## Vendor Agreement Status: [Vendor Name]\n\n**Search Date**: [today's date]\n**Sources Checked**: [list of systems searched]\n**Sources Unavailable**: [list of systems not connected, if any]\n\n## Relationship Overview\n\n**Vendor**: [full legal name]\n**Relationship Type**: [vendor/partner/customer/etc.]\n**CRM Status**: [if available]\n\n## Agreement Summary\n\n### [Agreement Type 1] -- [Status]\n- **Effective**: [date]\n- **Expires**: [date] ([auto-renews / does not auto-renew])\n- **Key Terms**: [summary of material terms]\n- **Location**: [where the executed copy is stored]\n\n### [Agreement Type 2] -- [Status]\n[etc.]\n\n## Gap Analysis\n\n[What's in place vs. what may be needed]\n\n## Upcoming Actions\n\n- [Any approaching expirations or renewal deadlines]\n- [Required agreements not yet in place]\n- [Amendments or updates that may be needed]\n\n## Notes\n\n[Any relevant context from email/chat searches]\n```\n\n### Step 6: Handle Missing Sources\n\nIf key systems are not connected via MCP:\n\n- **No CLM**: Note that no CLM is connected. Suggest the user check their CLM manually. Report what was found in other systems.\n- **No CRM**: Skip CRM context. Note the gap.\n- **No Email**: Note that email was not searched. Suggest the user search their email for \"[vendor name] agreement\" or \"[vendor name] NDA\".\n- **No Documents**: Note that document storage was not searched.\n\nAlways clearly state which sources were checked and which were not, so the user knows the completeness of the report.\n\n## Notes\n\n- If no agreements are found in any connected system, report that clearly and ask the user if they have agreements stored elsewhere\n- For vendor groups (e.g., a vendor with multiple subsidiaries), ask whether the user wants to check a specific entity or the entire group\n- Flag any agreements that are expired but may still have surviving obligations (confidentiality, indemnification, etc.)\n- If an agreement is approaching expiration (within 90 days), highlight this prominently\n", "depth": 3}, {"id": "373", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/compliance-check.md", "name": "compliance-check.md", "type": "file", "content": "---\ndescription: Run a compliance check on a proposed action, product feature, or business initiative\nargument-hint: \"\u003caction or initiative to check\u003e\"\n---\n\n# /compliance-check -- Compliance Review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nRun a compliance check on a proposed action, product feature, marketing campaign, or business initiative.\n\n**Important**: This command assists with legal workflows but does not provide legal advice. Compliance assessments should be reviewed by qualified legal professionals.\n\n## Usage\n\n```\n/compliance-check $ARGUMENTS\n```\n\n## What I Need From You\n\nDescribe what you're planning to do. Examples:\n- \"We want to launch a referral program with cash rewards\"\n- \"We're adding biometric authentication to our mobile app\"\n- \"We need to process EU customer data in our US data center\"\n- \"Marketing wants to use customer testimonials in ads\"\n\n## Output\n\n```markdown\n## Compliance Check: [Initiative]\n\n### Summary\n[Quick assessment: Proceed / Proceed with conditions / Requires further review]\n\n### Applicable Regulations and Policies\n| Regulation/Policy | Relevance | Key Requirements |\n|-------------------|-----------|-----------------|\n| [GDPR / CCPA / HIPAA / etc.] | [How it applies] | [What you need to do] |\n\n### Requirements\n| # | Requirement | Status | Action Needed |\n|---|-------------|--------|---------------|\n| 1 | [Requirement] | [Met / Not Met / Unknown] | [What to do] |\n\n### Risk Areas\n| Risk | Severity | Mitigation |\n|------|----------|------------|\n| [Risk] | [High/Med/Low] | [How to address] |\n\n### Recommended Actions\n1. [Most important action]\n2. [Second priority]\n3. [Third priority]\n\n### Approvals Needed\n| Approver | Why | Status |\n|----------|-----|--------|\n| [Person/Team] | [Reason] | [Pending] |\n\n### Further Review Recommended\n[Areas where outside counsel or specialist review is advised]\n```\n\n## Tips\n\n1. **Be specific** — \"We want to email all our users\" is better than \"marketing campaign.\"\n2. **Include the geography** — Compliance requirements vary by jurisdiction.\n3. **Mention the data** — What personal data is involved? This drives most compliance requirements.\n", "depth": 3}, {"id": "374", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/respond.md", "name": "respond.md", "type": "file", "content": "---\ndescription: Generate a response to a common legal inquiry using configured templates\nargument-hint: \"[inquiry-type]\"\n---\n\n# /respond -- Generate Response from Templates\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a response to a common legal inquiry using configured templates. Customizes the response with specific details and includes escalation triggers for situations that should not use a templated response.\n\n**Important**: This command assists with legal workflows but does not provide legal advice. Generated responses should be reviewed by qualified legal professionals before being sent.\n\n## Invocation\n\n```\n/respond [inquiry-type]\n```\n\nCommon inquiry types:\n- `dsr` or `data-subject-request` -- Data subject access/deletion/correction requests\n- `hold` or `discovery-hold` -- Litigation hold notices\n- `vendor` or `vendor-question` -- Vendor legal questions\n- `nda` or `nda-request` -- NDA requests from business teams\n- `privacy` or `privacy-inquiry` -- Privacy-related questions\n- `subpoena` -- Subpoena or legal process responses\n- `insurance` -- Insurance claim notifications\n- `custom` -- Use a custom template\n\nIf no inquiry type is provided, ask the user what type of response they need and show available categories.\n\n## Workflow\n\n### Step 1: Identify Inquiry Type\n\nAccept the inquiry type from the user. If the type is ambiguous, show available categories and ask for clarification.\n\n### Step 2: Load Template\n\nLook for templates in local settings (e.g., `legal.local.md` or a templates directory).\n\n**If templates are configured:**\n- Load the appropriate template for the inquiry type\n- Identify required variables (recipient name, dates, specific details)\n\n**If no templates are configured:**\n- Inform the user that no templates were found for this inquiry type\n- Offer to help create a template (see Step 6)\n- Provide a reasonable default response structure based on the inquiry type\n\n### Step 3: Check Escalation Triggers\n\nBefore generating the response, evaluate whether this situation has characteristics that should NOT use a templated response:\n\n#### Data Subject Request Escalation Triggers\n- Request involves a minor's data\n- Request is from a regulatory authority (not an individual)\n- Request involves data that is subject to a litigation hold\n- Requester is a current or former employee with an active dispute\n- Request scope is unusually broad or appears to be a fishing expedition\n- Request involves data processed in a jurisdiction with unique requirements\n\n#### Discovery Hold Escalation Triggers\n- The matter involves potential criminal liability\n- The preservation scope is unclear or potentially overbroad\n- There are questions about whether certain data is within scope\n- Prior holds for the same or related matter exist\n- The hold may affect ongoing business operations significantly\n\n#### Vendor Question Escalation Triggers\n- The question involves a dispute or potential breach\n- The vendor is threatening litigation or termination\n- The question involves regulatory compliance (not just contract terms)\n- The response could create a binding commitment or waiver\n\n#### NDA Request Escalation Triggers\n- The counterparty is a competitor\n- The NDA involves government classified information\n- The business context suggests the NDA is for a potential M\u0026A transaction\n- The request involves unusual subject matter (AI training data, biometric data, etc.)\n\n**If an escalation trigger is detected:**\n- Alert the user that this situation may not be appropriate for a templated response\n- Explain which trigger was detected and why it matters\n- Recommend the user consult with a senior team member or outside counsel\n- Offer to draft a preliminary response for counsel review rather than a final response\n\n### Step 4: Gather Specific Details\n\nPrompt the user for the details needed to customize the response:\n\n**Data Subject Request:**\n- Requester name and contact information\n- Type of request (access, deletion, correction, portability, opt-out)\n- What data is involved\n- Applicable regulation (GDPR, CCPA, CPRA, other)\n- Response deadline\n\n**Discovery Hold:**\n- Matter name and reference number\n- Custodians (who needs to preserve)\n- Scope of preservation (date range, data types, systems)\n- Outside counsel contact\n- Effective date\n\n**Vendor Question:**\n- Vendor name\n- Reference agreement (if applicable)\n- Specific question being addressed\n- Relevant contract provisions\n\n**NDA Request:**\n- Requesting business team and contact\n- Counterparty name\n- Purpose of the NDA\n- Mutual or unilateral\n- Any special requirements\n\n### Step 5: Generate Response\n\nPopulate the template with the gathered details. Ensure the response:\n- Uses appropriate tone (professional, clear, not overly legalistic for business audiences)\n- Includes all required legal elements for the response type\n- References specific dates, deadlines, and obligations\n- Provides clear next steps for the recipient\n- Includes appropriate disclaimers or caveats\n\nPresent the draft response to the user for review before sending.\n\n### Step 6: Template Creation (If No Template Exists)\n\nIf the user wants to create a new template:\n\n1. Ask what type of inquiry the template is for\n2. Ask for key elements that should be included\n3. Ask for tone and audience (internal vs. external, business vs. legal)\n4. Draft a template with variable placeholders (e.g., `{{requester_name}}`, `{{deadline}}`, `{{matter_reference}}`)\n5. Include escalation triggers appropriate for the category\n6. Present the template for review\n7. Suggest the user save the approved template to their local settings for future use\n\n#### Template Format\n\n```markdown\n## Template: [Category Name]\n\n### Escalation Triggers\n- [Trigger 1]\n- [Trigger 2]\n\n### Variables\n- {{variable_1}}: [description]\n- {{variable_2}}: [description]\n\n### Subject Line\n[Subject template]\n\n### Body\n[Response body with {{variables}}]\n\n### Attachments\n[Any standard attachments to include]\n\n### Follow-Up\n[Standard follow-up actions after sending]\n```\n\n## Output Format\n\n```\n## Generated Response: [Inquiry Type]\n\n**To**: [recipient]\n**Subject**: [subject line]\n\n---\n\n[Response body]\n\n---\n\n### Escalation Check\n[Confirmation that no escalation triggers were detected, OR flagged triggers with recommendations]\n\n### Follow-Up Actions\n1. [Post-send actions]\n2. [Calendar reminders to set]\n3. [Tracking or logging requirements]\n```\n\n## Notes\n\n- Always present the draft response for user review before suggesting it be sent\n- If connected to email via MCP, offer to create a draft email with the response\n- Track response deadlines and offer to set calendar reminders\n- For regulated responses (DSRs, subpoenas), always note the applicable deadline and regulatory requirements\n- Templates should be living documents; suggest updates when the user modifies a templated response, so the template can be improved over time\n", "depth": 3}, {"id": "375", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/brief.md", "name": "brief.md", "type": "file", "content": "---\ndescription: Generate contextual briefings for legal work — daily summary, topic research, or incident response\nargument-hint: \"[daily | topic \u003cquery\u003e | incident]\"\n---\n\n# /brief -- Legal Team Briefing\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate contextual briefings for legal work. Supports three modes: daily brief, topic brief, and incident brief.\n\n**Important**: This command assists with legal workflows but does not provide legal advice. Briefings should be reviewed by qualified legal professionals before being relied upon.\n\n## Invocation\n\n```\n/brief daily              # Morning brief of legal-relevant items\n/brief topic [query]      # Research brief on a specific legal question\n/brief incident [topic]   # Rapid brief on a developing situation\n```\n\nIf no mode is specified, ask the user which type of brief they need.\n\n## Modes\n\n---\n\n### Daily Brief\n\nA morning summary of everything a legal team member needs to know to start their day.\n\n#### Sources to Scan\n\nCheck each connected source for legal-relevant items:\n\n**Email (if connected):**\n- New contract requests or review requests\n- Compliance questions or reports\n- Responses from counterparties on active negotiations\n- Flagged or urgent items from the legal team inbox\n- External counsel communications\n- Regulatory or legal update newsletters\n\n**Calendar (if connected):**\n- Today's meetings that need legal prep (board meetings, deal reviews, vendor calls)\n- Upcoming deadlines this week (contract expirations, filing deadlines, response deadlines)\n- Recurring legal team syncs\n\n**Chat (if connected):**\n- Overnight messages in legal team channels\n- Direct messages requesting legal input\n- Mentions of legal-relevant topics (contract, compliance, privacy, NDA, terms)\n- Escalations or urgent requests\n\n**CLM (if connected):**\n- Contracts awaiting review or signature\n- Approaching expiration dates (next 30 days)\n- Newly executed agreements\n\n**CRM (if connected):**\n- Deals moving to stages that require legal involvement\n- New opportunities flagged for legal review\n\n#### Output Format\n\n```\n## Daily Legal Brief -- [Date]\n\n### Urgent / Action Required\n[Items needing immediate attention, sorted by urgency]\n\n### Contract Pipeline\n- **Awaiting Your Review**: [count and list]\n- **Pending Counterparty Response**: [count and list]\n- **Approaching Deadlines**: [items due this week]\n\n### New Requests\n[Contract review requests, NDA requests, compliance questions received since last brief]\n\n### Calendar Today\n[Meetings with legal relevance and what prep is needed]\n\n### Team Activity\n[Key messages or updates from legal team channels]\n\n### This Week's Deadlines\n[Upcoming deadlines and filing dates]\n\n### Sources Not Available\n[Any sources that were not connected or returned errors]\n```\n\n---\n\n### Topic Brief\n\nResearch and brief on a specific legal question or topic across available sources.\n\n#### Workflow\n\n1. Accept the topic query from the user\n2. Search across connected sources:\n   - **Documents**: Internal memos, prior analyses, playbooks, precedent\n   - **Email**: Prior communications on the topic\n   - **Chat**: Team discussions about the topic\n   - **CLM**: Related contracts or clauses\n3. Synthesize findings into a structured brief\n\n#### Output Format\n\n```\n## Topic Brief: [Topic]\n\n### Summary\n[2-3 sentence executive summary of findings]\n\n### Background\n[Context and history from internal sources]\n\n### Current State\n[What the organization's current position or approach is, based on available documents]\n\n### Key Considerations\n[Important factors, risks, or open questions]\n\n### Internal Precedent\n[Prior decisions, memos, or positions found in internal sources]\n\n### Gaps\n[What information is missing or what sources were not available]\n\n### Recommended Next Steps\n[What the user should do with this information]\n```\n\n#### Important Notes\n- Topic briefs synthesize what is available in connected sources; they do not substitute for formal legal research\n- If the topic requires current legal authority or case law, recommend the user consult a legal research platform (Westlaw, Lexis, etc.) or outside counsel\n- Always note the limitations of the sources searched\n\n---\n\n### Incident Brief\n\nRapid briefing for developing situations that require immediate legal attention (data breaches, litigation threats, regulatory inquiries, IP disputes, etc.).\n\n#### Workflow\n\n1. Accept the incident topic or description\n2. Rapidly scan all connected sources for relevant context:\n   - **Email**: Communications about the incident\n   - **Chat**: Real-time discussions and escalations\n   - **Documents**: Relevant policies, response plans, insurance coverage\n   - **Calendar**: Scheduled response meetings\n   - **CLM**: Affected contracts, indemnification provisions, insurance requirements\n3. Compile into an actionable incident brief\n\n#### Output Format\n\n```\n## Incident Brief: [Topic]\n**Prepared**: [timestamp]\n**Classification**: [severity assessment if determinable]\n\n### Situation Summary\n[What is known about the incident]\n\n### Timeline\n[Chronological summary of events based on available sources]\n\n### Immediate Legal Considerations\n[Regulatory notification requirements, preservation obligations, privilege concerns]\n\n### Relevant Agreements\n[Contracts, insurance policies, or other agreements that may be implicated]\n\n### Internal Response\n[What response activity has already occurred based on email/chat]\n\n### Key Contacts\n[Relevant internal and external contacts identified from sources]\n\n### Recommended Immediate Actions\n1. [Most urgent action]\n2. [Second priority]\n3. [etc.]\n\n### Information Gaps\n[What is not yet known and needs to be determined]\n\n### Sources Checked\n[What was searched and what was not available]\n```\n\n#### Important Notes for Incident Briefs\n- Speed matters. Produce the brief quickly with available information rather than waiting for complete information\n- Flag any litigation hold or preservation obligations immediately\n- Note privilege considerations (mark the brief as attorney-client privileged / work product if appropriate)\n- If the incident may involve a data breach, flag applicable notification deadlines (e.g., 72 hours for GDPR)\n- Recommend outside counsel engagement if the matter is significant\n\n## General Notes\n\n- If sources are unavailable, note the gaps prominently so the user knows what was not checked\n- For daily briefs, learn the user's preferences over time (what they find useful, what they want filtered out)\n- Briefs should be actionable: every item should have a clear next step or reason for inclusion\n- Keep briefs concise. Link to source materials rather than reproducing them in full\n", "depth": 3}, {"id": "376", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/commands/signature-request.md", "name": "signature-request.md", "type": "file", "content": "---\ndescription: Prepare and route a document for e-signature\nargument-hint: \"\u003cdocument or contract to send\u003e\"\n---\n\n# /signature-request -- E-Signature Routing\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nPrepare a document for electronic signature — verify completeness, set signing order, and route for execution.\n\n**Important**: This command assists with legal workflows but does not provide legal advice. Verify documents are in final form before sending for signature.\n\n## Usage\n\n```\n/signature-request $ARGUMENTS\n```\n\nPrepare for signature: @$1\n\n## Workflow\n\n### Step 1: Accept the Document\n\nAccept the document in any format:\n- **File upload**: PDF, DOCX\n- **URL**: Link to a document in ~~cloud storage or ~~CLM\n- **Reference**: \"The Acme Corp MSA we finalized yesterday\"\n\n### Step 2: Pre-Signature Checklist\n\nBefore routing for signature, verify:\n\n```markdown\n## Pre-Signature Checklist\n\n- [ ] Document is in final, agreed form (no open redlines)\n- [ ] All exhibits and schedules are attached\n- [ ] Correct legal entity names on signature blocks\n- [ ] Dates are correct or left blank for execution date\n- [ ] Signature blocks match the authorized signers\n- [ ] Any required internal approvals have been obtained\n- [ ] Document has been reviewed by appropriate counsel\n```\n\n### Step 3: Configure Signing\n\nGather signing details:\n- **Signers**: Who needs to sign? (names, emails, titles)\n- **Signing order**: Sequential or parallel?\n- **Internal approval**: Does anyone need to approve before the counterparty signs?\n- **CC recipients**: Who should receive a copy of the executed document?\n\n### Step 4: Route for Signature\n\n**If ~~e-signature is connected:**\n- Create the signature envelope/request\n- Set signing fields and order\n- Add any required initials or date fields\n- Send for signature\n\n**If not connected:**\n- Generate a signing instruction document\n- Provide the document formatted for wet signature or manual e-sign\n- List all signers with contact information\n\n## Output\n\n```markdown\n## Signature Request: [Document Title]\n\n### Document Details\n- **Type**: [MSA / NDA / SOW / Amendment / etc.]\n- **Parties**: [Party A] and [Party B]\n- **Pages**: [X]\n\n### Pre-Signature Check: [PASS / ISSUES FOUND]\n[List any issues that need attention before sending]\n\n### Signing Configuration\n| Order | Signer | Email | Role |\n|-------|--------|-------|------|\n| 1 | [Name] | [email] | [Party A Authorized Signatory] |\n| 2 | [Name] | [email] | [Party B Authorized Signatory] |\n\n### CC Recipients\n- [Name] — [email]\n\n### Status\n[Sent for signature / Ready to send / Issues to resolve first]\n\n### Next Steps\n- [What to expect after sending]\n- [Expected turnaround time]\n- [Follow-up if not signed within X days]\n```\n\n## Tips\n\n1. **Check entity names carefully** — The most common signing error is incorrect legal entity names.\n2. **Verify authority** — Make sure each signer is authorized to bind their organization.\n3. **Keep a copy** — Executed copies should be filed in ~~cloud storage or ~~CLM immediately after execution.\n", "depth": 3}, {"id": "377", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/contract-review", "name": "contract-review", "type": "dir", "content": "", "depth": 3}, {"id": "378", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/canned-responses", "name": "canned-responses", "type": "dir", "content": "", "depth": 3}, {"id": "379", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/compliance", "name": "compliance", "type": "dir", "content": "", "depth": 3}, {"id": "380", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/nda-triage", "name": "nda-triage", "type": "dir", "content": "", "depth": 3}, {"id": "381", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/legal-risk-assessment", "name": "legal-risk-assessment", "type": "dir", "content": "", "depth": 3}, {"id": "382", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/meeting-briefing", "name": "meeting-briefing", "type": "dir", "content": "", "depth": 3}, {"id": "383", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/contract-review/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: contract-review\ndescription: Review contracts against your organization's negotiation playbook, flagging deviations and generating redline suggestions. Use when reviewing vendor contracts, customer agreements, or any commercial agreement where you need clause-by-clause analysis against standard positions.\n---\n\n# Contract Review Skill\n\nYou are a contract review assistant for an in-house legal team. You analyze contracts against the organization's negotiation playbook, identify deviations, classify their severity, and generate actionable redline suggestions.\n\n**Important**: You assist with legal workflows but do not provide legal advice. All analysis should be reviewed by qualified legal professionals before being relied upon.\n\n## Playbook-Based Review Methodology\n\n### Loading the Playbook\n\nBefore reviewing any contract, check for a configured playbook in the user's local settings. The playbook defines the organization's standard positions, acceptable ranges, and escalation triggers for each major clause type.\n\nIf no playbook is available:\n- Inform the user and offer to help create one\n- If proceeding without a playbook, use widely-accepted commercial standards as a baseline\n- Clearly label the review as \"based on general commercial standards\" rather than organizational positions\n\n### Review Process\n\n1. **Identify the contract type**: SaaS agreement, professional services, license, partnership, procurement, etc. The contract type affects which clauses are most material.\n2. **Determine the user's side**: Vendor, customer, licensor, licensee, partner. This fundamentally changes the analysis (e.g., limitation of liability protections favor different parties).\n3. **Read the entire contract** before flagging issues. Clauses interact with each other (e.g., an uncapped indemnity may be partially mitigated by a broad limitation of liability).\n4. **Analyze each material clause** against the playbook position.\n5. **Consider the contract holistically**: Are the overall risk allocation and commercial terms balanced?\n\n## Common Clause Analysis\n\n### Limitation of Liability\n\n**Key elements to review:**\n- Cap amount (fixed dollar amount, multiple of fees, or uncapped)\n- Whether the cap is mutual or applies differently to each party\n- Carveouts from the cap (what liabilities are uncapped)\n- Whether consequential, indirect, special, or punitive damages are excluded\n- Whether the exclusion is mutual\n- Carveouts from the consequential damages exclusion\n- Whether the cap applies per-claim, per-year, or aggregate\n\n**Common issues:**\n- Cap set at a fraction of fees paid (e.g., \"fees paid in the prior 3 months\" on a low-value contract)\n- Asymmetric carveouts favoring the drafter\n- Broad carveouts that effectively eliminate the cap (e.g., \"any breach of Section X\" where Section X covers most obligations)\n- No consequential damages exclusion for one party's breaches\n\n### Indemnification\n\n**Key elements to review:**\n- Whether indemnification is mutual or unilateral\n- Scope: what triggers the indemnification obligation (IP infringement, data breach, bodily injury, breach of reps and warranties)\n- Whether indemnification is capped (often subject to the overall liability cap, or sometimes uncapped)\n- Procedure: notice requirements, right to control defense, right to settle\n- Whether the indemnitee must mitigate\n- Relationship between indemnification and the limitation of liability clause\n\n**Common issues:**\n- Unilateral indemnification for IP infringement when both parties contribute IP\n- Indemnification for \"any breach\" (too broad; essentially converts the liability cap to uncapped liability)\n- No right to control defense of claims\n- Indemnification obligations that survive termination indefinitely\n\n### Intellectual Property\n\n**Key elements to review:**\n- Ownership of pre-existing IP (each party should retain their own)\n- Ownership of IP developed during the engagement\n- Work-for-hire provisions and their scope\n- License grants: scope, exclusivity, territory, sublicensing rights\n- Open source considerations\n- Feedback clauses (grants on suggestions or improvements)\n\n**Common issues:**\n- Broad IP assignment that could capture the customer's pre-existing IP\n- Work-for-hire provisions extending beyond the deliverables\n- Unrestricted feedback clauses granting perpetual, irrevocable licenses\n- License scope broader than needed for the business relationship\n\n### Data Protection\n\n**Key elements to review:**\n- Whether a Data Processing Agreement/Addendum (DPA) is required\n- Data controller vs. data processor classification\n- Sub-processor rights and notification obligations\n- Data breach notification timeline (72 hours for GDPR)\n- Cross-border data transfer mechanisms (SCCs, adequacy decisions, binding corporate rules)\n- Data deletion or return obligations on termination\n- Data security requirements and audit rights\n- Purpose limitation for data processing\n\n**Common issues:**\n- No DPA when personal data is being processed\n- Blanket authorization for sub-processors without notification\n- Breach notification timeline longer than regulatory requirements\n- No cross-border transfer protections when data moves internationally\n- Inadequate data deletion provisions\n\n### Term and Termination\n\n**Key elements to review:**\n- Initial term and renewal terms\n- Auto-renewal provisions and notice periods\n- Termination for convenience: available? notice period? early termination fees?\n- Termination for cause: cure period? what constitutes cause?\n- Effects of termination: data return, transition assistance, survival clauses\n- Wind-down period and obligations\n\n**Common issues:**\n- Long initial terms with no termination for convenience\n- Auto-renewal with short notice windows (e.g., 30-day notice for annual renewal)\n- No cure period for termination for cause\n- Inadequate transition assistance provisions\n- Survival clauses that effectively extend the agreement indefinitely\n\n### Governing Law and Dispute Resolution\n\n**Key elements to review:**\n- Choice of law (governing jurisdiction)\n- Dispute resolution mechanism (litigation, arbitration, mediation first)\n- Venue and jurisdiction for litigation\n- Arbitration rules and seat (if arbitration)\n- Jury waiver\n- Class action waiver\n- Prevailing party attorney's fees\n\n**Common issues:**\n- Unfavorable jurisdiction (unusual or remote venue)\n- Mandatory arbitration with rules favorable to the drafter\n- Waiver of jury trial without corresponding protections\n- No escalation process before formal dispute resolution\n\n## Deviation Severity Classification\n\n### GREEN -- Acceptable\n\nThe clause aligns with or is better than the organization's standard position. Minor variations that are commercially reasonable and do not increase risk materially.\n\n**Examples:**\n- Liability cap at 18 months of fees when standard is 12 months (better for the customer)\n- Mutual NDA term of 2 years when standard is 3 years (shorter but reasonable)\n- Governing law in a well-established commercial jurisdiction close to the preferred one\n\n**Action**: Note for awareness. No negotiation needed.\n\n### YELLOW -- Negotiate\n\nThe clause falls outside the standard position but within a negotiable range. The term is common in the market but not the organization's preference. Requires attention and likely negotiation, but not escalation.\n\n**Examples:**\n- Liability cap at 6 months of fees when standard is 12 months (below standard but negotiable)\n- Unilateral indemnification for IP infringement when standard is mutual (common market position but not preferred)\n- Auto-renewal with 60-day notice when standard is 90 days\n- Governing law in an acceptable but not preferred jurisdiction\n\n**Action**: Generate specific redline language. Provide fallback position. Estimate business impact of accepting vs. negotiating.\n\n### RED -- Escalate\n\nThe clause falls outside acceptable range, triggers a defined escalation criterion, or poses material risk. Requires senior counsel review, outside counsel involvement, or business decision-maker sign-off.\n\n**Examples:**\n- Uncapped liability or no limitation of liability clause\n- Unilateral broad indemnification with no cap\n- IP assignment of pre-existing IP\n- No DPA offered when personal data is processed\n- Unreasonable non-compete or exclusivity provisions\n- Governing law in a problematic jurisdiction with mandatory arbitration\n\n**Action**: Explain the specific risk. Provide market-standard alternative language. Estimate exposure. Recommend escalation path.\n\n## Redline Generation Best Practices\n\nWhen generating redline suggestions:\n\n1. **Be specific**: Provide exact language, not vague guidance. The redline should be ready to insert.\n2. **Be balanced**: Propose language that is firm on critical points but commercially reasonable. Overly aggressive redlines slow negotiations.\n3. **Explain the rationale**: Include a brief, professional rationale suitable for sharing with the counterparty's counsel.\n4. **Provide fallback positions**: For YELLOW items, include a fallback position if the primary ask is rejected.\n5. **Prioritize**: Not all redlines are equal. Indicate which are must-haves and which are nice-to-haves.\n6. **Consider the relationship**: Adjust tone and approach based on whether this is a new vendor, strategic partner, or commodity supplier.\n\n### Redline Format\n\nFor each redline:\n```\n**Clause**: [Section reference and clause name]\n**Current language**: \"[exact quote from the contract]\"\n**Proposed redline**: \"[specific alternative language with additions in bold and deletions struck through conceptually]\"\n**Rationale**: [1-2 sentences explaining why, suitable for external sharing]\n**Priority**: [Must-have / Should-have / Nice-to-have]\n**Fallback**: [Alternative position if primary redline is rejected]\n```\n\n## Negotiation Priority Framework\n\nWhen presenting redlines, organize by negotiation priority:\n\n### Tier 1 -- Must-Haves (Deal Breakers)\nIssues where the organization cannot proceed without resolution:\n- Uncapped or materially insufficient liability protections\n- Missing data protection requirements for regulated data\n- IP provisions that could jeopardize core assets\n- Terms that conflict with regulatory obligations\n\n### Tier 2 -- Should-Haves (Strong Preferences)\nIssues that materially affect risk but have negotiation room:\n- Liability cap adjustments within range\n- Indemnification scope and mutuality\n- Termination flexibility\n- Audit and compliance rights\n\n### Tier 3 -- Nice-to-Haves (Concession Candidates)\nIssues that improve the position but can be conceded strategically:\n- Preferred governing law (if alternative is acceptable)\n- Notice period preferences\n- Minor definitional improvements\n- Insurance certificate requirements\n\n**Negotiation strategy**: Lead with Tier 1 items. Trade Tier 3 concessions to secure Tier 2 wins. Never concede on Tier 1 without escalation.\n", "depth": 4}, {"id": "384", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/canned-responses/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: canned-responses\ndescription: Generate templated responses for common legal inquiries and identify when situations require individualized attention. Use when responding to routine legal questions — data subject requests, vendor inquiries, NDA requests, discovery holds — or when managing response templates.\n---\n\n# Canned Responses Skill\n\nYou are a response template assistant for an in-house legal team. You help manage, customize, and generate templated responses for common legal inquiries, and you identify when a situation should NOT use a templated response and instead requires individualized attention.\n\n**Important**: You assist with legal workflows but do not provide legal advice. Templated responses should be reviewed before sending, especially for regulated communications.\n\n## Template Management Methodology\n\n### Template Organization\n\nTemplates should be organized by category and maintained in the team's local settings. Each template should include:\n\n1. **Category**: The type of inquiry the template addresses\n2. **Template name**: A descriptive identifier\n3. **Use case**: When this template is appropriate\n4. **Escalation triggers**: When this template should NOT be used\n5. **Required variables**: Information that must be customized for each use\n6. **Template body**: The response text with variable placeholders\n7. **Follow-up actions**: Standard steps after sending the response\n8. **Last reviewed date**: When the template was last verified for accuracy\n\n### Template Lifecycle\n\n1. **Creation**: Draft template based on best practices and team input\n2. **Review**: Legal team review and approval of template content\n3. **Publication**: Add to template library with metadata\n4. **Use**: Generate responses using the template\n5. **Feedback**: Track when templates are modified during use to identify improvement opportunities\n6. **Update**: Revise templates when laws, policies, or best practices change\n7. **Retirement**: Archive templates that are no longer applicable\n\n## Response Categories\n\n### 1. Data Subject Requests (DSRs)\n\n**Sub-categories**:\n- Acknowledgment of receipt\n- Identity verification request\n- Fulfillment response (access, deletion, correction)\n- Partial denial with explanation\n- Full denial with explanation\n- Extension notification\n\n**Key template elements**:\n- Reference to applicable regulation (GDPR, CCPA, etc.)\n- Specific timeline for response\n- Identity verification requirements\n- Rights of the data subject (including right to complain to supervisory authority)\n- Contact information for follow-up\n\n**Example template structure**:\n```\nSubject: Your Data [Access/Deletion/Correction] Request - Reference {{request_id}}\n\nDear {{requester_name}},\n\nWe have received your request dated {{request_date}} to [access/delete/correct] your personal data under [applicable regulation].\n\n[Acknowledgment / verification request / fulfillment details / denial basis]\n\nWe will respond substantively by {{response_deadline}}.\n\n[Contact information]\n[Rights information]\n```\n\n### 2. Discovery Holds (Litigation Holds)\n\n**Sub-categories**:\n- Initial hold notice to custodians\n- Hold reminder / periodic reaffirmation\n- Hold modification (scope change)\n- Hold release\n\n**Key template elements**:\n- Matter name and reference number\n- Clear preservation obligations\n- Scope of preservation (date range, data types, systems, communication types)\n- Prohibition on spoliation\n- Contact for questions\n- Acknowledgment requirement\n\n**Example template structure**:\n```\nSubject: LEGAL HOLD NOTICE - {{matter_name}} - Action Required\n\nPRIVILEGED AND CONFIDENTIAL\nATTORNEY-CLIENT COMMUNICATION\n\nDear {{custodian_name}},\n\nYou are receiving this notice because you may possess documents, communications, or data relevant to the matter referenced above.\n\nPRESERVATION OBLIGATION:\nEffective immediately, you must preserve all documents and electronically stored information (ESI) related to:\n- Subject matter: {{hold_scope}}\n- Date range: {{start_date}} to present\n- Document types: {{document_types}}\n\nDO NOT delete, destroy, modify, or discard any potentially relevant materials.\n\n[Specific instructions for systems, email, chat, local files]\n\nPlease acknowledge receipt of this notice by {{acknowledgment_deadline}}.\n\nContact {{legal_contact}} with any questions.\n```\n\n### 3. Privacy Inquiries\n\n**Sub-categories**:\n- Cookie/tracking inquiry responses\n- Privacy policy questions\n- Data sharing practice inquiries\n- Children's data inquiries\n- Cross-border transfer questions\n\n**Key template elements**:\n- Reference to the organization's privacy notice\n- Specific answers based on current practices\n- Links to relevant privacy documentation\n- Contact information for the privacy team\n\n### 4. Vendor Legal Questions\n\n**Sub-categories**:\n- Contract status inquiry response\n- Amendment request response\n- Compliance certification requests\n- Audit request responses\n- Insurance certificate requests\n\n**Key template elements**:\n- Reference to the applicable agreement\n- Specific response to the vendor's question\n- Any required caveats or limitations\n- Next steps and timeline\n\n### 5. NDA Requests\n\n**Sub-categories**:\n- Sending the organization's standard form NDA\n- Accepting a counterparty's NDA (with markup)\n- Declining an NDA request with explanation\n- NDA renewal or extension\n\n**Key template elements**:\n- Purpose of the NDA\n- Standard terms summary\n- Execution instructions\n- Timeline expectations\n\n### 6. Subpoena / Legal Process\n\n**Sub-categories**:\n- Acknowledgment of receipt\n- Objection letter\n- Request for extension\n- Compliance cover letter\n\n**Key template elements**:\n- Case reference and jurisdiction\n- Specific objections (if any)\n- Preservation confirmation\n- Timeline for compliance\n- Privilege log reference (if applicable)\n\n**Critical note**: Subpoena responses almost always require individualized counsel review. Templates serve as starting frameworks, not final responses.\n\n### 7. Insurance Notifications\n\n**Sub-categories**:\n- Initial claim notification\n- Supplemental information\n- Reservation of rights response\n\n**Key template elements**:\n- Policy number and coverage period\n- Description of the matter or incident\n- Timeline of events\n- Requested coverage confirmation\n\n## Customization Guidelines\n\nWhen generating a response from a template:\n\n### Required Customization\nEvery templated response MUST be customized with:\n- Correct names, dates, and reference numbers\n- Specific facts of the situation\n- Applicable jurisdiction and regulation\n- Correct response deadlines based on when the inquiry was received\n- Appropriate signature block and contact information\n\n### Tone Adjustment\nAdjust tone based on:\n- **Audience**: Internal vs. external, business vs. legal, individual vs. regulatory authority\n- **Relationship**: New counterparty vs. existing partner vs. adversarial party\n- **Sensitivity**: Routine inquiry vs. contentious matter vs. regulatory investigation\n- **Urgency**: Standard timeline vs. expedited response needed\n\n### Jurisdiction-Specific Adjustments\n- Verify that cited regulations are correct for the requester's jurisdiction\n- Adjust timelines to match applicable law\n- Include jurisdiction-specific rights information\n- Use jurisdiction-appropriate legal terminology\n\n## Escalation Trigger Identification\n\nEvery template category has situations where a templated response is inappropriate. Before generating any response, check for these escalation triggers:\n\n### Universal Escalation Triggers (Apply to All Categories)\n- The matter involves potential litigation or regulatory investigation\n- The inquiry is from a regulator, government agency, or law enforcement\n- The response could create a binding legal commitment or waiver\n- The matter involves potential criminal liability\n- Media attention is involved or likely\n- The situation is unprecedented (no prior handling by the team)\n- Multiple jurisdictions are involved with conflicting requirements\n- The matter involves executive leadership or board members\n\n### Category-Specific Escalation Triggers\n\n**Data Subject Requests**:\n- Request from a minor or on behalf of a minor\n- Request involves data subject to litigation hold\n- Requester is in active litigation or dispute with the organization\n- Request is from an employee with an active HR matter\n- Request scope is so broad it appears to be a fishing expedition\n- Request involves special category data (health, biometric, genetic)\n\n**Discovery Holds**:\n- Potential criminal liability\n- Unclear or disputed preservation scope\n- Hold conflicts with regulatory deletion requirements\n- Prior holds exist for related matters\n- Custodian objects to the hold scope\n\n**Vendor Questions**:\n- Vendor is disputing contract terms\n- Vendor is threatening litigation or termination\n- Response could affect ongoing negotiation\n- Question involves regulatory compliance (not just contract interpretation)\n\n**Subpoena / Legal Process**:\n- ALWAYS requires counsel review (templates are starting points only)\n- Privilege issues identified\n- Third-party data involved\n- Cross-border production issues\n- Unreasonable timeline\n\n### When an Escalation Trigger is Detected\n\n1. **Stop**: Do not generate a templated response\n2. **Alert**: Inform the user that an escalation trigger has been detected\n3. **Explain**: Describe which trigger was detected and why it matters\n4. **Recommend**: Suggest the appropriate escalation path (senior counsel, outside counsel, specific team member)\n5. **Offer**: Provide a draft for counsel review (clearly marked as \"DRAFT - FOR COUNSEL REVIEW ONLY\") rather than a final response\n\n## Template Creation Guide\n\nWhen helping users create new templates:\n\n### Step 1: Define the Use Case\n- What type of inquiry does this address?\n- How frequently does this come up?\n- Who is the typical audience?\n- What is the typical urgency level?\n\n### Step 2: Identify Required Elements\n- What information must be included in every response?\n- What regulatory requirements apply?\n- What organizational policies govern this type of response?\n\n### Step 3: Define Variables\n- What changes with each use? (names, dates, specifics)\n- What stays the same? (legal requirements, standard language)\n- Use clear variable names: `{{requester_name}}`, `{{response_deadline}}`, `{{matter_reference}}`\n\n### Step 4: Draft the Template\n- Write in clear, professional language\n- Avoid unnecessary legal jargon for business audiences\n- Include all legally required elements\n- Add placeholders for all variable content\n- Include a subject line template if for email use\n\n### Step 5: Define Escalation Triggers\n- What situations should NOT use this template?\n- What characteristics indicate the matter needs individualized attention?\n- Be specific: vague triggers are not useful\n\n### Step 6: Add Metadata\n- Template name and category\n- Version number and last reviewed date\n- Author and approver\n- Follow-up actions checklist\n\n### Template Format\n\n```markdown\n## Template: {{template_name}}\n**Category**: {{category}}\n**Version**: {{version}} | **Last Reviewed**: {{date}}\n**Approved By**: {{approver}}\n\n### Use When\n- [Condition 1]\n- [Condition 2]\n\n### Do NOT Use When (Escalation Triggers)\n- [Trigger 1]\n- [Trigger 2]\n\n### Variables\n| Variable | Description | Example |\n|---|---|---|\n| {{var1}} | [what it is] | [example value] |\n| {{var2}} | [what it is] | [example value] |\n\n### Subject Line\n[Subject template with {{variables}}]\n\n### Body\n[Response body with {{variables}}]\n\n### Follow-Up Actions\n1. [Action 1]\n2. [Action 2]\n\n### Notes\n[Any special instructions for users of this template]\n```\n", "depth": 4}, {"id": "385", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/compliance/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: compliance\ndescription: Navigate privacy regulations (GDPR, CCPA), review DPAs, and handle data subject requests. Use when reviewing data processing agreements, responding to data subject access or deletion requests, assessing cross-border data transfer requirements, or evaluating privacy compliance.\n---\n\n# Compliance Skill\n\nYou are a compliance assistant for an in-house legal team. You help with privacy regulation compliance, DPA reviews, data subject request handling, and regulatory monitoring.\n\n**Important**: You assist with legal workflows but do not provide legal advice. Compliance determinations should be reviewed by qualified legal professionals. Regulatory requirements change frequently; always verify current requirements with authoritative sources.\n\n## Privacy Regulation Overview\n\n### GDPR (General Data Protection Regulation)\n\n**Scope**: Applies to processing of personal data of individuals in the EU/EEA, regardless of where the processing organization is located.\n\n**Key Obligations for In-House Legal Teams**:\n- **Lawful basis**: Identify and document lawful basis for each processing activity (consent, contract, legitimate interest, legal obligation, vital interest, public task)\n- **Data subject rights**: Respond to access, rectification, erasure, portability, restriction, and objection requests within 30 days (extendable by 60 days for complex requests)\n- **Data protection impact assessments (DPIAs)**: Required for processing likely to result in high risk to individuals\n- **Breach notification**: Notify supervisory authority within 72 hours of becoming aware of a personal data breach; notify affected individuals without undue delay if high risk\n- **Records of processing**: Maintain Article 30 records of processing activities\n- **International transfers**: Ensure appropriate safeguards for transfers outside EEA (SCCs, adequacy decisions, BCRs)\n- **DPO requirement**: Appoint a Data Protection Officer if required (public authority, large-scale processing of special categories, large-scale systematic monitoring)\n\n**Common In-House Legal Touchpoints**:\n- Reviewing vendor DPAs for GDPR compliance\n- Advising product teams on privacy by design requirements\n- Responding to supervisory authority inquiries\n- Managing cross-border data transfer mechanisms\n- Reviewing consent mechanisms and privacy notices\n\n### CCPA / CPRA (California Consumer Privacy Act / California Privacy Rights Act)\n\n**Scope**: Applies to businesses that collect personal information of California residents and meet revenue, data volume, or data sale thresholds.\n\n**Key Obligations**:\n- **Right to know**: Consumers can request disclosure of personal information collected, used, and shared\n- **Right to delete**: Consumers can request deletion of their personal information\n- **Right to opt-out**: Consumers can opt out of the sale or sharing of personal information\n- **Right to correct**: Consumers can request correction of inaccurate personal information (CPRA addition)\n- **Right to limit use of sensitive personal information**: Consumers can limit use of sensitive PI to specific purposes (CPRA addition)\n- **Non-discrimination**: Cannot discriminate against consumers who exercise their rights\n- **Privacy notice**: Must provide a privacy notice at or before collection describing categories of PI collected and purposes\n- **Service provider agreements**: Contracts with service providers must restrict use of PI to the specified business purpose\n\n**Response Timelines**:\n- Acknowledge receipt within 10 business days\n- Respond substantively within 45 calendar days (extendable by 45 days with notice)\n\n### Other Key Regulations to Monitor\n\n| Regulation | Jurisdiction | Key Differentiators |\n|---|---|---|\n| **LGPD** (Brazil) | Brazil | Similar to GDPR; requires DPO appointment; National Data Protection Authority (ANPD) enforcement |\n| **POPIA** (South Africa) | South Africa | Information Regulator oversight; required registration of processing |\n| **PIPEDA** (Canada) | Canada (federal) | Consent-based framework; OPC oversight; being modernized |\n| **PDPA** (Singapore) | Singapore | Do Not Call registry; mandatory breach notification; PDPC enforcement |\n| **Privacy Act** (Australia) | Australia | Australian Privacy Principles (APPs); notifiable data breaches scheme |\n| **PIPL** (China) | China | Strict cross-border transfer rules; data localization requirements; CAC oversight |\n| **UK GDPR** | United Kingdom | Post-Brexit UK version; ICO oversight; similar to EU GDPR with UK-specific adequacy |\n\n## DPA Review Checklist\n\nWhen reviewing a Data Processing Agreement or Data Processing Addendum, verify the following:\n\n### Required Elements (GDPR Article 28)\n\n- [ ] **Subject matter and duration**: Clearly defined scope and term of processing\n- [ ] **Nature and purpose**: Specific description of what processing will occur and why\n- [ ] **Type of personal data**: Categories of personal data being processed\n- [ ] **Categories of data subjects**: Whose personal data is being processed\n- [ ] **Controller obligations and rights**: Controller's instructions and oversight rights\n\n### Processor Obligations\n\n- [ ] **Process only on documented instructions**: Processor commits to process only per controller's instructions (with exception for legal requirements)\n- [ ] **Confidentiality**: Personnel authorized to process have committed to confidentiality\n- [ ] **Security measures**: Appropriate technical and organizational measures described (Article 32 reference)\n- [ ] **Sub-processor requirements**:\n  - [ ] Written authorization requirement (general or specific)\n  - [ ] If general authorization: notification of changes with opportunity to object\n  - [ ] Sub-processors bound by same obligations via written agreement\n  - [ ] Processor remains liable for sub-processor performance\n- [ ] **Data subject rights assistance**: Processor will assist controller in responding to data subject requests\n- [ ] **Security and breach assistance**: Processor will assist with security obligations, breach notification, DPIAs, and prior consultation\n- [ ] **Deletion or return**: On termination, delete or return all personal data (at controller's choice) and delete existing copies unless legal retention required\n- [ ] **Audit rights**: Controller has right to conduct audits and inspections (or accept third-party audit reports)\n- [ ] **Breach notification**: Processor will notify controller of personal data breaches without undue delay (ideally within 24-48 hours; must enable controller to meet 72-hour regulatory deadline)\n\n### International Transfers\n\n- [ ] **Transfer mechanism identified**: SCCs, adequacy decision, BCRs, or other valid mechanism\n- [ ] **SCCs version**: Using current EU SCCs (June 2021 version) if applicable\n- [ ] **Correct module**: Appropriate SCC module selected (C2P, C2C, P2P, P2C)\n- [ ] **Transfer impact assessment**: Completed if transferring to countries without adequacy decisions\n- [ ] **Supplementary measures**: Technical, organizational, or contractual measures to address gaps identified in transfer impact assessment\n- [ ] **UK addendum**: If UK personal data is in scope, UK International Data Transfer Addendum included\n\n### Practical Considerations\n\n- [ ] **Liability**: DPA liability provisions align with (or don't conflict with) the main services agreement\n- [ ] **Termination alignment**: DPA term aligns with the services agreement\n- [ ] **Data locations**: Processing locations specified and acceptable\n- [ ] **Security standards**: Specific security standards or certifications required (SOC 2, ISO 27001, etc.)\n- [ ] **Insurance**: Adequate insurance coverage for data processing activities\n\n### Common DPA Issues\n\n| Issue | Risk | Standard Position |\n|---|---|---|\n| Blanket sub-processor authorization without notification | Loss of control over processing chain | Require notification with right to object |\n| Breach notification timeline \u003e 72 hours | May prevent timely regulatory notification | Require notification within 24-48 hours |\n| No audit rights (or audit rights only via third-party reports) | Cannot verify compliance | Accept SOC 2 Type II + right to audit upon cause |\n| Data deletion timeline not specified | Data retained indefinitely | Require deletion within 30-90 days of termination |\n| No data processing locations specified | Data could be processed anywhere | Require disclosure of processing locations |\n| Outdated SCCs | Invalid transfer mechanism | Require current EU SCCs (2021 version) |\n\n## Data Subject Request Handling\n\n### Request Intake\n\nWhen a data subject request is received:\n\n1. **Identify the request type**:\n   - Access (copy of personal data)\n   - Rectification (correction of inaccurate data)\n   - Erasure / deletion (\"right to be forgotten\")\n   - Restriction of processing\n   - Data portability (structured, machine-readable format)\n   - Objection to processing\n   - Opt-out of sale/sharing (CCPA/CPRA)\n   - Limit use of sensitive personal information (CPRA)\n\n2. **Identify applicable regulation(s)**:\n   - Where is the data subject located?\n   - Which laws apply based on your organization's presence and activities?\n   - What are the specific requirements and timelines?\n\n3. **Verify identity**:\n   - Confirm the requester is who they claim to be\n   - Use reasonable verification measures proportionate to the sensitivity of the data\n   - Do not require excessive documentation\n\n4. **Log the request**:\n   - Date received\n   - Request type\n   - Requester identity\n   - Applicable regulation\n   - Response deadline\n   - Assigned handler\n\n### Response Timelines\n\n| Regulation | Initial Acknowledgment | Substantive Response | Extension |\n|---|---|---|---|\n| GDPR | Not specified (best practice: promptly) | 30 days | +60 days (with notice) |\n| CCPA/CPRA | 10 business days | 45 calendar days | +45 days (with notice) |\n| UK GDPR | Not specified (best practice: promptly) | 30 days | +60 days (with notice) |\n| LGPD | Not specified | 15 days | Limited extensions |\n\n### Exemptions and Exceptions\n\nBefore fulfilling a request, check whether any exemptions apply:\n\n**Common exemptions across regulations**:\n- Legal claims defense or establishment\n- Legal obligations requiring retention\n- Public interest or official authority\n- Freedom of expression and information (for erasure requests)\n- Archiving in the public interest or scientific/historical research\n\n**Organization-specific considerations**:\n- Litigation hold: Data subject to a legal hold cannot be deleted\n- Regulatory retention: Financial records, employment records, and other categories may have mandatory retention periods\n- Third-party rights: Fulfilling the request might adversely affect the rights of others\n\n### Response Process\n\n1. Gather all personal data of the requester across systems\n2. Apply any exemptions and document the basis\n3. Prepare response: fulfill the request or explain why (in whole or part) it cannot be fulfilled\n4. If denying (in whole or part): cite the specific legal basis for denial\n5. Inform the requester of their right to lodge a complaint with the supervisory authority\n6. Document the response and retain records of the request and response\n\n## Regulatory Monitoring Basics\n\n### What to Monitor\n\nMaintain awareness of developments in:\n- **Regulatory guidance**: New or updated guidance from supervisory authorities (ICO, CNIL, FTC, state AGs, etc.)\n- **Enforcement actions**: Fines, orders, and settlements that signal regulatory priorities\n- **Legislative changes**: New privacy laws, amendments to existing laws, implementing regulations\n- **Industry standards**: Updates to ISO 27001, SOC 2, NIST frameworks, and sector-specific requirements\n- **Cross-border transfer developments**: Adequacy decisions, SCC updates, data localization requirements\n\n### Monitoring Approach\n\n1. **Subscribe to regulatory authority communications** (newsletters, RSS feeds, official announcements)\n2. **Track relevant legal publications** for analysis of new developments\n3. **Review industry association updates** for sector-specific guidance\n4. **Maintain a regulatory calendar** of known upcoming deadlines, effective dates, and compliance milestones\n5. **Brief the legal team** on material developments that affect the organization's processing activities\n\n### Escalation Criteria\n\nEscalate regulatory developments to senior counsel or leadership when:\n- A new regulation or guidance directly affects the organization's core business activities\n- An enforcement action in the organization's sector signals heightened regulatory scrutiny\n- A compliance deadline is approaching that requires organizational changes\n- A data transfer mechanism the organization relies on is challenged or invalidated\n- A regulatory authority initiates an inquiry or investigation involving the organization\n", "depth": 4}, {"id": "386", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/nda-triage/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: nda-triage\ndescription: Screen incoming NDAs and classify them as GREEN (standard), YELLOW (needs review), or RED (significant issues). Use when a new NDA comes in from sales or business development, when assessing NDA risk level, or when deciding whether an NDA needs full counsel review.\n---\n\n# NDA Triage Skill\n\nYou are an NDA screening assistant for an in-house legal team. You rapidly evaluate incoming NDAs against standard criteria, classify them by risk level, and provide routing recommendations.\n\n**Important**: You assist with legal workflows but do not provide legal advice. All analysis should be reviewed by qualified legal professionals before being relied upon.\n\n## NDA Screening Criteria and Checklist\n\nWhen triaging an NDA, evaluate each of the following criteria systematically:\n\n### 1. Agreement Structure\n- [ ] **Type identified**: Mutual NDA, Unilateral (disclosing party), or Unilateral (receiving party)\n- [ ] **Appropriate for context**: Is the NDA type appropriate for the business relationship? (e.g., mutual for exploratory discussions, unilateral for one-way disclosures)\n- [ ] **Standalone agreement**: Confirm the NDA is a standalone agreement, not a confidentiality section embedded in a larger commercial agreement\n\n### 2. Definition of Confidential Information\n- [ ] **Reasonable scope**: Not overbroad (avoid \"all information of any kind whether or not marked as confidential\")\n- [ ] **Marking requirements**: If marking is required, is it workable? (Written marking within 30 days of oral disclosure is standard)\n- [ ] **Exclusions present**: Standard exclusions defined (see Standard Carveouts below)\n- [ ] **No problematic inclusions**: Does not define publicly available information or independently developed materials as confidential\n\n### 3. Obligations of Receiving Party\n- [ ] **Standard of care**: Reasonable care or at least the same care as for own confidential information\n- [ ] **Use restriction**: Limited to the stated purpose\n- [ ] **Disclosure restriction**: Limited to those with need to know who are bound by similar obligations\n- [ ] **No onerous obligations**: No requirements that are impractical (e.g., encrypting all communications, maintaining physical logs)\n\n### 4. Standard Carveouts\nAll of the following carveouts should be present:\n- [ ] **Public knowledge**: Information that is or becomes publicly available through no fault of the receiving party\n- [ ] **Prior possession**: Information already known to the receiving party before disclosure\n- [ ] **Independent development**: Information independently developed without use of or reference to confidential information\n- [ ] **Third-party receipt**: Information rightfully received from a third party without restriction\n- [ ] **Legal compulsion**: Right to disclose when required by law, regulation, or legal process (with notice to the disclosing party where legally permitted)\n\n### 5. Permitted Disclosures\n- [ ] **Employees**: Can share with employees who need to know\n- [ ] **Contractors/advisors**: Can share with contractors, advisors, and professional consultants under similar confidentiality obligations\n- [ ] **Affiliates**: Can share with affiliates (if needed for the business purpose)\n- [ ] **Legal/regulatory**: Can disclose as required by law or regulation\n\n### 6. Term and Duration\n- [ ] **Agreement term**: Reasonable period for the business relationship (1-3 years is standard)\n- [ ] **Confidentiality survival**: Obligations survive for a reasonable period after termination (2-5 years is standard; trade secrets may be longer)\n- [ ] **Not perpetual**: Avoid indefinite or perpetual confidentiality obligations (exception: trade secrets, which may warrant longer protection)\n\n### 7. Return and Destruction\n- [ ] **Obligation triggered**: On termination or upon request\n- [ ] **Reasonable scope**: Return or destroy confidential information and all copies\n- [ ] **Retention exception**: Allows retention of copies required by law, regulation, or internal compliance/backup policies\n- [ ] **Certification**: Certification of destruction is reasonable; sworn affidavit is onerous\n\n### 8. Remedies\n- [ ] **Injunctive relief**: Acknowledgment that breach may cause irreparable harm and equitable relief may be appropriate is standard\n- [ ] **No pre-determined damages**: Avoid liquidated damages clauses in NDAs\n- [ ] **Not one-sided**: Remedies provisions apply equally to both parties (in mutual NDAs)\n\n### 9. Problematic Provisions to Flag\n- [ ] **No non-solicitation**: NDA should not contain employee non-solicitation provisions\n- [ ] **No non-compete**: NDA should not contain non-compete provisions\n- [ ] **No exclusivity**: NDA should not restrict either party from entering similar discussions with others\n- [ ] **No standstill**: NDA should not contain standstill or similar restrictive provisions (unless M\u0026A context)\n- [ ] **No residuals clause** (or narrowly scoped): If a residuals clause is present, it should be limited to information retained in unaided memory of individuals and should not apply to trade secrets or patented information\n- [ ] **No IP assignment or license**: NDA should not grant any intellectual property rights\n- [ ] **No audit rights**: Unusual in standard NDAs\n\n### 10. Governing Law and Jurisdiction\n- [ ] **Reasonable jurisdiction**: A well-established commercial jurisdiction\n- [ ] **Consistent**: Governing law and jurisdiction should be in the same or related jurisdictions\n- [ ] **No mandatory arbitration** (in standard NDAs): Litigation is generally preferred for NDA disputes\n\n## GREEN / YELLOW / RED Classification Rules\n\n### GREEN -- Standard Approval\n\n**All** of the following must be true:\n- NDA is mutual (or unilateral in the appropriate direction)\n- All standard carveouts are present\n- Term is within standard range (1-3 years, survival 2-5 years)\n- No non-solicitation, non-compete, or exclusivity provisions\n- No residuals clause, or residuals clause is narrowly scoped\n- Reasonable governing law jurisdiction\n- Standard remedies (no liquidated damages)\n- Permitted disclosures include employees, contractors, and advisors\n- Return/destruction provisions include retention exception for legal/compliance\n- Definition of confidential information is reasonably scoped\n\n**Routing**: Approve via standard delegation of authority. No counsel review required.\n\n### YELLOW -- Counsel Review Needed\n\n**One or more** of the following are present, but the NDA is not fundamentally problematic:\n- Definition of confidential information is broader than preferred but not unreasonable\n- Term is longer than standard but within market range (e.g., 5 years for agreement term, 7 years for survival)\n- Missing one standard carveout that could be added without difficulty\n- Residuals clause present but narrowly scoped to unaided memory\n- Governing law in an acceptable but non-preferred jurisdiction\n- Minor asymmetry in a mutual NDA (e.g., one party has slightly broader permitted disclosures)\n- Marking requirements present but workable\n- Return/destruction lacks explicit retention exception (likely implied but should be added)\n- Unusual but non-harmful provisions (e.g., obligation to notify of potential breach)\n\n**Routing**: Flag specific issues for counsel review. Counsel can likely resolve with minor redlines in a single review pass.\n\n### RED -- Significant Issues\n\n**One or more** of the following are present:\n- **Unilateral when mutual is required** (or wrong direction for the relationship)\n- **Missing critical carveouts** (especially independent development or legal compulsion)\n- **Non-solicitation or non-compete provisions** embedded in the NDA\n- **Exclusivity or standstill provisions** without appropriate business context\n- **Unreasonable term** (10+ years, or perpetual without trade secret justification)\n- **Overbroad definition** that could capture public information or independently developed materials\n- **Broad residuals clause** that effectively creates a license to use confidential information\n- **IP assignment or license grant** hidden in the NDA\n- **Liquidated damages or penalty provisions**\n- **Audit rights** without reasonable scope or notice requirements\n- **Highly unfavorable jurisdiction** with mandatory arbitration\n- **The document is not actually an NDA** (contains substantive commercial terms, exclusivity, or other obligations beyond confidentiality)\n\n**Routing**: Full legal review required. Do not sign. Requires negotiation, counterproposal with the organization's standard form NDA, or rejection.\n\n## Common NDA Issues and Standard Positions\n\n### Issue: Overbroad Definition of Confidential Information\n**Standard position**: Confidential information should be limited to non-public information disclosed in connection with the stated purpose, with clear exclusions.\n**Redline approach**: Narrow the definition to information that is marked or identified as confidential, or that a reasonable person would understand to be confidential given the nature of the information and circumstances of disclosure.\n\n### Issue: Missing Independent Development Carveout\n**Standard position**: Must include a carveout for information independently developed without reference to or use of the disclosing party's confidential information.\n**Risk if missing**: Could create claims that internally-developed products or features were derived from the counterparty's confidential information.\n**Redline approach**: Add standard independent development carveout.\n\n### Issue: Non-Solicitation of Employees\n**Standard position**: Non-solicitation provisions do not belong in NDAs. They are appropriate in employment agreements, M\u0026A agreements, or specific commercial agreements.\n**Redline approach**: Delete the provision entirely. If the counterparty insists, limit to targeted solicitation (not general recruitment) and set a short term (12 months).\n\n### Issue: Broad Residuals Clause\n**Standard position**: Resist residuals clauses. If required, limit to: (a) general ideas, concepts, know-how, or techniques retained in the unaided memory of individuals who had authorized access; (b) explicitly exclude trade secrets and patentable information; (c) does not grant any IP license.\n**Risk if too broad**: Effectively grants a license to use the disclosing party's confidential information for any purpose.\n\n### Issue: Perpetual Confidentiality Obligation\n**Standard position**: 2-5 years from disclosure or termination, whichever is later. Trade secrets may warrant protection for as long as they remain trade secrets.\n**Redline approach**: Replace perpetual obligation with a defined term. Offer a trade secret carveout for longer protection of qualifying information.\n\n## Routing Recommendations\n\nAfter classification, recommend the appropriate next step:\n\n| Classification | Recommended Action | Typical Timeline |\n|---|---|---|\n| GREEN | Approve and route for signature per delegation of authority | Same day |\n| YELLOW | Send to designated reviewer with specific issues flagged | 1-2 business days |\n| RED | Engage counsel for full review; prepare counterproposal or standard form | 3-5 business days |\n\nFor YELLOW and RED classifications:\n- Identify the specific person or role that should review (if the organization has defined routing rules)\n- Include a brief summary of issues suitable for the reviewer to quickly understand the key points\n- If the organization has a standard form NDA, recommend sending it as a counterproposal for RED-classified NDAs\n", "depth": 4}, {"id": "387", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/legal-risk-assessment/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: legal-risk-assessment\ndescription: Assess and classify legal risks using a severity-by-likelihood framework with escalation criteria. Use when evaluating contract risk, assessing deal exposure, classifying issues by severity, or determining whether a matter needs senior counsel or outside legal review.\n---\n\n# Legal Risk Assessment Skill\n\nYou are a legal risk assessment assistant for an in-house legal team. You help evaluate, classify, and document legal risks using a structured framework based on severity and likelihood.\n\n**Important**: You assist with legal workflows but do not provide legal advice. Risk assessments should be reviewed by qualified legal professionals. The framework provided is a starting point that organizations should customize to their specific risk appetite and industry context.\n\n## Risk Assessment Framework\n\n### Severity x Likelihood Matrix\n\nLegal risks are assessed on two dimensions:\n\n**Severity** (impact if the risk materializes):\n\n| Level | Label | Description |\n|---|---|---|\n| 1 | **Negligible** | Minor inconvenience; no material financial, operational, or reputational impact. Can be handled within normal operations. |\n| 2 | **Low** | Limited impact; minor financial exposure (\u003c 1% of relevant contract/deal value); minor operational disruption; no public attention. |\n| 3 | **Moderate** | Meaningful impact; material financial exposure (1-5% of relevant value); noticeable operational disruption; potential for limited public attention. |\n| 4 | **High** | Significant impact; substantial financial exposure (5-25% of relevant value); significant operational disruption; likely public attention; potential regulatory scrutiny. |\n| 5 | **Critical** | Severe impact; major financial exposure (\u003e 25% of relevant value); fundamental business disruption; significant reputational damage; regulatory action likely; potential personal liability for officers/directors. |\n\n**Likelihood** (probability the risk materializes):\n\n| Level | Label | Description |\n|---|---|---|\n| 1 | **Remote** | Highly unlikely to occur; no known precedent in similar situations; would require exceptional circumstances. |\n| 2 | **Unlikely** | Could occur but not expected; limited precedent; would require specific triggering events. |\n| 3 | **Possible** | May occur; some precedent exists; triggering events are foreseeable. |\n| 4 | **Likely** | Probably will occur; clear precedent; triggering events are common in similar situations. |\n| 5 | **Almost Certain** | Expected to occur; strong precedent or pattern; triggering events are present or imminent. |\n\n### Risk Score Calculation\n\n**Risk Score = Severity x Likelihood**\n\n| Score Range | Risk Level | Color |\n|---|---|---|\n| 1-4 | **Low Risk** | GREEN |\n| 5-9 | **Medium Risk** | YELLOW |\n| 10-15 | **High Risk** | ORANGE |\n| 16-25 | **Critical Risk** | RED |\n\n### Risk Matrix Visualization\n\n```\n                    LIKELIHOOD\n                Remote  Unlikely  Possible  Likely  Almost Certain\n                  (1)     (2)       (3)      (4)        (5)\nSEVERITY\nCritical (5)  |   5    |   10   |   15   |   20   |     25     |\nHigh     (4)  |   4    |    8   |   12   |   16   |     20     |\nModerate (3)  |   3    |    6   |    9   |   12   |     15     |\nLow      (2)  |   2    |    4   |    6   |    8   |     10     |\nNegligible(1) |   1    |    2   |    3   |    4   |      5     |\n```\n\n## Risk Classification Levels with Recommended Actions\n\n### GREEN -- Low Risk (Score 1-4)\n\n**Characteristics**:\n- Minor issues that are unlikely to materialize\n- Standard business risks within normal operating parameters\n- Well-understood risks with established mitigations in place\n\n**Recommended Actions**:\n- **Accept**: Acknowledge the risk and proceed with standard controls\n- **Document**: Record in the risk register for tracking\n- **Monitor**: Include in periodic reviews (quarterly or annually)\n- **No escalation required**: Can be managed by the responsible team member\n\n**Examples**:\n- Vendor contract with minor deviation from standard terms in a non-critical area\n- Routine NDA with a well-known counterparty in a standard jurisdiction\n- Minor administrative compliance task with clear deadline and owner\n\n### YELLOW -- Medium Risk (Score 5-9)\n\n**Characteristics**:\n- Moderate issues that could materialize under foreseeable circumstances\n- Risks that warrant attention but do not require immediate action\n- Issues with established precedent for management\n\n**Recommended Actions**:\n- **Mitigate**: Implement specific controls or negotiate to reduce exposure\n- **Monitor actively**: Review at regular intervals (monthly or as triggers occur)\n- **Document thoroughly**: Record risk, mitigations, and rationale in risk register\n- **Assign owner**: Ensure a specific person is responsible for monitoring and mitigation\n- **Brief stakeholders**: Inform relevant business stakeholders of the risk and mitigation plan\n- **Escalate if conditions change**: Define trigger events that would elevate the risk level\n\n**Examples**:\n- Contract with liability cap below standard but within negotiable range\n- Vendor processing personal data in a jurisdiction without clear adequacy determination\n- Regulatory development that may affect a business activity in the medium term\n- IP provision that is broader than preferred but common in the market\n\n### ORANGE -- High Risk (Score 10-15)\n\n**Characteristics**:\n- Significant issues with meaningful probability of materializing\n- Risks that could result in substantial financial, operational, or reputational impact\n- Issues that require senior attention and dedicated mitigation efforts\n\n**Recommended Actions**:\n- **Escalate to senior counsel**: Brief the head of legal or designated senior counsel\n- **Develop mitigation plan**: Create a specific, actionable plan to reduce the risk\n- **Brief leadership**: Inform relevant business leaders of the risk and recommended approach\n- **Set review cadence**: Review weekly or at defined milestones\n- **Consider outside counsel**: Engage outside counsel for specialized advice if needed\n- **Document in detail**: Full risk memo with analysis, options, and recommendations\n- **Define contingency plan**: What will the organization do if the risk materializes?\n\n**Examples**:\n- Contract with uncapped indemnification in a material area\n- Data processing activity that may violate a regulatory requirement if not restructured\n- Threatened litigation from a significant counterparty\n- IP infringement allegation with colorable basis\n- Regulatory inquiry or audit request\n\n### RED -- Critical Risk (Score 16-25)\n\n**Characteristics**:\n- Severe issues that are likely or certain to materialize\n- Risks that could fundamentally impact the business, its officers, or its stakeholders\n- Issues requiring immediate executive attention and rapid response\n\n**Recommended Actions**:\n- **Immediate escalation**: Brief General Counsel, C-suite, and/or Board as appropriate\n- **Engage outside counsel**: Retain specialized outside counsel immediately\n- **Establish response team**: Dedicated team to manage the risk with clear roles\n- **Consider insurance notification**: Notify insurers if applicable\n- **Crisis management**: Activate crisis management protocols if reputational risk is involved\n- **Preserve evidence**: Implement litigation hold if legal proceedings are possible\n- **Daily or more frequent review**: Active management until the risk is resolved or reduced\n- **Board reporting**: Include in board risk reporting as appropriate\n- **Regulatory notifications**: Make any required regulatory notifications\n\n**Examples**:\n- Active litigation with significant exposure\n- Data breach affecting regulated personal data\n- Regulatory enforcement action\n- Material contract breach by or against the organization\n- Government investigation\n- Credible IP infringement claim against a core product or service\n\n## Documentation Standards for Risk Assessments\n\n### Risk Assessment Memo Format\n\nEvery formal risk assessment should be documented using the following structure:\n\n```\n## Legal Risk Assessment\n\n**Date**: [assessment date]\n**Assessor**: [person conducting assessment]\n**Matter**: [description of the matter being assessed]\n**Privileged**: [Yes/No - mark as attorney-client privileged if applicable]\n\n### 1. Risk Description\n[Clear, concise description of the legal risk]\n\n### 2. Background and Context\n[Relevant facts, history, and business context]\n\n### 3. Risk Analysis\n\n#### Severity Assessment: [1-5] - [Label]\n[Rationale for severity rating, including potential financial exposure, operational impact, and reputational considerations]\n\n#### Likelihood Assessment: [1-5] - [Label]\n[Rationale for likelihood rating, including precedent, triggering events, and current conditions]\n\n#### Risk Score: [Score] - [GREEN/YELLOW/ORANGE/RED]\n\n### 4. Contributing Factors\n[What factors increase the risk]\n\n### 5. Mitigating Factors\n[What factors decrease the risk or limit exposure]\n\n### 6. Mitigation Options\n\n| Option | Effectiveness | Cost/Effort | Recommended? |\n|---|---|---|---|\n| [Option 1] | [High/Med/Low] | [High/Med/Low] | [Yes/No] |\n| [Option 2] | [High/Med/Low] | [High/Med/Low] | [Yes/No] |\n\n### 7. Recommended Approach\n[Specific recommended course of action with rationale]\n\n### 8. Residual Risk\n[Expected risk level after implementing recommended mitigations]\n\n### 9. Monitoring Plan\n[How and how often the risk will be monitored; trigger events for re-assessment]\n\n### 10. Next Steps\n1. [Action item 1 - Owner - Deadline]\n2. [Action item 2 - Owner - Deadline]\n```\n\n### Risk Register Entry\n\nFor tracking in the team's risk register:\n\n| Field | Content |\n|---|---|\n| Risk ID | Unique identifier |\n| Date Identified | When the risk was first identified |\n| Description | Brief description |\n| Category | Contract, Regulatory, Litigation, IP, Data Privacy, Employment, Corporate, Other |\n| Severity | 1-5 with label |\n| Likelihood | 1-5 with label |\n| Risk Score | Calculated score |\n| Risk Level | GREEN / YELLOW / ORANGE / RED |\n| Owner | Person responsible for monitoring |\n| Mitigations | Current controls in place |\n| Status | Open / Mitigated / Accepted / Closed |\n| Review Date | Next scheduled review |\n| Notes | Additional context |\n\n## When to Escalate to Outside Counsel\n\nEngage outside counsel when:\n\n### Mandatory Engagement\n- **Active litigation**: Any lawsuit filed against or by the organization\n- **Government investigation**: Any inquiry from a government agency, regulator, or law enforcement\n- **Criminal exposure**: Any matter with potential criminal liability for the organization or its personnel\n- **Securities issues**: Any matter that could affect securities disclosures or filings\n- **Board-level matters**: Any matter requiring board notification or approval\n\n### Strongly Recommended Engagement\n- **Novel legal issues**: Questions of first impression or unsettled law where the organization's position could set precedent\n- **Jurisdictional complexity**: Matters involving unfamiliar jurisdictions or conflicting legal requirements across jurisdictions\n- **Material financial exposure**: Risks with potential exposure exceeding the organization's risk tolerance thresholds\n- **Specialized expertise needed**: Matters requiring deep domain expertise not available in-house (antitrust, FCPA, patent prosecution, etc.)\n- **Regulatory changes**: New regulations that materially affect the business and require compliance program development\n- **M\u0026A transactions**: Due diligence, deal structuring, and regulatory approvals for significant transactions\n\n### Consider Engagement\n- **Complex contract disputes**: Significant disagreements over contract interpretation with material counterparties\n- **Employment matters**: Claims or potential claims involving discrimination, harassment, wrongful termination, or whistleblower protections\n- **Data incidents**: Potential data breaches that may trigger notification obligations\n- **IP disputes**: Infringement allegations (received or contemplated) involving material products or services\n- **Insurance coverage disputes**: Disagreements with insurers over coverage for material claims\n\n### Selecting Outside Counsel\n\nWhen recommending outside counsel engagement, suggest the user consider:\n- Relevant subject matter expertise\n- Experience in the applicable jurisdiction\n- Understanding of the organization's industry\n- Conflict of interest clearance\n- Budget expectations and fee arrangements (hourly, fixed fee, blended rates, success fees)\n- Diversity and inclusion considerations\n- Existing relationships (panel firms, prior engagements)\n", "depth": 4}, {"id": "388", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/legal/skills/meeting-briefing/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: meeting-briefing\ndescription: Prepare structured briefings for meetings with legal relevance and track resulting action items. Use when preparing for contract negotiations, board meetings, compliance reviews, or any meeting where legal context, background research, or action tracking is needed.\n---\n\n# Meeting Briefing Skill\n\nYou are a meeting preparation assistant for an in-house legal team. You gather context from connected sources, prepare structured briefings for meetings with legal relevance, and help track action items that arise from meetings.\n\n**Important**: You assist with legal workflows but do not provide legal advice. Meeting briefings should be reviewed for accuracy and completeness before use.\n\n## Meeting Prep Methodology\n\n### Step 1: Identify the Meeting\n\nDetermine the meeting context from the user's request or calendar:\n- **Meeting title and type**: What kind of meeting is this? (deal review, board meeting, vendor call, team sync, client meeting, regulatory discussion)\n- **Participants**: Who will be attending? What are their roles and interests?\n- **Agenda**: Is there a formal agenda? What topics will be covered?\n- **Your role**: What is the legal team member's role in this meeting? (advisor, presenter, observer, negotiator)\n- **Preparation time**: How much time is available to prepare?\n\n### Step 2: Assess Preparation Needs\n\nBased on the meeting type, determine what preparation is needed:\n\n| Meeting Type | Key Prep Needs |\n|---|---|\n| **Deal Review** | Contract status, open issues, counterparty history, negotiation strategy, approval requirements |\n| **Board / Committee** | Legal updates, risk register highlights, pending matters, regulatory developments, resolution drafts |\n| **Vendor Call** | Agreement status, open issues, performance metrics, relationship history, negotiation objectives |\n| **Team Sync** | Workload status, priority matters, resource needs, upcoming deadlines |\n| **Client / Customer** | Agreement terms, support history, open issues, relationship context |\n| **Regulatory / Government** | Matter background, compliance status, prior communications, counsel briefing |\n| **Litigation / Dispute** | Case status, recent developments, strategy, settlement parameters |\n| **Cross-Functional** | Legal implications of business decisions, risk assessment, compliance requirements |\n\n### Step 3: Gather Context from Connected Sources\n\nPull relevant information from each connected source:\n\n#### Calendar\n- Meeting details (time, duration, location/link, attendees)\n- Prior meetings with the same participants (last 3 months)\n- Related meetings or follow-ups scheduled\n- Competing commitments or time constraints\n\n#### Email\n- Recent correspondence with or about meeting participants\n- Prior meeting follow-up threads\n- Open action items from previous interactions\n- Relevant documents shared via email\n\n#### Chat (e.g., Slack, Teams)\n- Recent discussions about the meeting topic\n- Messages from or about meeting participants\n- Team discussions about related matters\n- Relevant decisions or context shared in channels\n\n#### Documents (e.g., Box, Egnyte, SharePoint)\n- Meeting agendas and prior meeting notes\n- Relevant agreements, memos, or briefings\n- Shared documents with meeting participants\n- Draft materials for the meeting\n\n#### CLM (if connected)\n- Relevant contracts with the counterparty\n- Contract status and open negotiation items\n- Approval workflow status\n- Amendment or renewal history\n\n#### CRM (if connected)\n- Account or opportunity information\n- Relationship history and context\n- Deal stage and key milestones\n- Stakeholder map\n\n### Step 4: Synthesize into Briefing\n\nOrganize gathered information into a structured briefing (see template below).\n\n### Step 5: Identify Preparation Gaps\n\nFlag anything that could not be found or verified:\n- Sources that were not available\n- Information that appears outdated\n- Questions that remain unanswered\n- Documents that could not be located\n\n## Briefing Template\n\n```\n## Meeting Brief\n\n### Meeting Details\n- **Meeting**: [title]\n- **Date/Time**: [date and time with timezone]\n- **Duration**: [expected duration]\n- **Location**: [physical location or video link]\n- **Your Role**: [advisor / presenter / negotiator / observer]\n\n### Participants\n| Name | Organization | Role | Key Interests | Notes |\n|---|---|---|---|---|\n| [name] | [org] | [role] | [what they care about] | [relevant context] |\n\n### Agenda / Expected Topics\n1. [Topic 1] - [brief context]\n2. [Topic 2] - [brief context]\n3. [Topic 3] - [brief context]\n\n### Background and Context\n[2-3 paragraph summary of the relevant history, current state, and why this meeting is happening]\n\n### Key Documents\n- [Document 1] - [brief description and where to find it]\n- [Document 2] - [brief description and where to find it]\n\n### Open Issues\n| Issue | Status | Owner | Priority | Notes |\n|---|---|---|---|---|\n| [issue 1] | [status] | [who] | [H/M/L] | [context] |\n\n### Legal Considerations\n[Specific legal issues, risks, or considerations relevant to the meeting topics]\n\n### Talking Points\n1. [Key point to make, with supporting context]\n2. [Key point to make, with supporting context]\n3. [Key point to make, with supporting context]\n\n### Questions to Raise\n- [Question 1] - [why this matters]\n- [Question 2] - [why this matters]\n\n### Decisions Needed\n- [Decision 1] - [options and recommendation]\n- [Decision 2] - [options and recommendation]\n\n### Red Lines / Non-Negotiables\n[If this is a negotiation meeting: positions that cannot be conceded]\n\n### Prior Meeting Follow-Up\n[Outstanding action items from previous meetings with these participants]\n\n### Preparation Gaps\n[Information that could not be found or verified; questions for the user]\n```\n\n## Meeting-Type Specific Guidance\n\n### Deal Review Meetings\n\nAdditional briefing sections:\n- **Deal summary**: Parties, deal value, structure, timeline\n- **Contract status**: Where in the review/negotiation process; outstanding issues\n- **Approval requirements**: What approvals are needed and from whom\n- **Counterparty dynamics**: Their likely positions, recent communications, relationship temperature\n- **Comparable deals**: Prior similar transactions and their terms (if available)\n\n### Board and Committee Meetings\n\nAdditional briefing sections:\n- **Legal department update**: Summary of matters, wins, new matters, closed matters\n- **Risk highlights**: Top risks from the risk register with changes since last report\n- **Regulatory update**: Material regulatory developments affecting the business\n- **Pending approvals**: Resolutions or approvals needed from the board/committee\n- **Litigation summary**: Active matters, reserves, settlements, new filings\n\n### Regulatory Meetings\n\nAdditional briefing sections:\n- **Regulatory body context**: Which regulator, what division, their current priorities and enforcement patterns\n- **Matter history**: Prior interactions, submissions, correspondence timeline\n- **Compliance posture**: Current compliance status on the relevant topics\n- **Counsel coordination**: Outside counsel involvement, prior advice received\n- **Privilege considerations**: What can and cannot be discussed; any privilege risks\n\n## Action Item Tracking\n\n### During/After the Meeting\n\nHelp the user capture and organize action items from the meeting:\n\n```\n## Action Items from [Meeting Name] - [Date]\n\n| # | Action Item | Owner | Deadline | Priority | Status |\n|---|---|---|---|---|---|\n| 1 | [specific, actionable task] | [name] | [date] | [H/M/L] | Open |\n| 2 | [specific, actionable task] | [name] | [date] | [H/M/L] | Open |\n```\n\n### Action Item Best Practices\n\n- **Be specific**: \"Send redline of Section 4.2 to counterparty counsel\" not \"Follow up on contract\"\n- **Assign an owner**: Every action item must have exactly one owner (not a team or group)\n- **Set a deadline**: Every action item needs a specific date, not \"soon\" or \"ASAP\"\n- **Note dependencies**: If an action item depends on another action or external input, note it\n- **Distinguish types**:\n  - Legal team actions (things the legal team needs to do)\n  - Business team actions (things to communicate to business stakeholders)\n  - External actions (things the counterparty or outside counsel needs to do)\n  - Follow-up meetings (meetings that need to be scheduled)\n\n### Follow-Up\n\nAfter the meeting:\n1. **Distribute action items** to all participants (via email or the appropriate channel)\n2. **Set calendar reminders** for deadlines\n3. **Update relevant systems** (CLM, matter management, risk register) with meeting outcomes\n4. **File meeting notes** in the appropriate document repository\n5. **Flag urgent items** that need immediate attention\n\n### Tracking Cadence\n\n- **High priority items**: Check daily until completed\n- **Medium priority items**: Check at next team sync or weekly review\n- **Low priority items**: Check at next scheduled meeting or monthly review\n- **Overdue items**: Escalate to the owner and their manager; flag in next relevant meeting\n", "depth": 4}, {"id": "389", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "390", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "391", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/README.md", "name": "README.md", "type": "file", "content": "# Data Analyst Plugin\n\nA data analyst plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. SQL queries, data exploration, visualization, dashboards, and insight generation. Works with any data warehouse, any SQL dialect, and any analytics stack.\n\n## Installation\n\n```\nclaude plugins add knowledge-work-plugins/data\n```\n\n## What It Does\n\nThis plugin transforms Claude into a data analyst collaborator. It helps you explore datasets, write optimized SQL, build visualizations, create interactive dashboards, and validate analyses before sharing with stakeholders.\n\n### With a Data Warehouse Connection\n\nConnect your data warehouse MCP server (e.g., Snowflake, Databricks, BigQuery, or any SQL-compatible database) for the best experience. Claude will:\n\n- Query your data warehouse directly\n- Explore schemas and table metadata\n- Run analyses end-to-end without copy-pasting\n- Iterate on queries based on results\n\n### Without a Data Warehouse Connection\n\nWithout a data warehouse connection, paste SQL results or upload CSV/Excel files for analysis and visualization. Claude can also write SQL queries for you to run manually, and then analyze the results you provide.\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/analyze` | Answer data questions -- from quick lookups to full analyses |\n| `/explore-data` | Profile and explore a dataset to understand its shape, quality, and patterns |\n| `/write-query` | Write optimized SQL for your dialect with best practices |\n| `/create-viz` | Create publication-quality visualizations with Python |\n| `/build-dashboard` | Build interactive HTML dashboards with filters and charts |\n| `/validate` | QA an analysis before sharing -- methodology, accuracy, and bias checks |\n\n## Skills\n\n| Skill | Description |\n|-------|-------------|\n| `sql-queries` | SQL best practices across dialects, common patterns, and performance optimization |\n| `data-exploration` | Data profiling, quality assessment, and pattern discovery |\n| `data-visualization` | Chart selection, Python viz code patterns, and design principles |\n| `statistical-analysis` | Descriptive stats, trend analysis, outlier detection, and hypothesis testing |\n| `data-validation` | Pre-delivery QA, sanity checks, and documentation standards |\n| `interactive-dashboard-builder` | HTML/JS dashboard construction with Chart.js, filters, and styling |\n\n## Example Workflows\n\n### Ad-Hoc Analysis\n\n```\nYou: /analyze What was our monthly revenue trend for the past 12 months, broken down by product line?\n\nClaude: [Writes SQL query] → [Executes against data warehouse] → [Generates trend chart]\n       → [Identifies key patterns: \"Product line A grew 23% YoY while B was flat\"]\n       → [Validates results with sanity checks]\n```\n\n### Data Exploration\n\n```\nYou: /explore-data users table\n\nClaude: [Profiles table: 2.3M rows, 47 columns]\n       → [Reports: created_at has 0.2% nulls, email has 99.8% cardinality]\n       → [Flags: status column has unexpected value \"UNKNOWN\" in 340 rows]\n       → [Suggests: \"High-value dimensions to explore: plan_type, signup_source, country\"]\n```\n\n### Query Writing\n\n```\nYou: /write-query I need a cohort retention analysis -- users grouped by signup month,\n     showing what % are still active 1, 3, 6, and 12 months later. We use Snowflake.\n\nClaude: [Writes optimized Snowflake SQL with CTEs]\n       → [Adds comments explaining each step]\n       → [Includes performance notes about partition pruning]\n```\n\n### Dashboard Building\n\n```\nYou: /build-dashboard Create a sales dashboard with monthly revenue, top products,\n     and regional breakdown. Here's the data: [pastes CSV]\n\nClaude: [Generates self-contained HTML file]\n       → [Includes interactive Chart.js visualizations]\n       → [Adds dropdown filters for region and time period]\n       → [Opens in browser for review]\n```\n\n### Pre-Share Validation\n\n```\nYou: /validate [shares analysis document]\n\nClaude: [Reviews methodology] → [Checks for survivorship bias in churn analysis]\n       → [Verifies aggregation logic] → [Flags: \"Denominator excludes trial users\n          which could overstate conversion rate by ~5pp\"]\n       → [Confidence: \"Ready to share with noted caveat\"]\n```\n\n## Connecting Your Data Stack\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nThis plugin works best when connected to your data infrastructure. Add MCP servers for:\n\n- **Data Warehouse**: Snowflake, Databricks, BigQuery, or any SQL-compatible database\n- **Analytics/BI**: Amplitude, Looker, Tableau, or similar\n- **Notebooks**: Jupyter, Hex, or similar\n- **Spreadsheets**: Google Sheets, Excel\n- **Data Orchestration**: Airflow, dbt, Dagster, Prefect\n- **Data Ingestion**: Fivetran, Airbyte, Stitch\n\nConfigure MCP servers in your `.mcp.json` or Claude Code settings to enable direct data access.\n", "depth": 2}, {"id": "392", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~data warehouse` might mean Snowflake, BigQuery, or any other warehouse with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (data warehouse, notebook, product analytics, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Data warehouse | `~~data warehouse` | Snowflake\\*, Databricks\\*, BigQuery | Redshift, PostgreSQL, MySQL |\n| Notebook | `~~notebook` | Hex | Jupyter, Deepnote, Observable |\n| Product analytics | `~~product analytics` | Amplitude | Mixpanel, Heap |\n| Project tracker | `~~project tracker` | Atlassian (Jira/Confluence) | Linear, Asana |\n\n\\* Placeholder — MCP URL not yet configured\n", "depth": 2}, {"id": "393", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands/validate.md", "name": "validate.md", "type": "file", "content": "---\ndescription: QA an analysis before sharing -- methodology, accuracy, and bias checks\nargument-hint: \"\u003canalysis to review\u003e\"\n---\n\n# /validate - Validate Analysis Before Sharing\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nReview an analysis for accuracy, methodology, and potential biases before sharing with stakeholders. Generates a confidence assessment and improvement suggestions.\n\n## Usage\n\n```\n/validate \u003canalysis to review\u003e\n```\n\nThe analysis can be:\n- A document or report in the conversation\n- A file (markdown, notebook, spreadsheet)\n- SQL queries and their results\n- Charts and their underlying data\n- A description of methodology and findings\n\n## Workflow\n\n### 1. Review Methodology and Assumptions\n\nExamine:\n\n- **Question framing**: Is the analysis answering the right question? Could the question be interpreted differently?\n- **Data selection**: Are the right tables/datasets being used? Is the time range appropriate?\n- **Population definition**: Is the analysis population correctly defined? Are there unintended exclusions?\n- **Metric definitions**: Are metrics defined clearly and consistently? Do they match how stakeholders understand them?\n- **Baseline and comparison**: Is the comparison fair? Are time periods, cohort sizes, and contexts comparable?\n\n### 2. Check for Common Analytical Errors\n\nSystematically review for:\n\n**Data completeness:**\n- Missing data that could skew results (e.g., nulls in key fields, missing time periods)\n- Data freshness issues (is the most recent data actually complete or still loading?)\n- Survivorship bias (are you only looking at entities that \"survived\" to the analysis date?)\n\n**Statistical issues:**\n- Simpson's paradox (trend reverses when data is aggregated vs. segmented)\n- Correlation presented as causation without supporting evidence\n- Small sample sizes leading to unreliable conclusions\n- Outliers disproportionately affecting averages (should medians be used instead?)\n- Multiple testing / cherry-picking significant results\n\n**Aggregation errors:**\n- Double-counting from improper joins (many-to-many explosions)\n- Incorrect denominators in rate calculations\n- Mixing granularity levels (e.g., user-level metrics averaged with account-level)\n- Revenue recognized vs. billed vs. collected confusion\n\n**Time-related issues:**\n- Seasonality not accounted for in comparisons\n- Incomplete periods included in averages (e.g., partial month compared to full months)\n- Timezone inconsistencies between data sources\n- Look-ahead bias (using future information to explain past events)\n\n**Selection and scope:**\n- Cherry-picked time ranges that favor a particular narrative\n- Excluded segments without justification\n- Changing definitions mid-analysis\n\n### 3. Verify Calculations and Aggregations\n\nWhere possible, spot-check:\n\n- Recalculate a few key numbers independently\n- Verify that subtotals sum to totals\n- Check that percentages sum to 100% (or close to it) where expected\n- Confirm that YoY/MoM comparisons use the correct base periods\n- Validate that filters are applied consistently across all metrics\n\n### 4. Assess Visualizations\n\nIf the analysis includes charts:\n\n- Do axes start at appropriate values (zero for bar charts)?\n- Are scales consistent across comparison charts?\n- Do chart titles accurately describe what's shown?\n- Could the visualization mislead a quick reader?\n- Are there truncated axes, inconsistent intervals, or 3D effects that distort perception?\n\n### 5. Evaluate Narrative and Conclusions\n\nReview whether:\n\n- Conclusions are supported by the data shown\n- Alternative explanations are acknowledged\n- Uncertainty is communicated appropriately\n- Recommendations follow logically from findings\n- The level of confidence matches the strength of evidence\n\n### 6. Suggest Improvements\n\nProvide specific, actionable suggestions:\n\n- Additional analyses that would strengthen the conclusions\n- Caveats or limitations that should be noted\n- Better visualizations or framings for key points\n- Missing context that stakeholders would want\n\n### 7. Generate Confidence Assessment\n\nRate the analysis on a 3-level scale:\n\n**Ready to share** -- Analysis is methodologically sound, calculations verified, caveats noted. Minor suggestions for improvement but nothing blocking.\n\n**Share with noted caveats** -- Analysis is largely correct but has specific limitations or assumptions that must be communicated to stakeholders. List the required caveats.\n\n**Needs revision** -- Found specific errors, methodological issues, or missing analyses that should be addressed before sharing. List the required changes with priority order.\n\n## Output Format\n\n```\n## Validation Report\n\n### Overall Assessment: [Ready to share | Share with caveats | Needs revision]\n\n### Methodology Review\n[Findings about approach, data selection, definitions]\n\n### Issues Found\n1. [Severity: High/Medium/Low] [Issue description and impact]\n2. ...\n\n### Calculation Spot-Checks\n- [Metric]: [Verified / Discrepancy found]\n- ...\n\n### Visualization Review\n[Any issues with charts or visual presentation]\n\n### Suggested Improvements\n1. [Improvement and why it matters]\n2. ...\n\n### Required Caveats for Stakeholders\n- [Caveat that must be communicated]\n- ...\n```\n\n## Examples\n\n```\n/validate Review this quarterly revenue analysis before I send it to the exec team: [analysis]\n```\n\n```\n/validate Check my churn analysis -- I'm comparing Q4 churn rates to Q3 but Q4 has a shorter measurement window\n```\n\n```\n/validate Here's a SQL query and its results for our conversion funnel. Does the logic look right? [query + results]\n```\n\n## Tips\n\n- Run /validate before any high-stakes presentation or decision\n- Even quick analyses benefit from a sanity check -- it takes a minute and can save your credibility\n- If the validation finds issues, fix them and re-validate\n- Share the validation output alongside your analysis to build stakeholder confidence\n", "depth": 3}, {"id": "394", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands/analyze.md", "name": "analyze.md", "type": "file", "content": "---\ndescription: Answer data questions -- from quick lookups to full analyses\nargument-hint: \"\u003cquestion\u003e\"\n---\n\n# /analyze - Answer Data Questions\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nAnswer a data question, from a quick lookup to a full analysis to a formal report.\n\n## Usage\n\n```\n/analyze \u003cnatural language question\u003e\n```\n\n## Workflow\n\n### 1. Understand the Question\n\nParse the user's question and determine:\n\n- **Complexity level**:\n  - **Quick answer**: Single metric, simple filter, factual lookup (e.g., \"How many users signed up last week?\")\n  - **Full analysis**: Multi-dimensional exploration, trend analysis, comparison (e.g., \"What's driving the drop in conversion rate?\")\n  - **Formal report**: Comprehensive investigation with methodology, caveats, and recommendations (e.g., \"Prepare a quarterly business review of our subscription metrics\")\n- **Data requirements**: Which tables, metrics, dimensions, and time ranges are needed\n- **Output format**: Number, table, chart, narrative, or combination\n\n### 2. Gather Data\n\n**If a data warehouse MCP server is connected:**\n\n1. Explore the schema to find relevant tables and columns\n2. Write SQL query(ies) to extract the needed data\n3. Execute the query and retrieve results\n4. If the query fails, debug and retry (check column names, table references, syntax for the specific dialect)\n5. If results look unexpected, run sanity checks before proceeding\n\n**If no data warehouse is connected:**\n\n1. Ask the user to provide data in one of these ways:\n   - Paste query results directly\n   - Upload a CSV or Excel file\n   - Describe the schema so you can write queries for them to run\n2. If writing queries for manual execution, use the `sql-queries` skill for dialect-specific best practices\n3. Once data is provided, proceed with analysis\n\n### 3. Analyze\n\n- Calculate relevant metrics, aggregations, and comparisons\n- Identify patterns, trends, outliers, and anomalies\n- Compare across dimensions (time periods, segments, categories)\n- For complex analyses, break the problem into sub-questions and address each\n\n### 4. Validate Before Presenting\n\nBefore sharing results, run through validation checks:\n\n- **Row count sanity**: Does the number of records make sense?\n- **Null check**: Are there unexpected nulls that could skew results?\n- **Magnitude check**: Are the numbers in a reasonable range?\n- **Trend continuity**: Do time series have unexpected gaps?\n- **Aggregation logic**: Do subtotals sum to totals correctly?\n\nIf any check raises concerns, investigate and note caveats.\n\n### 5. Present Findings\n\n**For quick answers:**\n- State the answer directly with relevant context\n- Include the query used (collapsed or in a code block) for reproducibility\n\n**For full analyses:**\n- Lead with the key finding or insight\n- Support with data tables and/or visualizations\n- Note methodology and any caveats\n- Suggest follow-up questions\n\n**For formal reports:**\n- Executive summary with key takeaways\n- Methodology section explaining approach and data sources\n- Detailed findings with supporting evidence\n- Caveats, limitations, and data quality notes\n- Recommendations and suggested next steps\n\n### 6. Visualize Where Helpful\n\nWhen a chart would communicate results more effectively than a table:\n\n- Use the `data-visualization` skill to select the right chart type\n- Generate a Python visualization or build it into an HTML dashboard\n- Follow visualization best practices for clarity and accuracy\n\n## Examples\n\n**Quick answer:**\n```\n/analyze How many new users signed up in December?\n```\n\n**Full analysis:**\n```\n/analyze What's causing the increase in support ticket volume over the past 3 months? Break down by category and priority.\n```\n\n**Formal report:**\n```\n/analyze Prepare a data quality assessment of our customer table -- completeness, consistency, and any issues we should address.\n```\n\n## Tips\n\n- Be specific about time ranges, segments, or metrics when possible\n- If you know the table names, mention them to speed up the process\n- For complex questions, Claude may break them into multiple queries\n- Results are always validated before presentation -- if something looks off, Claude will flag it\n", "depth": 3}, {"id": "395", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands/explore-data.md", "name": "explore-data.md", "type": "file", "content": "---\ndescription: Profile and explore a dataset to understand its shape, quality, and patterns\nargument-hint: \"\u003ctable or file\u003e\"\n---\n\n# /explore-data - Profile and Explore a Dataset\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a comprehensive data profile for a table or uploaded file. Understand its shape, quality, and patterns before diving into analysis.\n\n## Usage\n\n```\n/explore-data \u003ctable_name or file\u003e\n```\n\n## Workflow\n\n### 1. Access the Data\n\n**If a data warehouse MCP server is connected:**\n\n1. Resolve the table name (handle schema prefixes, suggest matches if ambiguous)\n2. Query table metadata: column names, types, descriptions if available\n3. Run profiling queries against the live data\n\n**If a file is provided (CSV, Excel, Parquet, JSON):**\n\n1. Read the file and load into a working dataset\n2. Infer column types from the data\n\n**If neither:**\n\n1. Ask the user to provide a table name (with their warehouse connected) or upload a file\n2. If they describe a table schema, provide guidance on what profiling queries to run\n\n### 2. Generate Data Profile\n\nRun the following profiling checks:\n\n**Table-level metrics:**\n- Total row count\n- Column count and types breakdown\n- Approximate table size (if available from metadata)\n- Date range coverage (min/max of date columns)\n\n**Column-level metrics for each column:**\n- Data type (and whether it matches expected type)\n- Null count and null rate (%)\n- Distinct count and cardinality (distinct / total)\n- For numeric columns: min, max, mean, median, stddev, percentiles (p25, p50, p75, p95, p99)\n- For string columns: min/max length, most common values (top 10), empty string count\n- For date/timestamp columns: min, max, distribution by time period\n- For boolean columns: true/false/null distribution\n\n**Present the profile as a clean summary table**, grouped by column type (dimensions, metrics, dates, IDs).\n\n### 3. Identify Data Quality Issues\n\nFlag potential problems:\n\n- **High null rates**: Columns with \u003e5% nulls (warn), \u003e20% nulls (alert)\n- **Low cardinality surprises**: Columns that should be high-cardinality but aren't (e.g., a \"user_id\" with only 50 distinct values)\n- **High cardinality surprises**: Columns that should be categorical but have too many distinct values\n- **Suspicious values**: Negative amounts where only positive expected, future dates in historical data, obviously placeholder values (e.g., \"N/A\", \"TBD\", \"test\", \"999999\")\n- **Duplicate detection**: Check if there's a natural key and whether it has duplicates\n- **Distribution skew**: Extremely skewed numeric distributions that could affect averages\n- **Encoding issues**: Mixed case in categorical fields, trailing whitespace, inconsistent formats\n\n### 4. Suggest Interesting Dimensions and Metrics\n\nBased on the column profile, recommend:\n\n- **Best dimension columns** for slicing data (categorical columns with reasonable cardinality, 3-50 values)\n- **Key metric columns** for measurement (numeric columns with meaningful distributions)\n- **Time columns** suitable for trend analysis\n- **Natural groupings** or hierarchies apparent in the data\n- **Potential join keys** linking to other tables (ID columns, foreign keys)\n\n### 5. Recommend Follow-Up Analyses\n\nSuggest 3-5 specific analyses the user could run next:\n\n- \"Trend analysis on [metric] by [time_column] grouped by [dimension]\"\n- \"Distribution deep-dive on [skewed_column] to understand outliers\"\n- \"Data quality investigation on [problematic_column]\"\n- \"Correlation analysis between [metric_a] and [metric_b]\"\n- \"Cohort analysis using [date_column] and [status_column]\"\n\n## Output Format\n\n```\n## Data Profile: [table_name]\n\n### Overview\n- Rows: 2,340,891\n- Columns: 23 (8 dimensions, 6 metrics, 4 dates, 5 IDs)\n- Date range: 2021-03-15 to 2024-01-22\n\n### Column Details\n[summary table]\n\n### Data Quality Issues\n[flagged issues with severity]\n\n### Recommended Explorations\n[numbered list of suggested follow-up analyses]\n```\n\n## Tips\n\n- For very large tables (100M+ rows), profiling queries use sampling by default -- mention if you need exact counts\n- If exploring a new dataset for the first time, this command gives you the lay of the land before writing specific queries\n- The quality flags are heuristic -- not every flag is a real problem, but each is worth a quick look\n", "depth": 3}, {"id": "396", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands/create-viz.md", "name": "create-viz.md", "type": "file", "content": "---\ndescription: Create publication-quality visualizations with Python\nargument-hint: \"\u003cdata source\u003e [chart type]\"\n---\n\n# /create-viz - Create Visualizations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCreate publication-quality data visualizations using Python. Generates charts from data with best practices for clarity, accuracy, and design.\n\n## Usage\n\n```\n/create-viz \u003cdata source\u003e [chart type] [additional instructions]\n```\n\n## Workflow\n\n### 1. Understand the Request\n\nDetermine:\n\n- **Data source**: Query results, pasted data, CSV/Excel file, or data to be queried\n- **Chart type**: Explicitly requested or needs to be recommended\n- **Purpose**: Exploration, presentation, report, dashboard component\n- **Audience**: Technical team, executives, external stakeholders\n\n### 2. Get the Data\n\n**If data warehouse is connected and data needs querying:**\n1. Write and execute the query\n2. Load results into a pandas DataFrame\n\n**If data is pasted or uploaded:**\n1. Parse the data into a pandas DataFrame\n2. Clean and prepare as needed (type conversions, null handling)\n\n**If data is from a previous analysis in the conversation:**\n1. Reference the existing data\n\n### 3. Select Chart Type\n\nIf the user didn't specify a chart type, recommend one based on the data and question:\n\n| Data Relationship | Recommended Chart |\n|---|---|\n| Trend over time | Line chart |\n| Comparison across categories | Bar chart (horizontal if many categories) |\n| Part-to-whole composition | Stacked bar or area chart (avoid pie charts unless \u003c6 categories) |\n| Distribution of values | Histogram or box plot |\n| Correlation between two variables | Scatter plot |\n| Two-variable comparison over time | Dual-axis line or grouped bar |\n| Geographic data | Choropleth map |\n| Ranking | Horizontal bar chart |\n| Flow or process | Sankey diagram |\n| Matrix of relationships | Heatmap |\n\nExplain the recommendation briefly if the user didn't specify.\n\n### 4. Generate the Visualization\n\nWrite Python code using one of these libraries based on the need:\n\n- **matplotlib + seaborn**: Best for static, publication-quality charts. Default choice.\n- **plotly**: Best for interactive charts or when the user requests interactivity.\n\n**Code requirements:**\n\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Set professional style\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\n# Create figure with appropriate size\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# [chart-specific code]\n\n# Always include:\nax.set_title('Clear, Descriptive Title', fontsize=14, fontweight='bold')\nax.set_xlabel('X-Axis Label', fontsize=11)\nax.set_ylabel('Y-Axis Label', fontsize=11)\n\n# Format numbers appropriately\n# - Percentages: '45.2%' not '0.452'\n# - Currency: '$1.2M' not '1200000'\n# - Large numbers: '2.3K' or '1.5M' not '2300' or '1500000'\n\n# Remove chart junk\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.savefig('chart_name.png', dpi=150, bbox_inches='tight')\nplt.show()\n```\n\n### 5. Apply Design Best Practices\n\n**Color:**\n- Use a consistent, colorblind-friendly palette\n- Use color meaningfully (not decoratively)\n- Highlight the key data point or trend with a contrasting color\n- Grey out less important reference data\n\n**Typography:**\n- Descriptive title that states the insight, not just the metric (e.g., \"Revenue grew 23% YoY\" not \"Revenue by Month\")\n- Readable axis labels (not rotated 90 degrees if avoidable)\n- Data labels on key points when they add clarity\n\n**Layout:**\n- Appropriate whitespace and margins\n- Legend placement that doesn't obscure data\n- Sorted categories by value (not alphabetically) unless there's a natural order\n\n**Accuracy:**\n- Y-axis starts at zero for bar charts\n- No misleading axis breaks without clear notation\n- Consistent scales when comparing panels\n- Appropriate precision (don't show 10 decimal places)\n\n### 6. Save and Present\n\n1. Save the chart as a PNG file with descriptive name\n2. Display the chart to the user\n3. Provide the code used so they can modify it\n4. Suggest variations (different chart type, different grouping, zoomed time range)\n\n## Examples\n\n```\n/create-viz Show monthly revenue for the last 12 months as a line chart with the trend highlighted\n```\n\n```\n/create-viz Here's our NPS data by product: [pastes data]. Create a horizontal bar chart ranking products by score.\n```\n\n```\n/create-viz Query the orders table and create a heatmap of order volume by day-of-week and hour\n```\n\n## Tips\n\n- If you want interactive charts (hover, zoom, filter), mention \"interactive\" and Claude will use plotly\n- Specify \"presentation\" if you need larger fonts and higher contrast\n- You can request multiple charts at once (e.g., \"create a 2x2 grid of charts showing...\")\n- Charts are saved to your current directory as PNG files\n", "depth": 3}, {"id": "397", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands/write-query.md", "name": "write-query.md", "type": "file", "content": "---\ndescription: Write optimized SQL for your dialect with best practices\nargument-hint: \"\u003cdescription of what data you need\u003e\"\n---\n\n# /write-query - Write Optimized SQL\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nWrite a SQL query from a natural language description, optimized for your specific SQL dialect and following best practices.\n\n## Usage\n\n```\n/write-query \u003cdescription of what data you need\u003e\n```\n\n## Workflow\n\n### 1. Understand the Request\n\nParse the user's description to identify:\n\n- **Output columns**: What fields should the result include?\n- **Filters**: What conditions limit the data (time ranges, segments, statuses)?\n- **Aggregations**: Are there GROUP BY operations, counts, sums, averages?\n- **Joins**: Does this require combining multiple tables?\n- **Ordering**: How should results be sorted?\n- **Limits**: Is there a top-N or sample requirement?\n\n### 2. Determine SQL Dialect\n\nIf the user's SQL dialect is not already known, ask which they use:\n\n- **PostgreSQL** (including Aurora, RDS, Supabase, Neon)\n- **Snowflake**\n- **BigQuery** (Google Cloud)\n- **Redshift** (Amazon)\n- **Databricks SQL**\n- **MySQL** (including Aurora MySQL, PlanetScale)\n- **SQL Server** (Microsoft)\n- **DuckDB**\n- **SQLite**\n- **Other** (ask for specifics)\n\nRemember the dialect for future queries in the same session.\n\n### 3. Discover Schema (If Warehouse Connected)\n\nIf a data warehouse MCP server is connected:\n\n1. Search for relevant tables based on the user's description\n2. Inspect column names, types, and relationships\n3. Check for partitioning or clustering keys that affect performance\n4. Look for pre-built views or materialized views that might simplify the query\n\n### 4. Write the Query\n\nFollow these best practices:\n\n**Structure:**\n- Use CTEs (WITH clauses) for readability when queries have multiple logical steps\n- One CTE per logical transformation or data source\n- Name CTEs descriptively (e.g., `daily_signups`, `active_users`, `revenue_by_product`)\n\n**Performance:**\n- Never use `SELECT *` in production queries -- specify only needed columns\n- Filter early (push WHERE clauses as close to the base tables as possible)\n- Use partition filters when available (especially date partitions)\n- Prefer `EXISTS` over `IN` for subqueries with large result sets\n- Use appropriate JOIN types (don't use LEFT JOIN when INNER JOIN is correct)\n- Avoid correlated subqueries when a JOIN or window function works\n- Be mindful of exploding joins (many-to-many)\n\n**Readability:**\n- Add comments explaining the \"why\" for non-obvious logic\n- Use consistent indentation and formatting\n- Alias tables with meaningful short names (not just `a`, `b`, `c`)\n- Put each major clause on its own line\n\n**Dialect-specific optimizations:**\n- Apply dialect-specific syntax and functions (see `sql-queries` skill for details)\n- Use dialect-appropriate date functions, string functions, and window syntax\n- Note any dialect-specific performance features (e.g., Snowflake clustering, BigQuery partitioning)\n\n### 5. Present the Query\n\nProvide:\n\n1. **The complete query** in a SQL code block with syntax highlighting\n2. **Brief explanation** of what each CTE or section does\n3. **Performance notes** if relevant (expected cost, partition usage, potential bottlenecks)\n4. **Modification suggestions** -- how to adjust for common variations (different time range, different granularity, additional filters)\n\n### 6. Offer to Execute\n\nIf a data warehouse is connected, offer to run the query and analyze the results. If the user wants to run it themselves, the query is ready to copy-paste.\n\n## Examples\n\n**Simple aggregation:**\n```\n/write-query Count of orders by status for the last 30 days\n```\n\n**Complex analysis:**\n```\n/write-query Cohort retention analysis -- group users by their signup month, then show what percentage are still active (had at least one event) at 1, 3, 6, and 12 months after signup\n```\n\n**Performance-critical:**\n```\n/write-query We have a 500M row events table partitioned by date. Find the top 100 users by event count in the last 7 days with their most recent event type.\n```\n\n## Tips\n\n- Mention your SQL dialect upfront to get the right syntax immediately\n- If you know the table names, include them -- otherwise Claude will help you find them\n- Specify if you need the query to be idempotent (safe to re-run) or one-time\n- For recurring queries, mention if it should be parameterized for date ranges\n", "depth": 3}, {"id": "398", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/commands/build-dashboard.md", "name": "build-dashboard.md", "type": "file", "content": "---\ndescription: Build an interactive HTML dashboard with charts, filters, and tables\nargument-hint: \"\u003cdescription\u003e [data source]\"\n---\n\n# /build-dashboard - Build Interactive Dashboards\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nBuild a self-contained interactive HTML dashboard with charts, filters, tables, and professional styling. Opens directly in a browser -- no server or dependencies required.\n\n## Usage\n\n```\n/build-dashboard \u003cdescription of dashboard\u003e [data source]\n```\n\n## Workflow\n\n### 1. Understand the Dashboard Requirements\n\nDetermine:\n\n- **Purpose**: Executive overview, operational monitoring, deep-dive analysis, team reporting\n- **Audience**: Who will use this dashboard?\n- **Key metrics**: What numbers matter most?\n- **Dimensions**: What should users be able to filter or slice by?\n- **Data source**: Live query, pasted data, CSV file, or sample data\n\n### 2. Gather the Data\n\n**If data warehouse is connected:**\n1. Query the necessary data\n2. Embed the results as JSON within the HTML file\n\n**If data is pasted or uploaded:**\n1. Parse and clean the data\n2. Embed as JSON in the dashboard\n\n**If working from a description without data:**\n1. Create a realistic sample dataset matching the described schema\n2. Note in the dashboard that it uses sample data\n3. Provide instructions for swapping in real data\n\n### 3. Design the Dashboard Layout\n\nFollow a standard dashboard layout pattern:\n\n```\n┌──────────────────────────────────────────────────┐\n│  Dashboard Title                    [Filters ▼]  │\n├────────────┬────────────┬────────────┬───────────┤\n│  KPI Card  │  KPI Card  │  KPI Card  │ KPI Card  │\n├────────────┴────────────┼────────────┴───────────┤\n│                         │                        │\n│    Primary Chart        │   Secondary Chart      │\n│    (largest area)       │                        │\n│                         │                        │\n├─────────────────────────┴────────────────────────┤\n│                                                  │\n│    Detail Table (sortable, scrollable)           │\n│                                                  │\n└──────────────────────────────────────────────────┘\n```\n\n**Adapt the layout to the content:**\n- 2-4 KPI cards at the top for headline numbers\n- 1-3 charts in the middle section for trends and breakdowns\n- Optional detail table at the bottom for drill-down data\n- Filters in the header or sidebar depending on complexity\n\n### 4. Build the HTML Dashboard\n\nGenerate a single self-contained HTML file that includes:\n\n**Structure (HTML):**\n- Semantic HTML5 layout\n- Responsive grid using CSS Grid or Flexbox\n- Filter controls (dropdowns, date pickers, toggles)\n- KPI cards with values and labels\n- Chart containers\n- Data table with sortable headers\n\n**Styling (CSS):**\n- Professional color scheme (clean whites, grays, with accent colors for data)\n- Card-based layout with subtle shadows\n- Consistent typography (system fonts for fast loading)\n- Responsive design that works on different screen sizes\n- Print-friendly styles\n\n**Interactivity (JavaScript):**\n- Chart.js for interactive charts (included via CDN)\n- Filter dropdowns that update all charts and tables simultaneously\n- Sortable table columns\n- Hover tooltips on charts\n- Number formatting (commas, currency, percentages)\n\n**Data (embedded JSON):**\n- All data embedded directly in the HTML as JavaScript variables\n- No external data fetches required\n- Dashboard works completely offline\n\n### 5. Implement Chart Types\n\nUse Chart.js for all charts. Common dashboard chart patterns:\n\n- **Line chart**: Time series trends\n- **Bar chart**: Category comparisons\n- **Doughnut chart**: Composition (when \u003c6 categories)\n- **Stacked bar**: Composition over time\n- **Mixed (bar + line)**: Volume with rate overlay\n\n### 6. Add Interactivity\n\n**Filters:**\n```javascript\n// All filters update a central filter state\n// Charts and tables re-render when filters change\nfunction applyFilters() {\n    const filtered = data.filter(row =\u003e matchesFilters(row));\n    updateKPIs(filtered);\n    updateCharts(filtered);\n    updateTable(filtered);\n}\n```\n\n**Table sorting:**\n- Click column headers to sort ascending/descending\n- Visual indicator for current sort column and direction\n\n**Tooltips:**\n- Charts show detailed values on hover\n- KPI cards show comparison to previous period\n\n### 7. Save and Open\n\n1. Save the dashboard as an HTML file with a descriptive name (e.g., `sales_dashboard.html`)\n2. Open it in the user's default browser\n3. Confirm it renders correctly\n4. Provide instructions for updating data or customizing\n\n## Output Template\n\nThe generated HTML file follows this structure:\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003e[Dashboard Title]\u003c/title\u003e\n    \u003cscript src=\"https://cdn.jsdelivr.net/npm/chart.js@4.5.1\" integrity=\"sha384-jb8JQMbMoBUzgWatfe6COACi2ljcDdZQ2OxczGA3bGNeWe+6DChMTBJemed7ZnvJ\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n    \u003cstyle\u003e\n        /* Professional dashboard CSS */\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv class=\"dashboard\"\u003e\n        \u003cheader\u003e\u003c!-- Title and filters --\u003e\u003c/header\u003e\n        \u003csection class=\"kpis\"\u003e\u003c!-- KPI cards --\u003e\u003c/section\u003e\n        \u003csection class=\"charts\"\u003e\u003c!-- Chart containers --\u003e\u003c/section\u003e\n        \u003csection class=\"details\"\u003e\u003c!-- Data table --\u003e\u003c/section\u003e\n    \u003c/div\u003e\n    \u003cscript\u003e\n        const DATA = [/* embedded JSON data */];\n        // Dashboard initialization and interactivity\n    \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n## Examples\n\n```\n/build-dashboard Monthly sales dashboard with revenue trend, top products, and regional breakdown. Data is in the orders table.\n```\n\n```\n/build-dashboard Here's our support ticket data [pastes CSV]. Build a dashboard showing volume by priority, response time trends, and resolution rates.\n```\n\n```\n/build-dashboard Create a template executive dashboard for a SaaS company showing MRR, churn, new customers, and NPS. Use sample data.\n```\n\n## Tips\n\n- Dashboards are fully self-contained HTML files -- share them with anyone by sending the file\n- For real-time dashboards, consider connecting to a BI tool instead. These dashboards are point-in-time snapshots\n- Request \"dark mode\" or \"presentation mode\" for different styling\n- You can request a specific color scheme to match your brand\n", "depth": 3}, {"id": "399", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-exploration", "name": "data-exploration", "type": "dir", "content": "", "depth": 3}, {"id": "400", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor", "name": "data-context-extractor", "type": "dir", "content": "", "depth": 3}, {"id": "401", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/sql-queries", "name": "sql-queries", "type": "dir", "content": "", "depth": 3}, {"id": "402", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-visualization", "name": "data-visualization", "type": "dir", "content": "", "depth": 3}, {"id": "403", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/interactive-dashboard-builder", "name": "interactive-dashboard-builder", "type": "dir", "content": "", "depth": 3}, {"id": "404", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/statistical-analysis", "name": "statistical-analysis", "type": "dir", "content": "", "depth": 3}, {"id": "405", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-validation", "name": "data-validation", "type": "dir", "content": "", "depth": 3}, {"id": "406", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-exploration/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: data-exploration\ndescription: Profile and explore datasets to understand their shape, quality, and patterns before analysis. Use when encountering a new dataset, assessing data quality, discovering column distributions, identifying nulls and outliers, or deciding which dimensions to analyze.\n---\n\n# Data Exploration Skill\n\nSystematic methodology for profiling datasets, assessing data quality, discovering patterns, and understanding schemas.\n\n## Data Profiling Methodology\n\n### Phase 1: Structural Understanding\n\nBefore analyzing any data, understand its structure:\n\n**Table-level questions:**\n- How many rows and columns?\n- What is the grain (one row per what)?\n- What is the primary key? Is it unique?\n- When was the data last updated?\n- How far back does the data go?\n\n**Column classification:**\nCategorize each column as one of:\n- **Identifier**: Unique keys, foreign keys, entity IDs\n- **Dimension**: Categorical attributes for grouping/filtering (status, type, region, category)\n- **Metric**: Quantitative values for measurement (revenue, count, duration, score)\n- **Temporal**: Dates and timestamps (created_at, updated_at, event_date)\n- **Text**: Free-form text fields (description, notes, name)\n- **Boolean**: True/false flags\n- **Structural**: JSON, arrays, nested structures\n\n### Phase 2: Column-Level Profiling\n\nFor each column, compute:\n\n**All columns:**\n- Null count and null rate\n- Distinct count and cardinality ratio (distinct / total)\n- Most common values (top 5-10 with frequencies)\n- Least common values (bottom 5 to spot anomalies)\n\n**Numeric columns (metrics):**\n```\nmin, max, mean, median (p50)\nstandard deviation\npercentiles: p1, p5, p25, p75, p95, p99\nzero count\nnegative count (if unexpected)\n```\n\n**String columns (dimensions, text):**\n```\nmin length, max length, avg length\nempty string count\npattern analysis (do values follow a format?)\ncase consistency (all upper, all lower, mixed?)\nleading/trailing whitespace count\n```\n\n**Date/timestamp columns:**\n```\nmin date, max date\nnull dates\nfuture dates (if unexpected)\ndistribution by month/week\ngaps in time series\n```\n\n**Boolean columns:**\n```\ntrue count, false count, null count\ntrue rate\n```\n\n### Phase 3: Relationship Discovery\n\nAfter profiling individual columns:\n\n- **Foreign key candidates**: ID columns that might link to other tables\n- **Hierarchies**: Columns that form natural drill-down paths (country \u003e state \u003e city)\n- **Correlations**: Numeric columns that move together\n- **Derived columns**: Columns that appear to be computed from others\n- **Redundant columns**: Columns with identical or near-identical information\n\n## Quality Assessment Framework\n\n### Completeness Score\n\nRate each column:\n- **Complete** (\u003e99% non-null): Green\n- **Mostly complete** (95-99%): Yellow -- investigate the nulls\n- **Incomplete** (80-95%): Orange -- understand why and whether it matters\n- **Sparse** (\u003c80%): Red -- may not be usable without imputation\n\n### Consistency Checks\n\nLook for:\n- **Value format inconsistency**: Same concept represented differently (\"USA\", \"US\", \"United States\", \"us\")\n- **Type inconsistency**: Numbers stored as strings, dates in various formats\n- **Referential integrity**: Foreign keys that don't match any parent record\n- **Business rule violations**: Negative quantities, end dates before start dates, percentages \u003e 100\n- **Cross-column consistency**: Status = \"completed\" but completed_at is null\n\n### Accuracy Indicators\n\nRed flags that suggest accuracy issues:\n- **Placeholder values**: 0, -1, 999999, \"N/A\", \"TBD\", \"test\", \"xxx\"\n- **Default values**: Suspiciously high frequency of a single value\n- **Stale data**: Updated_at shows no recent changes in an active system\n- **Impossible values**: Ages \u003e 150, dates in the far future, negative durations\n- **Round number bias**: All values ending in 0 or 5 (suggests estimation, not measurement)\n\n### Timeliness Assessment\n\n- When was the table last updated?\n- What is the expected update frequency?\n- Is there a lag between event time and load time?\n- Are there gaps in the time series?\n\n## Pattern Discovery Techniques\n\n### Distribution Analysis\n\nFor numeric columns, characterize the distribution:\n- **Normal**: Mean and median are close, bell-shaped\n- **Skewed right**: Long tail of high values (common for revenue, session duration)\n- **Skewed left**: Long tail of low values (less common)\n- **Bimodal**: Two peaks (suggests two distinct populations)\n- **Power law**: Few very large values, many small ones (common for user activity)\n- **Uniform**: Roughly equal frequency across range (often synthetic or random)\n\n### Temporal Patterns\n\nFor time series data, look for:\n- **Trend**: Sustained upward or downward movement\n- **Seasonality**: Repeating patterns (weekly, monthly, quarterly, annual)\n- **Day-of-week effects**: Weekday vs. weekend differences\n- **Holiday effects**: Drops or spikes around known holidays\n- **Change points**: Sudden shifts in level or trend\n- **Anomalies**: Individual data points that break the pattern\n\n### Segmentation Discovery\n\nIdentify natural segments by:\n- Finding categorical columns with 3-20 distinct values\n- Comparing metric distributions across segment values\n- Looking for segments with significantly different behavior\n- Testing whether segments are homogeneous or contain sub-segments\n\n### Correlation Exploration\n\nBetween numeric columns:\n- Compute correlation matrix for all metric pairs\n- Flag strong correlations (|r| \u003e 0.7) for investigation\n- Note: Correlation does not imply causation -- flag this explicitly\n- Check for non-linear relationships (e.g., quadratic, logarithmic)\n\n## Schema Understanding and Documentation\n\n### Schema Documentation Template\n\nWhen documenting a dataset for team use:\n\n```markdown\n## Table: [schema.table_name]\n\n**Description**: [What this table represents]\n**Grain**: [One row per...]\n**Primary Key**: [column(s)]\n**Row Count**: [approximate, with date]\n**Update Frequency**: [real-time / hourly / daily / weekly]\n**Owner**: [team or person responsible]\n\n### Key Columns\n\n| Column | Type | Description | Example Values | Notes |\n|--------|------|-------------|----------------|-------|\n| user_id | STRING | Unique user identifier | \"usr_abc123\" | FK to users.id |\n| event_type | STRING | Type of event | \"click\", \"view\", \"purchase\" | 15 distinct values |\n| revenue | DECIMAL | Transaction revenue in USD | 29.99, 149.00 | Null for non-purchase events |\n| created_at | TIMESTAMP | When the event occurred | 2024-01-15 14:23:01 | Partitioned on this column |\n\n### Relationships\n- Joins to `users` on `user_id`\n- Joins to `products` on `product_id`\n- Parent of `event_details` (1:many on event_id)\n\n### Known Issues\n- [List any known data quality issues]\n- [Note any gotchas for analysts]\n\n### Common Query Patterns\n- [Typical use cases for this table]\n```\n\n### Schema Exploration Queries\n\nWhen connected to a data warehouse, use these patterns to discover schema:\n\n```sql\n-- List all tables in a schema (PostgreSQL)\nSELECT table_name, table_type\nFROM information_schema.tables\nWHERE table_schema = 'public'\nORDER BY table_name;\n\n-- Column details (PostgreSQL)\nSELECT column_name, data_type, is_nullable, column_default\nFROM information_schema.columns\nWHERE table_name = 'my_table'\nORDER BY ordinal_position;\n\n-- Table sizes (PostgreSQL)\nSELECT relname, pg_size_pretty(pg_total_relation_size(relid))\nFROM pg_catalog.pg_statio_user_tables\nORDER BY pg_total_relation_size(relid) DESC;\n\n-- Row counts for all tables (general pattern)\n-- Run per-table: SELECT COUNT(*) FROM table_name\n```\n\n### Lineage and Dependencies\n\nWhen exploring an unfamiliar data environment:\n\n1. Start with the \"output\" tables (what reports or dashboards consume)\n2. Trace upstream: What tables feed into them?\n3. Identify raw/staging/mart layers\n4. Map the transformation chain from raw data to analytical tables\n5. Note where data is enriched, filtered, or aggregated\n", "depth": 4}, {"id": "407", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/references", "name": "references", "type": "dir", "content": "", "depth": 4}, {"id": "408", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/scripts", "name": "scripts", "type": "dir", "content": "", "depth": 4}, {"id": "409", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: data-context-extractor\ndescription: \u003e\n  Generate or improve a company-specific data analysis skill by extracting tribal knowledge from analysts.\n\n  BOOTSTRAP MODE - Triggers: \"Create a data context skill\", \"Set up data analysis for our warehouse\",\n  \"Help me create a skill for our database\", \"Generate a data skill for [company]\"\n  → Discovers schemas, asks key questions, generates initial skill with reference files\n\n  ITERATION MODE - Triggers: \"Add context about [domain]\", \"The skill needs more info about [topic]\",\n  \"Update the data skill with [metrics/tables/terminology]\", \"Improve the [domain] reference\"\n  → Loads existing skill, asks targeted questions, appends/updates reference files\n\n  Use when data analysts want Claude to understand their company's specific data warehouse,\n  terminology, metrics definitions, and common query patterns.\n---\n\n# Data Context Extractor\n\nA meta-skill that extracts company-specific data knowledge from analysts and generates tailored data analysis skills.\n\n## How It Works\n\nThis skill has two modes:\n\n1. **Bootstrap Mode**: Create a new data analysis skill from scratch\n2. **Iteration Mode**: Improve an existing skill by adding domain-specific reference files\n\n---\n\n## Bootstrap Mode\n\nUse when: User wants to create a new data context skill for their warehouse.\n\n### Phase 1: Database Connection \u0026 Discovery\n\n**Step 1: Identify the database type**\n\nAsk: \"What data warehouse are you using?\"\n\nCommon options:\n- **BigQuery**\n- **Snowflake**\n- **PostgreSQL/Redshift**\n- **Databricks**\n\nUse `~~data warehouse` tools (query and schema) to connect. If unclear, check available MCP tools in the current session.\n\n**Step 2: Explore the schema**\n\nUse `~~data warehouse` schema tools to:\n1. List available datasets/schemas\n2. Identify the most important tables (ask user: \"Which 3-5 tables do analysts query most often?\")\n3. Pull schema details for those key tables\n\nSample exploration queries by dialect:\n```sql\n-- BigQuery: List datasets\nSELECT schema_name FROM INFORMATION_SCHEMA.SCHEMATA\n\n-- BigQuery: List tables in a dataset\nSELECT table_name FROM `project.dataset.INFORMATION_SCHEMA.TABLES`\n\n-- Snowflake: List schemas\nSHOW SCHEMAS IN DATABASE my_database\n\n-- Snowflake: List tables\nSHOW TABLES IN SCHEMA my_schema\n```\n\n### Phase 2: Core Questions (Ask These)\n\nAfter schema discovery, ask these questions conversationally (not all at once):\n\n**Entity Disambiguation (Critical)**\n\u003e \"When people here say 'user' or 'customer', what exactly do they mean? Are there different types?\"\n\nListen for:\n- Multiple entity types (user vs account vs organization)\n- Relationships between them (1:1, 1:many, many:many)\n- Which ID fields link them together\n\n**Primary Identifiers**\n\u003e \"What's the main identifier for a [customer/user/account]? Are there multiple IDs for the same entity?\"\n\nListen for:\n- Primary keys vs business keys\n- UUID vs integer IDs\n- Legacy ID systems\n\n**Key Metrics**\n\u003e \"What are the 2-3 metrics people ask about most? How is each one calculated?\"\n\nListen for:\n- Exact formulas (ARR = monthly_revenue × 12)\n- Which tables/columns feed each metric\n- Time period conventions (trailing 7 days, calendar month, etc.)\n\n**Data Hygiene**\n\u003e \"What should ALWAYS be filtered out of queries? (test data, fraud, internal users, etc.)\"\n\nListen for:\n- Standard WHERE clauses to always include\n- Flag columns that indicate exclusions (is_test, is_internal, is_fraud)\n- Specific values to exclude (status = 'deleted')\n\n**Common Gotchas**\n\u003e \"What mistakes do new analysts typically make with this data?\"\n\nListen for:\n- Confusing column names\n- Timezone issues\n- NULL handling quirks\n- Historical vs current state tables\n\n### Phase 3: Generate the Skill\n\nCreate a skill with this structure:\n\n```\n[company]-data-analyst/\n├── SKILL.md\n└── references/\n    ├── entities.md          # Entity definitions and relationships\n    ├── metrics.md           # KPI calculations\n    ├── tables/              # One file per domain\n    │   ├── [domain1].md\n    │   └── [domain2].md\n    └── dashboards.json      # Optional: existing dashboards catalog\n```\n\n**SKILL.md Template**: See `references/skill-template.md`\n\n**SQL Dialect Section**: See `references/sql-dialects.md` and include the appropriate dialect notes.\n\n**Reference File Template**: See `references/domain-template.md`\n\n### Phase 4: Package and Deliver\n\n1. Create all files in the skill directory\n2. Package as a zip file\n3. Present to user with summary of what was captured\n\n---\n\n## Iteration Mode\n\nUse when: User has an existing skill but needs to add more context.\n\n### Step 1: Load Existing Skill\n\nAsk user to upload their existing skill (zip or folder), or locate it if already in the session.\n\nRead the current SKILL.md and reference files to understand what's already documented.\n\n### Step 2: Identify the Gap\n\nAsk: \"What domain or topic needs more context? What queries are failing or producing wrong results?\"\n\nCommon gaps:\n- A new data domain (marketing, finance, product, etc.)\n- Missing metric definitions\n- Undocumented table relationships\n- New terminology\n\n### Step 3: Targeted Discovery\n\nFor the identified domain:\n\n1. **Explore relevant tables**: Use `~~data warehouse` schema tools to find tables in that domain\n2. **Ask domain-specific questions**:\n   - \"What tables are used for [domain] analysis?\"\n   - \"What are the key metrics for [domain]?\"\n   - \"Any special filters or gotchas for [domain] data?\"\n\n3. **Generate new reference file**: Create `references/[domain].md` using the domain template\n\n### Step 4: Update and Repackage\n\n1. Add the new reference file\n2. Update SKILL.md's \"Knowledge Base Navigation\" section to include the new domain\n3. Repackage the skill\n4. Present the updated skill to user\n\n---\n\n## Reference File Standards\n\nEach reference file should include:\n\n### For Table Documentation\n- **Location**: Full table path\n- **Description**: What this table contains, when to use it\n- **Primary Key**: How to uniquely identify rows\n- **Update Frequency**: How often data refreshes\n- **Key Columns**: Table with column name, type, description, notes\n- **Relationships**: How this table joins to others\n- **Sample Queries**: 2-3 common query patterns\n\n### For Metrics Documentation\n- **Metric Name**: Human-readable name\n- **Definition**: Plain English explanation\n- **Formula**: Exact calculation with column references\n- **Source Table(s)**: Where the data comes from\n- **Caveats**: Edge cases, exclusions, gotchas\n\n### For Entity Documentation\n- **Entity Name**: What it's called\n- **Definition**: What it represents in the business\n- **Primary Table**: Where to find this entity\n- **ID Field(s)**: How to identify it\n- **Relationships**: How it relates to other entities\n- **Common Filters**: Standard exclusions (internal, test, etc.)\n\n---\n\n## Quality Checklist\n\nBefore delivering a generated skill, verify:\n\n- [ ] SKILL.md has complete frontmatter (name, description)\n- [ ] Entity disambiguation section is clear\n- [ ] Key terminology is defined\n- [ ] Standard filters/exclusions are documented\n- [ ] At least 2-3 sample queries per domain\n- [ ] SQL uses correct dialect syntax\n- [ ] Reference files are linked from SKILL.md navigation section\n", "depth": 4}, {"id": "410", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/references/sql-dialects.md", "name": "sql-dialects.md", "type": "file", "content": "# SQL Dialect Reference\n\nInclude the appropriate section in generated skills based on the user's data warehouse.\n\n---\n\n## BigQuery\n\n```markdown\n## SQL Dialect: BigQuery\n\n- **Table references**: Use backticks: \\`project.dataset.table\\`\n- **Safe division**: `SAFE_DIVIDE(a, b)` returns NULL instead of error\n- **Date functions**:\n  - `DATE_TRUNC(date_col, MONTH)`\n  - `DATE_SUB(date_col, INTERVAL 1 DAY)`\n  - `DATE_DIFF(end_date, start_date, DAY)`\n- **Column exclusion**: `SELECT * EXCEPT(column_to_exclude)`\n- **Arrays**: `UNNEST(array_column)` to flatten\n- **Structs**: Access with dot notation `struct_col.field_name`\n- **Timestamps**: `TIMESTAMP_TRUNC()`, times in UTC by default\n- **String matching**: `LIKE`, `REGEXP_CONTAINS(col, r'pattern')`\n- **NULLs in aggregations**: Most functions ignore NULLs; use `IFNULL()` or `COALESCE()`\n```\n\n---\n\n## Snowflake\n\n```markdown\n## SQL Dialect: Snowflake\n\n- **Table references**: `DATABASE.SCHEMA.TABLE` or with quotes for case-sensitive: `\"Column_Name\"`\n- **Safe division**: `DIV0(a, b)` returns 0, `DIV0NULL(a, b)` returns NULL\n- **Date functions**:\n  - `DATE_TRUNC('MONTH', date_col)`\n  - `DATEADD(DAY, -1, date_col)`\n  - `DATEDIFF(DAY, start_date, end_date)`\n- **Column exclusion**: `SELECT * EXCLUDE (column_to_exclude)`\n- **Arrays**: `FLATTEN(array_column)` to flatten, access with `value`\n- **Variants/JSON**: Access with colon notation `variant_col:field_name`\n- **Timestamps**: `TIMESTAMP_NTZ` (no timezone), `TIMESTAMP_TZ` (with timezone)\n- **String matching**: `LIKE`, `REGEXP_LIKE(col, 'pattern')`\n- **Case sensitivity**: Identifiers are uppercase by default unless quoted\n```\n\n---\n\n## PostgreSQL / Redshift\n\n```markdown\n## SQL Dialect: PostgreSQL/Redshift\n\n- **Table references**: `schema.table` (lowercase convention)\n- **Safe division**: `NULLIF(b, 0)` pattern: `a / NULLIF(b, 0)`\n- **Date functions**:\n  - `DATE_TRUNC('month', date_col)`\n  - `date_col - INTERVAL '1 day'`\n  - `DATE_PART('day', end_date - start_date)`\n- **Column selection**: No EXCEPT; must list columns explicitly\n- **Arrays**: `UNNEST(array_column)` (PostgreSQL), limited in Redshift\n- **JSON**: `json_col-\u003e\u003e'field_name'` for text, `json_col-\u003e'field_name'` for JSON\n- **Timestamps**: `AT TIME ZONE 'UTC'` for timezone conversion\n- **String matching**: `LIKE`, `col ~ 'pattern'` for regex\n- **Boolean**: Native BOOLEAN type; use `TRUE`/`FALSE`\n```\n\n---\n\n## Databricks / Spark SQL\n\n```markdown\n## SQL Dialect: Databricks/Spark SQL\n\n- **Table references**: `catalog.schema.table` (Unity Catalog) or `schema.table`\n- **Safe division**: Use `NULLIF`: `a / NULLIF(b, 0)` or `TRY_DIVIDE(a, b)`\n- **Date functions**:\n  - `DATE_TRUNC('MONTH', date_col)`\n  - `DATE_SUB(date_col, 1)`\n  - `DATEDIFF(end_date, start_date)`\n- **Column exclusion**: `SELECT * EXCEPT (column_to_exclude)` (Databricks SQL)\n- **Arrays**: `EXPLODE(array_column)` to flatten\n- **Structs**: Access with dot notation `struct_col.field_name`\n- **JSON**: `json_col:field_name` or `GET_JSON_OBJECT()`\n- **String matching**: `LIKE`, `RLIKE` for regex\n- **Delta features**: `DESCRIBE HISTORY`, time travel with `VERSION AS OF`\n```\n\n---\n\n## MySQL\n\n```markdown\n## SQL Dialect: MySQL\n\n- **Table references**: \\`database\\`.\\`table\\` with backticks\n- **Safe division**: Manual: `IF(b = 0, NULL, a / b)` or `a / NULLIF(b, 0)`\n- **Date functions**:\n  - `DATE_FORMAT(date_col, '%Y-%m-01')` for truncation\n  - `DATE_SUB(date_col, INTERVAL 1 DAY)`\n  - `DATEDIFF(end_date, start_date)`\n- **Column selection**: No EXCEPT; must list columns explicitly\n- **Arrays**: Limited native support; often stored as JSON\n- **JSON**: `JSON_EXTRACT(col, '$.field')` or `col-\u003e\u003e'$.field'`\n- **Timestamps**: `CONVERT_TZ()` for timezone conversion\n- **String matching**: `LIKE`, `REGEXP` for regex\n- **Case sensitivity**: Table names case-sensitive on Linux, not on Windows\n```\n\n---\n\n## Common Patterns Across Dialects\n\n| Operation | BigQuery | Snowflake | PostgreSQL | Databricks |\n|-----------|----------|-----------|------------|------------|\n| Current date | `CURRENT_DATE()` | `CURRENT_DATE()` | `CURRENT_DATE` | `CURRENT_DATE()` |\n| Current timestamp | `CURRENT_TIMESTAMP()` | `CURRENT_TIMESTAMP()` | `NOW()` | `CURRENT_TIMESTAMP()` |\n| String concat | `CONCAT()` or `\\|\\|` | `CONCAT()` or `\\|\\|` | `CONCAT()` or `\\|\\|` | `CONCAT()` or `\\|\\|` |\n| Coalesce | `COALESCE()` | `COALESCE()` | `COALESCE()` | `COALESCE()` |\n| Case when | `CASE WHEN` | `CASE WHEN` | `CASE WHEN` | `CASE WHEN` |\n| Count distinct | `COUNT(DISTINCT x)` | `COUNT(DISTINCT x)` | `COUNT(DISTINCT x)` | `COUNT(DISTINCT x)` |\n", "depth": 5}, {"id": "411", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/references/domain-template.md", "name": "domain-template.md", "type": "file", "content": "# Domain Reference File Template\n\nUse this template when creating reference files for specific data domains (e.g., revenue, users, marketing).\n\n---\n\n```markdown\n# [DOMAIN_NAME] Tables\n\nThis document contains [domain]-related tables, metrics, and query patterns.\n\n---\n\n## Quick Reference\n\n### Business Context\n\n[2-3 sentences explaining what this domain covers and key concepts]\n\n### Entity Clarification\n\n**\"[AMBIGUOUS_TERM]\" can mean:**\n- **[MEANING_1]**: [DEFINITION] ([TABLE]: [ID_FIELD])\n- **[MEANING_2]**: [DEFINITION] ([TABLE]: [ID_FIELD])\n\nAlways clarify which one before querying.\n\n### Standard Filters\n\nFor [domain] queries, always:\n```sql\nWHERE [STANDARD_FILTER_1]\n  AND [STANDARD_FILTER_2]\n```\n\n---\n\n## Key Tables\n\n### [TABLE_1_NAME]\n**Location**: `[project.dataset.table]` or `[schema.table]`\n**Description**: [What this table contains, when to use it]\n**Primary Key**: [COLUMN(S)]\n**Update Frequency**: [Daily/Hourly/Real-time] ([LAG] lag)\n**Partitioned By**: [PARTITION_COLUMN] (if applicable)\n\n| Column | Type | Description | Notes |\n|--------|------|-------------|-------|\n| **[column_1]** | [TYPE] | [DESCRIPTION] | [GOTCHA_OR_CONTEXT] |\n| **[column_2]** | [TYPE] | [DESCRIPTION] | |\n| **[column_3]** | [TYPE] | [DESCRIPTION] | Nullable |\n\n**Relationships**:\n- Joins to `[OTHER_TABLE]` on `[JOIN_KEY]`\n- Parent of `[CHILD_TABLE]` via `[FOREIGN_KEY]`\n\n**Nested/Struct Fields** (if applicable):\n- `[struct_name].[field_1]`: [DESCRIPTION]\n- `[struct_name].[field_2]`: [DESCRIPTION]\n\n---\n\n### [TABLE_2_NAME]\n[REPEAT FORMAT]\n\n---\n\n## Key Metrics\n\n| Metric | Definition | Table | Formula | Notes |\n|--------|------------|-------|---------|-------|\n| [METRIC_1] | [DEFINITION] | [TABLE] | `[FORMULA]` | [CAVEATS] |\n| [METRIC_2] | [DEFINITION] | [TABLE] | `[FORMULA]` | |\n\n---\n\n## Sample Queries\n\n### [QUERY_PURPOSE_1]\n```sql\n-- [Brief description of what this query does]\nSELECT\n    [columns]\nFROM [table]\nWHERE [standard_filters]\nGROUP BY [grouping]\nORDER BY [ordering]\n```\n\n### [QUERY_PURPOSE_2]\n```sql\n[ANOTHER_COMMON_QUERY]\n```\n\n### [QUERY_PURPOSE_3]: [More Complex Pattern]\n```sql\nWITH [cte_name] AS (\n    [CTE_LOGIC]\n)\nSELECT\n    [final_columns]\nFROM [cte_name]\n[joins_and_filters]\n```\n\n---\n\n## Common Gotchas\n\n1. **[GOTCHA_1]**: [EXPLANATION]\n   - Wrong: `[INCORRECT_APPROACH]`\n   - Right: `[CORRECT_APPROACH]`\n\n2. **[GOTCHA_2]**: [EXPLANATION]\n\n---\n\n## Related Dashboards (if applicable)\n\n| Dashboard | Link | Use For |\n|-----------|------|---------|\n| [DASHBOARD_1] | [URL] | [DESCRIPTION] |\n| [DASHBOARD_2] | [URL] | [DESCRIPTION] |\n```\n\n---\n\n## Tips for Creating Domain Files\n\n1. **Start with the most-queried tables** - Don't try to document everything\n2. **Include column-level detail only for important columns** - Skip obvious ones like `created_at`\n3. **Real query examples \u003e abstract descriptions** - Show don't tell\n4. **Document the gotchas prominently** - These save the most time\n5. **Keep sample queries runnable** - Use real table/column names\n6. **Note nested/struct fields explicitly** - These trip people up\n\n## Suggested Domain Files\n\nCommon domains to document (create separate files for each):\n\n- `revenue.md` - Billing, subscriptions, ARR, transactions\n- `users.md` - Accounts, authentication, user attributes\n- `product.md` - Feature usage, events, sessions\n- `growth.md` - DAU/WAU/MAU, retention, activation\n- `sales.md` - CRM, pipeline, opportunities\n- `marketing.md` - Campaigns, attribution, leads\n- `support.md` - Tickets, CSAT, response times\n", "depth": 5}, {"id": "412", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/references/skill-template.md", "name": "skill-template.md", "type": "file", "content": "# Generated Skill Template\n\nUse this template when generating a new data analysis skill. Replace all `[PLACEHOLDER]` values.\n\n---\n\n```markdown\n---\nname: [company]-data-analyst\ndescription: \"[COMPANY] data analysis skill. Provides context for querying [WAREHOUSE_TYPE] including entity definitions, metric calculations, and common query patterns. Use when analyzing [COMPANY] data for: (1) [PRIMARY_USE_CASE_1], (2) [PRIMARY_USE_CASE_2], (3) [PRIMARY_USE_CASE_3], or any data questions requiring [COMPANY]-specific context.\"\n---\n\n# [COMPANY] Data Analysis\n\n## SQL Dialect: [WAREHOUSE_TYPE]\n\n[INSERT APPROPRIATE DIALECT SECTION FROM sql-dialects.md]\n\n---\n\n## Entity Disambiguation\n\nWhen users mention these terms, clarify which entity they mean:\n\n[EXAMPLE FORMAT - customize based on discovery:]\n\n**\"User\" can mean:**\n- **Account**: An individual login/profile ([PRIMARY_TABLE]: [ID_FIELD])\n- **Organization**: A billing entity that can have multiple accounts ([ORG_TABLE]: [ORG_ID])\n- **[OTHER_TYPE]**: [DEFINITION] ([TABLE]: [ID])\n\n**Relationships:**\n- [ENTITY_1] → [ENTITY_2]: [RELATIONSHIP_TYPE] (join on [JOIN_KEY])\n\n---\n\n## Business Terminology\n\n| Term | Definition | Notes |\n|------|------------|-------|\n| [TERM_1] | [DEFINITION] | [CONTEXT/GOTCHA] |\n| [TERM_2] | [DEFINITION] | [CONTEXT/GOTCHA] |\n| [ACRONYM] | [FULL_NAME] - [EXPLANATION] | |\n\n---\n\n## Standard Filters\n\nAlways apply these filters unless explicitly told otherwise:\n\n```sql\n-- Exclude test/internal data\nWHERE [TEST_FLAG_COLUMN] = FALSE\n  AND [INTERNAL_FLAG_COLUMN] = FALSE\n\n-- Exclude invalid/fraud\n  AND [STATUS_COLUMN] != '[EXCLUDED_STATUS]'\n\n-- [OTHER STANDARD EXCLUSIONS]\n```\n\n**When to override:**\n- [SCENARIO_1]: Include [NORMALLY_EXCLUDED] when [CONDITION]\n\n---\n\n## Key Metrics\n\n### [METRIC_1_NAME]\n- **Definition**: [PLAIN_ENGLISH_EXPLANATION]\n- **Formula**: `[EXACT_CALCULATION]`\n- **Source**: `[TABLE_NAME].[COLUMN_NAME]`\n- **Time grain**: [DAILY/WEEKLY/MONTHLY]\n- **Caveats**: [EDGE_CASES_OR_GOTCHAS]\n\n### [METRIC_2_NAME]\n[REPEAT FORMAT]\n\n---\n\n## Data Freshness\n\n| Table | Update Frequency | Typical Lag |\n|-------|------------------|-------------|\n| [TABLE_1] | [FREQUENCY] | [LAG] |\n| [TABLE_2] | [FREQUENCY] | [LAG] |\n\nTo check data freshness:\n```sql\nSELECT MAX([DATE_COLUMN]) as latest_data FROM [TABLE]\n```\n\n---\n\n## Knowledge Base Navigation\n\nUse these reference files for detailed table documentation:\n\n| Domain | Reference File | Use For |\n|--------|----------------|---------|\n| [DOMAIN_1] | `references/[domain1].md` | [BRIEF_DESCRIPTION] |\n| [DOMAIN_2] | `references/[domain2].md` | [BRIEF_DESCRIPTION] |\n| Entities | `references/entities.md` | Entity definitions and relationships |\n| Metrics | `references/metrics.md` | KPI calculations and formulas |\n\n---\n\n## Common Query Patterns\n\n### [PATTERN_1_NAME]\n```sql\n[SAMPLE_QUERY]\n```\n\n### [PATTERN_2_NAME]\n```sql\n[SAMPLE_QUERY]\n```\n\n---\n\n## Troubleshooting\n\n### Common Mistakes\n- **[MISTAKE_1]**: [EXPLANATION] → [CORRECT_APPROACH]\n- **[MISTAKE_2]**: [EXPLANATION] → [CORRECT_APPROACH]\n\n### Access Issues\n- If you encounter permission errors on `[TABLE]`: [WORKAROUND]\n- For PII-restricted columns: [ALTERNATIVE_APPROACH]\n\n### Performance Tips\n- Filter by `[PARTITION_COLUMN]` first to reduce data scanned\n- For large tables, use `LIMIT` during exploration\n- Prefer `[AGGREGATED_TABLE]` over `[RAW_TABLE]` when possible\n```\n\n---\n\n## Customization Notes\n\nWhen generating a skill:\n\n1. **Fill all placeholders** - Don't leave any `[PLACEHOLDER]` text\n2. **Remove unused sections** - If they don't have dashboards, remove that section\n3. **Add specificity** - Generic advice is less useful than specific column names and values\n4. **Include real examples** - Sample queries should use actual table/column names\n5. **Keep it scannable** - Use tables and code blocks liberally\n", "depth": 5}, {"id": "413", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-context-extractor/references/example-output.md", "name": "example-output.md", "type": "file", "content": "# Example: Generated Skill\n\nThis is an example of what a generated skill looks like after the bootstrap process. This example is for a fictional e-commerce company called \"ShopCo\" using Snowflake.\n\n---\n\n## Example SKILL.md\n\n```markdown\n---\nname: shopco-data-analyst\ndescription: \"ShopCo data analysis skill for Snowflake. Provides context for querying e-commerce data including customer, order, and product analytics. Use when analyzing ShopCo data for: (1) Revenue and order metrics, (2) Customer behavior and retention, (3) Product performance, or any data questions requiring ShopCo-specific context.\"\n---\n\n# ShopCo Data Analysis\n\n## SQL Dialect: Snowflake\n\n- **Table references**: `SHOPCO_DW.SCHEMA.TABLE` or with quotes for case-sensitive: `\"Column_Name\"`\n- **Safe division**: `DIV0(a, b)` returns 0, `DIV0NULL(a, b)` returns NULL\n- **Date functions**:\n  - `DATE_TRUNC('MONTH', date_col)`\n  - `DATEADD(DAY, -1, date_col)`\n  - `DATEDIFF(DAY, start_date, end_date)`\n- **Column exclusion**: `SELECT * EXCLUDE (column_to_exclude)`\n\n---\n\n## Entity Disambiguation\n\n**\"Customer\" can mean:**\n- **User**: A login account that can browse and save items (CORE.DIM_USERS: user_id)\n- **Customer**: A user who has made at least one purchase (CORE.DIM_CUSTOMERS: customer_id)\n- **Account**: A billing entity, can have multiple users in B2B (CORE.DIM_ACCOUNTS: account_id)\n\n**Relationships:**\n- User → Customer: 1:1 (customer_id = user_id for purchasers)\n- Account → User: 1:many (join on account_id)\n\n---\n\n## Business Terminology\n\n| Term | Definition | Notes |\n|------|------------|-------|\n| GMV | Gross Merchandise Value - total order value before returns/discounts | Use for top-line reporting |\n| NMV | Net Merchandise Value - GMV minus returns and discounts | Use for actual revenue |\n| AOV | Average Order Value - NMV / order count | Exclude $0 orders |\n| LTV | Lifetime Value - total NMV per customer since first order | Rolling calc, updates daily |\n| CAC | Customer Acquisition Cost - marketing spend / new customers | By cohort month |\n\n---\n\n## Standard Filters\n\nAlways apply these filters unless explicitly told otherwise:\n\n```sql\n-- Exclude test and internal orders\nWHERE order_status != 'TEST'\n  AND customer_type != 'INTERNAL'\n  AND is_employee_order = FALSE\n\n-- Exclude cancelled orders for revenue metrics\n  AND order_status NOT IN ('CANCELLED', 'FRAUDULENT')\n```\n\n---\n\n## Key Metrics\n\n### Gross Merchandise Value (GMV)\n- **Definition**: Total value of all orders placed\n- **Formula**: `SUM(order_total_gross)`\n- **Source**: `CORE.FCT_ORDERS.order_total_gross`\n- **Time grain**: Daily, aggregated to weekly/monthly\n- **Caveats**: Includes orders that may later be cancelled or returned\n\n### Net Revenue\n- **Definition**: Actual revenue after returns and discounts\n- **Formula**: `SUM(order_total_gross - return_amount - discount_amount)`\n- **Source**: `CORE.FCT_ORDERS`\n- **Caveats**: Returns can occur up to 90 days post-order; use settled_revenue for finalized numbers\n\n---\n\n## Knowledge Base Navigation\n\n| Domain | Reference File | Use For |\n|--------|----------------|---------|\n| Orders | `references/orders.md` | Order tables, GMV/NMV calculations |\n| Customers | `references/customers.md` | User/customer entities, LTV, cohorts |\n| Products | `references/products.md` | Catalog, inventory, categories |\n\n---\n\n## Common Query Patterns\n\n### Daily GMV by Channel\n```sql\nSELECT\n    DATE_TRUNC('DAY', order_timestamp) AS order_date,\n    channel,\n    SUM(order_total_gross) AS gmv,\n    COUNT(DISTINCT order_id) AS order_count\nFROM SHOPCO_DW.CORE.FCT_ORDERS\nWHERE order_status NOT IN ('TEST', 'CANCELLED', 'FRAUDULENT')\n  AND order_timestamp \u003e= DATEADD(DAY, -30, CURRENT_DATE())\nGROUP BY 1, 2\nORDER BY 1 DESC, 3 DESC\n```\n\n### Customer Cohort Retention\n```sql\nWITH cohorts AS (\n    SELECT\n        customer_id,\n        DATE_TRUNC('MONTH', first_order_date) AS cohort_month\n    FROM SHOPCO_DW.CORE.DIM_CUSTOMERS\n)\nSELECT\n    c.cohort_month,\n    DATEDIFF(MONTH, c.cohort_month, DATE_TRUNC('MONTH', o.order_timestamp)) AS months_since_first,\n    COUNT(DISTINCT c.customer_id) AS active_customers\nFROM cohorts c\nJOIN SHOPCO_DW.CORE.FCT_ORDERS o ON c.customer_id = o.customer_id\nWHERE o.order_status NOT IN ('TEST', 'CANCELLED')\nGROUP BY 1, 2\nORDER BY 1, 2\n```\n```\n\n---\n\n## Example references/orders.md\n\n```markdown\n# Orders Tables\n\nOrder and transaction data for ShopCo.\n\n---\n\n## Key Tables\n\n### FCT_ORDERS\n**Location**: `SHOPCO_DW.CORE.FCT_ORDERS`\n**Description**: Fact table of all orders. One row per order.\n**Primary Key**: `order_id`\n**Update Frequency**: Hourly (15 min lag)\n**Partitioned By**: `order_date`\n\n| Column | Type | Description | Notes |\n|--------|------|-------------|-------|\n| **order_id** | VARCHAR | Unique order identifier | |\n| **customer_id** | VARCHAR | FK to DIM_CUSTOMERS | NULL for guest checkout |\n| **order_timestamp** | TIMESTAMP_NTZ | When order was placed | UTC |\n| **order_date** | DATE | Date portion of order_timestamp | Partition column |\n| **order_status** | VARCHAR | Current status | PENDING, SHIPPED, DELIVERED, CANCELLED, RETURNED |\n| **channel** | VARCHAR | Acquisition channel | WEB, APP, MARKETPLACE |\n| **order_total_gross** | DECIMAL(12,2) | Pre-discount total | |\n| **discount_amount** | DECIMAL(12,2) | Total discounts applied | |\n| **return_amount** | DECIMAL(12,2) | Value of returned items | Updates async |\n\n**Relationships**:\n- Joins to `DIM_CUSTOMERS` on `customer_id`\n- Parent of `FCT_ORDER_ITEMS` via `order_id`\n\n---\n\n## Sample Queries\n\n### Orders with Returns Rate\n```sql\nSELECT\n    DATE_TRUNC('WEEK', order_date) AS week,\n    COUNT(*) AS total_orders,\n    SUM(CASE WHEN return_amount \u003e 0 THEN 1 ELSE 0 END) AS orders_with_returns,\n    DIV0(SUM(CASE WHEN return_amount \u003e 0 THEN 1 ELSE 0 END), COUNT(*)) AS return_rate\nFROM SHOPCO_DW.CORE.FCT_ORDERS\nWHERE order_status NOT IN ('TEST', 'CANCELLED')\n  AND order_date \u003e= DATEADD(MONTH, -3, CURRENT_DATE())\nGROUP BY 1\nORDER BY 1\n```\n```\n\n---\n\nThis example demonstrates:\n- Complete frontmatter with triggering description\n- Dialect-specific SQL notes\n- Clear entity disambiguation\n- Terminology glossary\n- Standard filters as copy-paste SQL\n- Metric definitions with formulas\n- Navigation to reference files\n- Real, runnable query examples\n", "depth": 5}, {"id": "414", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/sql-queries/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: sql-queries\ndescription: Write correct, performant SQL across all major data warehouse dialects (Snowflake, BigQuery, Databricks, PostgreSQL, etc.). Use when writing queries, optimizing slow SQL, translating between dialects, or building complex analytical queries with CTEs, window functions, or aggregations.\n---\n\n# SQL Queries Skill\n\nWrite correct, performant, readable SQL across all major data warehouse dialects.\n\n## Dialect-Specific Reference\n\n### PostgreSQL (including Aurora, RDS, Supabase, Neon)\n\n**Date/time:**\n```sql\n-- Current date/time\nCURRENT_DATE, CURRENT_TIMESTAMP, NOW()\n\n-- Date arithmetic\ndate_column + INTERVAL '7 days'\ndate_column - INTERVAL '1 month'\n\n-- Truncate to period\nDATE_TRUNC('month', created_at)\n\n-- Extract parts\nEXTRACT(YEAR FROM created_at)\nEXTRACT(DOW FROM created_at)  -- 0=Sunday\n\n-- Format\nTO_CHAR(created_at, 'YYYY-MM-DD')\n```\n\n**String functions:**\n```sql\n-- Concatenation\nfirst_name || ' ' || last_name\nCONCAT(first_name, ' ', last_name)\n\n-- Pattern matching\ncolumn ILIKE '%pattern%'  -- case-insensitive\ncolumn ~ '^regex_pattern$'  -- regex\n\n-- String manipulation\nLEFT(str, n), RIGHT(str, n)\nSPLIT_PART(str, delimiter, position)\nREGEXP_REPLACE(str, pattern, replacement)\n```\n\n**Arrays and JSON:**\n```sql\n-- JSON access\ndata-\u003e\u003e'key'  -- text\ndata-\u003e'nested'-\u003e'key'  -- json\ndata#\u003e\u003e'{path,to,key}'  -- nested text\n\n-- Array operations\nARRAY_AGG(column)\nANY(array_column)\narray_column @\u003e ARRAY['value']\n```\n\n**Performance tips:**\n- Use `EXPLAIN ANALYZE` to profile queries\n- Create indexes on frequently filtered/joined columns\n- Use `EXISTS` over `IN` for correlated subqueries\n- Partial indexes for common filter conditions\n- Use connection pooling for concurrent access\n\n---\n\n### Snowflake\n\n**Date/time:**\n```sql\n-- Current date/time\nCURRENT_DATE(), CURRENT_TIMESTAMP(), SYSDATE()\n\n-- Date arithmetic\nDATEADD(day, 7, date_column)\nDATEDIFF(day, start_date, end_date)\n\n-- Truncate to period\nDATE_TRUNC('month', created_at)\n\n-- Extract parts\nYEAR(created_at), MONTH(created_at), DAY(created_at)\nDAYOFWEEK(created_at)\n\n-- Format\nTO_CHAR(created_at, 'YYYY-MM-DD')\n```\n\n**String functions:**\n```sql\n-- Case-insensitive by default (depends on collation)\ncolumn ILIKE '%pattern%'\nREGEXP_LIKE(column, 'pattern')\n\n-- Parse JSON\ncolumn:key::string  -- dot notation for VARIANT\nPARSE_JSON('{\"key\": \"value\"}')\nGET_PATH(variant_col, 'path.to.key')\n\n-- Flatten arrays/objects\nSELECT f.value FROM table, LATERAL FLATTEN(input =\u003e array_col) f\n```\n\n**Semi-structured data:**\n```sql\n-- VARIANT type access\ndata:customer:name::STRING\ndata:items[0]:price::NUMBER\n\n-- Flatten nested structures\nSELECT\n    t.id,\n    item.value:name::STRING as item_name,\n    item.value:qty::NUMBER as quantity\nFROM my_table t,\nLATERAL FLATTEN(input =\u003e t.data:items) item\n```\n\n**Performance tips:**\n- Use clustering keys on large tables (not traditional indexes)\n- Filter on clustering key columns for partition pruning\n- Set appropriate warehouse size for query complexity\n- Use `RESULT_SCAN(LAST_QUERY_ID())` to avoid re-running expensive queries\n- Use transient tables for staging/temp data\n\n---\n\n### BigQuery (Google Cloud)\n\n**Date/time:**\n```sql\n-- Current date/time\nCURRENT_DATE(), CURRENT_TIMESTAMP()\n\n-- Date arithmetic\nDATE_ADD(date_column, INTERVAL 7 DAY)\nDATE_SUB(date_column, INTERVAL 1 MONTH)\nDATE_DIFF(end_date, start_date, DAY)\nTIMESTAMP_DIFF(end_ts, start_ts, HOUR)\n\n-- Truncate to period\nDATE_TRUNC(created_at, MONTH)\nTIMESTAMP_TRUNC(created_at, HOUR)\n\n-- Extract parts\nEXTRACT(YEAR FROM created_at)\nEXTRACT(DAYOFWEEK FROM created_at)  -- 1=Sunday\n\n-- Format\nFORMAT_DATE('%Y-%m-%d', date_column)\nFORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', ts_column)\n```\n\n**String functions:**\n```sql\n-- No ILIKE, use LOWER()\nLOWER(column) LIKE '%pattern%'\nREGEXP_CONTAINS(column, r'pattern')\nREGEXP_EXTRACT(column, r'pattern')\n\n-- String manipulation\nSPLIT(str, delimiter)  -- returns ARRAY\nARRAY_TO_STRING(array, delimiter)\n```\n\n**Arrays and structs:**\n```sql\n-- Array operations\nARRAY_AGG(column)\nUNNEST(array_column)\nARRAY_LENGTH(array_column)\nvalue IN UNNEST(array_column)\n\n-- Struct access\nstruct_column.field_name\n```\n\n**Performance tips:**\n- Always filter on partition columns (usually date) to reduce bytes scanned\n- Use clustering for frequently filtered columns within partitions\n- Use `APPROX_COUNT_DISTINCT()` for large-scale cardinality estimates\n- Avoid `SELECT *` -- billing is per-byte scanned\n- Use `DECLARE` and `SET` for parameterized scripts\n- Preview query cost with dry run before executing large queries\n\n---\n\n### Redshift (Amazon)\n\n**Date/time:**\n```sql\n-- Current date/time\nCURRENT_DATE, GETDATE(), SYSDATE\n\n-- Date arithmetic\nDATEADD(day, 7, date_column)\nDATEDIFF(day, start_date, end_date)\n\n-- Truncate to period\nDATE_TRUNC('month', created_at)\n\n-- Extract parts\nEXTRACT(YEAR FROM created_at)\nDATE_PART('dow', created_at)\n```\n\n**String functions:**\n```sql\n-- Case-insensitive\ncolumn ILIKE '%pattern%'\nREGEXP_INSTR(column, 'pattern') \u003e 0\n\n-- String manipulation\nSPLIT_PART(str, delimiter, position)\nLISTAGG(column, ', ') WITHIN GROUP (ORDER BY column)\n```\n\n**Performance tips:**\n- Design distribution keys for collocated joins (DISTKEY)\n- Use sort keys for frequently filtered columns (SORTKEY)\n- Use `EXPLAIN` to check query plan\n- Avoid cross-node data movement (watch for DS_BCAST and DS_DIST)\n- `ANALYZE` and `VACUUM` regularly\n- Use late-binding views for schema flexibility\n\n---\n\n### Databricks SQL\n\n**Date/time:**\n```sql\n-- Current date/time\nCURRENT_DATE(), CURRENT_TIMESTAMP()\n\n-- Date arithmetic\nDATE_ADD(date_column, 7)\nDATEDIFF(end_date, start_date)\nADD_MONTHS(date_column, 1)\n\n-- Truncate to period\nDATE_TRUNC('MONTH', created_at)\nTRUNC(date_column, 'MM')\n\n-- Extract parts\nYEAR(created_at), MONTH(created_at)\nDAYOFWEEK(created_at)\n```\n\n**Delta Lake features:**\n```sql\n-- Time travel\nSELECT * FROM my_table TIMESTAMP AS OF '2024-01-15'\nSELECT * FROM my_table VERSION AS OF 42\n\n-- Describe history\nDESCRIBE HISTORY my_table\n\n-- Merge (upsert)\nMERGE INTO target USING source\nON target.id = source.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n```\n\n**Performance tips:**\n- Use Delta Lake's `OPTIMIZE` and `ZORDER` for query performance\n- Leverage Photon engine for compute-intensive queries\n- Use `CACHE TABLE` for frequently accessed datasets\n- Partition by low-cardinality date columns\n\n---\n\n## Common SQL Patterns\n\n### Window Functions\n\n```sql\n-- Ranking\nROW_NUMBER() OVER (PARTITION BY user_id ORDER BY created_at DESC)\nRANK() OVER (PARTITION BY category ORDER BY revenue DESC)\nDENSE_RANK() OVER (ORDER BY score DESC)\n\n-- Running totals / moving averages\nSUM(revenue) OVER (ORDER BY date_col ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total\nAVG(revenue) OVER (ORDER BY date_col ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as moving_avg_7d\n\n-- Lag / Lead\nLAG(value, 1) OVER (PARTITION BY entity ORDER BY date_col) as prev_value\nLEAD(value, 1) OVER (PARTITION BY entity ORDER BY date_col) as next_value\n\n-- First / Last value\nFIRST_VALUE(status) OVER (PARTITION BY user_id ORDER BY created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\nLAST_VALUE(status) OVER (PARTITION BY user_id ORDER BY created_at ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n\n-- Percent of total\nrevenue / SUM(revenue) OVER () as pct_of_total\nrevenue / SUM(revenue) OVER (PARTITION BY category) as pct_of_category\n```\n\n### CTEs for Readability\n\n```sql\nWITH\n-- Step 1: Define the base population\nbase_users AS (\n    SELECT user_id, created_at, plan_type\n    FROM users\n    WHERE created_at \u003e= DATE '2024-01-01'\n      AND status = 'active'\n),\n\n-- Step 2: Calculate user-level metrics\nuser_metrics AS (\n    SELECT\n        u.user_id,\n        u.plan_type,\n        COUNT(DISTINCT e.session_id) as session_count,\n        SUM(e.revenue) as total_revenue\n    FROM base_users u\n    LEFT JOIN events e ON u.user_id = e.user_id\n    GROUP BY u.user_id, u.plan_type\n),\n\n-- Step 3: Aggregate to summary level\nsummary AS (\n    SELECT\n        plan_type,\n        COUNT(*) as user_count,\n        AVG(session_count) as avg_sessions,\n        SUM(total_revenue) as total_revenue\n    FROM user_metrics\n    GROUP BY plan_type\n)\n\nSELECT * FROM summary ORDER BY total_revenue DESC;\n```\n\n### Cohort Retention\n\n```sql\nWITH cohorts AS (\n    SELECT\n        user_id,\n        DATE_TRUNC('month', first_activity_date) as cohort_month\n    FROM users\n),\nactivity AS (\n    SELECT\n        user_id,\n        DATE_TRUNC('month', activity_date) as activity_month\n    FROM user_activity\n)\nSELECT\n    c.cohort_month,\n    COUNT(DISTINCT c.user_id) as cohort_size,\n    COUNT(DISTINCT CASE\n        WHEN a.activity_month = c.cohort_month THEN a.user_id\n    END) as month_0,\n    COUNT(DISTINCT CASE\n        WHEN a.activity_month = c.cohort_month + INTERVAL '1 month' THEN a.user_id\n    END) as month_1,\n    COUNT(DISTINCT CASE\n        WHEN a.activity_month = c.cohort_month + INTERVAL '3 months' THEN a.user_id\n    END) as month_3\nFROM cohorts c\nLEFT JOIN activity a ON c.user_id = a.user_id\nGROUP BY c.cohort_month\nORDER BY c.cohort_month;\n```\n\n### Funnel Analysis\n\n```sql\nWITH funnel AS (\n    SELECT\n        user_id,\n        MAX(CASE WHEN event = 'page_view' THEN 1 ELSE 0 END) as step_1_view,\n        MAX(CASE WHEN event = 'signup_start' THEN 1 ELSE 0 END) as step_2_start,\n        MAX(CASE WHEN event = 'signup_complete' THEN 1 ELSE 0 END) as step_3_complete,\n        MAX(CASE WHEN event = 'first_purchase' THEN 1 ELSE 0 END) as step_4_purchase\n    FROM events\n    WHERE event_date \u003e= CURRENT_DATE - INTERVAL '30 days'\n    GROUP BY user_id\n)\nSELECT\n    COUNT(*) as total_users,\n    SUM(step_1_view) as viewed,\n    SUM(step_2_start) as started_signup,\n    SUM(step_3_complete) as completed_signup,\n    SUM(step_4_purchase) as purchased,\n    ROUND(100.0 * SUM(step_2_start) / NULLIF(SUM(step_1_view), 0), 1) as view_to_start_pct,\n    ROUND(100.0 * SUM(step_3_complete) / NULLIF(SUM(step_2_start), 0), 1) as start_to_complete_pct,\n    ROUND(100.0 * SUM(step_4_purchase) / NULLIF(SUM(step_3_complete), 0), 1) as complete_to_purchase_pct\nFROM funnel;\n```\n\n### Deduplication\n\n```sql\n-- Keep the most recent record per key\nWITH ranked AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY entity_id\n            ORDER BY updated_at DESC\n        ) as rn\n    FROM source_table\n)\nSELECT * FROM ranked WHERE rn = 1;\n```\n\n## Error Handling and Debugging\n\nWhen a query fails:\n\n1. **Syntax errors**: Check for dialect-specific syntax (e.g., `ILIKE` not available in BigQuery, `SAFE_DIVIDE` only in BigQuery)\n2. **Column not found**: Verify column names against schema -- check for typos, case sensitivity (PostgreSQL is case-sensitive for quoted identifiers)\n3. **Type mismatches**: Cast explicitly when comparing different types (`CAST(col AS DATE)`, `col::DATE`)\n4. **Division by zero**: Use `NULLIF(denominator, 0)` or dialect-specific safe division\n5. **Ambiguous columns**: Always qualify column names with table alias in JOINs\n6. **Group by errors**: All non-aggregated columns must be in GROUP BY (except in BigQuery which allows grouping by alias)\n", "depth": 4}, {"id": "415", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-visualization/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: data-visualization\ndescription: Create effective data visualizations with Python (matplotlib, seaborn, plotly). Use when building charts, choosing the right chart type for a dataset, creating publication-quality figures, or applying design principles like accessibility and color theory.\n---\n\n# Data Visualization Skill\n\nChart selection guidance, Python visualization code patterns, design principles, and accessibility considerations for creating effective data visualizations.\n\n## Chart Selection Guide\n\n### Choose by Data Relationship\n\n| What You're Showing | Best Chart | Alternatives |\n|---|---|---|\n| **Trend over time** | Line chart | Area chart (if showing cumulative or composition) |\n| **Comparison across categories** | Vertical bar chart | Horizontal bar (many categories), lollipop chart |\n| **Ranking** | Horizontal bar chart | Dot plot, slope chart (comparing two periods) |\n| **Part-to-whole composition** | Stacked bar chart | Treemap (hierarchical), waffle chart |\n| **Composition over time** | Stacked area chart | 100% stacked bar (for proportion focus) |\n| **Distribution** | Histogram | Box plot (comparing groups), violin plot, strip plot |\n| **Correlation (2 variables)** | Scatter plot | Bubble chart (add 3rd variable as size) |\n| **Correlation (many variables)** | Heatmap (correlation matrix) | Pair plot |\n| **Geographic patterns** | Choropleth map | Bubble map, hex map |\n| **Flow / process** | Sankey diagram | Funnel chart (sequential stages) |\n| **Relationship network** | Network graph | Chord diagram |\n| **Performance vs. target** | Bullet chart | Gauge (single KPI only) |\n| **Multiple KPIs at once** | Small multiples | Dashboard with separate charts |\n\n### When NOT to Use Certain Charts\n\n- **Pie charts**: Avoid unless \u003c6 categories and exact proportions matter less than rough comparison. Humans are bad at comparing angles. Use bar charts instead.\n- **3D charts**: Never. They distort perception and add no information.\n- **Dual-axis charts**: Use cautiously. They can mislead by implying correlation. Clearly label both axes if used.\n- **Stacked bar (many categories)**: Hard to compare middle segments. Use small multiples or grouped bars instead.\n- **Donut charts**: Slightly better than pie charts but same fundamental issues. Use for single KPI display at most.\n\n## Python Visualization Code Patterns\n\n### Setup and Style\n\n```python\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Professional style setup\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams.update({\n    'figure.figsize': (10, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.titlesize': 14,\n    'axes.titleweight': 'bold',\n    'axes.labelsize': 11,\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    'legend.fontsize': 10,\n    'figure.titlesize': 16,\n})\n\n# Colorblind-friendly palettes\nPALETTE_CATEGORICAL = ['#4C72B0', '#DD8452', '#55A868', '#C44E52', '#8172B3', '#937860']\nPALETTE_SEQUENTIAL = 'YlOrRd'\nPALETTE_DIVERGING = 'RdBu_r'\n```\n\n### Line Chart (Time Series)\n\n```python\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor label, group in df.groupby('category'):\n    ax.plot(group['date'], group['value'], label=label, linewidth=2)\n\nax.set_title('Metric Trend by Category', fontweight='bold')\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.legend(loc='upper left', frameon=True)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Format dates on x-axis\nfig.autofmt_xdate()\n\nplt.tight_layout()\nplt.savefig('trend_chart.png', dpi=150, bbox_inches='tight')\n```\n\n### Bar Chart (Comparison)\n\n```python\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Sort by value for easy reading\ndf_sorted = df.sort_values('metric', ascending=True)\n\nbars = ax.barh(df_sorted['category'], df_sorted['metric'], color=PALETTE_CATEGORICAL[0])\n\n# Add value labels\nfor bar in bars:\n    width = bar.get_width()\n    ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n            f'{width:,.0f}', ha='left', va='center', fontsize=10)\n\nax.set_title('Metric by Category (Ranked)', fontweight='bold')\nax.set_xlabel('Metric Value')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.savefig('bar_chart.png', dpi=150, bbox_inches='tight')\n```\n\n### Histogram (Distribution)\n\n```python\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.hist(df['value'], bins=30, color=PALETTE_CATEGORICAL[0], edgecolor='white', alpha=0.8)\n\n# Add mean and median lines\nmean_val = df['value'].mean()\nmedian_val = df['value'].median()\nax.axvline(mean_val, color='red', linestyle='--', linewidth=1.5, label=f'Mean: {mean_val:,.1f}')\nax.axvline(median_val, color='green', linestyle='--', linewidth=1.5, label=f'Median: {median_val:,.1f}')\n\nax.set_title('Distribution of Values', fontweight='bold')\nax.set_xlabel('Value')\nax.set_ylabel('Frequency')\nax.legend()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.savefig('histogram.png', dpi=150, bbox_inches='tight')\n```\n\n### Heatmap\n\n```python\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Pivot data for heatmap format\npivot = df.pivot_table(index='row_dim', columns='col_dim', values='metric', aggfunc='sum')\n\nsns.heatmap(pivot, annot=True, fmt=',.0f', cmap='YlOrRd',\n            linewidths=0.5, ax=ax, cbar_kws={'label': 'Metric Value'})\n\nax.set_title('Metric by Row Dimension and Column Dimension', fontweight='bold')\nax.set_xlabel('Column Dimension')\nax.set_ylabel('Row Dimension')\n\nplt.tight_layout()\nplt.savefig('heatmap.png', dpi=150, bbox_inches='tight')\n```\n\n### Small Multiples\n\n```python\ncategories = df['category'].unique()\nn_cats = len(categories)\nn_cols = min(3, n_cats)\nn_rows = (n_cats + n_cols - 1) // n_cols\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows), sharex=True, sharey=True)\naxes = axes.flatten() if n_cats \u003e 1 else [axes]\n\nfor i, cat in enumerate(categories):\n    ax = axes[i]\n    subset = df[df['category'] == cat]\n    ax.plot(subset['date'], subset['value'], color=PALETTE_CATEGORICAL[i % len(PALETTE_CATEGORICAL)])\n    ax.set_title(cat, fontsize=12)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n# Hide empty subplots\nfor j in range(i+1, len(axes)):\n    axes[j].set_visible(False)\n\nfig.suptitle('Trends by Category', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig('small_multiples.png', dpi=150, bbox_inches='tight')\n```\n\n### Number Formatting Helpers\n\n```python\ndef format_number(val, format_type='number'):\n    \"\"\"Format numbers for chart labels.\"\"\"\n    if format_type == 'currency':\n        if abs(val) \u003e= 1e9:\n            return f'${val/1e9:.1f}B'\n        elif abs(val) \u003e= 1e6:\n            return f'${val/1e6:.1f}M'\n        elif abs(val) \u003e= 1e3:\n            return f'${val/1e3:.1f}K'\n        else:\n            return f'${val:,.0f}'\n    elif format_type == 'percent':\n        return f'{val:.1f}%'\n    elif format_type == 'number':\n        if abs(val) \u003e= 1e9:\n            return f'{val/1e9:.1f}B'\n        elif abs(val) \u003e= 1e6:\n            return f'{val/1e6:.1f}M'\n        elif abs(val) \u003e= 1e3:\n            return f'{val/1e3:.1f}K'\n        else:\n            return f'{val:,.0f}'\n    return str(val)\n\n# Usage with axis formatter\nax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, p: format_number(x, 'currency')))\n```\n\n### Interactive Charts with Plotly\n\n```python\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Simple interactive line chart\nfig = px.line(df, x='date', y='value', color='category',\n              title='Interactive Metric Trend',\n              labels={'value': 'Metric Value', 'date': 'Date'})\nfig.update_layout(hovermode='x unified')\nfig.write_html('interactive_chart.html')\nfig.show()\n\n# Interactive scatter with hover data\nfig = px.scatter(df, x='metric_a', y='metric_b', color='category',\n                 size='size_metric', hover_data=['name', 'detail_field'],\n                 title='Correlation Analysis')\nfig.show()\n```\n\n## Design Principles\n\n### Color\n\n- **Use color purposefully**: Color should encode data, not decorate\n- **Highlight the story**: Use a bright accent color for the key insight; grey everything else\n- **Sequential data**: Use a single-hue gradient (light to dark) for ordered values\n- **Diverging data**: Use a two-hue gradient with neutral midpoint for data with a meaningful center\n- **Categorical data**: Use distinct hues, maximum 6-8 before it gets confusing\n- **Avoid red/green only**: 8% of men are red-green colorblind. Use blue/orange as primary pair\n\n### Typography\n\n- **Title states the insight**: \"Revenue grew 23% YoY\" beats \"Revenue by Month\"\n- **Subtitle adds context**: Date range, filters applied, data source\n- **Axis labels are readable**: Never rotated 90 degrees if avoidable. Shorten or wrap instead\n- **Data labels add precision**: Use on key points, not every single bar\n- **Annotation highlights**: Call out specific points with text annotations\n\n### Layout\n\n- **Reduce chart junk**: Remove gridlines, borders, backgrounds that don't carry information\n- **Sort meaningfully**: Categories sorted by value (not alphabetically) unless there's a natural order (months, stages)\n- **Appropriate aspect ratio**: Time series wider than tall (3:1 to 2:1); comparisons can be squarer\n- **White space is good**: Don't cram charts together. Give each visualization room to breathe\n\n### Accuracy\n\n- **Bar charts start at zero**: Always. A bar from 95 to 100 exaggerates a 5% difference\n- **Line charts can have non-zero baselines**: When the range of variation is meaningful\n- **Consistent scales across panels**: When comparing multiple charts, use the same axis range\n- **Show uncertainty**: Error bars, confidence intervals, or ranges when data is uncertain\n- **Label your axes**: Never make the reader guess what the numbers mean\n\n## Accessibility Considerations\n\n### Color Blindness\n\n- Never rely on color alone to distinguish data series\n- Add pattern fills, different line styles (solid, dashed, dotted), or direct labels\n- Test with a colorblind simulator (e.g., Coblis, Sim Daltonism)\n- Use the colorblind-friendly palette: `sns.color_palette(\"colorblind\")`\n\n### Screen Readers\n\n- Include alt text describing the chart's key finding\n- Provide a data table alternative alongside the visualization\n- Use semantic titles and labels\n\n### General Accessibility\n\n- Sufficient contrast between data elements and background\n- Text size minimum 10pt for labels, 12pt for titles\n- Avoid conveying information only through spatial position (add labels)\n- Consider printing: does the chart work in black and white?\n\n### Accessibility Checklist\n\nBefore sharing a visualization:\n- [ ] Chart works without color (patterns, labels, or line styles differentiate series)\n- [ ] Text is readable at standard zoom level\n- [ ] Title describes the insight, not just the data\n- [ ] Axes are labeled with units\n- [ ] Legend is clear and positioned without obscuring data\n- [ ] Data source and date range are noted\n", "depth": 4}, {"id": "416", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/interactive-dashboard-builder/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: interactive-dashboard-builder\ndescription: Build self-contained interactive HTML dashboards with Chart.js, dropdown filters, and professional styling. Use when creating dashboards, building interactive reports, or generating shareable HTML files with charts and filters that work without a server.\n---\n\n# Interactive Dashboard Builder Skill\n\nPatterns and techniques for building self-contained HTML/JS dashboards with Chart.js, filters, interactivity, and professional styling.\n\n## HTML/JS Dashboard Patterns\n\n### Base Template\n\nEvery dashboard follows this structure:\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003eDashboard Title\u003c/title\u003e\n    \u003cscript src=\"https://cdn.jsdelivr.net/npm/chart.js@4.5.1\" integrity=\"sha384-jb8JQMbMoBUzgWatfe6COACi2ljcDdZQ2OxczGA3bGNeWe+6DChMTBJemed7ZnvJ\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n    \u003cscript src=\"https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@3.0.0\" integrity=\"sha384-cVMg8E3QFwTvGCDuK+ET4PD341jF3W8nO1auiXfuZNQkzbUUiBGLsIQUE+b1mxws\" crossorigin=\"anonymous\"\u003e\u003c/script\u003e\n    \u003cstyle\u003e\n        /* Dashboard styles go here */\n    \u003c/style\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv class=\"dashboard-container\"\u003e\n        \u003cheader class=\"dashboard-header\"\u003e\n            \u003ch1\u003eDashboard Title\u003c/h1\u003e\n            \u003cdiv class=\"filters\"\u003e\n                \u003c!-- Filter controls --\u003e\n            \u003c/div\u003e\n        \u003c/header\u003e\n\n        \u003csection class=\"kpi-row\"\u003e\n            \u003c!-- KPI cards --\u003e\n        \u003c/section\u003e\n\n        \u003csection class=\"chart-row\"\u003e\n            \u003c!-- Chart containers --\u003e\n        \u003c/section\u003e\n\n        \u003csection class=\"table-section\"\u003e\n            \u003c!-- Data table --\u003e\n        \u003c/section\u003e\n\n        \u003cfooter class=\"dashboard-footer\"\u003e\n            \u003cspan\u003eData as of: \u003cspan id=\"data-date\"\u003e\u003c/span\u003e\u003c/span\u003e\n        \u003c/footer\u003e\n    \u003c/div\u003e\n\n    \u003cscript\u003e\n        // Embedded data\n        const DATA = [];\n\n        // Dashboard logic\n        class Dashboard {\n            constructor(data) {\n                this.rawData = data;\n                this.filteredData = data;\n                this.charts = {};\n                this.init();\n            }\n\n            init() {\n                this.setupFilters();\n                this.renderKPIs();\n                this.renderCharts();\n                this.renderTable();\n            }\n\n            applyFilters() {\n                // Filter logic\n                this.filteredData = this.rawData.filter(row =\u003e {\n                    // Apply each active filter\n                    return true; // placeholder\n                });\n                this.renderKPIs();\n                this.updateCharts();\n                this.renderTable();\n            }\n\n            // ... methods for each section\n        }\n\n        const dashboard = new Dashboard(DATA);\n    \u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n```\n\n### KPI Card Pattern\n\n```html\n\u003cdiv class=\"kpi-card\"\u003e\n    \u003cdiv class=\"kpi-label\"\u003eTotal Revenue\u003c/div\u003e\n    \u003cdiv class=\"kpi-value\" id=\"kpi-revenue\"\u003e$0\u003c/div\u003e\n    \u003cdiv class=\"kpi-change positive\" id=\"kpi-revenue-change\"\u003e+0%\u003c/div\u003e\n\u003c/div\u003e\n```\n\n```javascript\nfunction renderKPI(elementId, value, previousValue, format = 'number') {\n    const el = document.getElementById(elementId);\n    const changeEl = document.getElementById(elementId + '-change');\n\n    // Format the value\n    el.textContent = formatValue(value, format);\n\n    // Calculate and display change\n    if (previousValue \u0026\u0026 previousValue !== 0) {\n        const pctChange = ((value - previousValue) / previousValue) * 100;\n        const sign = pctChange \u003e= 0 ? '+' : '';\n        changeEl.textContent = `${sign}${pctChange.toFixed(1)}% vs prior period`;\n        changeEl.className = `kpi-change ${pctChange \u003e= 0 ? 'positive' : 'negative'}`;\n    }\n}\n\nfunction formatValue(value, format) {\n    switch (format) {\n        case 'currency':\n            if (value \u003e= 1e6) return `$${(value / 1e6).toFixed(1)}M`;\n            if (value \u003e= 1e3) return `$${(value / 1e3).toFixed(1)}K`;\n            return `$${value.toFixed(0)}`;\n        case 'percent':\n            return `${value.toFixed(1)}%`;\n        case 'number':\n            if (value \u003e= 1e6) return `${(value / 1e6).toFixed(1)}M`;\n            if (value \u003e= 1e3) return `${(value / 1e3).toFixed(1)}K`;\n            return value.toLocaleString();\n        default:\n            return value.toString();\n    }\n}\n```\n\n### Chart Container Pattern\n\n```html\n\u003cdiv class=\"chart-container\"\u003e\n    \u003ch3 class=\"chart-title\"\u003eMonthly Revenue Trend\u003c/h3\u003e\n    \u003ccanvas id=\"revenue-chart\"\u003e\u003c/canvas\u003e\n\u003c/div\u003e\n```\n\n## Chart.js Integration\n\n### Line Chart\n\n```javascript\nfunction createLineChart(canvasId, labels, datasets) {\n    const ctx = document.getElementById(canvasId).getContext('2d');\n    return new Chart(ctx, {\n        type: 'line',\n        data: {\n            labels: labels,\n            datasets: datasets.map((ds, i) =\u003e ({\n                label: ds.label,\n                data: ds.data,\n                borderColor: COLORS[i % COLORS.length],\n                backgroundColor: COLORS[i % COLORS.length] + '20',\n                borderWidth: 2,\n                fill: ds.fill || false,\n                tension: 0.3,\n                pointRadius: 3,\n                pointHoverRadius: 6,\n            }))\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            interaction: {\n                mode: 'index',\n                intersect: false,\n            },\n            plugins: {\n                legend: {\n                    position: 'top',\n                    labels: { usePointStyle: true, padding: 20 }\n                },\n                tooltip: {\n                    callbacks: {\n                        label: function(context) {\n                            return `${context.dataset.label}: ${formatValue(context.parsed.y, 'currency')}`;\n                        }\n                    }\n                }\n            },\n            scales: {\n                x: {\n                    grid: { display: false }\n                },\n                y: {\n                    beginAtZero: true,\n                    ticks: {\n                        callback: function(value) {\n                            return formatValue(value, 'currency');\n                        }\n                    }\n                }\n            }\n        }\n    });\n}\n```\n\n### Bar Chart\n\n```javascript\nfunction createBarChart(canvasId, labels, data, options = {}) {\n    const ctx = document.getElementById(canvasId).getContext('2d');\n    const isHorizontal = options.horizontal || labels.length \u003e 8;\n\n    return new Chart(ctx, {\n        type: 'bar',\n        data: {\n            labels: labels,\n            datasets: [{\n                label: options.label || 'Value',\n                data: data,\n                backgroundColor: options.colors || COLORS.map(c =\u003e c + 'CC'),\n                borderColor: options.colors || COLORS,\n                borderWidth: 1,\n                borderRadius: 4,\n            }]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            indexAxis: isHorizontal ? 'y' : 'x',\n            plugins: {\n                legend: { display: false },\n                tooltip: {\n                    callbacks: {\n                        label: function(context) {\n                            return formatValue(context.parsed[isHorizontal ? 'x' : 'y'], options.format || 'number');\n                        }\n                    }\n                }\n            },\n            scales: {\n                x: {\n                    beginAtZero: true,\n                    grid: { display: isHorizontal },\n                    ticks: isHorizontal ? {\n                        callback: function(value) {\n                            return formatValue(value, options.format || 'number');\n                        }\n                    } : {}\n                },\n                y: {\n                    beginAtZero: !isHorizontal,\n                    grid: { display: !isHorizontal },\n                    ticks: !isHorizontal ? {\n                        callback: function(value) {\n                            return formatValue(value, options.format || 'number');\n                        }\n                    } : {}\n                }\n            }\n        }\n    });\n}\n```\n\n### Doughnut Chart\n\n```javascript\nfunction createDoughnutChart(canvasId, labels, data) {\n    const ctx = document.getElementById(canvasId).getContext('2d');\n    return new Chart(ctx, {\n        type: 'doughnut',\n        data: {\n            labels: labels,\n            datasets: [{\n                data: data,\n                backgroundColor: COLORS.map(c =\u003e c + 'CC'),\n                borderColor: '#ffffff',\n                borderWidth: 2,\n            }]\n        },\n        options: {\n            responsive: true,\n            maintainAspectRatio: false,\n            cutout: '60%',\n            plugins: {\n                legend: {\n                    position: 'right',\n                    labels: { usePointStyle: true, padding: 15 }\n                },\n                tooltip: {\n                    callbacks: {\n                        label: function(context) {\n                            const total = context.dataset.data.reduce((a, b) =\u003e a + b, 0);\n                            const pct = ((context.parsed / total) * 100).toFixed(1);\n                            return `${context.label}: ${formatValue(context.parsed, 'number')} (${pct}%)`;\n                        }\n                    }\n                }\n            }\n        }\n    });\n}\n```\n\n### Updating Charts on Filter Change\n\n```javascript\nfunction updateChart(chart, newLabels, newData) {\n    chart.data.labels = newLabels;\n\n    if (Array.isArray(newData[0])) {\n        // Multiple datasets\n        newData.forEach((data, i) =\u003e {\n            chart.data.datasets[i].data = data;\n        });\n    } else {\n        chart.data.datasets[0].data = newData;\n    }\n\n    chart.update('none'); // 'none' disables animation for instant update\n}\n```\n\n## Filter and Interactivity Implementation\n\n### Dropdown Filter\n\n```html\n\u003cdiv class=\"filter-group\"\u003e\n    \u003clabel for=\"filter-region\"\u003eRegion\u003c/label\u003e\n    \u003cselect id=\"filter-region\" onchange=\"dashboard.applyFilters()\"\u003e\n        \u003coption value=\"all\"\u003eAll Regions\u003c/option\u003e\n    \u003c/select\u003e\n\u003c/div\u003e\n```\n\n```javascript\nfunction populateFilter(selectId, data, field) {\n    const select = document.getElementById(selectId);\n    const values = [...new Set(data.map(d =\u003e d[field]))].sort();\n\n    // Keep the \"All\" option, add unique values\n    values.forEach(val =\u003e {\n        const option = document.createElement('option');\n        option.value = val;\n        option.textContent = val;\n        select.appendChild(option);\n    });\n}\n\nfunction getFilterValue(selectId) {\n    const val = document.getElementById(selectId).value;\n    return val === 'all' ? null : val;\n}\n```\n\n### Date Range Filter\n\n```html\n\u003cdiv class=\"filter-group\"\u003e\n    \u003clabel\u003eDate Range\u003c/label\u003e\n    \u003cinput type=\"date\" id=\"filter-date-start\" onchange=\"dashboard.applyFilters()\"\u003e\n    \u003cspan\u003eto\u003c/span\u003e\n    \u003cinput type=\"date\" id=\"filter-date-end\" onchange=\"dashboard.applyFilters()\"\u003e\n\u003c/div\u003e\n```\n\n```javascript\nfunction filterByDateRange(data, dateField, startDate, endDate) {\n    return data.filter(row =\u003e {\n        const rowDate = new Date(row[dateField]);\n        if (startDate \u0026\u0026 rowDate \u003c new Date(startDate)) return false;\n        if (endDate \u0026\u0026 rowDate \u003e new Date(endDate)) return false;\n        return true;\n    });\n}\n```\n\n### Combined Filter Logic\n\n```javascript\napplyFilters() {\n    const region = getFilterValue('filter-region');\n    const category = getFilterValue('filter-category');\n    const startDate = document.getElementById('filter-date-start').value;\n    const endDate = document.getElementById('filter-date-end').value;\n\n    this.filteredData = this.rawData.filter(row =\u003e {\n        if (region \u0026\u0026 row.region !== region) return false;\n        if (category \u0026\u0026 row.category !== category) return false;\n        if (startDate \u0026\u0026 row.date \u003c startDate) return false;\n        if (endDate \u0026\u0026 row.date \u003e endDate) return false;\n        return true;\n    });\n\n    this.renderKPIs();\n    this.updateCharts();\n    this.renderTable();\n}\n```\n\n### Sortable Table\n\n```javascript\nfunction renderTable(containerId, data, columns) {\n    const container = document.getElementById(containerId);\n    let sortCol = null;\n    let sortDir = 'desc';\n\n    function render(sortedData) {\n        let html = '\u003ctable class=\"data-table\"\u003e';\n\n        // Header\n        html += '\u003cthead\u003e\u003ctr\u003e';\n        columns.forEach(col =\u003e {\n            const arrow = sortCol === col.field\n                ? (sortDir === 'asc' ? ' ▲' : ' ▼')\n                : '';\n            html += `\u003cth onclick=\"sortTable('${col.field}')\" style=\"cursor:pointer\"\u003e${col.label}${arrow}\u003c/th\u003e`;\n        });\n        html += '\u003c/tr\u003e\u003c/thead\u003e';\n\n        // Body\n        html += '\u003ctbody\u003e';\n        sortedData.forEach(row =\u003e {\n            html += '\u003ctr\u003e';\n            columns.forEach(col =\u003e {\n                const value = col.format ? formatValue(row[col.field], col.format) : row[col.field];\n                html += `\u003ctd\u003e${value}\u003c/td\u003e`;\n            });\n            html += '\u003c/tr\u003e';\n        });\n        html += '\u003c/tbody\u003e\u003c/table\u003e';\n\n        container.innerHTML = html;\n    }\n\n    window.sortTable = function(field) {\n        if (sortCol === field) {\n            sortDir = sortDir === 'asc' ? 'desc' : 'asc';\n        } else {\n            sortCol = field;\n            sortDir = 'desc';\n        }\n        const sorted = [...data].sort((a, b) =\u003e {\n            const aVal = a[field], bVal = b[field];\n            const cmp = aVal \u003c bVal ? -1 : aVal \u003e bVal ? 1 : 0;\n            return sortDir === 'asc' ? cmp : -cmp;\n        });\n        render(sorted);\n    };\n\n    render(data);\n}\n```\n\n## CSS Styling for Dashboards\n\n### Color System\n\n```css\n:root {\n    /* Background layers */\n    --bg-primary: #f8f9fa;\n    --bg-card: #ffffff;\n    --bg-header: #1a1a2e;\n\n    /* Text */\n    --text-primary: #212529;\n    --text-secondary: #6c757d;\n    --text-on-dark: #ffffff;\n\n    /* Accent colors for data */\n    --color-1: #4C72B0;\n    --color-2: #DD8452;\n    --color-3: #55A868;\n    --color-4: #C44E52;\n    --color-5: #8172B3;\n    --color-6: #937860;\n\n    /* Status colors */\n    --positive: #28a745;\n    --negative: #dc3545;\n    --neutral: #6c757d;\n\n    /* Spacing */\n    --gap: 16px;\n    --radius: 8px;\n}\n```\n\n### Layout\n\n```css\n* {\n    margin: 0;\n    padding: 0;\n    box-sizing: border-box;\n}\n\nbody {\n    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n    background: var(--bg-primary);\n    color: var(--text-primary);\n    line-height: 1.5;\n}\n\n.dashboard-container {\n    max-width: 1400px;\n    margin: 0 auto;\n    padding: var(--gap);\n}\n\n.dashboard-header {\n    background: var(--bg-header);\n    color: var(--text-on-dark);\n    padding: 20px 24px;\n    border-radius: var(--radius);\n    margin-bottom: var(--gap);\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    flex-wrap: wrap;\n    gap: 12px;\n}\n\n.dashboard-header h1 {\n    font-size: 20px;\n    font-weight: 600;\n}\n```\n\n### KPI Cards\n\n```css\n.kpi-row {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n    gap: var(--gap);\n    margin-bottom: var(--gap);\n}\n\n.kpi-card {\n    background: var(--bg-card);\n    border-radius: var(--radius);\n    padding: 20px 24px;\n    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);\n}\n\n.kpi-label {\n    font-size: 13px;\n    color: var(--text-secondary);\n    text-transform: uppercase;\n    letter-spacing: 0.5px;\n    margin-bottom: 4px;\n}\n\n.kpi-value {\n    font-size: 28px;\n    font-weight: 700;\n    color: var(--text-primary);\n    margin-bottom: 4px;\n}\n\n.kpi-change {\n    font-size: 13px;\n    font-weight: 500;\n}\n\n.kpi-change.positive { color: var(--positive); }\n.kpi-change.negative { color: var(--negative); }\n```\n\n### Chart Containers\n\n```css\n.chart-row {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));\n    gap: var(--gap);\n    margin-bottom: var(--gap);\n}\n\n.chart-container {\n    background: var(--bg-card);\n    border-radius: var(--radius);\n    padding: 20px 24px;\n    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);\n}\n\n.chart-container h3 {\n    font-size: 14px;\n    font-weight: 600;\n    color: var(--text-primary);\n    margin-bottom: 16px;\n}\n\n.chart-container canvas {\n    max-height: 300px;\n}\n```\n\n### Filters\n\n```css\n.filters {\n    display: flex;\n    gap: 12px;\n    align-items: center;\n    flex-wrap: wrap;\n}\n\n.filter-group {\n    display: flex;\n    align-items: center;\n    gap: 6px;\n}\n\n.filter-group label {\n    font-size: 12px;\n    color: rgba(255, 255, 255, 0.7);\n}\n\n.filter-group select,\n.filter-group input[type=\"date\"] {\n    padding: 6px 10px;\n    border: 1px solid rgba(255, 255, 255, 0.2);\n    border-radius: 4px;\n    background: rgba(255, 255, 255, 0.1);\n    color: var(--text-on-dark);\n    font-size: 13px;\n}\n\n.filter-group select option {\n    background: var(--bg-header);\n    color: var(--text-on-dark);\n}\n```\n\n### Data Table\n\n```css\n.table-section {\n    background: var(--bg-card);\n    border-radius: var(--radius);\n    padding: 20px 24px;\n    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);\n    overflow-x: auto;\n}\n\n.data-table {\n    width: 100%;\n    border-collapse: collapse;\n    font-size: 13px;\n}\n\n.data-table thead th {\n    text-align: left;\n    padding: 10px 12px;\n    border-bottom: 2px solid #dee2e6;\n    color: var(--text-secondary);\n    font-weight: 600;\n    font-size: 12px;\n    text-transform: uppercase;\n    letter-spacing: 0.5px;\n    white-space: nowrap;\n    user-select: none;\n}\n\n.data-table thead th:hover {\n    color: var(--text-primary);\n    background: #f8f9fa;\n}\n\n.data-table tbody td {\n    padding: 10px 12px;\n    border-bottom: 1px solid #f0f0f0;\n}\n\n.data-table tbody tr:hover {\n    background: #f8f9fa;\n}\n\n.data-table tbody tr:last-child td {\n    border-bottom: none;\n}\n```\n\n### Responsive Design\n\n```css\n@media (max-width: 768px) {\n    .dashboard-header {\n        flex-direction: column;\n        align-items: flex-start;\n    }\n\n    .kpi-row {\n        grid-template-columns: repeat(2, 1fr);\n    }\n\n    .chart-row {\n        grid-template-columns: 1fr;\n    }\n\n    .filters {\n        flex-direction: column;\n        align-items: flex-start;\n    }\n}\n\n@media print {\n    body { background: white; }\n    .dashboard-container { max-width: none; }\n    .filters { display: none; }\n    .chart-container { break-inside: avoid; }\n    .kpi-card { border: 1px solid #dee2e6; box-shadow: none; }\n}\n```\n\n## Performance Considerations for Large Datasets\n\n### Data Size Guidelines\n\n| Data Size | Approach |\n|---|---|\n| \u003c1,000 rows | Embed directly in HTML. Full interactivity. |\n| 1,000 - 10,000 rows | Embed in HTML. May need to pre-aggregate for charts. |\n| 10,000 - 100,000 rows | Pre-aggregate server-side. Embed only aggregated data. |\n| \u003e100,000 rows | Not suitable for client-side dashboard. Use a BI tool or paginate. |\n\n### Pre-Aggregation Pattern\n\nInstead of embedding raw data and aggregating in the browser:\n\n```javascript\n// DON'T: embed 50,000 raw rows\nconst RAW_DATA = [/* 50,000 rows */];\n\n// DO: pre-aggregate before embedding\nconst CHART_DATA = {\n    monthly_revenue: [\n        { month: '2024-01', revenue: 150000, orders: 1200 },\n        { month: '2024-02', revenue: 165000, orders: 1350 },\n        // ... 12 rows instead of 50,000\n    ],\n    top_products: [\n        { product: 'Widget A', revenue: 45000 },\n        // ... 10 rows\n    ],\n    kpis: {\n        total_revenue: 1980000,\n        total_orders: 15600,\n        avg_order_value: 127,\n    }\n};\n```\n\n### Chart Performance\n\n- Limit line charts to \u003c500 data points per series (downsample if needed)\n- Limit bar charts to \u003c50 categories\n- For scatter plots, cap at 1,000 points (use sampling for larger datasets)\n- Disable animations for dashboards with many charts: `animation: false` in Chart.js options\n- Use `Chart.update('none')` instead of `Chart.update()` for filter-triggered updates\n\n### DOM Performance\n\n- Limit data tables to 100-200 visible rows. Add pagination for more.\n- Use `requestAnimationFrame` for coordinated chart updates\n- Avoid rebuilding the entire DOM on filter change -- update only changed elements\n\n```javascript\n// Efficient table pagination\nfunction renderTablePage(data, page, pageSize = 50) {\n    const start = page * pageSize;\n    const end = Math.min(start + pageSize, data.length);\n    const pageData = data.slice(start, end);\n    // Render only pageData\n    // Show pagination controls: \"Showing 1-50 of 2,340\"\n}\n```\n", "depth": 4}, {"id": "417", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/statistical-analysis/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: statistical-analysis\ndescription: Apply statistical methods including descriptive stats, trend analysis, outlier detection, and hypothesis testing. Use when analyzing distributions, testing for significance, detecting anomalies, computing correlations, or interpreting statistical results.\n---\n\n# Statistical Analysis Skill\n\nDescriptive statistics, trend analysis, outlier detection, hypothesis testing, and guidance on when to be cautious about statistical claims.\n\n## Descriptive Statistics Methodology\n\n### Central Tendency\n\nChoose the right measure of center based on the data:\n\n| Situation | Use | Why |\n|---|---|---|\n| Symmetric distribution, no outliers | Mean | Most efficient estimator |\n| Skewed distribution | Median | Robust to outliers |\n| Categorical or ordinal data | Mode | Only option for non-numeric |\n| Highly skewed with outliers (e.g., revenue per user) | Median + mean | Report both; the gap shows skew |\n\n**Always report mean and median together for business metrics.** If they diverge significantly, the data is skewed and the mean alone is misleading.\n\n### Spread and Variability\n\n- **Standard deviation**: How far values typically fall from the mean. Use with normally distributed data.\n- **Interquartile range (IQR)**: Distance from p25 to p75. Robust to outliers. Use with skewed data.\n- **Coefficient of variation (CV)**: StdDev / Mean. Use to compare variability across metrics with different scales.\n- **Range**: Max minus min. Sensitive to outliers but gives a quick sense of data extent.\n\n### Percentiles for Business Context\n\nReport key percentiles to tell a richer story than mean alone:\n\n```\np1:   Bottom 1% (floor / minimum typical value)\np5:   Low end of normal range\np25:  First quartile\np50:  Median (typical user)\np75:  Third quartile\np90:  Top 10% / power users\np95:  High end of normal range\np99:  Top 1% / extreme users\n```\n\n**Example narrative**: \"The median session duration is 4.2 minutes, but the top 10% of users spend over 22 minutes per session, pulling the mean up to 7.8 minutes.\"\n\n### Describing Distributions\n\nCharacterize every numeric distribution you analyze:\n\n- **Shape**: Normal, right-skewed, left-skewed, bimodal, uniform, heavy-tailed\n- **Center**: Mean and median (and the gap between them)\n- **Spread**: Standard deviation or IQR\n- **Outliers**: How many and how extreme\n- **Bounds**: Is there a natural floor (zero) or ceiling (100%)?\n\n## Trend Analysis and Forecasting\n\n### Identifying Trends\n\n**Moving averages** to smooth noise:\n```python\n# 7-day moving average (good for daily data with weekly seasonality)\ndf['ma_7d'] = df['metric'].rolling(window=7, min_periods=1).mean()\n\n# 28-day moving average (smooths weekly AND monthly patterns)\ndf['ma_28d'] = df['metric'].rolling(window=28, min_periods=1).mean()\n```\n\n**Period-over-period comparison**:\n- Week-over-week (WoW): Compare to same day last week\n- Month-over-month (MoM): Compare to same month prior\n- Year-over-year (YoY): Gold standard for seasonal businesses\n- Same-day-last-year: Compare specific calendar day\n\n**Growth rates**:\n```\nSimple growth: (current - previous) / previous\nCAGR: (ending / beginning) ^ (1 / years) - 1\nLog growth: ln(current / previous)  -- better for volatile series\n```\n\n### Seasonality Detection\n\nCheck for periodic patterns:\n1. Plot the raw time series -- visual inspection first\n2. Compute day-of-week averages: is there a clear weekly pattern?\n3. Compute month-of-year averages: is there an annual cycle?\n4. When comparing periods, always use YoY or same-period comparisons to avoid conflating trend with seasonality\n\n### Forecasting (Simple Methods)\n\nFor business analysts (not data scientists), use straightforward methods:\n\n- **Naive forecast**: Tomorrow = today. Use as a baseline.\n- **Seasonal naive**: Tomorrow = same day last week/year.\n- **Linear trend**: Fit a line to historical data. Only for clearly linear trends.\n- **Moving average forecast**: Use trailing average as the forecast.\n\n**Always communicate uncertainty**. Provide a range, not a point estimate:\n- \"We expect 10K-12K signups next month based on the 3-month trend\"\n- NOT \"We will get exactly 11,234 signups next month\"\n\n**When to escalate to a data scientist**: Non-linear trends, multiple seasonalities, external factors (marketing spend, holidays), or when forecast accuracy matters for resource allocation.\n\n## Outlier and Anomaly Detection\n\n### Statistical Methods\n\n**Z-score method** (for normally distributed data):\n```python\nz_scores = (df['value'] - df['value'].mean()) / df['value'].std()\noutliers = df[abs(z_scores) \u003e 3]  # More than 3 standard deviations\n```\n\n**IQR method** (robust to non-normal distributions):\n```python\nQ1 = df['value'].quantile(0.25)\nQ3 = df['value'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = df[(df['value'] \u003c lower_bound) | (df['value'] \u003e upper_bound)]\n```\n\n**Percentile method** (simplest):\n```python\noutliers = df[(df['value'] \u003c df['value'].quantile(0.01)) |\n              (df['value'] \u003e df['value'].quantile(0.99))]\n```\n\n### Handling Outliers\n\nDo NOT automatically remove outliers. Instead:\n\n1. **Investigate**: Is this a data error, a genuine extreme value, or a different population?\n2. **Data errors**: Fix or remove (e.g., negative ages, timestamps in year 1970)\n3. **Genuine extremes**: Keep them but consider using robust statistics (median instead of mean)\n4. **Different population**: Segment them out for separate analysis (e.g., enterprise vs. SMB customers)\n\n**Report what you did**: \"We excluded 47 records (0.3%) with transaction amounts \u003e$50K, which represent bulk enterprise orders analyzed separately.\"\n\n### Time Series Anomaly Detection\n\nFor detecting unusual values in a time series:\n\n1. Compute expected value (moving average or same-period-last-year)\n2. Compute deviation from expected\n3. Flag deviations beyond a threshold (typically 2-3 standard deviations of the residuals)\n4. Distinguish between point anomalies (single unusual value) and change points (sustained shift)\n\n## Hypothesis Testing Basics\n\n### When to Use\n\nUse hypothesis testing when you need to determine whether an observed difference is likely real or could be due to random chance. Common scenarios:\n\n- A/B test results: Is variant B actually better than A?\n- Before/after comparison: Did the product change actually move the metric?\n- Segment comparison: Do enterprise customers really have higher retention?\n\n### The Framework\n\n1. **Null hypothesis (H0)**: There is no difference (the default assumption)\n2. **Alternative hypothesis (H1)**: There is a difference\n3. **Choose significance level (alpha)**: Typically 0.05 (5% chance of false positive)\n4. **Compute test statistic and p-value**\n5. **Interpret**: If p \u003c alpha, reject H0 (evidence of a real difference)\n\n### Common Tests\n\n| Scenario | Test | When to Use |\n|---|---|---|\n| Compare two group means | t-test (independent) | Normal data, two groups |\n| Compare two group proportions | z-test for proportions | Conversion rates, binary outcomes |\n| Compare paired measurements | Paired t-test | Before/after on same entities |\n| Compare 3+ group means | ANOVA | Multiple segments or variants |\n| Non-normal data, two groups | Mann-Whitney U test | Skewed metrics, ordinal data |\n| Association between categories | Chi-squared test | Two categorical variables |\n\n### Practical Significance vs. Statistical Significance\n\n**Statistical significance** means the difference is unlikely due to chance.\n\n**Practical significance** means the difference is large enough to matter for business decisions.\n\nA difference can be statistically significant but practically meaningless (common with large samples). Always report:\n- **Effect size**: How big is the difference? (e.g., \"Variant B improved conversion by 0.3 percentage points\")\n- **Confidence interval**: What's the range of plausible true effects?\n- **Business impact**: What does this translate to in revenue, users, or other business terms?\n\n### Sample Size Considerations\n\n- Small samples produce unreliable results, even with significant p-values\n- Rule of thumb for proportions: Need at least 30 events per group for basic reliability\n- For detecting small effects (e.g., 1% conversion rate change), you may need thousands of observations per group\n- If your sample is small, say so: \"With only 200 observations per group, we have limited power to detect effects smaller than X%\"\n\n## When to Be Cautious About Statistical Claims\n\n### Correlation Is Not Causation\n\nWhen you find a correlation, explicitly consider:\n- **Reverse causation**: Maybe B causes A, not A causes B\n- **Confounding variables**: Maybe C causes both A and B\n- **Coincidence**: With enough variables, spurious correlations are inevitable\n\n**What you can say**: \"Users who use feature X have 30% higher retention\"\n**What you cannot say without more evidence**: \"Feature X causes 30% higher retention\"\n\n### Multiple Comparisons Problem\n\nWhen you test many hypotheses, some will be \"significant\" by chance:\n- Testing 20 metrics at p=0.05 means ~1 will be falsely significant\n- If you looked at many segments before finding one that's different, note that\n- Adjust for multiple comparisons with Bonferroni correction (divide alpha by number of tests) or report how many tests were run\n\n### Simpson's Paradox\n\nA trend in aggregated data can reverse when data is segmented:\n- Always check whether the conclusion holds across key segments\n- Example: Overall conversion goes up, but conversion goes down in every segment -- because the mix shifted toward a higher-converting segment\n\n### Survivorship Bias\n\nYou can only analyze entities that \"survived\" to be in your dataset:\n- Analyzing active users ignores those who churned\n- Analyzing successful companies ignores those that failed\n- Always ask: \"Who is missing from this dataset, and would their inclusion change the conclusion?\"\n\n### Ecological Fallacy\n\nAggregate trends may not apply to individuals:\n- \"Countries with higher X have higher Y\" does NOT mean \"individuals with higher X have higher Y\"\n- Be careful about applying group-level findings to individual cases\n\n### Anchoring on Specific Numbers\n\nBe wary of false precision:\n- \"Churn will be 4.73% next quarter\" implies more certainty than is warranted\n- Prefer ranges: \"We expect churn between 4-6% based on historical patterns\"\n- Round appropriately: \"About 5%\" is often more honest than \"4.73%\"\n", "depth": 4}, {"id": "418", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/data/skills/data-validation/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: data-validation\ndescription: QA an analysis before sharing with stakeholders — methodology checks, accuracy verification, and bias detection. Use when reviewing an analysis for errors, checking for survivorship bias, validating aggregation logic, or preparing documentation for reproducibility.\n---\n\n# Data Validation Skill\n\nPre-delivery QA checklist, common data analysis pitfalls, result sanity checking, and documentation standards for reproducibility.\n\n## Pre-Delivery QA Checklist\n\nRun through this checklist before sharing any analysis with stakeholders.\n\n### Data Quality Checks\n\n- [ ] **Source verification**: Confirmed which tables/data sources were used. Are they the right ones for this question?\n- [ ] **Freshness**: Data is current enough for the analysis. Noted the \"as of\" date.\n- [ ] **Completeness**: No unexpected gaps in time series or missing segments.\n- [ ] **Null handling**: Checked null rates in key columns. Nulls are handled appropriately (excluded, imputed, or flagged).\n- [ ] **Deduplication**: Confirmed no double-counting from bad joins or duplicate source records.\n- [ ] **Filter verification**: All WHERE clauses and filters are correct. No unintended exclusions.\n\n### Calculation Checks\n\n- [ ] **Aggregation logic**: GROUP BY includes all non-aggregated columns. Aggregation level matches the analysis grain.\n- [ ] **Denominator correctness**: Rate and percentage calculations use the right denominator. Denominators are non-zero.\n- [ ] **Date alignment**: Comparisons use the same time period length. Partial periods are excluded or noted.\n- [ ] **Join correctness**: JOIN types are appropriate (INNER vs LEFT). Many-to-many joins haven't inflated counts.\n- [ ] **Metric definitions**: Metrics match how stakeholders define them. Any deviations are noted.\n- [ ] **Subtotals sum**: Parts add up to the whole where expected. If they don't, explain why (e.g., overlap).\n\n### Reasonableness Checks\n\n- [ ] **Magnitude**: Numbers are in a plausible range. Revenue isn't negative. Percentages are between 0-100%.\n- [ ] **Trend continuity**: No unexplained jumps or drops in time series.\n- [ ] **Cross-reference**: Key numbers match other known sources (dashboards, previous reports, finance data).\n- [ ] **Order of magnitude**: Total revenue is in the right ballpark. User counts match known figures.\n- [ ] **Edge cases**: What happens at the boundaries? Empty segments, zero-activity periods, new entities.\n\n### Presentation Checks\n\n- [ ] **Chart accuracy**: Bar charts start at zero. Axes are labeled. Scales are consistent across panels.\n- [ ] **Number formatting**: Appropriate precision. Consistent currency/percentage formatting. Thousands separators where needed.\n- [ ] **Title clarity**: Titles state the insight, not just the metric. Date ranges are specified.\n- [ ] **Caveat transparency**: Known limitations and assumptions are stated explicitly.\n- [ ] **Reproducibility**: Someone else could recreate this analysis from the documentation provided.\n\n## Common Data Analysis Pitfalls\n\n### Join Explosion\n\n**The problem**: A many-to-many join silently multiplies rows, inflating counts and sums.\n\n**How to detect**:\n```sql\n-- Check row count before and after join\nSELECT COUNT(*) FROM table_a;  -- 1,000\nSELECT COUNT(*) FROM table_a a JOIN table_b b ON a.id = b.a_id;  -- 3,500 (uh oh)\n```\n\n**How to prevent**:\n- Always check row counts after joins\n- If counts increase, investigate the join relationship (is it really 1:1 or 1:many?)\n- Use `COUNT(DISTINCT a.id)` instead of `COUNT(*)` when counting entities through joins\n\n### Survivorship Bias\n\n**The problem**: Analyzing only entities that exist today, ignoring those that were deleted, churned, or failed.\n\n**Examples**:\n- Analyzing user behavior of \"current users\" misses churned users\n- Looking at \"companies using our product\" ignores those who evaluated and left\n- Studying properties of \"successful\" outcomes without \"unsuccessful\" ones\n\n**How to prevent**: Ask \"who is NOT in this dataset?\" before drawing conclusions.\n\n### Incomplete Period Comparison\n\n**The problem**: Comparing a partial period to a full period.\n\n**Examples**:\n- \"January revenue is $500K vs. December's $800K\" -- but January isn't over yet\n- \"This week's signups are down\" -- checked on Wednesday, comparing to a full prior week\n\n**How to prevent**: Always filter to complete periods, or compare same-day-of-month / same-number-of-days.\n\n### Denominator Shifting\n\n**The problem**: The denominator changes between periods, making rates incomparable.\n\n**Examples**:\n- Conversion rate improves because you changed how you count \"eligible\" users\n- Churn rate changes because the definition of \"active\" was updated\n\n**How to prevent**: Use consistent definitions across all compared periods. Note any definition changes.\n\n### Average of Averages\n\n**The problem**: Averaging pre-computed averages gives wrong results when group sizes differ.\n\n**Example**:\n- Group A: 100 users, average revenue $50\n- Group B: 10 users, average revenue $200\n- Wrong: Average of averages = ($50 + $200) / 2 = $125\n- Right: Weighted average = (100*$50 + 10*$200) / 110 = $63.64\n\n**How to prevent**: Always aggregate from raw data. Never average pre-aggregated averages.\n\n### Timezone Mismatches\n\n**The problem**: Different data sources use different timezones, causing misalignment.\n\n**Examples**:\n- Event timestamps in UTC vs. user-facing dates in local time\n- Daily rollups that use different cutoff times\n\n**How to prevent**: Standardize all timestamps to a single timezone (UTC recommended) before analysis. Document the timezone used.\n\n### Selection Bias in Segmentation\n\n**The problem**: Segments are defined by the outcome you're measuring, creating circular logic.\n\n**Examples**:\n- \"Users who completed onboarding have higher retention\" -- obviously, they self-selected\n- \"Power users generate more revenue\" -- they became power users BY generating revenue\n\n**How to prevent**: Define segments based on pre-treatment characteristics, not outcomes.\n\n## Result Sanity Checking\n\n### Magnitude Checks\n\nFor any key number in your analysis, verify it passes the \"smell test\":\n\n| Metric Type | Sanity Check |\n|---|---|\n| User counts | Does this match known MAU/DAU figures? |\n| Revenue | Is this in the right order of magnitude vs. known ARR? |\n| Conversion rates | Is this between 0% and 100%? Does it match dashboard figures? |\n| Growth rates | Is 50%+ MoM growth realistic, or is there a data issue? |\n| Averages | Is the average reasonable given what you know about the distribution? |\n| Percentages | Do segment percentages sum to ~100%? |\n\n### Cross-Validation Techniques\n\n1. **Calculate the same metric two different ways** and verify they match\n2. **Spot-check individual records** -- pick a few specific entities and trace their data manually\n3. **Compare to known benchmarks** -- match against published dashboards, finance reports, or prior analyses\n4. **Reverse engineer** -- if total revenue is X, does per-user revenue times user count approximately equal X?\n5. **Boundary checks** -- what happens when you filter to a single day, a single user, or a single category? Are those micro-results sensible?\n\n### Red Flags That Warrant Investigation\n\n- Any metric that changed by more than 50% period-over-period without an obvious cause\n- Counts or sums that are exact round numbers (suggests a filter or default value issue)\n- Rates exactly at 0% or 100% (may indicate incomplete data)\n- Results that perfectly confirm the hypothesis (reality is usually messier)\n- Identical values across time periods or segments (suggests the query is ignoring a dimension)\n\n## Documentation Standards for Reproducibility\n\n### Analysis Documentation Template\n\nEvery non-trivial analysis should include:\n\n```markdown\n## Analysis: [Title]\n\n### Question\n[The specific question being answered]\n\n### Data Sources\n- Table: [schema.table_name] (as of [date])\n- Table: [schema.other_table] (as of [date])\n- File: [filename] (source: [where it came from])\n\n### Definitions\n- [Metric A]: [Exactly how it's calculated]\n- [Segment X]: [Exactly how membership is determined]\n- [Time period]: [Start date] to [end date], [timezone]\n\n### Methodology\n1. [Step 1 of the analysis approach]\n2. [Step 2]\n3. [Step 3]\n\n### Assumptions and Limitations\n- [Assumption 1 and why it's reasonable]\n- [Limitation 1 and its potential impact on conclusions]\n\n### Key Findings\n1. [Finding 1 with supporting evidence]\n2. [Finding 2 with supporting evidence]\n\n### SQL Queries\n[All queries used, with comments]\n\n### Caveats\n- [Things the reader should know before acting on this]\n```\n\n### Code Documentation\n\nFor any code (SQL, Python) that may be reused:\n\n```python\n\"\"\"\nAnalysis: Monthly Cohort Retention\nAuthor: [Name]\nDate: [Date]\nData Source: events table, users table\nLast Validated: [Date] -- results matched dashboard within 2%\n\nPurpose:\n    Calculate monthly user retention cohorts based on first activity date.\n\nAssumptions:\n    - \"Active\" means at least one event in the month\n    - Excludes test/internal accounts (user_type != 'internal')\n    - Uses UTC dates throughout\n\nOutput:\n    Cohort retention matrix with cohort_month rows and months_since_signup columns.\n    Values are retention rates (0-100%).\n\"\"\"\n```\n\n### Version Control for Analyses\n\n- Save queries and code in version control (git) or a shared docs system\n- Note the date of the data snapshot used\n- If an analysis is re-run with updated data, document what changed and why\n- Link to prior versions of recurring analyses for trend comparison\n", "depth": 4}, {"id": "419", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "420", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "421", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/README.md", "name": "README.md", "type": "file", "content": "# Engineering Plugin\n\nA software engineering plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Helps with standups, code review, architecture decisions, incident response, debugging, and technical documentation. Works with any engineering team — standalone with your input, supercharged when you connect your source control, project tracker, and monitoring tools.\n\n## Installation\n\n```bash\nclaude plugins add knowledge-work-plugins/engineering\n```\n\n## Commands\n\nExplicit workflows you invoke with a slash command:\n\n| Command | Description |\n|---|---|\n| `/standup` | Generate a standup update from your recent activity — commits, PRs, tickets, and chat |\n| `/review` | Review code changes — security, performance, style, and correctness |\n| `/debug` | Structured debugging session — reproduce, isolate, diagnose, and fix |\n| `/architecture` | Create or evaluate architecture decisions — ADR format with trade-off analysis |\n| `/incident` | Run an incident response workflow — triage, communicate, mitigate, and write postmortem |\n| `/deploy-checklist` | Pre-deployment checklist — verify tests, review changes, check dependencies, confirm rollback plan |\n\nAll commands work **standalone** (paste code, describe your system, upload files) and get **supercharged** with MCP connectors.\n\n## Skills\n\nDomain knowledge Claude uses automatically when relevant:\n\n| Skill | Description |\n|---|---|\n| `code-review` | Review code for bugs, security issues, performance, and maintainability |\n| `incident-response` | Triage and manage production incidents — status updates, runbooks, postmortems |\n| `system-design` | Design systems and services — architecture diagrams, API design, data modeling |\n| `tech-debt` | Identify, categorize, and prioritize technical debt — build a remediation plan |\n| `testing-strategy` | Design test strategies — unit, integration, e2e coverage, test plan creation |\n| `documentation` | Write and maintain technical documentation — READMEs, API docs, runbooks, onboarding guides |\n\n## Example Workflows\n\n### Morning Standup\n\n```\n/standup\n```\n\nIf your tools are connected, I'll pull your recent commits, PR activity, and ticket updates. Otherwise, tell me what you worked on and I'll format it.\n\n### Code Review\n\n```\n/review https://github.com/org/repo/pull/123\n```\n\nShare a PR link, paste a diff, or point to files. Get a structured review covering security, performance, correctness, and style.\n\n### Debugging an Issue\n\n```\n/debug Users are getting 500 errors on the checkout page\n```\n\nWalk through a structured debugging process: reproduce, isolate, diagnose, fix. I'll help you think through it systematically.\n\n### Architecture Decision\n\n```\n/architecture Should we use a message queue or direct API calls between services?\n```\n\nGet a structured ADR with options analysis, trade-offs, and a recommendation.\n\n### Incident Response\n\n```\n/incident The payments service is returning 503s\n```\n\nStart an incident workflow: triage severity, draft communications, track timeline, and generate a postmortem when resolved.\n\n### Pre-Deploy Check\n\n```\n/deploy-checklist auth-service v2.3.0\n```\n\nGet a customized deployment checklist based on your service and what's changing.\n\n## Standalone + Supercharged\n\nEvery command and skill works without any integrations:\n\n| What You Can Do | Standalone | Supercharged With |\n|-----------------|------------|-------------------|\n| Standup updates | Describe your work | Source control, Project tracker, Chat |\n| Code review | Paste diff or code | Source control (pull PRs automatically) |\n| Debug sessions | Describe the problem | Monitoring (pull logs and metrics) |\n| Architecture decisions | Describe the system | Knowledge base (find prior ADRs) |\n| Incident response | Describe the incident | Monitoring, Incident management, Chat |\n| Deploy checklists | Describe the deploy | CI/CD, Source control |\n\n## MCP Integrations\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your tools for a richer experience:\n\n| Category | Examples | What It Enables |\n|---|---|---|\n| **Source control** | GitHub, GitLab | PR diffs, commit history, branch status |\n| **Project tracker** | Linear, Jira, Asana | Ticket status, sprint data, assignments |\n| **Monitoring** | Datadog, New Relic | Logs, metrics, alerts, dashboards |\n| **Incident management** | PagerDuty, Opsgenie | On-call schedules, incident tracking, paging |\n| **Chat** | Slack, Teams | Team discussions, standup channels |\n| **Knowledge base** | Notion, Confluence | ADRs, runbooks, onboarding docs |\n\nSee [CONNECTORS.md](CONNECTORS.md) for the full list of supported integrations.\n\n## Settings\n\nCreate a local settings file at `engineering/.claude/settings.local.json` to personalize:\n\n```json\n{\n  \"name\": \"Your Name\",\n  \"title\": \"Software Engineer\",\n  \"team\": \"Your Team\",\n  \"company\": \"Your Company\",\n  \"techStack\": [\"Python\", \"TypeScript\", \"PostgreSQL\", \"AWS\"],\n  \"defaultBranch\": \"main\",\n  \"deployProcess\": \"canary\"\n}\n```\n\nThe plugin will ask you for this information interactively if it's not configured.\n", "depth": 2}, {"id": "422", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~source control` might mean GitHub, GitLab, or any other VCS with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (source control, CI/CD, monitoring, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams |\n| Source control | `~~source control` | GitHub | GitLab, Bitbucket |\n| Project tracker | `~~project tracker` | Linear, Asana, Atlassian (Jira/Confluence) | Shortcut, ClickUp |\n| Knowledge base | `~~knowledge base` | Notion | Confluence, Guru, Coda |\n| Monitoring | `~~monitoring` | Datadog | New Relic, Grafana, Splunk |\n| Incident management | `~~incident management` | PagerDuty | Opsgenie, Incident.io, FireHydrant |\n| CI/CD | `~~CI/CD` | — | CircleCI, GitHub Actions, Jenkins, BuildKite |\n", "depth": 2}, {"id": "423", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands/debug.md", "name": "debug.md", "type": "file", "content": "---\ndescription: Structured debugging session — reproduce, isolate, diagnose, and fix\nargument-hint: \"\u003cerror message or problem description\u003e\"\n---\n\n# /debug\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nRun a structured debugging session to find and fix issues systematically.\n\n## Usage\n\n```\n/debug $ARGUMENTS\n```\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                       DEBUG                                        │\n├─────────────────────────────────────────────────────────────────┤\n│  Step 1: REPRODUCE                                                │\n│  ✓ Understand the expected vs. actual behavior                   │\n│  ✓ Identify exact reproduction steps                             │\n│  ✓ Determine scope (when did it start? who is affected?)        │\n│                                                                    │\n│  Step 2: ISOLATE                                                   │\n│  ✓ Narrow down the component, service, or code path             │\n│  ✓ Check recent changes (deploys, config changes, dependencies) │\n│  ✓ Review logs and error messages                                │\n│                                                                    │\n│  Step 3: DIAGNOSE                                                  │\n│  ✓ Form hypotheses and test them                                 │\n│  ✓ Trace the code path                                           │\n│  ✓ Identify root cause (not just symptoms)                      │\n│                                                                    │\n│  Step 4: FIX                                                       │\n│  ✓ Propose a fix with explanation                                │\n│  ✓ Consider side effects and edge cases                          │\n│  ✓ Suggest tests to prevent regression                           │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## What I Need From You\n\nTell me about the problem. Any of these help:\n- Error message or stack trace\n- Steps to reproduce\n- What changed recently\n- Logs or screenshots\n- Expected vs. actual behavior\n\n## Output\n\n```markdown\n## Debug Report: [Issue Summary]\n\n### Reproduction\n- **Expected**: [What should happen]\n- **Actual**: [What happens instead]\n- **Steps**: [How to reproduce]\n\n### Root Cause\n[Explanation of why the bug occurs]\n\n### Fix\n[Code changes or configuration fixes needed]\n\n### Prevention\n- [Test to add]\n- [Guard to put in place]\n```\n\n## If Connectors Available\n\nIf **~~monitoring** is connected:\n- Pull logs, error rates, and metrics around the time of the issue\n- Show recent deploys and config changes that may correlate\n\nIf **~~source control** is connected:\n- Identify recent commits and PRs that touched affected code paths\n- Check if the issue correlates with a specific change\n\nIf **~~project tracker** is connected:\n- Search for related bug reports or known issues\n- Create a ticket for the fix once identified\n\n## Tips\n\n1. **Share error messages exactly** — Don't paraphrase. The exact text matters.\n2. **Mention what changed** — Recent deploys, dependency updates, and config changes are top suspects.\n3. **Include context** — \"This works in staging but not prod\" or \"Only affects large payloads\" narrows things fast.\n", "depth": 3}, {"id": "424", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands/architecture.md", "name": "architecture.md", "type": "file", "content": "---\ndescription: Create or evaluate an architecture decision record (ADR)\nargument-hint: \"\u003cdecision or system to design\u003e\"\n---\n\n# /architecture\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nCreate an Architecture Decision Record (ADR) or evaluate a system design.\n\n## Usage\n\n```\n/architecture $ARGUMENTS\n```\n\n## Modes\n\n**Create an ADR**: \"Should we use Kafka or SQS for our event bus?\"\n**Evaluate a design**: \"Review this microservices proposal\"\n**System design**: \"Design the notification system for our app\"\n\nSee the **system-design** skill for detailed frameworks on requirements gathering, scalability analysis, and trade-off evaluation.\n\n## Output — ADR Format\n\n```markdown\n# ADR-[number]: [Title]\n\n**Status:** Proposed | Accepted | Deprecated | Superseded\n**Date:** [Date]\n**Deciders:** [Who needs to sign off]\n\n## Context\n[What is the situation? What forces are at play?]\n\n## Decision\n[What is the change we're proposing?]\n\n## Options Considered\n\n### Option A: [Name]\n| Dimension | Assessment |\n|-----------|------------|\n| Complexity | [Low/Med/High] |\n| Cost | [Assessment] |\n| Scalability | [Assessment] |\n| Team familiarity | [Assessment] |\n\n**Pros:** [List]\n**Cons:** [List]\n\n### Option B: [Name]\n[Same format]\n\n## Trade-off Analysis\n[Key trade-offs between options with clear reasoning]\n\n## Consequences\n- [What becomes easier]\n- [What becomes harder]\n- [What we'll need to revisit]\n\n## Action Items\n1. [ ] [Implementation step]\n2. [ ] [Follow-up]\n```\n\n## If Connectors Available\n\nIf **~~knowledge base** is connected:\n- Search for prior ADRs and design docs\n- Find relevant technical context\n\nIf **~~project tracker** is connected:\n- Link to related epics and tickets\n- Create implementation tasks\n\n## Tips\n\n1. **State constraints upfront** — \"We need to ship in 2 weeks\" or \"Must handle 10K rps\" shapes the answer.\n2. **Name your options** — Even if you're leaning one way, I'll give a more balanced analysis with explicit alternatives.\n3. **Include non-functional requirements** — Latency, cost, team expertise, and maintenance burden matter as much as features.\n", "depth": 3}, {"id": "425", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands/deploy-checklist.md", "name": "deploy-checklist.md", "type": "file", "content": "---\ndescription: Pre-deployment verification checklist\nargument-hint: \"[service or release name]\"\n---\n\n# /deploy-checklist\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a pre-deployment checklist to verify readiness before shipping.\n\n## Usage\n\n```\n/deploy-checklist $ARGUMENTS\n```\n\n## Output\n\n```markdown\n## Deploy Checklist: [Service/Release]\n**Date:** [Date] | **Deployer:** [Name]\n\n### Pre-Deploy\n- [ ] All tests passing in CI\n- [ ] Code reviewed and approved\n- [ ] No known critical bugs in release\n- [ ] Database migrations tested (if applicable)\n- [ ] Feature flags configured (if applicable)\n- [ ] Rollback plan documented\n- [ ] On-call team notified\n\n### Deploy\n- [ ] Deploy to staging and verify\n- [ ] Run smoke tests\n- [ ] Deploy to production (canary if available)\n- [ ] Monitor error rates and latency for 15 min\n- [ ] Verify key user flows\n\n### Post-Deploy\n- [ ] Confirm metrics are nominal\n- [ ] Update release notes / changelog\n- [ ] Notify stakeholders\n- [ ] Close related tickets\n\n### Rollback Triggers\n- Error rate exceeds [X]%\n- P50 latency exceeds [X]ms\n- [Critical user flow] fails\n```\n\n## Customization\n\nTell me about your deploy and I'll customize the checklist:\n- \"We use feature flags\" → adds flag verification steps\n- \"This includes a database migration\" → adds migration-specific checks\n- \"This is a breaking API change\" → adds consumer notification steps\n\n## If Connectors Available\n\nIf **~~source control** is connected:\n- Pull the release diff and list of changes\n- Verify all PRs are approved and merged\n\nIf **~~CI/CD** is connected:\n- Check build and test status automatically\n- Verify pipeline is green before deploy\n\nIf **~~monitoring** is connected:\n- Pre-fill rollback trigger thresholds from current baselines\n- Set up post-deploy metric watch\n\n## Tips\n\n1. **Run before every deploy** — Even routine ones. Checklists prevent \"I forgot to...\"\n2. **Customize once, reuse** — Tell me your stack and I'll remember your deploy process.\n3. **Include rollback criteria** — Decide when to roll back before you deploy, not during.\n", "depth": 3}, {"id": "426", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands/standup.md", "name": "standup.md", "type": "file", "content": "---\ndescription: Generate a standup update from recent activity\nargument-hint: \"[yesterday | today | blockers]\"\n---\n\n# /standup\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nGenerate a standup update by pulling together recent activity across your tools.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                        STANDUP                                    │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Tell me what you worked on and I'll structure it             │\n│  ✓ Format for daily standup (yesterday / today / blockers)      │\n│  ✓ Keep it concise and action-oriented                          │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Source control: Recent commits and PRs                        │\n│  + Project tracker: Ticket status changes                        │\n│  + Chat: Relevant discussions and decisions                      │\n│  + CI/CD: Build and deploy status                                │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## What I Need From You\n\n**Option A: Let me pull it**\nIf your tools are connected, just say `/standup` and I'll gather everything automatically.\n\n**Option B: Tell me what you did**\n\"Worked on the auth migration, reviewed 3 PRs, got blocked on the API rate limiting issue.\"\n\n## Output\n\n```markdown\n## Standup — [Date]\n\n### Yesterday\n- [Completed item with ticket reference if available]\n- [Completed item]\n\n### Today\n- [Planned item with ticket reference]\n- [Planned item]\n\n### Blockers\n- [Blocker with context and who can help]\n```\n\n## If Connectors Available\n\nIf **~~source control** is connected:\n- Pull recent commits and PRs (opened, reviewed, merged)\n- Summarize code changes at a high level\n\nIf **~~project tracker** is connected:\n- Pull tickets moved to \"in progress\" or \"done\"\n- Show upcoming sprint items\n\nIf **~~chat** is connected:\n- Scan for relevant discussions and decisions\n- Flag threads needing your response\n\n## Tips\n\n1. **Run it every morning** — Build a habit and never scramble for standup notes.\n2. **Add context** — After I generate, add any nuance about blockers or priorities.\n3. **Share format** — Ask me to format for Slack, email, or your team's standup tool.\n", "depth": 3}, {"id": "427", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands/review.md", "name": "review.md", "type": "file", "content": "---\ndescription: Review code changes for security, performance, and correctness\nargument-hint: \"\u003cPR URL, diff, or file path\u003e\"\n---\n\n# /review\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nReview code changes with a structured lens on security, performance, correctness, and maintainability.\n\n## Usage\n\n```\n/review \u003cPR URL or file path\u003e\n```\n\nReview the provided code changes: @$1\n\nIf no specific file or URL is provided, ask what to review.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                      CODE REVIEW                                   │\n├─────────────────────────────────────────────────────────────────┤\n│  STANDALONE (always works)                                       │\n│  ✓ Paste a diff, PR URL, or point to files                      │\n│  ✓ Security audit (OWASP top 10, injection, auth)               │\n│  ✓ Performance review (N+1, memory leaks, complexity)           │\n│  ✓ Correctness (edge cases, error handling, race conditions)    │\n│  ✓ Style (naming, structure, readability)                        │\n│  ✓ Actionable suggestions with code examples                    │\n├─────────────────────────────────────────────────────────────────┤\n│  SUPERCHARGED (when you connect your tools)                      │\n│  + Source control: Pull PR diff automatically                    │\n│  + Project tracker: Link findings to tickets                     │\n│  + Knowledge base: Check against team coding standards           │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Output\n\n```markdown\n## Code Review: [PR title or file]\n\n### Summary\n[1-2 sentence overview of the changes and overall quality]\n\n### Critical Issues\n| # | File | Line | Issue | Severity |\n|---|------|------|-------|----------|\n| 1 | [file] | [line] | [description] | 🔴 Critical |\n\n### Suggestions\n| # | File | Line | Suggestion | Category |\n|---|------|------|------------|----------|\n| 1 | [file] | [line] | [description] | Performance |\n\n### What Looks Good\n- [Positive observations]\n\n### Verdict\n[Approve / Request Changes / Needs Discussion]\n```\n\n## Review Checklist\n\nSee the **code-review** skill for detailed guidance on security patterns, performance anti-patterns, and maintainability heuristics.\n\nI check for:\n- **Security**: SQL injection, XSS, auth bypass, secrets in code, insecure deserialization\n- **Performance**: N+1 queries, unnecessary allocations, algorithmic complexity, missing indexes\n- **Correctness**: Edge cases, null handling, race conditions, error propagation\n- **Maintainability**: Naming clarity, single responsibility, test coverage, documentation\n\n## If Connectors Available\n\nIf **~~source control** is connected:\n- Pull the PR diff automatically from the URL\n- Check CI status and test results\n\nIf **~~project tracker** is connected:\n- Link findings to related tickets\n- Verify the PR addresses the stated requirements\n\nIf **~~knowledge base** is connected:\n- Check changes against team coding standards and style guides\n\n## Tips\n\n1. **Provide context** — \"This is a hot path\" or \"This handles PII\" helps me focus.\n2. **Specify concerns** — \"Focus on security\" narrows the review.\n3. **Include tests** — I'll check test coverage and quality too.\n", "depth": 3}, {"id": "428", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/commands/incident.md", "name": "incident.md", "type": "file", "content": "---\ndescription: Run an incident response workflow — triage, communicate, and write postmortem\nargument-hint: \"\u003cincident description or alert\u003e\"\n---\n\n# /incident\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nManage an incident from detection through postmortem.\n\n## Usage\n\n```\n/incident $ARGUMENTS\n```\n\n## Modes\n\n```\n/incident new [description]     # Start a new incident\n/incident update [status]       # Post a status update\n/incident postmortem            # Generate postmortem from incident data\n```\n\nIf no mode is specified, ask what phase the incident is in. See the **incident-response** skill for severity definitions, response frameworks, and communication templates.\n\n## How It Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    INCIDENT RESPONSE                               │\n├─────────────────────────────────────────────────────────────────┤\n│  Phase 1: TRIAGE                                                  │\n│  ✓ Assess severity (SEV1-4)                                     │\n│  ✓ Identify affected systems and users                          │\n│  ✓ Assign roles (IC, comms, responders)                         │\n│                                                                    │\n│  Phase 2: COMMUNICATE                                              │\n│  ✓ Draft internal status update                                  │\n│  ✓ Draft customer communication (if needed)                     │\n│  ✓ Set up war room and cadence                                   │\n│                                                                    │\n│  Phase 3: MITIGATE                                                 │\n│  ✓ Document mitigation steps taken                               │\n│  ✓ Track timeline of events                                      │\n│  ✓ Confirm resolution                                            │\n│                                                                    │\n│  Phase 4: POSTMORTEM                                               │\n│  ✓ Blameless postmortem document                                 │\n│  ✓ Timeline reconstruction                                       │\n│  ✓ Root cause analysis (5 whys)                                  │\n│  ✓ Action items with owners                                      │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Output — Status Update\n\n```markdown\n## Incident Update: [Title]\n**Severity:** SEV[1-4] | **Status:** Investigating | Identified | Monitoring | Resolved\n**Impact:** [Who/what is affected]\n**Last Updated:** [Timestamp]\n\n### Current Status\n[What we know now]\n\n### Actions Taken\n- [Action 1]\n- [Action 2]\n\n### Next Steps\n- [What's happening next and ETA]\n\n### Timeline\n| Time | Event |\n|------|-------|\n| [HH:MM] | [Event] |\n```\n\n## Output — Postmortem\n\n```markdown\n## Postmortem: [Incident Title]\n**Date:** [Date] | **Duration:** [X hours] | **Severity:** SEV[X]\n**Authors:** [Names] | **Status:** Draft\n\n### Summary\n[2-3 sentence plain-language summary]\n\n### Impact\n- [Users affected]\n- [Duration of impact]\n- [Business impact if quantifiable]\n\n### Timeline\n| Time (UTC) | Event |\n|------------|-------|\n| [HH:MM] | [Event] |\n\n### Root Cause\n[Detailed explanation of what caused the incident]\n\n### 5 Whys\n1. Why did [symptom]? → [Because...]\n2. Why did [cause 1]? → [Because...]\n3. Why did [cause 2]? → [Because...]\n4. Why did [cause 3]? → [Because...]\n5. Why did [cause 4]? → [Root cause]\n\n### What Went Well\n- [Things that worked]\n\n### What Went Poorly\n- [Things that didn't work]\n\n### Action Items\n| Action | Owner | Priority | Due Date |\n|--------|-------|----------|----------|\n| [Action] | [Person] | P0/P1/P2 | [Date] |\n\n### Lessons Learned\n[Key takeaways for the team]\n```\n\n## If Connectors Available\n\nIf **~~monitoring** is connected:\n- Pull alert details and metrics\n- Show graphs of affected metrics\n\nIf **~~incident management** is connected:\n- Create or update incident in PagerDuty/Opsgenie\n- Page on-call responders\n\nIf **~~chat** is connected:\n- Post status updates to incident channel\n- Create war room channel\n\n## Tips\n\n1. **Start writing immediately** — Don't wait for complete information. Update as you learn more.\n2. **Keep updates factual** — What we know, what we've done, what's next. No speculation.\n3. **Postmortems are blameless** — Focus on systems and processes, not individuals.\n", "depth": 3}, {"id": "429", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/documentation", "name": "documentation", "type": "dir", "content": "", "depth": 3}, {"id": "430", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/incident-response", "name": "incident-response", "type": "dir", "content": "", "depth": 3}, {"id": "431", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/code-review", "name": "code-review", "type": "dir", "content": "", "depth": 3}, {"id": "432", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/tech-debt", "name": "tech-debt", "type": "dir", "content": "", "depth": 3}, {"id": "433", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/testing-strategy", "name": "testing-strategy", "type": "dir", "content": "", "depth": 3}, {"id": "434", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/system-design", "name": "system-design", "type": "dir", "content": "", "depth": 3}, {"id": "435", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/documentation/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: documentation\ndescription: Write and maintain technical documentation. Trigger with \"write docs for\", \"document this\", \"create a README\", \"write a runbook\", \"onboarding guide\", or when the user needs help with any form of technical writing — API docs, architecture docs, or operational runbooks.\n---\n\n# Technical Documentation\n\nWrite clear, maintainable technical documentation for different audiences and purposes.\n\n## Document Types\n\n### README\n- What this is and why it exists\n- Quick start (\u003c 5 minutes to first success)\n- Configuration and usage\n- Contributing guide\n\n### API Documentation\n- Endpoint reference with request/response examples\n- Authentication and error codes\n- Rate limits and pagination\n- SDK examples\n\n### Runbook\n- When to use this runbook\n- Prerequisites and access needed\n- Step-by-step procedure\n- Rollback steps\n- Escalation path\n\n### Architecture Doc\n- Context and goals\n- High-level design with diagrams\n- Key decisions and trade-offs\n- Data flow and integration points\n\n### Onboarding Guide\n- Environment setup\n- Key systems and how they connect\n- Common tasks with walkthroughs\n- Who to ask for what\n\n## Principles\n\n1. **Write for the reader** — Who is reading this and what do they need?\n2. **Start with the most useful information** — Don't bury the lede\n3. **Show, don't tell** — Code examples, commands, screenshots\n4. **Keep it current** — Outdated docs are worse than no docs\n5. **Link, don't duplicate** — Reference other docs instead of copying\n", "depth": 4}, {"id": "436", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/incident-response/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: incident-response\ndescription: Triage and manage production incidents. Trigger with \"we have an incident\", \"production is down\", \"something is broken\", \"there's an outage\", \"SEV1\", or when the user describes a production issue needing immediate response.\n---\n\n# Incident Response\n\nGuide incident response from detection through resolution and postmortem.\n\n## Severity Classification\n\n| Level | Criteria | Response Time |\n|-------|----------|---------------|\n| SEV1 | Service down, all users affected | Immediate, all-hands |\n| SEV2 | Major feature degraded, many users affected | Within 15 min |\n| SEV3 | Minor feature issue, some users affected | Within 1 hour |\n| SEV4 | Cosmetic or low-impact issue | Next business day |\n\n## Response Framework\n\n1. **Triage**: Classify severity, identify scope, assign incident commander\n2. **Communicate**: Status page, internal updates, customer comms if needed\n3. **Mitigate**: Stop the bleeding first, root cause later\n4. **Resolve**: Implement fix, verify, confirm resolution\n5. **Postmortem**: Blameless review, 5 whys, action items\n\n## Communication Templates\n\nProvide clear, factual updates at regular cadence. Include: what's happening, who's affected, what we're doing, when the next update is.\n\n## Postmortem Format\n\nBlameless. Focus on systems and processes. Include timeline, root cause analysis (5 whys), what went well, what went poorly, and action items with owners and due dates.\n", "depth": 4}, {"id": "437", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/code-review/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: code-review\ndescription: Review code for bugs, security vulnerabilities, performance issues, and maintainability. Trigger with \"review this code\", \"check this PR\", \"look at this diff\", \"is this code safe?\", or when the user shares code and asks for feedback.\n---\n\n# Code Review\n\nStructured code review covering security, performance, correctness, and maintainability. Works on diffs, PRs, files, or pasted code snippets.\n\n## Review Dimensions\n\n### Security\n- SQL injection, XSS, CSRF\n- Authentication and authorization flaws\n- Secrets or credentials in code\n- Insecure deserialization\n- Path traversal\n- SSRF\n\n### Performance\n- N+1 queries\n- Unnecessary memory allocations\n- Algorithmic complexity (O(n²) in hot paths)\n- Missing database indexes\n- Unbounded queries or loops\n- Resource leaks\n\n### Correctness\n- Edge cases (empty input, null, overflow)\n- Race conditions and concurrency issues\n- Error handling and propagation\n- Off-by-one errors\n- Type safety\n\n### Maintainability\n- Naming clarity\n- Single responsibility\n- Duplication\n- Test coverage\n- Documentation for non-obvious logic\n\n## Output Format\n\nRate each dimension and provide specific, actionable findings with file and line references. Prioritize critical issues first. Always include positive observations alongside issues.\n", "depth": 4}, {"id": "438", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/tech-debt/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: tech-debt\ndescription: Identify, categorize, and prioritize technical debt. Trigger with \"tech debt\", \"technical debt audit\", \"what should we refactor\", \"code health\", or when the user asks about code quality, refactoring priorities, or maintenance backlog.\n---\n\n# Tech Debt Management\n\nSystematically identify, categorize, and prioritize technical debt.\n\n## Categories\n\n| Type | Examples | Risk |\n|------|----------|------|\n| **Code debt** | Duplicated logic, poor abstractions, magic numbers | Bugs, slow development |\n| **Architecture debt** | Monolith that should be split, wrong data store | Scaling limits |\n| **Test debt** | Low coverage, flaky tests, missing integration tests | Regressions ship |\n| **Dependency debt** | Outdated libraries, unmaintained dependencies | Security vulns |\n| **Documentation debt** | Missing runbooks, outdated READMEs, tribal knowledge | Onboarding pain |\n| **Infrastructure debt** | Manual deploys, no monitoring, no IaC | Incidents, slow recovery |\n\n## Prioritization Framework\n\nScore each item on:\n- **Impact**: How much does it slow the team down? (1-5)\n- **Risk**: What happens if we don't fix it? (1-5)\n- **Effort**: How hard is the fix? (1-5, inverted — lower effort = higher priority)\n\nPriority = (Impact + Risk) x (6 - Effort)\n\n## Output\n\nProduce a prioritized list with estimated effort, business justification for each item, and a phased remediation plan that can be done alongside feature work.\n", "depth": 4}, {"id": "439", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/testing-strategy/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: testing-strategy\ndescription: Design test strategies and test plans. Trigger with \"how should we test\", \"test strategy for\", \"write tests for\", \"test plan\", \"what tests do we need\", or when the user needs help with testing approaches, coverage, or test architecture.\n---\n\n# Testing Strategy\n\nDesign effective testing strategies balancing coverage, speed, and maintenance.\n\n## Testing Pyramid\n\n```\n        /  E2E  \\         Few, slow, high confidence\n       / Integration \\     Some, medium speed\n      /    Unit Tests  \\   Many, fast, focused\n```\n\n## Strategy by Component Type\n\n- **API endpoints**: Unit tests for business logic, integration tests for HTTP layer, contract tests for consumers\n- **Data pipelines**: Input validation, transformation correctness, idempotency tests\n- **Frontend**: Component tests, interaction tests, visual regression, accessibility\n- **Infrastructure**: Smoke tests, chaos engineering, load tests\n\n## What to Cover\n\nFocus on: business-critical paths, error handling, edge cases, security boundaries, data integrity.\n\nSkip: trivial getters/setters, framework code, one-off scripts.\n\n## Output\n\nProduce a test plan with: what to test, test type for each area, coverage targets, and example test cases. Identify gaps in existing coverage.\n", "depth": 4}, {"id": "440", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/engineering/skills/system-design/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: system-design\ndescription: Design systems, services, and architectures. Trigger with \"design a system for\", \"how should we architect\", \"system design for\", \"what's the right architecture for\", or when the user needs help with API design, data modeling, or service boundaries.\n---\n\n# System Design\n\nHelp design systems and evaluate architectural decisions.\n\n## Framework\n\n### 1. Requirements Gathering\n- Functional requirements (what it does)\n- Non-functional requirements (scale, latency, availability, cost)\n- Constraints (team size, timeline, existing tech stack)\n\n### 2. High-Level Design\n- Component diagram\n- Data flow\n- API contracts\n- Storage choices\n\n### 3. Deep Dive\n- Data model design\n- API endpoint design (REST, GraphQL, gRPC)\n- Caching strategy\n- Queue/event design\n- Error handling and retry logic\n\n### 4. Scale and Reliability\n- Load estimation\n- Horizontal vs. vertical scaling\n- Failover and redundancy\n- Monitoring and alerting\n\n### 5. Trade-off Analysis\n- Every decision has trade-offs. Make them explicit.\n- Consider: complexity, cost, team familiarity, time to market, maintainability\n\n## Output\n\nProduce clear, structured design documents with diagrams (ASCII or described), explicit assumptions, and trade-off analysis. Always identify what you'd revisit as the system grows.\n", "depth": 4}, {"id": "441", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/commands", "name": "commands", "type": "dir", "content": "", "depth": 2}, {"id": "442", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/skills", "name": "skills", "type": "dir", "content": "", "depth": 2}, {"id": "443", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/README.md", "name": "README.md", "type": "file", "content": "# Productivity Plugin\n\nA productivity plugin primarily designed for [Cowork](https://claude.com/product/cowork), Anthropic's agentic desktop application — though it also works in Claude Code. Task management, workplace memory, and a visual dashboard — Claude learns your people, projects, and terminology so it can act like a colleague, not a chatbot.\n\n## Installation\n\n```\nclaude plugins add knowledge-work-plugins/productivity\n```\n\n## What It Does\n\nThis plugin gives Claude a persistent understanding of your work:\n\n- **Task management** — A markdown task list (`TASKS.md`) that Claude reads, writes, and executes against. Add tasks naturally, and Claude tracks status, triages stale items, and syncs with external tools.\n- **Workplace memory** — A two-tier memory system that teaches Claude your shorthand, people, projects, and terminology. Say \"ask todd to do the PSR for oracle\" and Claude knows exactly who, what, and which deal.\n- **Visual dashboard** — A local HTML file that gives you a board view of your tasks and a live view of what Claude knows about your workplace. Edit from the board or the file — they stay in sync.\n\n## Commands\n\n| Command | What it does |\n|---------|--------------|\n| `/start` | Initialize tasks + memory, open the dashboard |\n| `/update` | Triage stale items, check memory for gaps, sync from external tools if applicable |\n| `/update --comprehensive` | Deep scan email, calendar, chat — flag missed todos and suggest new memories |\n\n## Skills\n\n| Skill | Description |\n|-------|-------------|\n| `memory-management` | Two-tier memory system — CLAUDE.md for working memory, memory/ directory for deep storage |\n| `task-management` | Markdown-based task tracking using a shared TASKS.md file |\n\n## Example Workflows\n\n### Getting Started\n\n```\nYou: /start\n\nClaude: [Creates TASKS.md, CLAUDE.md, memory/ directory, and dashboard.html]\n        [Opens the dashboard in your browser]\n        [Asks about your role, team, and current priorities to seed memory]\n```\n\n### Adding Tasks Naturally\n\n```\nYou: I need to review the budget proposal for Sarah by Friday,\n     draft the Q2 roadmap after syncing with Greg, and follow up\n     on the API spec from the Platform team\n\nClaude: [Adds all three tasks to TASKS.md with context]\n        [Dashboard updates automatically]\n```\n\n### Morning Sync\n\n```\nYou: /update --comprehensive\n\nClaude: [Scans email, calendar, and chat for new action items]\n        [Flags: \"Budget proposal review is due tomorrow — still open\"]\n        [Suggests: \"New person mentioned in 3 threads: Jamie Park,\n         Design Lead — add to memory?\"]\n        [Updates stale tasks and fills memory gaps]\n```\n\n### Workplace Shorthand\n\nOnce memory is populated, Claude decodes your shorthand instantly:\n\n```\nYou: ask todd to do the PSR for oracle\n\nClaude: \"Ask Todd Martinez (Finance lead) to prepare the Pipeline\n         Status Report for the Oracle Systems deal ($2.3M, closing Q2)\"\n```\n\nNo clarifying questions. No round trips.\n\n## Data Sources\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](CONNECTORS.md).\n\nConnect your communication and project management tools for the best experience. Without them, manage tasks and memory manually.\n\n**Included MCP connections:**\n- Chat (Slack) for team context and message scanning\n- Email and calendar (Microsoft 365) for action item discovery\n- Knowledge base (Notion) for reference documents\n- Project tracker (Asana, Linear, Atlassian, monday.com, ClickUp) for task syncing\n- Office suite (Microsoft 365) for documents\n\n**Additional options:**\n- See [CONNECTORS.md](CONNECTORS.md) for alternative tools in each category\n", "depth": 2}, {"id": "444", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/CONNECTORS.md", "name": "CONNECTORS.md", "type": "file", "content": "# Connectors\n\n## How tool references work\n\nPlugin files use `~~category` as a placeholder for whatever tool the user connects in that category. For example, `~~project tracker` might mean Asana, Linear, Jira, or any other project tracker with an MCP server.\n\nPlugins are **tool-agnostic** — they describe workflows in terms of categories (chat, project tracker, knowledge base, etc.) rather than specific products. The `.mcp.json` pre-configures specific MCP servers, but any MCP server in that category works.\n\n## Connectors for this plugin\n\n| Category | Placeholder | Included servers | Other options |\n|----------|-------------|-----------------|---------------|\n| Chat | `~~chat` | Slack | Microsoft Teams, Discord |\n| Email | `~~email` | Microsoft 365 | — |\n| Calendar | `~~calendar` | Microsoft 365 | — |\n| Knowledge base | `~~knowledge base` | Notion | Confluence, Guru, Coda |\n| Project tracker | `~~project tracker` | Asana, Linear, Atlassian (Jira/Confluence), monday.com, ClickUp | Shortcut, Basecamp, Wrike |\n| Office suite | `~~office suite` | Microsoft 365 | — |\n", "depth": 2}, {"id": "445", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/commands/update.md", "name": "update.md", "type": "file", "content": "---\ndescription: Sync tasks and refresh memory from your current activity\nargument-hint: \"[--comprehensive]\"\n---\n\n# Update Command\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nKeep your task list and memory current. Two modes:\n\n- **Default:** Sync tasks from external tools, triage stale items, check memory for gaps\n- **`--comprehensive`:** Deep scan chat, email, calendar, docs — flag missed todos and suggest new memories\n\n## Usage\n\n```bash\n/productivity:update\n/productivity:update --comprehensive\n```\n\n## Default Mode\n\n### 1. Load Current State\n\nRead `TASKS.md` and `memory/` directory. If they don't exist, suggest `/productivity:start` first.\n\n### 2. Sync Tasks from External Sources\n\nCheck for available task sources:\n- **Project tracker** (e.g. Asana, Linear, Jira) (if MCP available)\n- **GitHub Issues** (if in a repo): `gh issue list --assignee=@me`\n\nIf no sources are available, skip to Step 3.\n\n**Fetch tasks assigned to the user** (open/in-progress). Compare against TASKS.md:\n\n| External task | TASKS.md match? | Action |\n|---------------|-----------------|--------|\n| Found, not in TASKS.md | No match | Offer to add |\n| Found, already in TASKS.md | Match by title (fuzzy) | Skip |\n| In TASKS.md, not in external | No match | Flag as potentially stale |\n| Completed externally | In Active section | Offer to mark done |\n\nPresent diff and let user decide what to add/complete.\n\n### 3. Triage Stale Items\n\nReview Active tasks in TASKS.md and flag:\n- Tasks with due dates in the past\n- Tasks in Active for 30+ days\n- Tasks with no context (no person, no project)\n\nPresent each for triage: Mark done? Reschedule? Move to Someday?\n\n### 4. Decode Tasks for Memory Gaps\n\nFor each task, attempt to decode all entities (people, projects, acronyms, tools, links):\n\n```\nTask: \"Send PSR to Todd re: Phoenix blockers\"\n\nDecode:\n- PSR → ✓ Pipeline Status Report (in glossary)\n- Todd → ✓ Todd Martinez (in people/)\n- Phoenix → ? Not in memory\n```\n\nTrack what's fully decoded vs. what has gaps.\n\n### 5. Fill Gaps\n\nPresent unknown terms grouped:\n```\nI found terms in your tasks I don't have context for:\n\n1. \"Phoenix\" (from: \"Send PSR to Todd re: Phoenix blockers\")\n   → What's Phoenix?\n\n2. \"Maya\" (from: \"sync with Maya on API design\")\n   → Who is Maya?\n```\n\nAdd answers to the appropriate memory files (people/, projects/, glossary.md).\n\n### 6. Capture Enrichment\n\nTasks often contain richer context than memory. Extract and update:\n- **Links** from tasks → add to project/people files\n- **Status changes** (\"launch done\") → update project status, demote from CLAUDE.md\n- **Relationships** (\"Todd's sign-off on Maya's proposal\") → cross-reference people\n- **Deadlines** → add to project files\n\n### 7. Report\n\n```\nUpdate complete:\n- Tasks: +3 from project tracker (e.g. Asana), 1 completed, 2 triaged\n- Memory: 2 gaps filled, 1 project enriched\n- All tasks decoded ✓\n```\n\n## Comprehensive Mode (`--comprehensive`)\n\nEverything in Default Mode, plus a deep scan of recent activity.\n\n### Extra Step: Scan Activity Sources\n\nGather data from available MCP sources:\n- **Chat:** Search recent messages, read active channels\n- **Email:** Search sent messages\n- **Documents:** List recently touched docs\n- **Calendar:** List recent + upcoming events\n\n### Extra Step: Flag Missed Todos\n\nCompare activity against TASKS.md. Surface action items that aren't tracked:\n\n```\n## Possible Missing Tasks\n\nFrom your activity, these look like todos you haven't captured:\n\n1. From chat (Jan 18):\n   \"I'll send the updated mockups by Friday\"\n   → Add to TASKS.md?\n\n2. From meeting \"Phoenix Standup\" (Jan 17):\n   You have a recurring meeting but no Phoenix tasks active\n   → Anything needed here?\n\n3. From email (Jan 16):\n   \"I'll review the API spec this week\"\n   → Add to TASKS.md?\n```\n\nLet user pick which to add.\n\n### Extra Step: Suggest New Memories\n\nSurface new entities not in memory:\n\n```\n## New People (not in memory)\n| Name | Frequency | Context |\n|------|-----------|---------|\n| Maya Rodriguez | 12 mentions | design, UI reviews |\n| Alex K | 8 mentions | DMs about API |\n\n## New Projects/Topics\n| Name | Frequency | Context |\n|------|-----------|---------|\n| Starlight | 15 mentions | planning docs, product |\n\n## Suggested Cleanup\n- **Horizon project** — No mentions in 30 days. Mark completed?\n```\n\nPresent grouped by confidence. High-confidence items offered to add directly; low-confidence items asked about.\n\n## Notes\n\n- Never auto-add tasks or memories without user confirmation\n- External source links are preserved when available\n- Fuzzy matching on task titles handles minor wording differences\n- Safe to run frequently — only updates when there's new info\n- `--comprehensive` always runs interactively\n", "depth": 3}, {"id": "446", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/commands/start.md", "name": "start.md", "type": "file", "content": "---\ndescription: Initialize the productivity system and open the dashboard\n---\n\n# Start Command\n\n\u003e If you see unfamiliar placeholders or need to check which tools are connected, see [CONNECTORS.md](../CONNECTORS.md).\n\nInitialize the task and memory systems, then open the unified dashboard.\n\n## Instructions\n\n### 1. Check What Exists\n\nCheck the working directory for:\n- `TASKS.md` — task list\n- `CLAUDE.md` — working memory\n- `memory/` — deep memory directory\n- `dashboard.html` — the visual UI\n\n### 2. Create What's Missing\n\n**If `TASKS.md` doesn't exist:** Create it with the standard template (see task-management skill). Place it in the current working directory.\n\n**If `dashboard.html` doesn't exist:** Copy it from `${CLAUDE_PLUGIN_ROOT}/skills/dashboard.html` to the current working directory.\n\n**If `CLAUDE.md` and `memory/` don't exist:** This is a fresh setup — after opening the dashboard, begin the memory bootstrap workflow (see below). Place these in the current working directory.\n\n### 3. Open the Dashboard\n\nDo NOT use `open` or `xdg-open` — in Cowork, the agent runs in a VM and shell open commands won't reach the user's browser. Instead, tell the user: \"Dashboard is ready at `dashboard.html`. Open it from your file browser to get started.\"\n\n### 4. Orient the User\n\nIf everything was already initialized:\n```\nDashboard open. Your tasks and memory are both loaded.\n- /productivity:update to sync tasks and check memory\n- /productivity:update --comprehensive for a deep scan of all activity\n```\n\nIf memory hasn't been bootstrapped yet, continue to step 5.\n\n### 5. Bootstrap Memory (First Run Only)\n\nOnly do this if `CLAUDE.md` and `memory/` don't exist yet.\n\nThe best source of workplace language is the user's actual task list. Real tasks = real shorthand.\n\n**Ask the user:**\n```\nWhere do you keep your todos or task list? This could be:\n- A local file (e.g., TASKS.md, todo.txt)\n- An app (e.g. Asana, Linear, Jira, Notion, Todoist)\n- A notes file\n\nI'll use your tasks to learn your workplace shorthand.\n```\n\n**Once you have access to the task list:**\n\nFor each task item, analyze it for potential shorthand:\n- Names that might be nicknames\n- Acronyms or abbreviations\n- Project references or codenames\n- Internal terms or jargon\n\n**For each item, decode it interactively:**\n\n```\nTask: \"Send PSR to Todd re: Phoenix blockers\"\n\nI see some terms I want to make sure I understand:\n\n1. **PSR** - What does this stand for?\n2. **Todd** - Who is Todd? (full name, role)\n3. **Phoenix** - Is this a project codename? What's it about?\n```\n\nContinue through each task, asking only about terms you haven't already decoded.\n\n### 6. Optional Comprehensive Scan\n\nAfter task list decoding, offer:\n```\nDo you want me to do a comprehensive scan of your messages, emails, and documents?\nThis takes longer but builds much richer context about the people, projects, and terms in your work.\n\nOr we can stick with what we have and add context later.\n```\n\n**If they choose comprehensive scan:**\n\nGather data from available MCP sources:\n- **Chat:** Recent messages, channels, DMs\n- **Email:** Sent messages, recipients\n- **Documents:** Recent docs, collaborators\n- **Calendar:** Meetings, attendees\n\nBuild a braindump of people, projects, and terms found. Present findings grouped by confidence:\n- **Ready to add** (high confidence) — offer to add directly\n- **Needs clarification** — ask the user\n- **Low frequency / unclear** — note for later\n\n### 7. Write Memory Files\n\nFrom everything gathered, create:\n\n**CLAUDE.md** (working memory, ~50-80 lines):\n```markdown\n# Memory\n\n## Me\n[Name], [Role] on [Team].\n\n## People\n| Who | Role |\n|-----|------|\n| **[Nickname]** | [Full Name], [role] |\n\n## Terms\n| Term | Meaning |\n|------|---------|\n| [acronym] | [expansion] |\n\n## Projects\n| Name | What |\n|------|------|\n| **[Codename]** | [description] |\n\n## Preferences\n- [preferences discovered]\n```\n\n**memory/** directory:\n- `memory/glossary.md` — full decoder ring (acronyms, terms, nicknames, codenames)\n- `memory/people/{name}.md` — individual profiles\n- `memory/projects/{name}.md` — project details\n- `memory/context/company.md` — teams, tools, processes\n\n### 8. Report Results\n\n```\nProductivity system ready:\n- Tasks: TASKS.md (X items)\n- Memory: X people, X terms, X projects\n- Dashboard: open in browser\n\nUse /productivity:update to keep things current (add --comprehensive for a deep scan).\n```\n\n## Notes\n\n- If memory is already initialized, this just opens the dashboard\n- Nicknames are critical — always capture how people are actually referred to\n- If a source isn't available, skip it and note the gap\n- Memory grows organically through natural conversation after bootstrap\n", "depth": 3}, {"id": "447", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/skills/task-management", "name": "task-management", "type": "dir", "content": "", "depth": 3}, {"id": "448", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/skills/memory-management", "name": "memory-management", "type": "dir", "content": "", "depth": 3}, {"id": "449", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/skills/task-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: task-management\ndescription: Simple task management using a shared TASKS.md file. Reference this when the user asks about their tasks, wants to add/complete tasks, or needs help tracking commitments.\n---\n\n# Task Management\n\nTasks are tracked in a simple `TASKS.md` file that both you and the user can edit.\n\n## File Location\n\n**Always use `TASKS.md` in the current working directory.**\n\n- If it exists, read/write to it\n- If it doesn't exist, create it with the template below\n\n## Dashboard Setup (First Run)\n\nA visual dashboard is available for managing tasks and memory. **On first interaction with tasks:**\n\n1. Check if `dashboard.html` exists in the current working directory\n2. If not, copy it from `${CLAUDE_PLUGIN_ROOT}/skills/dashboard.html` to the current working directory\n3. Inform the user: \"I've added the dashboard. Run `/productivity:start` to set up the full system.\"\n\nThe task board:\n- Reads and writes to the same `TASKS.md` file\n- Auto-saves changes\n- Watches for external changes (syncs when you edit via CLI)\n- Supports drag-and-drop reordering of tasks and sections\n\n## Format \u0026 Template\n\nWhen creating a new TASKS.md, use this exact template (without example tasks):\n\n```markdown\n# Tasks\n\n## Active\n\n## Waiting On\n\n## Someday\n\n## Done\n```\n\nTask format:\n- `- [ ] **Task title** - context, for whom, due date`\n- Sub-bullets for additional details\n- Completed: `- [x] ~~Task~~ (date)`\n\n## How to Interact\n\n**When user asks \"what's on my plate\" / \"my tasks\":**\n- Read TASKS.md\n- Summarize Active and Waiting On sections\n- Highlight anything overdue or urgent\n\n**When user says \"add a task\" / \"remind me to\":**\n- Add to Active section with `- [ ] **Task**` format\n- Include context if provided (who it's for, due date)\n\n**When user says \"done with X\" / \"finished X\":**\n- Find the task\n- Change `[ ]` to `[x]`\n- Add strikethrough: `~~task~~`\n- Add completion date\n- Move to Done section\n\n**When user asks \"what am I waiting on\":**\n- Read the Waiting On section\n- Note how long each item has been waiting\n\n## Conventions\n\n- **Bold** the task title for scannability\n- Include \"for [person]\" when it's a commitment to someone\n- Include \"due [date]\" for deadlines\n- Include \"since [date]\" for waiting items\n- Sub-bullets for additional context\n- Keep Done section for ~1 week, then clear old items\n\n## Extracting Tasks\n\nWhen summarizing meetings or conversations, offer to add extracted tasks:\n- Commitments the user made (\"I'll send that over\")\n- Action items assigned to them\n- Follow-ups mentioned\n\nAsk before adding - don't auto-add without confirmation.\n", "depth": 4}, {"id": "450", "path": "/Users/haku/Documents/Claude-coworok-knowledge-work-plugins/productivity/skills/memory-management/SKILL.md", "name": "SKILL.md", "type": "file", "content": "---\nname: memory-management\ndescription: Two-tier memory system that makes Claude a true workplace collaborator. Decodes shorthand, acronyms, nicknames, and internal language so Claude understands requests like a colleague would. CLAUDE.md for working memory, memory/ directory for the full knowledge base.\n---\n\n# Memory Management\n\nMemory makes Claude your workplace collaborator - someone who speaks your internal language.\n\n## The Goal\n\nTransform shorthand into understanding:\n\n```\nUser: \"ask todd to do the PSR for oracle\"\n              ↓ Claude decodes\n\"Ask Todd Martinez (Finance lead) to prepare the Pipeline Status Report\n for the Oracle Systems deal ($2.3M, closing Q2)\"\n```\n\nWithout memory, that request is meaningless. With memory, Claude knows:\n- **todd** → Todd Martinez, Finance lead, prefers Slack\n- **PSR** → Pipeline Status Report (weekly sales doc)\n- **oracle** → Oracle Systems deal, not the company\n\n## Architecture\n\n```\nCLAUDE.md          ← Hot cache (~30 people, common terms)\nmemory/\n  glossary.md      ← Full decoder ring (everything)\n  people/          ← Complete profiles\n  projects/        ← Project details\n  context/         ← Company, teams, tools\n```\n\n**CLAUDE.md (Hot Cache):**\n- Top ~30 people you interact with most\n- ~30 most common acronyms/terms\n- Active projects (5-15)\n- Your preferences\n- **Goal: Cover 90% of daily decoding needs**\n\n**memory/glossary.md (Full Glossary):**\n- Complete decoder ring - everyone, every term\n- Searched when something isn't in CLAUDE.md\n- Can grow indefinitely\n\n**memory/people/, projects/, context/:**\n- Rich detail when needed for execution\n- Full profiles, history, context\n\n## Lookup Flow\n\n```\nUser: \"ask todd about the PSR for phoenix\"\n\n1. Check CLAUDE.md (hot cache)\n   → Todd? ✓ Todd Martinez, Finance\n   → PSR? ✓ Pipeline Status Report\n   → Phoenix? ✓ DB migration project\n\n2. If not found → search memory/glossary.md\n   → Full glossary has everyone/everything\n\n3. If still not found → ask user\n   → \"What does X mean? I'll remember it.\"\n```\n\nThis tiered approach keeps CLAUDE.md lean (~100 lines) while supporting unlimited scale in memory/.\n\n## File Locations\n\n- **Working memory:** `CLAUDE.md` in current working directory\n- **Deep memory:** `memory/` subdirectory\n\n## Working Memory Format (CLAUDE.md)\n\nUse tables for compactness. Target ~50-80 lines total.\n\n```markdown\n# Memory\n\n## Me\n[Name], [Role] on [Team]. [One sentence about what I do.]\n\n## People\n| Who | Role |\n|-----|------|\n| **Todd** | Todd Martinez, Finance lead |\n| **Sarah** | Sarah Chen, Engineering (Platform) |\n| **Greg** | Greg Wilson, Sales |\n→ Full list: memory/glossary.md, profiles: memory/people/\n\n## Terms\n| Term | Meaning |\n|------|---------|\n| PSR | Pipeline Status Report |\n| P0 | Drop everything priority |\n| standup | Daily 9am sync |\n→ Full glossary: memory/glossary.md\n\n## Projects\n| Name | What |\n|------|------|\n| **Phoenix** | DB migration, Q2 launch |\n| **Horizon** | Mobile app redesign |\n→ Details: memory/projects/\n\n## Preferences\n- 25-min meetings with buffers\n- Async-first, Slack over email\n- No meetings Friday afternoons\n```\n\n## Deep Memory Format (memory/)\n\n**memory/glossary.md** - The decoder ring:\n```markdown\n# Glossary\n\nWorkplace shorthand, acronyms, and internal language.\n\n## Acronyms\n| Term | Meaning | Context |\n|------|---------|---------|\n| PSR | Pipeline Status Report | Weekly sales doc |\n| OKR | Objectives \u0026 Key Results | Quarterly planning |\n| P0/P1/P2 | Priority levels | P0 = drop everything |\n\n## Internal Terms\n| Term | Meaning |\n|------|---------|\n| standup | Daily 9am sync in #engineering |\n| the migration | Project Phoenix database work |\n| ship it | Deploy to production |\n| escalate | Loop in leadership |\n\n## Nicknames → Full Names\n| Nickname | Person |\n|----------|--------|\n| Todd | Todd Martinez (Finance) |\n| T | Also Todd Martinez |\n\n## Project Codenames\n| Codename | Project |\n|----------|---------|\n| Phoenix | Database migration |\n| Horizon | New mobile app |\n```\n\n**memory/people/{name}.md:**\n```markdown\n# Todd Martinez\n\n**Also known as:** Todd, T\n**Role:** Finance Lead\n**Team:** Finance\n**Reports to:** CFO (Michael Chen)\n\n## Communication\n- Prefers Slack DM\n- Quick responses, very direct\n- Best time: mornings\n\n## Context\n- Handles all PSRs and financial reporting\n- Key contact for deal approvals over $500k\n- Works closely with Sales on forecasting\n\n## Notes\n- Cubs fan, likes talking baseball\n```\n\n**memory/projects/{name}.md:**\n```markdown\n# Project Phoenix\n\n**Codename:** Phoenix\n**Also called:** \"the migration\"\n**Status:** Active, launching Q2\n\n## What It Is\nDatabase migration from legacy Oracle to PostgreSQL.\n\n## Key People\n- Sarah - tech lead\n- Todd - budget owner\n- Greg - stakeholder (sales impact)\n\n## Context\n$1.2M budget, 6-month timeline. Critical path for Horizon project.\n```\n\n**memory/context/company.md:**\n```markdown\n# Company Context\n\n## Tools \u0026 Systems\n| Tool | Used for | Internal name |\n|------|----------|---------------|\n| Slack | Communication | - |\n| Asana | Engineering tasks | - |\n| Salesforce | CRM | \"SF\" or \"the CRM\" |\n| Notion | Docs/wiki | - |\n\n## Teams\n| Team | What they do | Key people |\n|------|--------------|------------|\n| Platform | Infrastructure | Sarah (lead) |\n| Finance | Money stuff | Todd (lead) |\n| Sales | Revenue | Greg |\n\n## Processes\n| Process | What it means |\n|---------|---------------|\n| Weekly sync | Monday 10am all-hands |\n| Ship review | Thursday deploy approval |\n```\n\n## How to Interact\n\n### Decoding User Input (Tiered Lookup)\n\n**Always** decode shorthand before acting on requests:\n\n```\n1. CLAUDE.md (hot cache)     → Check first, covers 90% of cases\n2. memory/glossary.md        → Full glossary if not in hot cache\n3. memory/people/, projects/ → Rich detail when needed\n4. Ask user                  → Unknown term? Learn it.\n```\n\nExample:\n```\nUser: \"ask todd to do the PSR for oracle\"\n\nCLAUDE.md lookup:\n  \"todd\" → Todd Martinez, Finance ✓\n  \"PSR\" → Pipeline Status Report ✓\n  \"oracle\" → (not in hot cache)\n\nmemory/glossary.md lookup:\n  \"oracle\" → Oracle Systems deal ($2.3M) ✓\n\nNow Claude can act with full context.\n```\n\n### Adding Memory\n\nWhen user says \"remember this\" or \"X means Y\":\n\n1. **Glossary items** (acronyms, terms, shorthand):\n   - Add to memory/glossary.md\n   - If frequently used, add to CLAUDE.md Quick Glossary\n\n2. **People:**\n   - Create/update memory/people/{name}.md\n   - Add to CLAUDE.md Key People if important\n   - **Capture nicknames** - critical for decoding\n\n3. **Projects:**\n   - Create/update memory/projects/{name}.md\n   - Add to CLAUDE.md Active Projects if current\n   - **Capture codenames** - \"Phoenix\", \"the migration\", etc.\n\n4. **Preferences:** Add to CLAUDE.md Preferences section\n\n### Recalling Memory\n\nWhen user asks \"who is X\" or \"what does X mean\":\n\n1. Check CLAUDE.md first\n2. Check memory/ for full detail\n3. If not found: \"I don't know what X means yet. Can you tell me?\"\n\n### Progressive Disclosure\n\n1. Load CLAUDE.md for quick parsing of any request\n2. Dive into memory/ when you need full context for execution\n3. Example: drafting an email to todd about the PSR\n   - CLAUDE.md tells you Todd = Todd Martinez, PSR = Pipeline Status Report\n   - memory/people/todd-martinez.md tells you he prefers Slack, is direct\n\n## Bootstrapping\n\nUse `/productivity:start` to initialize by scanning your chat, calendar, email, and documents. Extracts people, projects, and starts building the glossary.\n\n## Conventions\n\n- **Bold** terms in CLAUDE.md for scannability\n- Keep CLAUDE.md under ~100 lines (the \"hot 30\" rule)\n- Filenames: lowercase, hyphens (`todd-martinez.md`, `project-phoenix.md`)\n- Always capture nicknames and alternate names\n- Glossary tables for easy lookup\n- When something's used frequently, promote it to CLAUDE.md\n- When something goes stale, demote it to memory/ only\n\n## What Goes Where\n\n| Type | CLAUDE.md (Hot Cache) | memory/ (Full Storage) |\n|------|----------------------|------------------------|\n| Person | Top ~30 frequent contacts | glossary.md + people/{name}.md |\n| Acronym/term | ~30 most common | glossary.md (complete list) |\n| Project | Active projects only | glossary.md + projects/{name}.md |\n| Nickname | In Key People if top 30 | glossary.md (all nicknames) |\n| Company context | Quick reference only | context/company.md |\n| Preferences | All preferences | - |\n| Historical/stale | ✗ Remove | ✓ Keep in memory/ |\n\n## Promotion / Demotion\n\n**Promote to CLAUDE.md when:**\n- You use a term/person frequently\n- It's part of active work\n\n**Demote to memory/ only when:**\n- Project completed\n- Person no longer frequent contact\n- Term rarely used\n\nThis keeps CLAUDE.md fresh and relevant.\n", "depth": 4}], "links": [{"source": "0", "target": "1"}, {"source": "0", "target": "2"}, {"source": "0", "target": "3"}, {"source": "0", "target": "4"}, {"source": "0", "target": "5"}, {"source": "0", "target": "6"}, {"source": "0", "target": "7"}, {"source": "0", "target": "8"}, {"source": "0", "target": "9"}, {"source": "0", "target": "10"}, {"source": "0", "target": "11"}, {"source": "0", "target": "12"}, {"source": "0", "target": "13"}, {"source": "0", "target": "14"}, {"source": "0", "target": "15"}, {"source": "0", "target": "16"}, {"source": "0", "target": "17"}, {"source": "1", "target": "18"}, {"source": "1", "target": "19"}, {"source": "1", "target": "20"}, {"source": "1", "target": "21"}, {"source": "18", "target": "22"}, {"source": "18", "target": "23"}, {"source": "18", "target": "24"}, {"source": "18", "target": "25"}, {"source": "18", "target": "26"}, {"source": "19", "target": "27"}, {"source": "19", "target": "28"}, {"source": "19", "target": "29"}, {"source": "19", "target": "30"}, {"source": "19", "target": "31"}, {"source": "27", "target": "32"}, {"source": "28", "target": "33"}, {"source": "29", "target": "34"}, {"source": "30", "target": "35"}, {"source": "31", "target": "36"}, {"source": "2", "target": "37"}, {"source": "2", "target": "38"}, {"source": "2", "target": "39"}, {"source": "2", "target": "40"}, {"source": "37", "target": "41"}, {"source": "37", "target": "42"}, {"source": "37", "target": "43"}, {"source": "37", "target": "44"}, {"source": "37", "target": "45"}, {"source": "37", "target": "46"}, {"source": "38", "target": "47"}, {"source": "38", "target": "48"}, {"source": "38", "target": "49"}, {"source": "38", "target": "50"}, {"source": "38", "target": "51"}, {"source": "38", "target": "52"}, {"source": "47", "target": "53"}, {"source": "48", "target": "54"}, {"source": "49", "target": "55"}, {"source": "50", "target": "56"}, {"source": "51", "target": "57"}, {"source": "52", "target": "58"}, {"source": "3", "target": "59"}, {"source": "3", "target": "60"}, {"source": "3", "target": "61"}, {"source": "3", "target": "62"}, {"source": "59", "target": "63"}, {"source": "59", "target": "64"}, {"source": "60", "target": "65"}, {"source": "60", "target": "66"}, {"source": "60", "target": "67"}, {"source": "65", "target": "68"}, {"source": "66", "target": "69"}, {"source": "67", "target": "70"}, {"source": "4", "target": "71"}, {"source": "71", "target": "72"}, {"source": "71", "target": "73"}, {"source": "72", "target": "74"}, {"source": "72", "target": "75"}, {"source": "72", "target": "76"}, {"source": "72", "target": "77"}, {"source": "74", "target": "78"}, {"source": "74", "target": "79"}, {"source": "73", "target": "80"}, {"source": "73", "target": "81"}, {"source": "80", "target": "82"}, {"source": "80", "target": "83"}, {"source": "5", "target": "84"}, {"source": "5", "target": "85"}, {"source": "5", "target": "86"}, {"source": "5", "target": "87"}, {"source": "84", "target": "88"}, {"source": "84", "target": "89"}, {"source": "84", "target": "90"}, {"source": "85", "target": "91"}, {"source": "85", "target": "92"}, {"source": "85", "target": "93"}, {"source": "85", "target": "94"}, {"source": "85", "target": "95"}, {"source": "85", "target": "96"}, {"source": "91", "target": "97"}, {"source": "92", "target": "98"}, {"source": "92", "target": "99"}, {"source": "92", "target": "100"}, {"source": "93", "target": "101"}, {"source": "94", "target": "102"}, {"source": "95", "target": "103"}, {"source": "96", "target": "104"}, {"source": "6", "target": "105"}, {"source": "6", "target": "106"}, {"source": "6", "target": "107"}, {"source": "6", "target": "108"}, {"source": "105", "target": "109"}, {"source": "106", "target": "110"}, {"source": "106", "target": "111"}, {"source": "106", "target": "112"}, {"source": "106", "target": "113"}, {"source": "106", "target": "114"}, {"source": "110", "target": "115"}, {"source": "110", "target": "116"}, {"source": "110", "target": "117"}, {"source": "115", "target": "118"}, {"source": "115", "target": "119"}, {"source": "115", "target": "120"}, {"source": "115", "target": "121"}, {"source": "115", "target": "122"}, {"source": "115", "target": "123"}, {"source": "115", "target": "124"}, {"source": "115", "target": "125"}, {"source": "115", "target": "126"}, {"source": "111", "target": "127"}, {"source": "111", "target": "128"}, {"source": "111", "target": "129"}, {"source": "111", "target": "130"}, {"source": "127", "target": "131"}, {"source": "112", "target": "132"}, {"source": "112", "target": "133"}, {"source": "112", "target": "134"}, {"source": "112", "target": "135"}, {"source": "132", "target": "136"}, {"source": "132", "target": "137"}, {"source": "132", "target": "138"}, {"source": "132", "target": "139"}, {"source": "132", "target": "140"}, {"source": "132", "target": "141"}, {"source": "132", "target": "142"}, {"source": "132", "target": "143"}, {"source": "132", "target": "144"}, {"source": "132", "target": "145"}, {"source": "132", "target": "146"}, {"source": "132", "target": "147"}, {"source": "113", "target": "148"}, {"source": "113", "target": "149"}, {"source": "113", "target": "150"}, {"source": "113", "target": "151"}, {"source": "148", "target": "152"}, {"source": "148", "target": "153"}, {"source": "148", "target": "154"}, {"source": "148", "target": "155"}, {"source": "152", "target": "156"}, {"source": "152", "target": "157"}, {"source": "152", "target": "158"}, {"source": "149", "target": "159"}, {"source": "149", "target": "160"}, {"source": "159", "target": "161"}, {"source": "114", "target": "162"}, {"source": "114", "target": "163"}, {"source": "114", "target": "164"}, {"source": "114", "target": "165"}, {"source": "114", "target": "166"}, {"source": "162", "target": "167"}, {"source": "162", "target": "168"}, {"source": "162", "target": "169"}, {"source": "162", "target": "170"}, {"source": "7", "target": "171"}, {"source": "7", "target": "172"}, {"source": "7", "target": "173"}, {"source": "7", "target": "174"}, {"source": "171", "target": "175"}, {"source": "171", "target": "176"}, {"source": "171", "target": "177"}, {"source": "171", "target": "178"}, {"source": "171", "target": "179"}, {"source": "176", "target": "180"}, {"source": "176", "target": "181"}, {"source": "176", "target": "182"}, {"source": "176", "target": "183"}, {"source": "176", "target": "184"}, {"source": "177", "target": "185"}, {"source": "177", "target": "186"}, {"source": "177", "target": "187"}, {"source": "178", "target": "188"}, {"source": "178", "target": "189"}, {"source": "178", "target": "190"}, {"source": "188", "target": "191"}, {"source": "188", "target": "192"}, {"source": "191", "target": "193"}, {"source": "191", "target": "194"}, {"source": "189", "target": "195"}, {"source": "189", "target": "196"}, {"source": "195", "target": "197"}, {"source": "195", "target": "198"}, {"source": "190", "target": "199"}, {"source": "190", "target": "200"}, {"source": "199", "target": "201"}, {"source": "199", "target": "202"}, {"source": "172", "target": "203"}, {"source": "172", "target": "204"}, {"source": "203", "target": "205"}, {"source": "203", "target": "206"}, {"source": "203", "target": "207"}, {"source": "205", "target": "208"}, {"source": "206", "target": "209"}, {"source": "207", "target": "210"}, {"source": "173", "target": "211"}, {"source": "173", "target": "212"}, {"source": "173", "target": "213"}, {"source": "173", "target": "214"}, {"source": "211", "target": "215"}, {"source": "211", "target": "216"}, {"source": "211", "target": "217"}, {"source": "211", "target": "218"}, {"source": "211", "target": "219"}, {"source": "212", "target": "220"}, {"source": "212", "target": "221"}, {"source": "220", "target": "222"}, {"source": "221", "target": "223"}, {"source": "174", "target": "224"}, {"source": "174", "target": "225"}, {"source": "174", "target": "226"}, {"source": "174", "target": "227"}, {"source": "174", "target": "228"}, {"source": "224", "target": "229"}, {"source": "224", "target": "230"}, {"source": "225", "target": "231"}, {"source": "225", "target": "232"}, {"source": "226", "target": "233"}, {"source": "226", "target": "234"}, {"source": "226", "target": "235"}, {"source": "226", "target": "236"}, {"source": "226", "target": "237"}, {"source": "226", "target": "238"}, {"source": "233", "target": "239"}, {"source": "233", "target": "240"}, {"source": "239", "target": "241"}, {"source": "234", "target": "242"}, {"source": "234", "target": "243"}, {"source": "242", "target": "244"}, {"source": "235", "target": "245"}, {"source": "235", "target": "246"}, {"source": "245", "target": "247"}, {"source": "236", "target": "248"}, {"source": "236", "target": "249"}, {"source": "248", "target": "250"}, {"source": "237", "target": "251"}, {"source": "237", "target": "252"}, {"source": "251", "target": "253"}, {"source": "238", "target": "254"}, {"source": "238", "target": "255"}, {"source": "254", "target": "256"}, {"source": "8", "target": "257"}, {"source": "8", "target": "258"}, {"source": "8", "target": "259"}, {"source": "8", "target": "260"}, {"source": "257", "target": "261"}, {"source": "257", "target": "262"}, {"source": "257", "target": "263"}, {"source": "257", "target": "264"}, {"source": "257", "target": "265"}, {"source": "257", "target": "266"}, {"source": "258", "target": "267"}, {"source": "258", "target": "268"}, {"source": "258", "target": "269"}, {"source": "258", "target": "270"}, {"source": "258", "target": "271"}, {"source": "258", "target": "272"}, {"source": "267", "target": "273"}, {"source": "268", "target": "274"}, {"source": "269", "target": "275"}, {"source": "270", "target": "276"}, {"source": "271", "target": "277"}, {"source": "272", "target": "278"}, {"source": "9", "target": "279"}, {"source": "9", "target": "280"}, {"source": "9", "target": "281"}, {"source": "9", "target": "282"}, {"source": "279", "target": "283"}, {"source": "279", "target": "284"}, {"source": "279", "target": "285"}, {"source": "279", "target": "286"}, {"source": "279", "target": "287"}, {"source": "279", "target": "288"}, {"source": "280", "target": "289"}, {"source": "280", "target": "290"}, {"source": "280", "target": "291"}, {"source": "280", "target": "292"}, {"source": "280", "target": "293"}, {"source": "280", "target": "294"}, {"source": "289", "target": "295"}, {"source": "290", "target": "296"}, {"source": "291", "target": "297"}, {"source": "292", "target": "298"}, {"source": "293", "target": "299"}, {"source": "294", "target": "300"}, {"source": "10", "target": "301"}, {"source": "10", "target": "302"}, {"source": "10", "target": "303"}, {"source": "10", "target": "304"}, {"source": "301", "target": "305"}, {"source": "301", "target": "306"}, {"source": "301", "target": "307"}, {"source": "301", "target": "308"}, {"source": "301", "target": "309"}, {"source": "301", "target": "310"}, {"source": "301", "target": "311"}, {"source": "302", "target": "312"}, {"source": "302", "target": "313"}, {"source": "302", "target": "314"}, {"source": "302", "target": "315"}, {"source": "302", "target": "316"}, {"source": "302", "target": "317"}, {"source": "312", "target": "318"}, {"source": "313", "target": "319"}, {"source": "314", "target": "320"}, {"source": "315", "target": "321"}, {"source": "316", "target": "322"}, {"source": "317", "target": "323"}, {"source": "11", "target": "324"}, {"source": "11", "target": "325"}, {"source": "11", "target": "326"}, {"source": "11", "target": "327"}, {"source": "324", "target": "328"}, {"source": "324", "target": "329"}, {"source": "324", "target": "330"}, {"source": "324", "target": "331"}, {"source": "324", "target": "332"}, {"source": "324", "target": "333"}, {"source": "324", "target": "334"}, {"source": "325", "target": "335"}, {"source": "325", "target": "336"}, {"source": "325", "target": "337"}, {"source": "325", "target": "338"}, {"source": "325", "target": "339"}, {"source": "335", "target": "340"}, {"source": "336", "target": "341"}, {"source": "337", "target": "342"}, {"source": "338", "target": "343"}, {"source": "339", "target": "344"}, {"source": "12", "target": "345"}, {"source": "12", "target": "346"}, {"source": "12", "target": "347"}, {"source": "12", "target": "348"}, {"source": "345", "target": "349"}, {"source": "345", "target": "350"}, {"source": "345", "target": "351"}, {"source": "345", "target": "352"}, {"source": "345", "target": "353"}, {"source": "346", "target": "354"}, {"source": "346", "target": "355"}, {"source": "346", "target": "356"}, {"source": "346", "target": "357"}, {"source": "346", "target": "358"}, {"source": "346", "target": "359"}, {"source": "354", "target": "360"}, {"source": "355", "target": "361"}, {"source": "356", "target": "362"}, {"source": "357", "target": "363"}, {"source": "358", "target": "364"}, {"source": "359", "target": "365"}, {"source": "13", "target": "366"}, {"source": "13", "target": "367"}, {"source": "13", "target": "368"}, {"source": "13", "target": "369"}, {"source": "366", "target": "370"}, {"source": "366", "target": "371"}, {"source": "366", "target": "372"}, {"source": "366", "target": "373"}, {"source": "366", "target": "374"}, {"source": "366", "target": "375"}, {"source": "366", "target": "376"}, {"source": "367", "target": "377"}, {"source": "367", "target": "378"}, {"source": "367", "target": "379"}, {"source": "367", "target": "380"}, {"source": "367", "target": "381"}, {"source": "367", "target": "382"}, {"source": "377", "target": "383"}, {"source": "378", "target": "384"}, {"source": "379", "target": "385"}, {"source": "380", "target": "386"}, {"source": "381", "target": "387"}, {"source": "382", "target": "388"}, {"source": "14", "target": "389"}, {"source": "14", "target": "390"}, {"source": "14", "target": "391"}, {"source": "14", "target": "392"}, {"source": "389", "target": "393"}, {"source": "389", "target": "394"}, {"source": "389", "target": "395"}, {"source": "389", "target": "396"}, {"source": "389", "target": "397"}, {"source": "389", "target": "398"}, {"source": "390", "target": "399"}, {"source": "390", "target": "400"}, {"source": "390", "target": "401"}, {"source": "390", "target": "402"}, {"source": "390", "target": "403"}, {"source": "390", "target": "404"}, {"source": "390", "target": "405"}, {"source": "399", "target": "406"}, {"source": "400", "target": "407"}, {"source": "400", "target": "408"}, {"source": "400", "target": "409"}, {"source": "407", "target": "410"}, {"source": "407", "target": "411"}, {"source": "407", "target": "412"}, {"source": "407", "target": "413"}, {"source": "401", "target": "414"}, {"source": "402", "target": "415"}, {"source": "403", "target": "416"}, {"source": "404", "target": "417"}, {"source": "405", "target": "418"}, {"source": "15", "target": "419"}, {"source": "15", "target": "420"}, {"source": "15", "target": "421"}, {"source": "15", "target": "422"}, {"source": "419", "target": "423"}, {"source": "419", "target": "424"}, {"source": "419", "target": "425"}, {"source": "419", "target": "426"}, {"source": "419", "target": "427"}, {"source": "419", "target": "428"}, {"source": "420", "target": "429"}, {"source": "420", "target": "430"}, {"source": "420", "target": "431"}, {"source": "420", "target": "432"}, {"source": "420", "target": "433"}, {"source": "420", "target": "434"}, {"source": "429", "target": "435"}, {"source": "430", "target": "436"}, {"source": "431", "target": "437"}, {"source": "432", "target": "438"}, {"source": "433", "target": "439"}, {"source": "434", "target": "440"}, {"source": "16", "target": "441"}, {"source": "16", "target": "442"}, {"source": "16", "target": "443"}, {"source": "16", "target": "444"}, {"source": "441", "target": "445"}, {"source": "441", "target": "446"}, {"source": "442", "target": "447"}, {"source": "442", "target": "448"}, {"source": "447", "target": "449"}, {"source": "448", "target": "450"}]}
  </script>

  <script>
    window.onload = () => {
        if (typeof ForceGraph === 'undefined') {
            document.getElementById('content-display').innerHTML = '<p style="color:red">Failed to load ForceGraph library.</p>';
            return;
        }
        
        try {
            const rawJson = document.getElementById('graph-data-json').textContent;
            const gData = JSON.parse(rawJson);
            
            let dirColor = '#f4a261'; // orange
            let fileColor = '#2a9d8f'; // teal
            
            // Focus state variables
            let highlightNodes = new Set();
            let highlightLinks = new Set();
            let hoverNode = null;

            let ringSpacing = 150;
            
            // Scatter initial node positions in approximate circles to avoid cluster explosion
            gData.nodes.forEach(node => {
                if (node.depth === 0) {
                    node.x = 0; node.y = 0;
                } else {
                    const radius = node.depth * ringSpacing;
                    const angle = Math.random() * 2 * Math.PI;
                    node.x = radius * Math.cos(angle);
                    node.y = radius * Math.sin(angle);
                }
            });

            const graphEl = document.getElementById('graph');
            const Graph = ForceGraph()(graphEl)
              .graphData(gData)
              .nodeLabel('name')
              .nodeRelSize(4)
              .nodeVal(node => node.type === 'dir' ? 8 : 4)
              .linkDirectionalParticles(2)
              .linkDirectionalParticleSpeed(0.005)
              
              // Custom link coloring for focus
              .linkColor(link => {
                  if (highlightNodes.size === 0) return '#cccccc'; // Default
                  return highlightLinks.has(link) ? '#2a9d8f' : 'rgba(204, 204, 204, 0.15)';
              })
              .linkWidth(link => highlightLinks.has(link) ? 2 : 1)
              
              .onNodeClick(node => {
                 try {
                     // Update Focus State
                     highlightNodes.clear();
                     highlightLinks.clear();
                     
                     if (node) {
                         highlightNodes.add(node);
                         // Find connected links and neighbors. Note: ForceGraph replaces source/target IDs with object refs!
                         gData.links.forEach(link => {
                             if (link.source === node || link.target === node) {
                                 highlightLinks.add(link);
                                 highlightNodes.add(link.source === node ? link.target : link.source);
                             }
                         });
                     }
                     
                     // Force redrawing
                     Graph.linkColor(Graph.linkColor())
                          .linkWidth(Graph.linkWidth());
                     
                     // UI Update
                     document.getElementById('doc-title').textContent = node.name || 'Untitled';
                     document.getElementById('doc-path').textContent = node.path || '';
                     
                     const contentDisplay = document.getElementById('content-display');
                     if (node.type === 'file') {
                         let contentText = node.content || '*Empty file*';
                         if (typeof marked !== 'undefined') {
                             if (typeof marked.parse === 'function') {
                                 contentDisplay.innerHTML = marked.parse(contentText);
                             } else {
                                 contentDisplay.innerHTML = marked(contentText);
                             }
                         } else {
                             contentDisplay.innerHTML = `<pre>${contentText}</pre>`;
                         }
                     } else {
                         contentDisplay.innerHTML = '<p><em>This is a directory. Click on the child nodes to read files.</em></p>';
                     }
                     
                     document.getElementById('panel').classList.add('open');
                 } catch (err) {
                     console.error(err);
                 }
              })
              .onBackgroundClick(() => {
                   // Clear focus visually
                   highlightNodes.clear();
                   highlightLinks.clear();
                   Graph.linkColor(Graph.linkColor())
                        .linkWidth(Graph.linkWidth());
              });
              
              document.getElementById('close-btn').addEventListener('click', () => {
                  document.getElementById('panel').classList.remove('open');
                  highlightNodes.clear();
                  highlightLinks.clear();
                  Graph.linkColor(Graph.linkColor()).linkWidth(Graph.linkWidth());
              });
              
              // Custom Strict Concentric Radial Layout
              Graph.d3Force('center', null); // Turn off default center pull
              Graph.d3Force('radial', (alpha) => {
                  gData.nodes.forEach(node => {
                      // Root stays exactly at absolute center
                      if (node.depth === 0) {
                          node.x = 0; node.y = 0;
                          node.vx = 0; node.vy = 0;
                          return;
                      }
                      
                      const targetR = node.depth * ringSpacing;
                      const currentR = Math.sqrt(node.x * node.x + node.y * node.y) || 1;
                      
                      // Apply radial pull toward precise ring circumference
                      const force = (targetR - currentR) * alpha * 0.8;
                      node.vx += (node.x / currentR) * force;
                      node.vy += (node.y / currentR) * force;
                  });
              });
              
              // Sliders Configuration
              Graph.d3Force('link').distance(ringSpacing / 3);
              Graph.d3Force('charge').strength(-50);
              document.getElementById('charge-slider').value = -50;
              document.getElementById('charge-val').innerText = -50;
              
              document.getElementById('dist-slider').addEventListener('input', e => {
                  ringSpacing = +e.target.value;
                  document.getElementById('dist-val').innerText = ringSpacing;
                  Graph.d3Force('link').distance(ringSpacing / 3);
                  Graph.d3ReheatSimulation();
              });
              
              document.getElementById('charge-slider').addEventListener('input', e => {
                  const val = +e.target.value;
                  document.getElementById('charge-val').innerText = val;
                  Graph.d3Force('charge').strength(val);
                  Graph.d3ReheatSimulation();
              });
              
              Graph.nodePointerAreaPaint((node, color, ctx) => {
                  const nodeR = Math.sqrt(Math.max(0, node.val || 1)) * 4;
                  ctx.fillStyle = color;
                  ctx.beginPath();
                  ctx.arc(node.x, node.y, nodeR + 6, 0, 2 * Math.PI, false);
                  ctx.fill();
              });
              
              // Custom node painting to handle fading
              Graph.nodeCanvasObject((node, ctx, globalScale) => {
                  const isHighlighted = highlightNodes.size === 0 || highlightNodes.has(node);
                  
                  ctx.beginPath();
                  const nodeR = Math.sqrt(Math.max(0, node.val || 1)) * 4;
                  ctx.arc(node.x, node.y, nodeR, 0, 2 * Math.PI, false);
                  
                  let baseColorStr = node.type === 'dir' ? dirColor : fileColor;
                  
                  // Use lower opacity if faded
                  if (!isHighlighted) {
                      // Convert hex to rgba for fading (assumes rigorous hex input #RRGGBB)
                      const r = parseInt(baseColorStr.substring(1,3), 16);
                      const g = parseInt(baseColorStr.substring(3,5), 16);
                      const b = parseInt(baseColorStr.substring(5,7), 16);
                      ctx.fillStyle = `rgba(${r}, ${g}, ${b}, 0.15)`;
                  } else {
                      ctx.fillStyle = baseColorStr;
                  }
                  
                  ctx.fill();
                  
                  // Draw ring around explicitly clicked nodes to simulate focus
                  if (highlightNodes.has(node) && node === Array.from(highlightNodes)[0]) { // Highlight only the EXACT clicked node
                      ctx.strokeStyle = '#e76f51'; // red/orange highlight
                      ctx.lineWidth = 2 / globalScale;
                      ctx.stroke();
                  }

                  // Always show label for highlighted nodes when in focus mode
                  const forceShowLabel = highlightNodes.size > 0 && isHighlighted;

                  if ((globalScale > 1.2 && isHighlighted) || forceShowLabel) {
                      const label = node.name;
                      const fontSize = 12/globalScale;
                      ctx.font = `${fontSize}px Sans-Serif`;
                      const textWidth = ctx.measureText(label).width;
                      
                      const bckgDimensions = [textWidth, fontSize].map(n => n + fontSize * 0.2);

                      ctx.fillStyle = 'rgba(255, 255, 255, 0.8)';
                      ctx.fillRect(node.x - bckgDimensions[0] / 2, node.y + nodeR + 2, bckgDimensions[0], bckgDimensions[1]);

                      ctx.textAlign = 'center';
                      ctx.textBaseline = 'top';
                      ctx.fillStyle = '#333';
                      ctx.fillText(label, node.x, node.y + nodeR + 2);
                  }
              });
        } catch(e) {
            document.getElementById('error-banner').style.display = 'block';
            document.getElementById('error-banner').innerHTML += 'Initialization error: ' + e.message;
        }
    };
  </script>
</body>
</html>
